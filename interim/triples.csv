info_unit,text,predicates,subj/obj,triple_A,triple_B,triple_C,triple_D,SPEC_0,SPEC_1,SPEC_2,SPEC_3,SPEC_4,topic,paper_idx,idx
research-problem,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",[],"[('Conditional computation', (0, 2)), ('increasing model capacity without a proportional increase in computation', (24, 33))]",[],[],[],[],"[['Contribution', 'has research problem', 'Conditional computation'], ['Contribution', 'has research problem', 'increasing model capacity without a proportional increase in computation']]",[],[],[],[],machine-translation,7,5
research-problem,Exploiting scale in both training data and model size has been central to the success of deep learning .,[],"[('Exploiting scale in both training data and model size', (0, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Exploiting scale in both training data and model size']]",[],[],[],[],machine-translation,7,16
approach,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,"[('to', (2, 3)), ('to introduce', (6, 8)), (':', (17, 18))]","[('conditional computation', (3, 5)), ('new type of general purpose neural network component', (9, 17)), ('Sparsely - Gated Mixture - of - Experts Layer ( MoE )', (19, 31))]","[['conditional computation', 'to introduce', 'new type of general purpose neural network component'], ['new type of general purpose neural network component', ':', 'Sparsely - Gated Mixture - of - Experts Layer ( MoE )']]",[],"[['Approach', 'to', 'conditional computation']]",[],[],[],[],[],[],machine-translation,7,45
approach,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .","[('consists of', (2, 4)), ('each', (9, 10)), ('selects', (24, 25))]","[('number of experts', (5, 8)), ('simple feed - forward neural network', (11, 17)), ('trainable gating network', (20, 23)), ('sparse combination of the experts to process each input', (26, 35))]","[['trainable gating network', 'selects', 'sparse combination of the experts to process each input'], ['number of experts', 'each', 'simple feed - forward neural network']]",[],"[['Approach', 'consists of', 'trainable gating network'], ['Approach', 'consists of', 'number of experts']]",[],[],[],[],[],[],machine-translation,7,46
approach,All parts of the network are trained jointly by back - propagation .,"[('All parts of', (0, 3)), ('trained', (6, 7)), ('by', (8, 9))]","[('network', (4, 5)), ('jointly', (7, 8)), ('back - propagation', (9, 12))]","[['jointly', 'by', 'back - propagation'], ['back - propagation', 'All parts of', 'network']]",[],"[['Approach', 'trained', 'jointly']]",[],[],[],[],[],[],machine-translation,7,47
tasks,100 BILLION WORD GOOGLE NEWS CORPUS,[],"[('100 BILLION WORD GOOGLE NEWS CORPUS', (0, 6))]",[],[],[],"[['Tasks', 'has', '100 BILLION WORD GOOGLE NEWS CORPUS']]",[],[],[],[],"[['100 BILLION WORD GOOGLE NEWS CORPUS', 'has', 'Hyperparameters']]",machine-translation,7,183
tasks,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .","[('MoE layers', (13, 15))]","[('32 , experts', (16, 19))]",[],[],[],[],[],"[['Hyperparameters', 'MoE layers', '32 , experts']]",[],[],[],machine-translation,7,186
tasks,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .","[('When', (0, 1)), ('test perplexity', (9, 11)), ('up to', (13, 15))]","[('training over the full 100 billion words', (1, 8)), ('improves significantly', (11, 13)), ('65536 experts ( 68 billion parameters )', (15, 22))]","[['training over the full 100 billion words', 'test perplexity', 'improves significantly'], ['improves significantly', 'up to', '65536 experts ( 68 billion parameters )']]",[],[],[],[],"[['Results', 'When', 'training over the full 100 billion words']]",[],[],[],machine-translation,7,190
tasks,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),[],"[('MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )', (0, 7))]",[],[],[],"[['Tasks', 'has', 'MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )']]",[],[],[],[],"[['MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )', 'has', 'Hyperparameters']]",machine-translation,7,193
tasks,Our model was a modified version of the GNMT model described in .,"[('model', (1, 2)), ('was a', (2, 4))]","[('modified version', (4, 6)), ('GNMT model', (8, 10))]","[['GNMT model', 'was a', 'modified version']]",[],[],[],[],"[['Hyperparameters', 'model', 'GNMT model']]",[],[],[],machine-translation,7,195
tasks,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .","[('To reduce', (0, 2)), ('decreased', (5, 6)), ('in', (11, 12)), ('from', (16, 17))]","[('computation', (2, 3)), ('LSTM layers', (9, 11)), ('encoder and decoder', (13, 16)), ('9 and 8 to 3 and 2 respectively', (17, 25))]","[['LSTM layers', 'in', 'encoder and decoder'], ['encoder and decoder', 'from', '9 and 8 to 3 and 2 respectively'], ['LSTM layers', 'To reduce', 'computation']]",[],[],[],[],"[['Hyperparameters', 'decreased', 'LSTM layers']]",[],[],[],machine-translation,7,196
tasks,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,[],[],"[['MoE layers', 'in both', 'decoder'], ['decoder', 'between', 'layers 1 and 2'], ['MoE layers', 'in both', 'encoder'], ['encoder', 'between', 'layers 2 and 3']]",[],[],[],[],"[['Hyperparameters', 'inserted', 'MoE layers']]",[],[],[],machine-translation,7,197
tasks,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .","[('Each', (0, 1)), ('contained up to', (3, 6)), ('with', (9, 10))]","[('MoE layer', (1, 3)), ('2048 experts each', (6, 9)), ('about two million parameters', (10, 14))]","[['MoE layer', 'contained up to', '2048 experts each'], ['2048 experts each', 'with', 'about two million parameters']]",[],[],[],[],"[['Hyperparameters', 'Each', 'MoE layer']]",[],[],[],machine-translation,7,198
tasks,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,"[('BLEU scores', (3, 5)), ('on', (9, 10))]","[('40.56 and 26.03', (6, 9)), (""WMT ' 14 En?Fr and En ? De benchmarks"", (11, 20))]","[['40.56 and 26.03', 'on', ""WMT ' 14 En?Fr and En ? De benchmarks""]]",[],[],[],[],"[['Results', 'BLEU scores', '40.56 and 26.03']]",[],[],[],machine-translation,7,206
tasks,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .","[('On', (0, 1)), ('test BLEU score', (11, 14))]","[('Google Production dataset', (2, 5)), ('1.01 higher', (9, 11))]","[['1.01 higher', 'On', 'Google Production dataset']]",[],[],[],[],"[['Results', 'test BLEU score', '1.01 higher']]",[],[],[],machine-translation,7,210
tasks,MULTILINGUAL MACHINE TRANSLATION,[],"[('MULTILINGUAL MACHINE TRANSLATION', (0, 3))]",[],[],[],"[['Tasks', 'has', 'MULTILINGUAL MACHINE TRANSLATION']]",[],[],[],[],"[['MULTILINGUAL MACHINE TRANSLATION', 'has', 'Results']]",machine-translation,7,211
tasks,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,"[('achieves', (3, 4)), ('on', (8, 9)), ('than', (12, 13))]","[('19 % lower perplexity', (4, 8)), ('dev set', (10, 12)), ('multilingual GNMT model', (14, 17))]","[['dev set', 'achieves', '19 % lower perplexity'], ['19 % lower perplexity', 'than', 'multilingual GNMT model']]",[],[],[],[],"[['Results', 'on', 'dev set']]",[],[],[],machine-translation,7,214
tasks,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",[],[],"[['BLEU score', 'significantly beats', 'multilingual GNMT model'], ['multilingual GNMT model', 'on', '11 of the 12 language pairs'], ['11 of the 12 language pairs', 'by as much as', '5.84 points'], ['BLEU score', 'even beats', 'monolingual GNMT models'], ['monolingual GNMT models', 'on', '8 of 12 language pairs']]",[],[],[],[],"[['Results', 'On', 'BLEU score']]",[],[],[],machine-translation,7,215
research-problem,Semi-supervised sequence tagging with bidirectional language models,[],"[('Semi-supervised sequence tagging', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semi-supervised sequence tagging']]",[],[],[],[],named-entity-recognition,3,2
research-problem,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",[],"[('general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems', (7, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems']]",[],[],[],[],named-entity-recognition,3,6
approach,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .","[('not require', (12, 14))]","[('semisupervised approach', (8, 10)), ('additional labeled data', (14, 17))]","[['semisupervised approach', 'not require', 'additional labeled data']]",[],[],"[['Approach', 'has', 'semisupervised approach']]",[],[],[],[],[],named-entity-recognition,3,16
approach,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .","[('use', (1, 2)), ('pre-trained on', (10, 12)), ('to compute', (17, 19)), ('use it in', (37, 40))]","[('neural language model ( LM )', (3, 9)), ('large , unlabeled corpus', (13, 17)), ('encoding of the context at each position in the sequence', (20, 30)), ('LM embedding', (33, 35)), ('supervised sequence tagging model', (41, 45))]","[['neural language model ( LM )', 'pre-trained on', 'large , unlabeled corpus'], ['large , unlabeled corpus', 'to compute', 'encoding of the context at each position in the sequence'], ['large , unlabeled corpus', 'use it in', 'supervised sequence tagging model']]","[['encoding of the context at each position in the sequence', 'name', 'LM embedding']]","[['Approach', 'use', 'neural language model ( LM )']]",[],[],[],[],[],[],named-entity-recognition,3,17
tasks,CoNLL 2003 NER .,[],"[('CoNLL 2003 NER', (0, 3))]",[],[],[],"[['Tasks', 'has', 'CoNLL 2003 NER']]",[],[],[],[],"[['CoNLL 2003 NER', 'Hyperparameters', 'two bidirectional GRUs']]",named-entity-recognition,3,75
tasks,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,"[('with', (5, 6)), ('for', (14, 15))]","[('two bidirectional GRUs', (2, 5)), ('80 hidden units', (6, 9)), ('25 dimensional character embeddings', (10, 14)), ('token character encoder', (16, 19))]","[['two bidirectional GRUs', 'for', 'token character encoder'], ['token character encoder', 'with', '80 hidden units'], ['token character encoder', 'with', '25 dimensional character embeddings']]",[],[],[],[],[],[],[],[],named-entity-recognition,3,80
tasks,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,"[('uses', (3, 4)), ('with', (7, 8))]","[('sequence layer', (1, 3)), ('bidirectional GRUs', (5, 7)), ('300 hidden units each', (8, 12))]","[['sequence layer', 'uses', 'bidirectional GRUs'], ['bidirectional GRUs', 'with', '300 hidden units each']]",[],[],[],[],[],[],"[['CoNLL 2003 NER', 'Hyperparameters', 'sequence layer']]",[],named-entity-recognition,3,81
tasks,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .","[('add', (4, 5)), ('to the input of', (8, 12))]","[('regularization', (1, 2)), ('25 % dropout', (5, 8)), ('each GRU', (12, 14))]","[['regularization', 'add', '25 % dropout'], ['25 % dropout', 'to the input of', 'each GRU']]",[],[],[],[],[],[],"[['CoNLL 2003 NER', 'Hyperparameters', 'regularization']]",[],named-entity-recognition,3,82
tasks,CoNLL 2000 chunking .,[],"[('CoNLL 2000 chunking', (0, 3))]",[],[],[],"[['Tasks', 'has', 'CoNLL 2000 chunking']]",[],[],[],[],"[['CoNLL 2000 chunking', 'Hyperparameters', 'baseline sequence tagger']]",named-entity-recognition,3,83
tasks,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,"[('uses', (4, 5)), ('with', (12, 13)), ('followed by', (19, 21))]","[('baseline sequence tagger', (1, 4)), ('30 dimensional character embeddings', (5, 9)), ('CNN', (11, 12)), ('30 filters of width 3 characters', (13, 19)), ('tanh non-linearity for the token character encoder', (22, 29))]","[['baseline sequence tagger', 'uses', '30 dimensional character embeddings'], ['baseline sequence tagger', 'uses', 'CNN'], ['CNN', 'with', '30 filters of width 3 characters'], ['30 filters of width 3 characters', 'followed by', 'tanh non-linearity for the token character encoder']]",[],[],[],[],[],[],[],[],named-entity-recognition,3,87
tasks,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,"[('uses', (3, 4)), ('hidden units', (9, 11))]","[('sequence layer', (1, 3)), ('two bidirectional LSTMs', (4, 7)), ('200', (8, 9))]","[['sequence layer', 'uses', 'two bidirectional LSTMs'], ['two bidirectional LSTMs', 'hidden units', '200']]",[],[],[],[],[],[],"[['CoNLL 2000 chunking', 'Hyperparameters', 'sequence layer']]",[],named-entity-recognition,3,88
tasks,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .","[('to', (6, 7))]","[('50 % dropout', (3, 6)), ('character embeddings', (8, 10)), ('input to each LSTM layer', (12, 17)), ('output of the final LSTM layer', (26, 32))]","[['50 % dropout', 'to', 'character embeddings'], ['50 % dropout', 'to', 'input to each LSTM layer'], ['50 % dropout', 'to', 'output of the final LSTM layer']]",[],[],[],[],[],[],"[['CoNLL 2000 chunking', 'Hyperparameters', '50 % dropout']]",[],named-entity-recognition,3,89
hyperparameters,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .","[('use', (2, 3)), ('with', (13, 14)), ('clipped at', (16, 18))]","[('Adam optimizer', (4, 6)), ('gradient norms', (14, 16)), ('5.0', (18, 19))]","[['Adam optimizer', 'with', 'gradient norms'], ['gradient norms', 'clipped at', '5.0']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],[],[],[],named-entity-recognition,3,104
hyperparameters,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .","[('fine tune', (5, 7)), ('fix', (13, 14)), ('in', (16, 17))]","[('pre-trained Senna word embeddings', (8, 12)), ('all weights', (14, 16)), ('pre-trained language models', (18, 21))]","[['all weights', 'in', 'pre-trained language models']]",[],"[['Hyperparameters', 'fine tune', 'pre-trained Senna word embeddings'], ['Hyperparameters', 'fix', 'all weights']]",[],[],[],[],[],[],named-entity-recognition,3,105
hyperparameters,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .","[('to prevent', (12, 14))]","[('early stopping', (10, 12)), ('over-fitting', (14, 15))]","[['early stopping', 'to prevent', 'over-fitting']]",[],[],"[['Hyperparameters', 'use', 'early stopping']]",[],[],[],[],[],named-entity-recognition,3,106
hyperparameters,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,"[('train with', (2, 4))]","[('constant learning rate ? = 0.001', (5, 11))]",[],[],"[['Hyperparameters', 'train with', 'constant learning rate ? = 0.001']]",[],[],[],[],[],[],named-entity-recognition,3,107
results,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .","[('In', (0, 1)), ('scores', (9, 10))]","[('CoNLL 2003 NER task', (2, 6)), ('91.93 mean F 1', (10, 14))]","[['CoNLL 2003 NER task', 'scores', '91.93 mean F 1']]",[],"[['Results', 'In', 'CoNLL 2003 NER task']]",[],[],[],[],[],[],named-entity-recognition,3,118
results,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .","[('achieves', (9, 10))]","[('CoNLL 2000 Chunking task', (2, 6)), ('96.37 mean F 1', (10, 14))]","[['CoNLL 2000 Chunking task', 'achieves', '96.37 mean F 1']]",[],[],"[['Results', 'In', 'CoNLL 2000 Chunking task']]",[],[],[],[],[],named-entity-recognition,3,119
research-problem,Deep contextualized word representations,[],"[('Deep contextualized word representations', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Deep contextualized word representations']]",[],[],[],[],named-entity-recognition,4,2
approach,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"[('differ', (2, 3)), ('assigned', (13, 14)), ('representation', (15, 16))]","[('traditional word type embeddings', (4, 8)), ('each token', (10, 12)), ('function of the entire input sentence', (19, 25))]","[['each token', 'representation', 'function of the entire input sentence']]",[],"[['Approach', 'assigned', 'each token'], ['Approach', 'differ', 'traditional word type embeddings']]",[],[],[],[],[],[],named-entity-recognition,4,13
approach,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"[('use', (1, 2)), ('derived from', (3, 5)), ('trained with', (10, 12)), ('on', (22, 23))]","[('vectors', (2, 3)), ('bidirectional LSTM', (6, 8)), ('coupled lan - guage model ( LM ) objective', (13, 22)), ('large text corpus', (24, 27))]","[['vectors', 'derived from', 'bidirectional LSTM'], ['vectors', 'trained with', 'coupled lan - guage model ( LM ) objective'], ['coupled lan - guage model ( LM ) objective', 'on', 'large text corpus']]",[],"[['Approach', 'use', 'vectors']]",[],[],[],[],[],[],named-entity-recognition,4,14
approach,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","[('call', (5, 6))]","[('ELMo ( Embeddings from Language Models ) representations', (7, 15))]",[],[],"[['Approach', 'call', 'ELMo ( Embeddings from Language Models ) representations']]",[],[],[],[],[],[],named-entity-recognition,4,15
approach,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","[('learn', (4, 5)), ('stacked above', (11, 13)), ('for', (16, 17)), ('markedly improves', (22, 24)), ('over', (25, 26))]","[('linear combination of the vectors', (6, 11)), ('each input word', (13, 16)), ('each end task', (17, 20)), ('performance', (24, 25)), ('using the top LSTM layer', (27, 32))]","[['linear combination of the vectors', 'stacked above', 'each input word'], ['each input word', 'for', 'each end task'], ['each end task', 'markedly improves', 'performance'], ['performance', 'over', 'using the top LSTM layer']]",[],"[['Approach', 'learn', 'linear combination of the vectors']]",[],[],[],[],[],[],named-entity-recognition,4,17
tasks,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",[],"[('Textual entailment', (0, 2))]",[],[],[],"[['Tasks', 'has', 'Textual entailment']]",[],[],[],[],"[['Textual entailment', 'has', 'Stanford Natural Language Inference ( SNLI )']]",named-entity-recognition,4,108
tasks,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,[],"[('Stanford Natural Language Inference ( SNLI )', (1, 8))]",[],[],[],[],[],[],[],[],[],named-entity-recognition,4,109
tasks,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","[('adding', (2, 3)), ('to', (4, 5)), ('improves', (8, 9)), ('by', (10, 11)), ('across', (16, 17))]","[('ELMo', (3, 4)), ('ESIM model', (6, 8)), ('accuracy', (9, 10)), ('average of 0.7 %', (12, 16)), ('five random seeds', (17, 20))]","[['accuracy', 'by', 'average of 0.7 %'], ['average of 0.7 %', 'across', 'five random seeds'], ['accuracy', 'adding', 'ELMo'], ['ELMo', 'to', 'ESIM model']]",[],[],[],[],"[['Stanford Natural Language Inference ( SNLI )', 'improves', 'accuracy']]",[],[],[],named-entity-recognition,4,111
tasks,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,[],"[('Coreference resolution', (3, 5))]",[],[],[],"[['Tasks', 'has', 'Coreference resolution']]",[],[],[],[],[],named-entity-recognition,4,117
tasks,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",[],[],"[['OntoNotes coreference annotations', 'improved', 'average F 1 by 3.2 %'], ['average F 1 by 3.2 %', 'adding', 'ELMo'], ['average F 1 by 3.2 %', 'from', '67.2 to 70.4'], ['OntoNotes coreference annotations', 'from', 'CoNLL 2012 shared task']]",[],[],[],[],"[['Coreference resolution', 'with', 'OntoNotes coreference annotations']]",[],[],[],named-entity-recognition,4,120
research-problem,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,[],"[('Language Understanding', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Language Understanding']]",[],[],[],[],named-entity-recognition,8,2
research-problem,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",[],"[('language representation model', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'language representation model']]",[],[],[],[],named-entity-recognition,8,4
research-problem,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,[],"[('Language model pre-training', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Language model pre-training']]",[],[],[],[],named-entity-recognition,8,14
approach,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .","[('proposing', (13, 14))]","[('BERT : Bidirectional Encoder Representations from Transformers', (14, 21))]",[],[],"[['Approach', 'proposing', 'BERT : Bidirectional Encoder Representations from Transformers']]",[],[],[],[],[],[],named-entity-recognition,8,24
approach,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .","[('alleviates', (1, 2)), ('using', (8, 9))]","[('BERT', (0, 1)), ('unidirectionality constraint', (5, 7)), ('"" masked language model "" ( MLM ) pre-training objective', (10, 20))]","[['BERT', 'alleviates', 'unidirectionality constraint'], ['unidirectionality constraint', 'using', '"" masked language model "" ( MLM ) pre-training objective']]",[],[],"[['Approach', 'name', 'BERT']]",[],[],[],[],[],named-entity-recognition,8,25
approach,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .","[('randomly masks', (4, 6)), ('from', (10, 11)), ('objective', (16, 17)), ('of', (24, 25)), ('based only on', (28, 31))]","[('masked language model', (1, 4)), ('some of the tokens', (6, 10)), ('input', (12, 13)), ('predict the original vocabulary id', (19, 24)), ('masked word', (26, 28)), ('context', (32, 33))]","[['masked language model', 'randomly masks', 'some of the tokens'], ['some of the tokens', 'from', 'input'], ['masked language model', 'objective', 'predict the original vocabulary id'], ['predict the original vocabulary id', 'of', 'masked word'], ['masked word', 'based only on', 'context']]",[],[],"[['Approach', 'has', 'masked language model']]",[],[],[],[],[],named-entity-recognition,8,26
approach,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .","[('enables', (11, 12)), ('allows us to pretrain', (24, 28))]","[('MLM objective', (9, 11)), ('representation to fuse the left and the right context', (13, 22)), ('deep bidirectional Transformer', (29, 32))]","[['MLM objective', 'enables', 'representation to fuse the left and the right context'], ['representation to fuse the left and the right context', 'allows us to pretrain', 'deep bidirectional Transformer']]",[],[],"[['Approach', 'has', 'MLM objective']]",[],[],[],[],[],named-entity-recognition,8,27
approach,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .","[('use', (10, 11)), ('jointly pretrains', (19, 21))]","[('"" next sentence prediction "" task', (12, 18)), ('text - pair representations', (21, 25))]","[['"" next sentence prediction "" task', 'jointly pretrains', 'text - pair representations']]",[],"[['Approach', 'use', '"" next sentence prediction "" task']]",[],[],[],[],[],[],named-entity-recognition,8,28
tasks,GLUE,[],"[('GLUE', (0, 1))]",[],[],[],"[['Tasks', 'has', 'GLUE']]",[],[],[],[],"[['GLUE', 'name', 'General Language Understanding Evaluation ( GLUE )']]",named-entity-recognition,8,155
tasks,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",[],"[('General Language Understanding Evaluation ( GLUE )', (1, 8))]",[],[],[],[],[],[],[],[],[],named-entity-recognition,8,156
tasks,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,"[('use', (1, 2)), ('of', (5, 6)), ('fine - tune', (8, 11)), ('over', (14, 15))]","[('batch size', (3, 5)), ('32', (6, 7)), ('for 3 epochs', (11, 14)), ('data for all GLUE tasks', (16, 21))]","[['batch size', 'of', '32'], ['for 3 epochs', 'over', 'data for all GLUE tasks']]",[],[],[],[],"[['Hyperparameters', 'use', 'batch size'], ['Hyperparameters', 'fine - tune', 'for 3 epochs']]",[],[],[],named-entity-recognition,8,170
tasks,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .","[('selected', (5, 6)), ('among', (14, 15)), ('on', (36, 37))]","[('best fine - tuning learning rate', (7, 13)), ('5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5', (15, 35)), ('Dev set', (38, 40))]","[['best fine - tuning learning rate', 'among', '5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5'], ['best fine - tuning learning rate', 'on', 'Dev set']]",[],[],[],[],"[['Hyperparameters', 'selected', 'best fine - tuning learning rate']]",[],[],[],named-entity-recognition,8,171
tasks,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .","[('for', (2, 3)), ('ran', (18, 19)), ('selected', (23, 24))]","[('BERT LARGE', (3, 5)), ('several random restarts', (19, 22)), ('best model on the Dev set', (25, 31))]","[['BERT LARGE', 'ran', 'several random restarts'], ['several random restarts', 'selected', 'best model on the Dev set']]",[],[],[],[],"[['Hyperparameters', 'for', 'BERT LARGE']]",[],[],[],named-entity-recognition,8,172
tasks,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .","[('outperform', (6, 7)), ('by', (12, 13)), ('obtaining', (17, 18)), ('over', (27, 28))]","[('BERT BASE and BERT LARGE', (1, 6)), ('all systems on all tasks', (7, 12)), ('substantial margin', (14, 16)), ('4.5 % and 7.0 % respective average accuracy improvement', (18, 27)), ('prior state of the art', (29, 34))]","[['BERT BASE and BERT LARGE', 'outperform', 'all systems on all tasks'], ['all systems on all tasks', 'by', 'substantial margin'], ['substantial margin', 'obtaining', '4.5 % and 7.0 % respective average accuracy improvement'], ['4.5 % and 7.0 % respective average accuracy improvement', 'over', 'prior state of the art']]",[],[],[],[],[],[],"[['Results', 'has', 'BERT BASE and BERT LARGE']]",[],named-entity-recognition,8,175
tasks,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .","[('significantly outperforms', (5, 7)), ('across', (9, 10))]","[('BERT LARGE', (3, 5)), ('BERT BASE', (7, 9)), ('all tasks', (10, 12))]","[['BERT LARGE', 'significantly outperforms', 'BERT BASE'], ['BERT BASE', 'across', 'all tasks']]",[],[],[],[],[],[],"[['Results', 'has', 'BERT LARGE'], ['Results', 'has', 'BERT LARGE']]",[],named-entity-recognition,8,179
tasks,SQuAD v 1.1,[],"[('SQuAD v 1.1', (0, 3))]",[],[],[],"[['Tasks', 'has', 'SQuAD v 1.1']]",[],[],[],[],"[['SQuAD v 1.1', 'name', 'Stanford Question Answering Dataset ( SQuAD v1.1 )']]",named-entity-recognition,8,181
tasks,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,[],"[('Stanford Question Answering Dataset ( SQuAD v1.1 )', (1, 9))]",[],[],[],[],[],[],[],[],[],named-entity-recognition,8,182
tasks,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,"[('fine - tune', (1, 4)), ('learning rate', (9, 11)), ('batch size', (18, 20))]","[('3 epochs', (5, 7)), ('5 e - 5', (12, 16)), ('32', (21, 22))]","[['3 epochs', 'learning rate', '5 e - 5'], ['3 epochs', 'batch size', '32']]",[],[],[],[],"[['Hyperparameters', 'fine - tune', '3 epochs'], ['Hyperparameters', 'fine - tune', '3 epochs']]",[],[],[],named-entity-recognition,8,195
tasks,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,"[('outperforms', (4, 5)), ('by', (9, 10)), ('in', (13, 14)), ('as', (19, 20))]","[('best performing system', (1, 4)), ('top leaderboard system', (6, 9)), ('+ 1.5 F1', (10, 13)), ('ensembling', (14, 15)), ('+ 1.3 F1', (16, 19)), ('single system', (21, 23))]","[['best performing system', 'outperforms', 'top leaderboard system'], ['top leaderboard system', 'by', '+ 1.5 F1'], ['+ 1.5 F1', 'in', 'ensembling'], ['top leaderboard system', 'by', '+ 1.3 F1'], ['+ 1.3 F1', 'as', 'single system']]",[],[],[],[],[],[],"[['Results', 'has', 'best performing system']]",[],named-entity-recognition,8,199
tasks,SQuAD v 2.0,[],"[('SQuAD v 2.0', (0, 3))]",[],[],[],"[['Tasks', 'has', 'SQuAD v 2.0']]",[],[],[],[],"[['SQuAD v 2.0', 'has', 'Hyperparameters']]",named-entity-recognition,8,203
tasks,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,"[('fine - tuned', (1, 4)), ('learning rate', (9, 11)), ('batch size', (18, 20))]","[('2 epochs', (5, 7)), ('5 e - 5', (12, 16)), ('48', (21, 22))]","[['2 epochs', 'learning rate', '5 e - 5'], ['2 epochs', 'batch size', '48']]",[],[],[],[],"[['Hyperparameters', 'fine - tuned', '2 epochs']]",[],[],[],named-entity-recognition,8,213
tasks,We observe a + 5.1 F1 improvement over the previous best system .,"[('observe', (1, 2)), ('over', (7, 8))]","[('+ 5.1 F1 improvement', (3, 7)), ('previous best system', (9, 12))]","[['+ 5.1 F1 improvement', 'over', 'previous best system']]",[],[],[],[],"[['Results', 'observe', '+ 5.1 F1 improvement']]",[],[],[],named-entity-recognition,8,215
tasks,SWAG,[],"[('SWAG', (0, 1))]",[],[],[],"[['Tasks', 'has', 'SWAG']]",[],[],[],[],"[['SWAG', 'name', 'Situations With Adversarial Generations']]",named-entity-recognition,8,216
tasks,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,[],"[('Situations With Adversarial Generations', (1, 5))]",[],[],[],[],[],[],[],[],[],named-entity-recognition,8,217
tasks,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,[],[],"[['3 epochs', 'learning rate', '2 e - 5'], ['3 epochs', 'batch size', '16']]",[],[],[],[],[],[],[],[],named-entity-recognition,8,221
tasks,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,[],[],"[['BERT LARGE', 'outperforms', 'OpenAI GPT'], ['OpenAI GPT', 'by', '8.3 %'], ['BERT LARGE', 'outperforms', ""authors ' baseline ESIM + ELMo system""], [""authors ' baseline ESIM + ELMo system"", 'by', '+ 27.1 %']]",[],[],[],[],[],[],[],[],named-entity-recognition,8,223
ablation-analysis,Effect of Model Size,[],"[('Effect of Model Size', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Effect of Model Size']]",[],[],[],[],[],named-entity-recognition,8,250
ablation-analysis,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .","[('is', (6, 7)), ('to demonstrate', (10, 12)), ('leads to', (20, 22)), ('on', (24, 25)), ('provided that', (30, 32))]","[('first work', (8, 10)), ('scaling to extreme model', (14, 18)), ('large improvements', (22, 24)), ('very small scale tasks', (25, 29)), ('model has been sufficiently pre-trained', (33, 38))]","[['first work', 'to demonstrate', 'scaling to extreme model'], ['scaling to extreme model', 'leads to', 'large improvements'], ['large improvements', 'on', 'very small scale tasks'], ['very small scale tasks', 'provided that', 'model has been sufficiently pre-trained']]",[],[],[],[],"[['Effect of Model Size', 'is', 'first work']]",[],[],[],named-entity-recognition,8,260
ablation-analysis,Feature - based Approach with BERT,[],"[('Feature - based Approach with BERT', (0, 6))]",[],[],[],"[['Ablation analysis', 'has', 'Feature - based Approach with BERT']]",[],[],[],[],"[['Feature - based Approach with BERT', 'has', 'BERT LARGE']]",named-entity-recognition,8,263
ablation-analysis,BERT LARGE performs competitively with state - of - the - art methods .,"[('performs competitively', (2, 4))]","[('BERT LARGE', (0, 2)), ('state - of - the - art methods', (5, 13))]","[['BERT LARGE', 'performs competitively', 'state - of - the - art methods']]",[],[],[],[],[],[],[],[],named-entity-recognition,8,275
ablation-analysis,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,"[('demonstrates', (1, 2)), ('for both', (6, 8))]","[('BERT is effective', (3, 6)), ('finetuning', (8, 9)), ('feature - based approaches', (10, 14))]","[['BERT is effective', 'for both', 'finetuning'], ['BERT is effective', 'for both', 'feature - based approaches']]",[],[],[],[],"[['Feature - based Approach with BERT', 'demonstrates', 'BERT is effective']]",[],[],[],named-entity-recognition,8,277
research-problem,Gated - Attention Readers for Text Comprehension,[],"[('Text Comprehension', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text Comprehension']]",[],[],[],[],natural_language_inference,0,2
code,Source code is available on github : https:// github.com/bdhingra/ga-reader,[],"[('https:// github.com/bdhingra/ga-reader', (7, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https:// github.com/bdhingra/ga-reader']]",[],[],[],[],natural_language_inference,0,11
research-problem,A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .,[],"[('machine reading', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading']]",[],[],[],[],natural_language_inference,0,13
model,"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .","[('applied', (11, 12)), ('allows', (37, 38)), ('to directly interact with', (40, 44)), ('of', (46, 47)), ('at', (50, 51)), ('as', (62, 63)), ('during', (65, 66))]","[('query', (8, 9)), ('Gated - Attention ( GA ) module', (26, 33)), ('each dimension', (44, 46)), ('token embeddings', (48, 50)), ('semantic - level', (52, 55)), ('layer - wise', (59, 62)), ('information filters', (63, 65)), ('multi-hop representation learning process', (67, 71))]","[['Gated - Attention ( GA ) module', 'allows', 'query'], ['query', 'to directly interact with', 'each dimension'], ['each dimension', 'of', 'token embeddings'], ['token embeddings', 'at', 'semantic - level'], ['Gated - Attention ( GA ) module', 'applied', 'layer - wise'], ['layer - wise', 'as', 'information filters'], ['information filters', 'during', 'multi-hop representation learning process']]",[],[],"[['Model', 'has', 'Gated - Attention ( GA ) module']]",[],[],[],[],[],natural_language_inference,0,23
model,"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .","[('enables', (6, 7)), ('to learn', (9, 11)), ('w.r.t.', (14, 15))]","[('fine - grained attention', (2, 6)), ('model', (8, 9)), ('conditional token representations', (11, 14)), ('given question', (16, 18))]","[['fine - grained attention', 'enables', 'model'], ['model', 'to learn', 'conditional token representations'], ['conditional token representations', 'w.r.t.', 'given question']]",[],[],"[['Model', 'has', 'fine - grained attention']]",[],[],[],[],[],natural_language_inference,0,24
results,"Interestingly , we observe that feature engineering leads to significant improvements for WDW and CBT datasets , but not for CNN and Daily Mail datasets .","[('observe that', (3, 5)), ('leads to', (7, 9)), ('for', (11, 12)), ('not for', (18, 20))]","[('feature engineering', (5, 7)), ('significant improvements', (9, 11)), ('WDW and CBT datasets', (12, 16)), ('CNN and Daily Mail datasets', (20, 25))]","[['feature engineering', 'leads to', 'significant improvements'], ['significant improvements', 'not for', 'CNN and Daily Mail datasets'], ['significant improvements', 'for', 'WDW and CBT datasets']]",[],"[['Results', 'observe that', 'feature engineering']]",[],[],[],[],[],[],natural_language_inference,0,137
results,"Similarly , fixing the word embeddings provides an improvement for the WDW and CBT , but not for CNN and Daily Mail .","[('fixing', (2, 3)), ('provides', (6, 7)), ('for', (9, 10)), ('not for', (16, 18))]","[('word embeddings', (4, 6)), ('improvement', (8, 9)), ('WDW and CBT', (11, 14)), ('CNN and Daily Mail', (18, 22))]","[['word embeddings', 'provides', 'improvement'], ['improvement', 'not for', 'CNN and Daily Mail'], ['improvement', 'for', 'WDW and CBT']]",[],"[['Results', 'fixing', 'word embeddings']]",[],[],[],[],[],[],natural_language_inference,0,140
results,"Comparing with prior work , on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting .","[('on', (5, 6)), ('outperforms', (16, 17)), ('when trained on', (21, 24))]","[('WDW dataset', (7, 9)), ('basic version of the GA Reader', (10, 16)), ('all previously published models', (17, 21)), ('Strict setting', (25, 27))]","[['basic version of the GA Reader', 'when trained on', 'Strict setting'], ['basic version of the GA Reader', 'outperforms', 'all previously published models']]","[['WDW dataset', 'has', 'basic version of the GA Reader']]","[['Results', 'on', 'WDW dataset']]",[],[],[],[],[],[],natural_language_inference,0,142
results,By adding the qecomm feature the performance increases by 3.2 % and 3.5 % on the Strict and Relaxed settings respectively to set a new state of the art on this dataset .,"[('adding', (1, 2)), ('by', (8, 9)), ('on', (14, 15))]","[('qecomm feature', (3, 5)), ('performance', (6, 7)), ('increases', (7, 8)), ('3.2 % and 3.5 %', (9, 14)), ('Strict and Relaxed settings', (16, 20))]","[['increases', 'by', '3.2 % and 3.5 %'], ['increases', 'on', 'Strict and Relaxed settings']]","[['qecomm feature', 'has', 'performance'], ['performance', 'has', 'increases']]","[['Results', 'adding', 'qecomm feature']]",[],[],[],[],[],[],natural_language_inference,0,143
results,On the CNN and Daily Mail datasets the GA Reader leads to an improvement of 3.2 % and 4.3 % respectively over the best previous single models .,"[('On', (0, 1)), ('leads to', (10, 12)), ('of', (14, 15)), ('over', (21, 22))]","[('CNN and Daily Mail datasets', (2, 7)), ('GA Reader', (8, 10)), ('improvement', (13, 14)), ('3.2 % and 4.3 %', (15, 20)), ('best previous single models', (23, 27))]","[['GA Reader', 'leads to', 'improvement'], ['improvement', 'of', '3.2 % and 4.3 %'], ['3.2 % and 4.3 %', 'over', 'best previous single models']]","[['CNN and Daily Mail datasets', 'has', 'GA Reader']]","[['Results', 'On', 'CNN and Daily Mail datasets']]",[],[],[],[],[],[],natural_language_inference,0,144
results,"For CBT - NE , GA Reader with the qecomm feature outperforms all previous single and ensemble models except the AS Reader trained on the much larger BookTest Corpus .","[('For', (0, 1)), ('with', (7, 8)), ('outperforms', (11, 12)), ('except', (18, 19)), ('trained on', (22, 24))]","[('CBT - NE', (1, 4)), ('GA Reader', (5, 7)), ('qecomm feature', (9, 11)), ('all previous single and ensemble models', (12, 18)), ('AS Reader', (20, 22)), ('much larger BookTest Corpus', (25, 29))]","[['GA Reader', 'with', 'qecomm feature'], ['GA Reader', 'outperforms', 'all previous single and ensemble models'], ['all previous single and ensemble models', 'except', 'AS Reader'], ['AS Reader', 'trained on', 'much larger BookTest Corpus']]","[['CBT - NE', 'has', 'GA Reader']]","[['Results', 'For', 'CBT - NE']]",[],[],[],[],[],[],natural_language_inference,0,146
results,"Lastly , on CBT - CN the GA Reader with the qe-comm feature outperforms all previously published single models except the NSE , and AS Reader trained on a larger corpus .","[('with', (9, 10)), ('outperforms', (13, 14)), ('except', (19, 20)), ('trained on', (26, 28))]","[('CBT - CN', (3, 6)), ('GA Reader', (7, 9)), ('qe-comm feature', (11, 13)), ('all previously published single models', (14, 19)), ('NSE', (21, 22)), ('AS Reader', (24, 26)), ('larger corpus', (29, 31))]","[['GA Reader', 'with', 'qe-comm feature'], ['GA Reader', 'outperforms', 'all previously published single models'], ['all previously published single models', 'except', 'NSE'], ['all previously published single models', 'except', 'AS Reader'], ['AS Reader', 'trained on', 'larger corpus']]","[['CBT - CN', 'has', 'GA Reader']]",[],"[['Results', 'on', 'CBT - CN']]",[],[],[],[],[],natural_language_inference,0,147
ablation-analysis,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .","[('observe', (3, 4)), ('when removing', (7, 9)), ('over', (11, 12)), ('allow gating', (20, 22)), ('in', (24, 25)), ('by', (28, 29)), ('relevant to', (33, 35)), ('rather than', (37, 39))]","[('substantial drop', (5, 7)), ('tokenspecific attentions', (9, 11)), ('query in the GA module', (13, 18)), ('individual tokens', (22, 24)), ('document', (26, 27)), ('parts of the query', (29, 33)), ('token', (36, 37)), ('over all query representation', (40, 44))]","[['substantial drop', 'when removing', 'tokenspecific attentions'], ['tokenspecific attentions', 'over', 'query in the GA module'], ['tokenspecific attentions', 'allow gating', 'individual tokens'], ['individual tokens', 'in', 'document'], ['individual tokens', 'by', 'parts of the query'], ['parts of the query', 'relevant to', 'token'], ['token', 'rather than', 'over all query representation']]",[],"[['Ablation analysis', 'observe', 'substantial drop']]",[],[],[],[],[],[],natural_language_inference,0,182
ablation-analysis,"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a reduction of about 1 % in the performance .","[('removing', (2, 3)), ('used for', (10, 12)), ('leads to', (16, 18)), ('of', (20, 21))]","[('character embeddings', (4, 6)), ('WDW and CBT', (12, 15)), ('reduction', (19, 20)), ('about 1 % in the performance', (21, 27))]","[['character embeddings', 'leads to', 'reduction'], ['reduction', 'of', 'about 1 % in the performance'], ['character embeddings', 'used for', 'WDW and CBT']]",[],"[['Ablation analysis', 'removing', 'character embeddings']]",[],[],[],[],[],[],natural_language_inference,0,183
research-problem,Large - scale Simple Question Answering with Memory Networks,[],"[('Large - scale Simple Question Answering', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Large - scale Simple Question Answering']]",[],[],[],[],natural_language_inference,1,2
research-problem,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,[],"[('large - scale question answering', (1, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'large - scale question answering']]",[],[],[],[],natural_language_inference,1,4
research-problem,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",[],"[('simple question answering', (11, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'simple question answering']]",[],[],[],[],natural_language_inference,1,5
research-problem,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",[],"[('Simple Question Answering', (46, 49))]",[],[],[],[],"[['Contribution', 'has research problem', 'Simple Question Answering']]",[],[],[],[],natural_language_inference,1,12
dataset,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .","[('collected', (26, 27)), ('based on', (37, 39)), ('called', (42, 43))]","[('first large - scale dataset of questions and answers', (28, 37)), ('KB', (40, 41)), ('SimpleQuestions', (43, 44))]","[['first large - scale dataset of questions and answers', 'called', 'SimpleQuestions'], ['first large - scale dataset of questions and answers', 'based on', 'KB']]",[],"[['Dataset', 'collected', 'first large - scale dataset of questions and answers']]",[],[],[],[],[],[],natural_language_inference,1,19
dataset,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?","[('contains', (10, 11))]","[('more than 100 k questions', (11, 16))]",[],[],"[['Dataset', 'contains', 'more than 100 k questions']]",[],[],[],[],[],[],natural_language_inference,1,20
model,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .","[('present', (9, 10)), ('developed under', (16, 18))]","[('embedding - based QA system', (11, 16)), ('framework of Memory Networks ( Mem NNs )', (19, 27))]","[['embedding - based QA system', 'developed under', 'framework of Memory Networks ( Mem NNs )']]",[],"[['Model', 'present', 'embedding - based QA system']]",[],[],[],[],[],[],natural_language_inference,1,24
model,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .","[('are', (2, 3)), ('centered around', (5, 7)), ('can be', (11, 13)), ('focus on cases where', (21, 25)), ('between', (27, 28)), ('performed by', (51, 53)), ('in', (57, 58))]","[('Memory Networks', (0, 2)), ('learning systems', (3, 5)), ('memory component', (8, 10)), ('read and written to', (13, 17)), ('relationship', (26, 27)), ('input and response languages ( here natural language )', (29, 38)), ('storage language ( here , the facts from KBs )', (40, 50)), ('embedding', (53, 54)), ('same vector space', (59, 62))]","[['Memory Networks', 'are', 'learning systems'], ['learning systems', 'focus on cases where', 'relationship'], ['relationship', 'performed by', 'embedding'], ['embedding', 'in', 'same vector space'], ['embedding', 'between', 'input and response languages ( here natural language )'], ['embedding', 'between', 'storage language ( here , the facts from KBs )'], ['learning systems', 'centered around', 'memory component'], ['memory component', 'can be', 'read and written to']]",[],[],"[['Model', 'has', 'Memory Networks']]",[],[],[],[],[],natural_language_inference,1,25
model,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,"[('setting of', (1, 3)), ('corresponds to', (6, 8)), ('of performing', (11, 13)), ('in', (16, 17))]","[('simple QA', (4, 6)), ('elementary operation', (9, 11)), ('single lookup', (14, 16)), ('memory', (18, 19))]","[['simple QA', 'corresponds to', 'elementary operation'], ['elementary operation', 'of performing', 'single lookup'], ['single lookup', 'in', 'memory']]",[],"[['Model', 'setting of', 'simple QA']]",[],[],[],[],[],[],natural_language_inference,1,26
hyperparameters,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?","[('chosen among', (8, 10))]","[('embedding dimension and the learning rate', (1, 7)), ('{ 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 }', (10, 29)), ('margin', (33, 34))]","[['embedding dimension and the learning rate', 'chosen among', '{ 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 }']]",[],[],"[['Hyperparameters', 'has', 'margin'], ['Hyperparameters', 'has', 'embedding dimension and the learning rate']]",[],[],[],[],[],natural_language_inference,1,229
hyperparameters,was set to 0.1 .,"[('set to', (1, 3))]","[('0.1', (3, 4))]",[],[],[],[],[],"[['margin', 'set to', '0.1']]",[],[],[],natural_language_inference,1,230
results,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .","[('On', (0, 1)), ('answered with', (20, 22)), ('from', (32, 33)), ('to', (35, 36))]","[('WebQuestions', (1, 2)), ('86 % of the questions', (12, 17)), ('single supporting fact', (23, 26)), ('performance', (28, 29)), ('increases significantly', (29, 31)), ('36.2 %', (33, 35)), ('41.0 % F1-score', (36, 39))]","[['performance', 'from', '36.2 %'], ['36.2 %', 'to', '41.0 % F1-score'], ['86 % of the questions', 'answered with', 'single supporting fact']]","[['WebQuestions', 'has', '86 % of the questions'], ['86 % of the questions', 'has', 'performance'], ['performance', 'has', 'increases significantly']]","[['Results', 'On', 'WebQuestions']]",[],[],[],[],[],[],natural_language_inference,1,254
results,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .","[('Using', (0, 1)), ('not change', (7, 9)), ('on', (10, 11))]","[('bigger FB5M as KB', (2, 6)), ('performance', (9, 10)), ('SimpleQuestions', (11, 12))]","[['bigger FB5M as KB', 'not change', 'performance'], ['performance', 'on', 'SimpleQuestions']]",[],"[['Results', 'Using', 'bigger FB5M as KB']]",[],[],[],[],[],[],natural_language_inference,1,255
results,Transfer learning on Reverb,[],"[('Transfer learning on Reverb', (0, 4))]",[],[],[],"[['Results', 'has', 'Transfer learning on Reverb']]",[],[],[],[],"[['Transfer learning on Reverb', 'has', 'best results']]",natural_language_inference,1,256
results,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .","[('are', (3, 4)), ('for', (11, 12))]","[('best results', (1, 3)), ('67 % accuracy', (4, 7)), ('68 %', (9, 11)), ('ensemble of 5 models', (13, 17))]","[['best results', 'are', '67 % accuracy'], ['best results', 'are', '68 %'], ['68 %', 'for', 'ensemble of 5 models']]",[],[],[],[],[],[],[],[],natural_language_inference,1,259
results,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .","[('notice', (2, 3)), ('trained on', (5, 7)), ('perform', (11, 12)), ('on', (13, 14))]","[('models', (4, 5)), ('single QA dataset', (8, 11)), ('poorly', (12, 13)), ('other datasets', (15, 17))]","[['models', 'trained on', 'single QA dataset'], ['single QA dataset', 'perform', 'poorly'], ['poorly', 'on', 'other datasets']]",[],[],[],[],"[['Transfer learning on Reverb', 'notice', 'models']]",[],[],[],natural_language_inference,1,262
results,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .","[('training on', (5, 7)), ('improves', (10, 11))]","[('both datasets', (7, 9)), ('performance', (11, 12))]","[['both datasets', 'improves', 'performance']]",[],[],[],[],"[['Transfer learning on Reverb', 'training on', 'both datasets']]",[],[],[],natural_language_inference,1,263
results,Importance of data sources,[],"[('Importance of data sources', (0, 4))]",[],[],[],"[['Results', 'has', 'Importance of data sources']]",[],[],[],[],"[['Importance of data sources', 'has', 'paraphrases']]",natural_language_inference,1,264
results,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .",[],[],"[['paraphrases', 'not seem to', 'help much'], ['help much', 'except when', 'training'], ['training', 'only with', 'synthetic questions'], ['synthetic questions', 'have', 'dramatic impact'], ['dramatic impact', 'on', 'performance'], ['performance', 'on', 'Reverb'], ['help much', 'on', 'WebQuestions and SimpleQuestions']]",[],[],[],[],[],[],[],[],natural_language_inference,1,266
research-problem,Learning to Compose Task - Specific Tree Structures,[],"[('Task - Specific Tree Structures', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Task - Specific Tree Structures']]",[],[],[],[],natural_language_inference,10,2
research-problem,"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently .",[],"[('task - specific tree structures only from plain text data', (27, 37))]",[],[],[],[],"[['Contribution', 'has research problem', 'task - specific tree structures only from plain text data']]",[],[],[],[],natural_language_inference,10,6
model,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .","[('propose', (5, 6)), ('is', (12, 13)), ('not require', (19, 21)), ('learns to compose', (24, 27)), ('without', (32, 33))]","[('Gumbel Tree - LSTM', (6, 10)), ('novel RvNN architecture', (14, 17)), ('structured data', (21, 23)), ('task - specific tree structures', (27, 32)), ('explicit guidance', (33, 35))]","[['Gumbel Tree - LSTM', 'is', 'novel RvNN architecture'], ['novel RvNN architecture', 'learns to compose', 'task - specific tree structures'], ['task - specific tree structures', 'without', 'explicit guidance'], ['novel RvNN architecture', 'not require', 'structured data']]",[],"[['Model', 'propose', 'Gumbel Tree - LSTM']]",[],[],[],[],[],[],natural_language_inference,10,23
model,"Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .","[('based on', (7, 9))]","[('Our Gumbel Tree - LSTM model', (0, 6)), ('tree - structured long short - term memory ( Tree - LSTM ) architecture', (9, 23))]","[['Our Gumbel Tree - LSTM model', 'based on', 'tree - structured long short - term memory ( Tree - LSTM ) architecture']]",[],[],"[['Model', 'has', 'Our Gumbel Tree - LSTM model']]",[],[],[],[],[],natural_language_inference,10,24
model,"To learn how to compose task - specific tree structures without depending on structured input , our model introduces composition query vector that measures validity of a composition .","[('introduces', (18, 19)), ('measures', (23, 24)), ('of', (25, 26))]","[('our model', (16, 18)), ('composition query vector', (19, 22)), ('validity', (24, 25)), ('composition', (27, 28))]","[['our model', 'introduces', 'composition query vector'], ['composition query vector', 'measures', 'validity'], ['validity', 'of', 'composition']]",[],[],"[['Model', 'has', 'our model']]",[],[],[],[],[],natural_language_inference,10,25
model,"Using validity scores computed by the composition query vector , our model recursively selects compositions until only a single representation remains .","[('Using', (0, 1)), ('computed by', (3, 5)), ('recursively selects', (12, 14)), ('until', (15, 16))]","[('validity scores', (1, 3)), ('composition query vector', (6, 9)), ('our model', (10, 12)), ('compositions', (14, 15)), ('only a single representation remains', (16, 21))]","[['validity scores', 'computed by', 'composition query vector'], ['our model', 'recursively selects', 'compositions'], ['compositions', 'until', 'only a single representation remains']]","[['validity scores', 'has', 'our model']]","[['Model', 'Using', 'validity scores']]",[],[],[],[],[],[],natural_language_inference,10,26
model,We use Straight - Through ( ST ) Gumbel - Softmax estimator to sample compositions in the training phase .,"[('use', (1, 2)), ('to sample', (12, 14)), ('in', (15, 16))]","[('Straight - Through ( ST ) Gumbel - Softmax estimator', (2, 12)), ('compositions', (14, 15)), ('training phase', (17, 19))]","[['Straight - Through ( ST ) Gumbel - Softmax estimator', 'to sample', 'compositions'], ['compositions', 'in', 'training phase']]",[],"[['Model', 'use', 'Straight - Through ( ST ) Gumbel - Softmax estimator']]",[],[],[],[],[],[],natural_language_inference,10,27
model,"ST Gumbel - Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass , thus our model can be trained via the standard backpropagation .","[('relaxes', (5, 6)), ('to be', (10, 12)), ('in', (13, 14))]","[('discrete sampling operation', (7, 10)), ('continuous', (12, 13)), ('backward pass', (15, 17))]","[['discrete sampling operation', 'to be', 'continuous'], ['continuous', 'in', 'backward pass']]",[],[],[],[],"[['Straight - Through ( ST ) Gumbel - Softmax estimator', 'relaxes', 'discrete sampling operation']]",[],[],[],natural_language_inference,10,28
experiments,Natural Language Inference,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Natural Language Inference']]","[['Natural Language Inference', 'has', 'Experimental setup']]",natural_language_inference,10,139
experiments,"Similar to 100D experiments , we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training .","[('initialize', (6, 7)), ('with', (11, 12))]","[('word embedding matrix', (8, 11)), ('GloVe 300D pretrained vectors', (12, 16))]","[['word embedding matrix', 'with', 'GloVe 300D pretrained vectors']]",[],[],[],[],"[['Experimental setup', 'initialize', 'word embedding matrix']]",[],[],[],natural_language_inference,10,150
experiments,The dropout probability is set to 0.2 and word embeddings are not updated during training .,"[('set to', (4, 6))]","[('dropout probability', (1, 3)), ('0.2', (6, 7))]","[['dropout probability', 'set to', '0.2']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'dropout probability']]",[],natural_language_inference,10,153
experiments,"The size of mini-batches is set to 128 in all experiments , and hyperparameters are tuned using the validation split .","[('of', (2, 3)), ('set to', (5, 7))]","[('size', (1, 2)), ('mini-batches', (3, 4)), ('128', (7, 8))]","[['size', 'of', 'mini-batches'], ['mini-batches', 'set to', '128'], ['size', 'of', 'mini-batches']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'size']]",[],natural_language_inference,10,154
experiments,"The temperature parameter ? of Gumbel - Softmax is set to 1.0 , and we did not find that temperature annealing improves performance .","[('of', (4, 5)), ('set to', (9, 11))]","[('temperature parameter', (1, 3)), ('Gumbel - Softmax', (5, 8)), ('1.0', (11, 12))]","[['temperature parameter', 'of', 'Gumbel - Softmax'], ['Gumbel - Softmax', 'set to', '1.0']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'temperature parameter']]",[],natural_language_inference,10,155
experiments,"For training models , Adam optimizer is used .","[('For training', (0, 2)), ('used', (7, 8))]","[('models', (2, 3)), ('Adam optimizer', (4, 6))]","[['models', 'used', 'Adam optimizer']]",[],[],[],[],"[['Experimental setup', 'For training', 'models']]",[],[],[],natural_language_inference,10,156
experiments,"First , we can see that LSTM - based leaf transformation has a clear advantage over the affine - transformation - based one .","[('see that', (4, 6)), ('over', (15, 16))]","[('LSTM - based leaf transformation', (6, 11)), ('clear advantage', (13, 15)), ('affine - transformation - based one', (17, 23))]","[['clear advantage', 'over', 'affine - transformation - based one']]","[['LSTM - based leaf transformation', 'has', 'clear advantage']]",[],[],[],"[['Results', 'see that', 'LSTM - based leaf transformation']]",[],[],[],natural_language_inference,10,158
experiments,"Secondly , comparing ours with other models , we find that our 100D and 300D model outperform all other models of similar numbers of parameters .","[('find that', (9, 11)), ('outperform', (16, 17)), ('of', (20, 21))]","[('our 100D and 300D model', (11, 16)), ('all other models', (17, 20)), ('similar numbers of parameters', (21, 25))]","[['our 100D and 300D model', 'outperform', 'all other models'], ['all other models', 'of', 'similar numbers of parameters']]",[],[],[],[],"[['Results', 'find that', 'our 100D and 300D model']]",[],[],[],natural_language_inference,10,160
experiments,"Our 600D model achieves the accuracy of 86.0 % , which is comparable to that of the state - of - the - art model , while using far less parameters .","[('achieves', (3, 4)), ('of', (6, 7)), ('comparable to', (12, 14))]","[('Our 600D model', (0, 3)), ('accuracy', (5, 6)), ('86.0 %', (7, 9)), ('state - of - the - art model', (17, 25))]","[['Our 600D model', 'achieves', 'accuracy'], ['accuracy', 'of', '86.0 %'], ['86.0 %', 'comparable to', 'state - of - the - art model']]",[],[],[],[],[],[],"[['Results', 'has', 'Our 600D model']]",[],natural_language_inference,10,161
experiments,All of our models converged within a few hours on a machine with NVIDIA Titan Xp GPU .,"[('on', (9, 10)), ('with', (12, 13))]","[('machine', (11, 12)), ('NVIDIA Titan Xp GPU', (13, 17))]","[['machine', 'with', 'NVIDIA Titan Xp GPU']]",[],[],[],[],"[['Experimental setup', 'on', 'machine']]",[],[],[],natural_language_inference,10,163
experiments,Sentiment Analysis,[],"[('Sentiment Analysis', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Sentiment Analysis']]","[['Sentiment Analysis', 'has', 'Hyperparameters']]",natural_language_inference,10,166
experiments,is a single - hidden layer MLP with the ReLU activation function .,"[('is', (0, 1)), ('with', (7, 8))]","[('single - hidden layer MLP', (2, 7)), ('ReLU activation function', (9, 12))]","[['single - hidden layer MLP', 'with', 'ReLU activation function']]",[],[],[],[],"[['Hyperparameters', 'is', 'single - hidden layer MLP']]",[],[],[],natural_language_inference,10,175
experiments,"We trained our SST - 2 model with hyperparameters D x = 300 , D h = 300 , D c = 300 .","[('trained', (1, 2)), ('with hyperparameters', (7, 9))]","[('SST - 2 model', (3, 7)), ('D x = 300 , D h = 300 , D c = 300', (9, 23))]","[['SST - 2 model', 'with hyperparameters', 'D x = 300 , D h = 300 , D c = 300']]",[],[],[],[],"[['Hyperparameters', 'trained', 'SST - 2 model']]",[],[],[],natural_language_inference,10,177
experiments,The word vectors are initialized with GloVe 300D pretrained vectors and fine - tuned during training .,"[('with', (5, 6)), ('during', (14, 15))]","[('word vectors', (1, 3)), ('initialized', (4, 5)), ('GloVe 300D pretrained vectors', (6, 10)), ('fine - tuned', (11, 14)), ('training', (15, 16))]","[['fine - tuned', 'during', 'training'], ['initialized', 'with', 'GloVe 300D pretrained vectors']]","[['word vectors', 'has', 'fine - tuned'], ['word vectors', 'has', 'initialized']]",[],[],[],[],[],"[['SST - 2 model', 'has', 'word vectors']]",[],natural_language_inference,10,178
experiments,We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the MLP layer .,[],[],"[['dropout ( p = 0.5 )', 'on', 'output'], ['output', 'of', 'word embedding layer'], ['dropout ( p = 0.5 )', 'on', 'input and the output'], ['input and the output', 'of', 'MLP layer']]",[],[],[],[],"[['SST - 2 model', 'apply', 'dropout ( p = 0.5 )']]",[],[],[],natural_language_inference,10,179
experiments,The size of mini-batches is set to 32 and Adadelta optimizer is used for optimization .,[],[],"[['Adadelta optimizer', 'used for', 'optimization'], ['mini-batches', 'set to', '32']]",[],[],[],[],[],[],"[['SST - 2 model', 'has', 'Adadelta optimizer'], ['SST - 2 model', 'has', 'size']]",[],natural_language_inference,10,180
experiments,"For our SST - 5 model , hyperparameters are set to D x = 300 , D h = 300 , D c = 1024 . Similar to the SST - 2 model , we optimize the model using Adadelta optimizer with batch size 64 and apply dropout with p = 0.5 .",[],[],"[['SST - 5 model', 'optimize', 'model'], ['model', 'using', 'Adadelta optimizer'], ['Adadelta optimizer', 'with', 'batch size 64'], ['Adadelta optimizer', 'apply', 'dropout'], ['dropout', 'with', 'p = 0.5'], ['hyperparameters', 'set to', 'D x = 300 , D h = 300 , D c = 1024']]","[['SST - 5 model', 'has', 'hyperparameters']]",[],[],[],"[['Hyperparameters', 'For', 'SST - 5 model']]",[],[],[],natural_language_inference,10,181
experiments,"Our SST - 2 model outperforms all other models substantially except byte - m LSTM , where a byte - level language model trained on the large product review dataset is used to obtain sentence representations .","[('outperforms', (5, 6)), ('except', (10, 11))]","[('SST - 2 model', (1, 5)), ('all other models', (6, 9)), ('substantially', (9, 10)), ('byte - m LSTM', (11, 15))]","[['SST - 2 model', 'outperforms', 'all other models'], ['all other models', 'except', 'byte - m LSTM']]","[['all other models', 'has', 'substantially']]",[],[],[],[],[],"[['Results', 'has', 'SST - 2 model']]",[],natural_language_inference,10,183
experiments,"We also see that the performance of our SST - 5 model is on par with that of the current state - of - the - art model , which is pretrained on large parallel datasets and uses character n-gram embeddings alongside word embeddings , even though our model does not utilize external resources other than GloVe vectors and only uses wordlevel representations .","[('see that', (2, 4)), ('of', (6, 7)), ('on par with', (13, 16))]","[('performance', (5, 6)), ('our SST - 5 model', (7, 12)), ('current state - of - the - art model', (19, 28))]","[['performance', 'of', 'our SST - 5 model'], ['our SST - 5 model', 'on par with', 'current state - of - the - art model']]",[],[],[],[],"[['Results', 'see that', 'performance']]",[],[],[],natural_language_inference,10,184
experiments,The authors of stated that utilizing pretraining and character n-gram embeddings improves validation accuracy by 2.8 % ( SST - 2 ) or 1.7 % ( SST - 5 ) .,"[('utilizing', (5, 6)), ('improves', (11, 12)), ('by', (14, 15))]","[('pretraining and character n-gram embeddings', (6, 11)), ('validation accuracy', (12, 14)), ('2.8 % ( SST - 2 )', (15, 22)), ('1.7 % ( SST - 5 )', (23, 30))]","[['pretraining and character n-gram embeddings', 'improves', 'validation accuracy'], ['validation accuracy', 'by', '2.8 % ( SST - 2 )'], ['validation accuracy', 'by', '1.7 % ( SST - 5 )']]",[],[],[],[],"[['Results', 'utilizing', 'pretraining and character n-gram embeddings']]",[],[],[],natural_language_inference,10,185
research-problem,"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers .",[],"[('question answering', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering']]",[],[],[],[],natural_language_inference,100,4
research-problem,"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search .",[],"[('Question answering ( QA )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question answering ( QA )']]",[],[],[],[],natural_language_inference,100,12
research-problem,"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features .",[],"[('QA', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,100,13
model,"To handle these issues in the existing deep learning architectures for ranking answers , we propose an attention based neural matching model ( a NMM ) .","[('propose', (15, 16))]","[('attention based neural matching model ( a NMM )', (17, 26))]",[],[],"[['Model', 'propose', 'attention based neural matching model ( a NMM )']]",[],[],[],[],[],[],natural_language_inference,100,48
model,Deep neural network with value - shared weights :,[],"[('Deep neural network with value - shared weights', (0, 8))]",[],[],[],"[['Model', 'has', 'Deep neural network with value - shared weights']]",[],[],[],[],[],natural_language_inference,100,51
model,"We introduce a novel value - shared weighting scheme in deep neural networks as a counterpart of the position - shared weighting scheme in CNNs , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regularities rather than spatial regularities .","[('introduce', (1, 2)), ('in', (9, 10))]","[('novel value - shared weighting scheme', (3, 9)), ('deep neural networks', (10, 13))]","[['novel value - shared weighting scheme', 'in', 'deep neural networks']]",[],[],[],[],"[['Deep neural network with value - shared weights', 'introduce', 'novel value - shared weighting scheme']]",[],[],[],natural_language_inference,100,52
model,Incorporate attention scheme over question terms :,[],"[('Incorporate attention scheme over question terms', (0, 6))]",[],[],[],"[['Model', 'has', 'Incorporate attention scheme over question terms']]",[],[],[],[],[],natural_language_inference,100,53
model,"We incorporate the attention scheme over the question terms using a gating function , so that we can explicitly discriminate the question term importance .","[('incorporate', (1, 2)), ('over', (5, 6)), ('using', (9, 10))]","[('attention scheme', (3, 5)), ('question terms', (7, 9)), ('gating function', (11, 13))]","[['attention scheme', 'over', 'question terms'], ['question terms', 'using', 'gating function']]",[],[],[],[],"[['Incorporate attention scheme over question terms', 'incorporate', 'attention scheme']]",[],[],[],natural_language_inference,100,54
hyperparameters,"For the setting of hyper - parameters , we set the number of bins as 600 , word embedding dimension as 700 for a NNM - 1 , the number of bins as 200 , word embedding dimension as 700 for a NNM - 2 after we tune hyper - parameters on the provided DEV set of TREC QA data .",[],[],"[['word embedding dimension', 'as', '700'], ['number of bins', 'as', '200'], ['word embedding dimension', 'as', '700'], ['number of bins', 'as', '600']]","[['NNM - 2', 'has', 'word embedding dimension'], ['NNM - 2', 'has', 'number of bins'], ['NNM - 1', 'has', 'word embedding dimension'], ['NNM - 1', 'has', 'number of bins']]","[['Hyperparameters', 'set', 'NNM - 2'], ['Hyperparameters', 'set', 'NNM - 1']]",[],[],[],[],[],[],natural_language_inference,100,248
results,We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,"[('see', (2, 3)), ('trained with', (5, 7)), ('beats', (11, 12)), ('including', (22, 23)), ('using', (25, 26))]","[('NMM', (4, 5)), ('TRAIN - ALL set', (7, 11)), ('all the previous state - of - the art systems', (12, 22)), ('both methods', (23, 25)), ('feature engineering and deep learning models', (26, 32))]","[['NMM', 'beats', 'all the previous state - of - the art systems'], ['all the previous state - of - the art systems', 'including', 'both methods'], ['both methods', 'using', 'feature engineering and deep learning models'], ['NMM', 'trained with', 'TRAIN - ALL set']]",[],"[['Results', 'see', 'NMM']]",[],[],[],[],[],[],natural_language_inference,100,341
results,"Furthermore , even without combining additional features , a NMM still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods .",[],[],"[['NMM', 'performs', 'well'], ['well', 'for', 'answer ranking'], ['well', 'showing', 'significant improvements'], ['significant improvements', 'over', 'previous deep learning model'], ['previous deep learning model', 'with no', 'additional features'], ['previous deep learning model', 'with no', 'linguistic feature engineering methods']]","[['additional features', 'has', 'NMM']]","[['Results', 'without combining', 'additional features']]",[],[],[],[],[],[],natural_language_inference,100,343
research-problem,Dynamic Integration of Background Knowledge in Neural NLU Systems,[],"[('Neural NLU', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural NLU']]",[],[],[],[],natural_language_inference,11,2
research-problem,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",[],"[('neural natural language understanding ( NLU )', (15, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural natural language understanding ( NLU )']]",[],[],[],[],natural_language_inference,11,4
model,"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .","[('develop', (5, 6)), ('for dynamically incorporating', (9, 12)), ('in', (15, 16))]","[('new architecture', (7, 9)), ('external background knowledge', (12, 15)), ('NLU models', (16, 18))]","[['new architecture', 'for dynamically incorporating', 'external background knowledge'], ['external background knowledge', 'in', 'NLU models']]",[],"[['Model', 'develop', 'new architecture']]",[],[],[],[],[],[],natural_language_inference,11,21
model,"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .","[('retrieved from', (17, 19)), ('assist with understanding', (32, 35))]","[('supplementary knowledge', (14, 16)), ('external knowledge sources', (19, 22)), ('ConceptNet', (27, 28)), ('Wikipedia', (29, 30)), ('text inputs', (35, 37))]","[['supplementary knowledge', 'retrieved from', 'external knowledge sources'], ['external knowledge sources', 'assist with understanding', 'text inputs']]","[['external knowledge sources', 'name', 'ConceptNet'], ['external knowledge sources', 'name', 'Wikipedia']]",[],"[['Model', 'has', 'supplementary knowledge']]",[],[],[],[],[],natural_language_inference,11,22
model,The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,11,24
model,These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,"[('are', (3, 4)), ('used as', (5, 7)), ('to', (8, 9))]","[('input', (7, 8)), ('task - specific NLU architecture', (10, 15))]","[['input', 'to', 'task - specific NLU architecture']]",[],[],[],[],"[['contextually refined word embeddings', 'used as', 'input']]",[],[],"[['outputs', 'are', 'contextually refined word embeddings']]",natural_language_inference,11,25
model,"The initial reading module and the task module are learnt jointly , end - to - end .","[('are learnt', (8, 10))]","[('initial reading module and the task module', (1, 8)), ('jointly , end - to - end', (10, 17))]","[['initial reading module and the task module', 'are learnt', 'jointly , end - to - end']]",[],[],"[['Model', 'has', 'initial reading module and the task module']]",[],[],[],[],[],natural_language_inference,11,26
hyperparameters,All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,"[('trained', (3, 4)), ('with', (10, 11)), ('of', (17, 18)), ('for', (21, 22)), ('had to reduce', (30, 33)), ('due to', (36, 38))]","[('All models', (0, 2)), ('end - to - end', (4, 9)), ('jointly', (9, 10)), ('refinement module', (12, 14)), ('using a dimensionality', (14, 17)), ('n = 300', (18, 21)), ('all but the TriviaQA experiments', (22, 27)), ('n to 150', (33, 36)), ('memory constraints', (38, 40))]","[['All models', 'trained', 'end - to - end'], ['All models', 'trained', 'jointly'], ['jointly', 'with', 'refinement module'], ['All models', 'trained', 'using a dimensionality'], ['using a dimensionality', 'of', 'n = 300'], ['n = 300', 'for', 'all but the TriviaQA experiments'], ['all but the TriviaQA experiments', 'had to reduce', 'n to 150'], ['n to 150', 'due to', 'memory constraints']]",[],[],"[['Hyperparameters', 'has', 'All models']]",[],[],[],[],[],natural_language_inference,11,101
hyperparameters,All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,"[('operate on', (2, 4))]","[('All baselines', (0, 2)), ('unrefined word embeddings', (5, 8))]","[['All baselines', 'operate on', 'unrefined word embeddings']]",[],[],"[['Hyperparameters', 'has', 'All baselines']]",[],[],[],[],[],natural_language_inference,11,102
hyperparameters,For the DQA baseline system we add the lemma in - question feature ( liq ) suggested in .,"[('For', (0, 1)), ('add', (6, 7)), ('in', (9, 10))]","[('DQA baseline system', (2, 5)), ('lemma', (8, 9)), ('question feature ( liq )', (11, 16))]","[['DQA baseline system', 'add', 'lemma'], ['lemma', 'in', 'question feature ( liq )']]",[],"[['Hyperparameters', 'For', 'DQA baseline system']]",[],[],[],[],[],[],natural_language_inference,11,103
results,"Wikipedia ( W ) yields further , significant improvements on TriviaQA , slightly outperforming the current state of the art model .","[('yields', (4, 5)), ('on', (9, 10))]","[('Wikipedia ( W )', (0, 4)), ('further , significant improvements', (5, 9)), ('TriviaQA', (10, 11)), ('slightly outperforming', (12, 14)), ('current state of the art model', (15, 21))]","[['Wikipedia ( W )', 'yields', 'further , significant improvements'], ['further , significant improvements', 'on', 'TriviaQA']]","[['further , significant improvements', 'has', 'slightly outperforming'], ['slightly outperforming', 'has', 'current state of the art model']]",[],"[['Results', 'has', 'Wikipedia ( W )']]",[],[],[],[],[],natural_language_inference,11,138
results,shows the results of our RTE experiments .,[],"[('RTE experiments', (5, 7))]",[],[],[],"[['Results', 'has', 'RTE experiments']]",[],[],[],[],[],natural_language_inference,11,147
results,"In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .","[('introduction of', (4, 6)), ('with and without', (14, 17))]","[('our refinement strategy', (6, 9)), ('almost always helps', (9, 12)), ('external knowledge', (17, 19))]","[['almost always helps', 'with and without', 'external knowledge']]","[['our refinement strategy', 'has', 'almost always helps']]",[],[],[],"[['RTE experiments', 'introduction of', 'our refinement strategy']]",[],[],[],natural_language_inference,11,148
results,"When providing additional background knowledge from ConceptNet , our BiLSTM based models improve substantially , while the ESIM - based models improve only on the more difficult MultiNLI dataset .","[('When providing', (0, 2)), ('from', (5, 6)), ('only on', (22, 24))]","[('additional background knowledge', (2, 5)), ('ConceptNet', (6, 7)), ('our BiLSTM based models', (8, 12)), ('improve substantially', (12, 14)), ('ESIM - based models', (17, 21)), ('improve', (21, 22)), ('more difficult MultiNLI dataset', (25, 29))]","[['additional background knowledge', 'from', 'ConceptNet'], ['improve', 'only on', 'more difficult MultiNLI dataset']]","[['additional background knowledge', 'has', 'ESIM - based models'], ['ESIM - based models', 'has', 'improve'], ['additional background knowledge', 'has', 'our BiLSTM based models'], ['our BiLSTM based models', 'has', 'improve substantially']]",[],[],[],"[['RTE experiments', 'When providing', 'additional background knowledge']]",[],[],[],natural_language_inference,11,149
results,"Compared to previously published state of the art systems , our models acquit themselves quite well on the MultiNLI benchmark , and competitively on the SNLI benchmark .",[],[],"[['our models', 'acquit', 'competitively'], ['competitively', 'on', 'SNLI benchmark'], ['our models', 'acquit', 'quite well'], ['quite well', 'on', 'MultiNLI benchmark']]",[],[],[],[],[],[],"[['RTE experiments', 'has', 'our models']]",[],natural_language_inference,11,150
results,"We do find that there is little impact of using external knowledge on the RTE task with ESIM , although the refinement strategy helps using just p + q.","[('find that', (2, 4)), ('of using', (8, 10)), ('on', (12, 13)), ('with', (16, 17))]","[('little impact', (6, 8)), ('external knowledge', (10, 12)), ('RTE task', (14, 16)), ('ESIM', (17, 18))]","[['little impact', 'of using', 'external knowledge'], ['external knowledge', 'on', 'RTE task'], ['RTE task', 'with', 'ESIM']]",[],[],[],[],"[['RTE experiments', 'find that', 'little impact']]",[],[],[],natural_language_inference,11,154
results,"Nevertheless , both ESIM and our BiL - STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5.3 .","[('trained with', (11, 13)), ('from', (14, 15)), ('sensitive to', (17, 19)), ('of', (21, 22))]","[('both ESIM and our BiL - STM models', (2, 10)), ('knowledge', (13, 14)), ('ConceptNet', (15, 16)), ('semantics', (20, 21)), ('provided assertions', (23, 25))]","[['both ESIM and our BiL - STM models', 'trained with', 'knowledge'], ['knowledge', 'sensitive to', 'semantics'], ['semantics', 'of', 'provided assertions'], ['knowledge', 'from', 'ConceptNet']]",[],[],[],[],[],[],"[['RTE experiments', 'has', 'both ESIM and our BiL - STM models']]",[],natural_language_inference,11,157
results,"Furthermore , increasing the coverage of assertions in ConceptNet would most likely yield improved performance even without retraining our models .","[('increasing', (2, 3)), ('of', (5, 6)), ('in', (7, 8)), ('most likely yield', (10, 13)), ('without retraining', (16, 18))]","[('coverage', (4, 5)), ('assertions', (6, 7)), ('ConceptNet', (8, 9)), ('improved performance', (13, 15)), ('our models', (18, 20))]","[['coverage', 'of', 'assertions'], ['assertions', 'in', 'ConceptNet'], ['assertions', 'most likely yield', 'improved performance'], ['improved performance', 'without retraining', 'our models']]",[],[],[],[],"[['RTE experiments', 'increasing', 'coverage']]",[],[],[],natural_language_inference,11,159
research-problem,Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,[],"[('Multi- Domain Inference', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi- Domain Inference']]",[],[],[],[],natural_language_inference,12,2
research-problem,We present a simple sequential sentence encoder for multi-domain natural language inference .,[],"[('multi-domain natural language inference', (8, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'multi-domain natural language inference']]",[],[],[],[],natural_language_inference,12,4
research-problem,Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,[],"[('Natural language inference ( NLI )', (0, 6)), ('recognizing textual entailment ( RTE )', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language inference ( NLI )'], ['Contribution', 'has research problem', 'recognizing textual entailment ( RTE )']]",[],[],[],[],natural_language_inference,12,10
model,"In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .","[('follow', (5, 6)), ('of', (9, 10)), ('propose', (16, 17)), ('for', (24, 25))]","[('former approach', (7, 9)), ('encoding - based models', (10, 14)), ('novel yet simple sequential sentence encoder', (18, 24)), ('Multi - NLI problem', (26, 30))]","[['former approach', 'of', 'encoding - based models'], ['novel yet simple sequential sentence encoder', 'for', 'Multi - NLI problem']]",[],"[['Model', 'follow', 'former approach'], ['Model', 'propose', 'novel yet simple sequential sentence encoder']]",[],[],[],[],[],[],natural_language_inference,12,15
model,It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .,"[('with', (12, 13))]","[('stacked ( multi-layered ) bidirectional LSTM - RNN', (4, 12)), ('shortcut connections', (13, 15)), ('word embedding fine - tuning', (30, 35))]","[['stacked ( multi-layered ) bidirectional LSTM - RNN', 'with', 'shortcut connections'], ['stacked ( multi-layered ) bidirectional LSTM - RNN', 'with', 'word embedding fine - tuning']]",[],[],"[['Model', 'has', 'stacked ( multi-layered ) bidirectional LSTM - RNN']]",[],[],[],[],[],natural_language_inference,12,18
model,"The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .","[('uses', (5, 6)), ('to encode', (9, 11)), ('into', (14, 15)), ('use', (21, 22)), ('over', (24, 25)), ('to label', (28, 30)), ('between', (32, 33)), ('as', (36, 37))]","[('over all supervised model', (1, 5)), ('shortcutstacked encoders', (7, 9)), ('two input sentences', (11, 14)), ('two vectors', (15, 17)), ('classifier', (23, 24)), ('vector combination', (26, 28)), ('relationship', (31, 32)), ('two sentences', (34, 36)), ('entailment', (39, 40)), ('contradiction', (41, 42)), ('neural', (44, 45))]","[['over all supervised model', 'use', 'classifier'], ['classifier', 'over', 'vector combination'], ['vector combination', 'to label', 'relationship'], ['relationship', 'as', 'entailment'], ['relationship', 'as', 'contradiction'], ['relationship', 'as', 'neural'], ['relationship', 'between', 'two sentences'], ['over all supervised model', 'uses', 'shortcutstacked encoders'], ['shortcutstacked encoders', 'to encode', 'two input sentences'], ['two input sentences', 'into', 'two vectors']]",[],[],"[['Model', 'has', 'over all supervised model']]",[],[],[],[],[],natural_language_inference,12,19
code,Github Code Link : https://github.com/ easonnie/multiNLI_encoder,[],"[('https://github.com/ easonnie/multiNLI_encoder', (4, 6))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/ easonnie/multiNLI_encoder']]",[],[],[],[],natural_language_inference,12,22
hyperparameters,We use cross - entropy loss as the training objective with Adam - based opti-Model Accuracy SNLI Multi - NLI Matched Multi - NLI Mismatched CBOW 80.6 65.2 64.6 biLSTM Encoder 81.5 67.5 67.1 300D Tree - CNN Encoder 82.1 --300D SPINN - PI Encoder 83.2 --300D NSE Encoder 84.6 --biLSTM -Max Encoder 84 . mization with 32 batch size .,[],[],"[['cross - entropy loss', 'as', 'training objective'], ['training objective', 'with', 'Adam'], ['Adam', 'with', '32 batch size']]",[],"[['Hyperparameters', 'use', 'cross - entropy loss']]",[],[],[],[],[],[],natural_language_inference,12,52
hyperparameters,The starting learning rate is 0.0002 with half decay every two epochs .,"[('is', (4, 5)), ('with', (6, 7)), ('every', (9, 10))]","[('starting learning rate', (1, 4)), ('0.0002', (5, 6)), ('half decay', (7, 9)), ('two epochs', (10, 12))]","[['starting learning rate', 'is', '0.0002'], ['0.0002', 'with', 'half decay'], ['half decay', 'every', 'two epochs']]",[],[],"[['Hyperparameters', 'has', 'starting learning rate']]",[],[],[],[],[],natural_language_inference,12,53
hyperparameters,The number of hidden units for MLP in classifier is 1600 .,"[('for', (5, 6)), ('in', (7, 8)), ('is', (9, 10))]","[('number of hidden units', (1, 5)), ('MLP', (6, 7)), ('classifier', (8, 9)), ('1600', (10, 11))]","[['number of hidden units', 'in', 'classifier'], ['number of hidden units', 'for', 'MLP'], ['number of hidden units', 'is', '1600']]",[],[],"[['Hyperparameters', 'has', 'number of hidden units']]",[],[],[],[],[],natural_language_inference,12,54
hyperparameters,"Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .","[('applied on', (4, 6)), ('of', (8, 9)), ('with', (14, 15)), ('set to', (17, 19))]","[('Dropout layer', (0, 2)), ('output', (7, 8)), ('each layer of MLP', (9, 13)), ('dropout rate', (15, 17)), ('0.1', (19, 20))]","[['Dropout layer', 'applied on', 'output'], ['output', 'with', 'dropout rate'], ['dropout rate', 'set to', '0.1'], ['output', 'of', 'each layer of MLP']]",[],[],"[['Hyperparameters', 'has', 'Dropout layer']]",[],[],[],[],[],natural_language_inference,12,55
hyperparameters,We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .,"[('used', (1, 2)), ('to initialize', (7, 9))]","[('pre-trained 300D Glove 840B vectors', (2, 7)), ('word embeddings', (10, 12))]","[['pre-trained 300D Glove 840B vectors', 'to initialize', 'word embeddings']]",[],"[['Hyperparameters', 'used', 'pre-trained 300D Glove 840B vectors']]",[],[],[],[],[],[],natural_language_inference,12,56
ablation-analysis,"These ablation results are shown in and 4 , all based on the Multi - NLI development sets .","[('based on', (10, 12))]","[('Multi - NLI development sets', (13, 18))]",[],[],"[['Ablation analysis', 'based on', 'Multi - NLI development sets']]",[],[],[],[],[],[],natural_language_inference,12,61
ablation-analysis,"As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .","[('improves', (7, 8)), ('achieve', (12, 13)), ('in', (16, 17)), ('on', (23, 24)), ('compared to', (30, 32))]","[('each added layer model', (3, 7)), ('accuracy', (9, 10)), ('substantial improvement', (14, 16)), ('accuracy ( around 2 % )', (17, 23)), ('matched and mismatched settings', (25, 29)), ('single - layer biLSTM', (33, 37))]","[['each added layer model', 'achieve', 'substantial improvement'], ['substantial improvement', 'in', 'accuracy ( around 2 % )'], ['substantial improvement', 'compared to', 'single - layer biLSTM'], ['substantial improvement', 'on', 'matched and mismatched settings'], ['each added layer model', 'improves', 'accuracy']]",[],[],"[['Ablation analysis', 'has', 'each added layer model']]",[],[],[],[],[],natural_language_inference,12,65
ablation-analysis,"Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .","[('show', (5, 6)), ('among', (10, 11)), ('is', (14, 15)), ('to', (19, 20)), ('around', (23, 24)), ('on top of', (26, 29))]","[('shortcut connections', (8, 10)), ('biLSTM layers', (12, 14)), ('important contributor', (17, 19)), ('accuracy improvement', (20, 22)), ('1.5 %', (24, 26)), ('full 3 - layered stacked - RNN model', (30, 38))]","[['shortcut connections', 'among', 'biLSTM layers'], ['shortcut connections', 'is', 'important contributor'], ['important contributor', 'to', 'accuracy improvement'], ['accuracy improvement', 'around', '1.5 %'], ['1.5 %', 'on top of', 'full 3 - layered stacked - RNN model']]",[],"[['Ablation analysis', 'show', 'shortcut connections']]",[],[],[],[],[],[],natural_language_inference,12,67
ablation-analysis,"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .","[('show that', (5, 7)), ('improves', (14, 15)), ('for both', (18, 20))]","[('fine - tuning', (7, 10)), ('word embeddings', (11, 13)), ('results', (15, 16)), ('in - domain task and cross - domain tasks', (21, 30))]","[['fine - tuning', 'improves', 'results'], ['results', 'for both', 'in - domain task and cross - domain tasks']]","[['fine - tuning', 'has', 'word embeddings']]","[['Ablation analysis', 'show that', 'fine - tuning']]",[],[],[],[],[],[],natural_language_inference,12,69
ablation-analysis,The last ablation in shows that a classifier with two layers of relu is preferable than other options .,"[('shows that', (4, 6)), ('with', (8, 9)), ('is', (13, 14))]","[('last ablation', (1, 3)), ('classifier', (7, 8)), ('two layers of relu', (9, 13)), ('preferable', (14, 15))]","[['last ablation', 'shows that', 'classifier'], ['classifier', 'with', 'two layers of relu'], ['two layers of relu', 'is', 'preferable']]",[],[],"[['Ablation analysis', 'has', 'last ablation']]",[],[],[],[],[],natural_language_inference,12,71
results,"First for Multi - NLI , we improve substantially over the CBOW and biL - STM Encoder baselines reported in the dataset paper .","[('for', (1, 2)), ('improve', (7, 8)), ('over', (9, 10))]","[('Multi - NLI', (2, 5)), ('substantially', (8, 9)), ('CBOW and biL - STM Encoder baselines', (11, 18))]","[['Multi - NLI', 'improve', 'substantially'], ['substantially', 'over', 'CBOW and biL - STM Encoder baselines']]",[],"[['Results', 'for', 'Multi - NLI']]",[],[],[],[],[],[],natural_language_inference,12,75
results,We also show that our final shortcut - based stacked encoder achieves around 3 % improvement as compared to the 1 layer biLSTM - Max Encoder in the second last row ( using the exact same classifier and optimizer settings ) .,"[('show that', (2, 4)), ('achieves', (11, 12)), ('compared to', (17, 19))]","[('our final shortcut - based stacked encoder', (4, 11)), ('around 3 % improvement', (12, 16)), ('1 layer biLSTM - Max Encoder', (20, 26))]","[['our final shortcut - based stacked encoder', 'achieves', 'around 3 % improvement'], ['around 3 % improvement', 'compared to', '1 layer biLSTM - Max Encoder']]",[],[],[],[],"[['Multi - NLI', 'show that', 'our final shortcut - based stacked encoder']]",[],[],[],natural_language_inference,12,76
results,Our shortcut - encoder was also the top singe - model ( non-ensemble ) result on the EMNLP RepEval Shared Task leaderboard .,"[('was', (4, 5)), ('on', (15, 16))]","[('shortcut - encoder', (1, 4)), ('top singe - model ( non-ensemble ) result', (7, 15)), ('EMNLP RepEval Shared Task leaderboard', (17, 22))]","[['shortcut - encoder', 'was', 'top singe - model ( non-ensemble ) result'], ['top singe - model ( non-ensemble ) result', 'on', 'EMNLP RepEval Shared Task leaderboard']]",[],[],[],[],[],[],"[['Multi - NLI', 'has', 'shortcut - encoder']]",[],natural_language_inference,12,77
results,"Next , for SNLI , we compare our shortcutstacked encoder with the current state - of - the - art encoders from the SNLI leaderboard ( https :// nlp.stanford.edu/projects/snli/ ) .","[('compare', (6, 7)), ('with', (10, 11)), ('from', (21, 22))]","[('SNLI', (3, 4)), ('shortcutstacked encoder', (8, 10)), ('current state - of - the - art encoders', (12, 21)), ('SNLI leaderboard', (23, 25))]","[['SNLI', 'compare', 'shortcutstacked encoder'], ['shortcutstacked encoder', 'with', 'current state - of - the - art encoders'], ['current state - of - the - art encoders', 'from', 'SNLI leaderboard']]",[],[],"[['Results', 'for', 'SNLI']]",[],[],[],[],[],natural_language_inference,12,78
results,"We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point .","[('compare to', (2, 4))]","[('recent biLSTM - Max Encoder', (5, 10))]",[],[],[],[],[],"[['SNLI', 'compare to', 'recent biLSTM - Max Encoder']]",[],[],"[['recent biLSTM - Max Encoder', 'has', 'results']]",natural_language_inference,12,79
results,"The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders .","[('indicate', (2, 3)), ('sur-passes', (11, 12)), ('achieves', (25, 26)), ('on', (33, 34))]","[('results', (1, 2)), ('Our Shortcut - Stacked Encoder', (5, 10)), ('all the previous state - of - the - art encoders', (12, 23)), ('new best encoding - based result', (27, 33)), ('SNLI', (34, 35))]","[['results', 'indicate', 'Our Shortcut - Stacked Encoder'], ['Our Shortcut - Stacked Encoder', 'sur-passes', 'all the previous state - of - the - art encoders'], ['Our Shortcut - Stacked Encoder', 'achieves', 'new best encoding - based result'], ['new best encoding - based result', 'on', 'SNLI']]",[],[],[],[],[],[],[],[],natural_language_inference,12,80
research-problem,Multi-range Reasoning for Machine Comprehension,[],"[('Machine Comprehension', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,13,2
research-problem,"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) .",[],"[('machine comprehension ( MC )', (17, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension ( MC )']]",[],[],[],[],natural_language_inference,13,4
research-problem,"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks .",[],"[('MC', (14, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'MC']]",[],[],[],[],natural_language_inference,13,19
model,"To this end , we propose a new compositional encoder that can either be used in - place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","[('propose', (5, 6)), ('used in - place of', (14, 19)), ('serve as', (23, 25)), ('complementary to', (30, 32))]","[('new compositional encoder', (7, 10)), ('standard RNN encoders', (19, 22)), ('new module', (26, 28)), ('existing neural architectures', (32, 35))]","[['new compositional encoder', 'serve as', 'new module'], ['new module', 'complementary to', 'existing neural architectures'], ['new compositional encoder', 'used in - place of', 'standard RNN encoders']]",[],"[['Model', 'propose', 'new compositional encoder']]",[],[],[],[],[],[],natural_language_inference,13,24
model,Our proposed MRU encoders learns gating vectors via multiple contract - and - expand layers at multiple dilated resolutions .,"[('learns', (4, 5)), ('via', (7, 8)), ('at', (15, 16))]","[('Our proposed MRU encoders', (0, 4)), ('gating vectors', (5, 7)), ('multiple contract - and - expand layers', (8, 15)), ('multiple dilated resolutions', (16, 19))]","[['Our proposed MRU encoders', 'learns', 'gating vectors'], ['gating vectors', 'via', 'multiple contract - and - expand layers'], ['multiple contract - and - expand layers', 'at', 'multiple dilated resolutions']]",[],[],"[['Model', 'has', 'Our proposed MRU encoders']]",[],[],[],[],[],natural_language_inference,13,25
model,"Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .","[('compress', (3, 4)), ('into', (26, 27))]","[('input document', (5, 7)), ('arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 )', (8, 26)), ('neural bag - of - words ( summed ) representation', (28, 38))]","[['input document', 'into', 'neural bag - of - words ( summed ) representation']]","[['input document', 'has', 'arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 )']]","[['Model', 'compress', 'input document']]",[],[],[],[],[],[],natural_language_inference,13,26
model,The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .,"[('passed through', (5, 7)), ('re-expanded to', (12, 14))]","[('affine transformation layers', (7, 10)), ('original sequence length', (15, 18))]",[],[],[],[],[],"[['neural bag - of - words ( summed ) representation', 're-expanded to', 'original sequence length'], ['neural bag - of - words ( summed ) representation', 'passed through', 'affine transformation layers']]",[],[],[],natural_language_inference,13,27
model,The k document representations ( at multiple ranges and n-gram blocks ) are then combined and modeled with fully connected layers to form the final compositional gate which are applied onto the original input document .,"[('at', (5, 6)), ('combined and modeled with', (14, 18)), ('to form', (21, 23)), ('applied onto', (29, 31))]","[('k document representations', (1, 4)), ('multiple ranges and n-gram blocks', (6, 11)), ('fully connected layers', (18, 21)), ('final compositional gate', (24, 27)), ('original input document', (32, 35))]","[['k document representations', 'combined and modeled with', 'fully connected layers'], ['fully connected layers', 'to form', 'final compositional gate'], ['final compositional gate', 'applied onto', 'original input document'], ['k document representations', 'at', 'multiple ranges and n-gram blocks']]",[],[],"[['Model', 'has', 'k document representations']]",[],[],[],[],[],natural_language_inference,13,28
baselines,"RACE - the key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","[('are', (5, 6))]","[('RACE', (0, 1)), ('key competitors', (3, 5)), ('Stanford Attention Reader ( Stanford AR )', (7, 14)), ('Gated Attention Reader ( GA )', (15, 21)), ('Dynamic Fusion Networks ( DFN )', (23, 29))]","[['key competitors', 'are', 'Stanford Attention Reader ( Stanford AR )'], ['key competitors', 'are', 'Gated Attention Reader ( GA )'], ['key competitors', 'are', 'Dynamic Fusion Networks ( DFN )']]","[['RACE', 'has', 'key competitors']]",[],"[['Baselines', 'has', 'RACE']]",[],[],[],[],[],natural_language_inference,13,153
baselines,Search QA - the main competitor baseline is the AMANDA model proposed by .,"[('is', (7, 8))]","[('Search QA', (0, 2)), ('main competitor baseline', (4, 7)), ('AMANDA model', (9, 11))]","[['main competitor baseline', 'is', 'AMANDA model']]","[['Search QA', 'has', 'main competitor baseline']]",[],"[['Baselines', 'has', 'Search QA']]",[],[],[],[],[],natural_language_inference,13,157
baselines,NarrativeQA,[],"[('NarrativeQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'NarrativeQA']]",[],[],[],[],"[['NarrativeQA', 'has', 'baselines']]",natural_language_inference,13,161
baselines,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","[('are', (11, 12))]","[('baselines', (10, 11)), ('context - less sequence to sequence ( seq2seq ) model', (13, 23)), ('ASR', (24, 25)), ('BiDAF', (26, 27))]","[['baselines', 'are', 'context - less sequence to sequence ( seq2seq ) model'], ['baselines', 'are', 'ASR'], ['baselines', 'are', 'BiDAF']]",[],[],[],[],[],[],[],[],natural_language_inference,13,163
experimental-setup,We implement all models in TensorFlow .,"[('implement', (1, 2)), ('in', (4, 5))]","[('all models', (2, 4)), ('TensorFlow', (5, 6))]","[['all models', 'in', 'TensorFlow']]",[],[],[],[],"[['Experimental Setup', 'implement', 'all models']]",[],[],[],natural_language_inference,13,176
experimental-setup,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"[('initialized with', (3, 5)), ('not fine - tuned during', (11, 16))]","[('Word embeddings', (0, 2)), ('300d Glo Ve vectors', (5, 9)), ('training', (16, 17))]","[['Word embeddings', 'initialized with', '300d Glo Ve vectors'], ['Word embeddings', 'not fine - tuned during', 'training']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Word embeddings']]",[],natural_language_inference,13,177
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","[('tuned amongst', (3, 5)), ('on', (12, 13)), ('including', (15, 16))]","[('Dropout rate', (0, 2)), ('{ 0.1 , 0.2 , 0.3 }', (5, 12)), ('all layers', (13, 15)), ('embedding layer', (17, 19))]","[['Dropout rate', 'tuned amongst', '{ 0.1 , 0.2 , 0.3 }'], ['{ 0.1 , 0.2 , 0.3 }', 'on', 'all layers'], ['all layers', 'including', 'embedding layer']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Dropout rate']]",[],natural_language_inference,13,178
experimental-setup,We adopt the Adam optimizer with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / NarrativeQA respectively .,"[('adopt', (1, 2)), ('with', (5, 6)), ('of', (9, 10)), ('for', (12, 13))]","[('Adam optimizer', (3, 5)), ('learning rate', (7, 9)), ('0.0003/ 0.001/0.001', (10, 12)), ('RACE / SearchQA / NarrativeQA', (13, 18))]","[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.0003/ 0.001/0.001'], ['0.0003/ 0.001/0.001', 'for', 'RACE / SearchQA / NarrativeQA']]",[],[],[],[],"[['Experimental Setup', 'adopt', 'Adam optimizer']]",[],[],[],natural_language_inference,13,185
experimental-setup,The batch size is set to 64/256/32 accordingly .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('64/256/32', (6, 7))]","[['batch size', 'set to', '64/256/32']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'batch size']]",[],natural_language_inference,13,186
experimental-setup,The maximum sequence lengths are 500/200/1100 respectively .,"[('are', (4, 5))]","[('maximum sequence lengths', (1, 4)), ('500/200/1100', (5, 6))]","[['maximum sequence lengths', 'are', '500/200/1100']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'maximum sequence lengths']]",[],natural_language_inference,13,187
experimental-setup,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"[('trained', (3, 4)), ('based on', (9, 11))]","[('All models', (0, 2)), ('all runtime benchmarks', (5, 8)), ('TitanXP GPU', (12, 14))]","[['TitanXP GPU', 'trained', 'All models']]","[['TitanXP GPU', 'has', 'all runtime benchmarks']]",[],[],[],"[['Experimental Setup', 'based on', 'TitanXP GPU']]",[],[],[],natural_language_inference,13,189
results,"Overall , there is a 6 % improvement on the RACE - H dataset and 1.8 % improvement on the RACE - M dataset .",[],[],"[['1.8 % improvement', 'on', 'RACE - M dataset']]",[],[],[],[],[],[],"[['RACE', 'has', '1.8 % improvement']]",[],natural_language_inference,13,190
results,Experimental Results on RACE,"[('on', (2, 3))]","[('RACE', (3, 4))]",[],[],"[['Results', 'on', 'RACE']]",[],[],[],[],[],"[['RACE', 'has', '6 % improvement'], ['6 % improvement', 'on', 'RACE - H dataset']]",natural_language_inference,13,194
results,"In general , we also found that the usage of a recurrent cell is not really crucial on this dataset since ( 1 ) Sim . MRU and MRU can achieve comparable performance to each other , ( 2 ) GRU and LSTM models do not have a competitive edge and ( 3 ) Using no encoder already achieves comparable 1 performance to DFN .",[],[],"[['Sim . MRU and MRU', 'achieve', 'comparable performance'], ['comparable performance', 'to', 'each other'], ['GRU and LSTM models', 'do not have', 'competitive edge'], ['no encoder', 'achieves', 'comparable 1 performance'], ['comparable 1 performance', 'to', 'DFN']]",[],[],[],[],[],[],"[['RACE', 'has', 'Sim . MRU and MRU'], ['RACE', 'has', 'GRU and LSTM models'], ['RACE', 'has', 'no encoder']]",[],natural_language_inference,13,195
results,"Finally , an ensemble of Sim . MRU models achieve state - of - the - art performance on the RACE dataset , achieving and over all score of 53.3 % . :","[('achieve', (9, 10)), ('on', (18, 19)), ('achieving', (23, 24)), ('of', (28, 29))]","[('ensemble of Sim . MRU models', (3, 9)), ('state - of - the - art performance', (10, 18)), ('RACE dataset', (20, 22)), ('over all score', (25, 28)), ('53.3 %', (29, 31))]","[['ensemble of Sim . MRU models', 'achieve', 'state - of - the - art performance'], ['state - of - the - art performance', 'achieving', 'over all score'], ['over all score', 'of', '53.3 %'], ['state - of - the - art performance', 'on', 'RACE dataset']]",[],[],[],[],[],[],"[['RACE', 'has', 'ensemble of Sim . MRU models']]",[],natural_language_inference,13,196
results,are baselines reported by . reports our results on the Narrative QA benchmark .,[],"[('Narrative QA benchmark', (10, 13))]",[],[],[],"[['Results', 'on', 'Narrative QA benchmark']]",[],[],[],[],[],natural_language_inference,13,198
results,"First , we observe that 300d MRU can achieve comparable performance with BiDAF .","[('observe that', (3, 5)), ('can achieve', (7, 9)), ('with', (11, 12))]","[('300d MRU', (5, 7)), ('comparable performance', (9, 11)), ('BiDAF', (12, 13))]","[['300d MRU', 'can achieve', 'comparable performance'], ['comparable performance', 'with', 'BiDAF']]",[],[],[],[],"[['Narrative QA benchmark', 'observe that', '300d MRU']]",[],[],[],natural_language_inference,13,199
results,"When compared with a BiLSTM of equal output dimensions ( 150 d ) , we find that our MRU model performs competitively , with less than 1 % deprovement across all metrics .","[('compared with', (1, 3)), ('of', (5, 6)), ('find that', (15, 17)), ('performs', (20, 21)), ('with', (23, 24)), ('across', (29, 30))]","[('BiLSTM', (4, 5)), ('equal output dimensions ( 150 d )', (6, 13)), ('MRU model', (18, 20)), ('competitively', (21, 22)), ('less than 1 % deprovement', (24, 29)), ('all metrics', (30, 32))]","[['BiLSTM', 'find that', 'MRU model'], ['MRU model', 'performs', 'competitively'], ['competitively', 'with', 'less than 1 % deprovement'], ['less than 1 % deprovement', 'across', 'all metrics'], ['BiLSTM', 'of', 'equal output dimensions ( 150 d )']]",[],[],[],[],"[['Narrative QA benchmark', 'compared with', 'BiLSTM']]",[],[],[],natural_language_inference,13,200
results,The performance of our model is significantly better than 300d LSTM model while also being significantly faster .,"[('performance of', (1, 3)), ('is', (5, 6)), ('than', (8, 9)), ('also being', (13, 15))]","[('our model', (3, 5)), ('significantly better', (6, 8)), ('300d LSTM model', (9, 12)), ('significantly faster', (15, 17))]","[['our model', 'is', 'significantly better'], ['significantly better', 'than', '300d LSTM model'], ['our model', 'also being', 'significantly faster']]",[],[],[],[],"[['Narrative QA benchmark', 'performance of', 'our model']]",[],[],[],natural_language_inference,13,202
results,"Finally , the MRU - LSTM significantly outperforms all models , including BiDAF on this dataset .","[('including', (11, 12))]","[('MRU - LSTM', (3, 6)), ('significantly outperforms', (6, 8)), ('all models', (8, 10)), ('BiDAF', (12, 13))]","[['all models', 'including', 'BiDAF']]","[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]",[],[],[],[],[],"[['Narrative QA benchmark', 'has', 'MRU - LSTM']]",[],natural_language_inference,13,206
results,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that MRU encoders are also effective as a complementary neural building block .","[('over', (2, 3)), ('ranges from', (7, 9)), ('across', (14, 15))]","[('Performance improvement', (0, 2)), ('vanilla BiLSTM model', (4, 7)), ('1 % ? 3 %', (9, 14)), ('all metrics', (15, 17))]","[['Performance improvement', 'over', 'vanilla BiLSTM model'], ['vanilla BiLSTM model', 'ranges from', '1 % ? 3 %'], ['1 % ? 3 %', 'across', 'all metrics']]",[],[],[],[],[],[],"[['Narrative QA benchmark', 'has', 'Performance improvement']]",[],natural_language_inference,13,207
research-problem,CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense,[],"[('Question Answering', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering']]",[],[],[],[],natural_language_inference,14,2
research-problem,"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense .",[],"[('Commonsense reasoning', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Commonsense reasoning']]",[],[],[],[],natural_language_inference,14,4
research-problem,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,[],"[('commonsense reasoning over text', (14, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'commonsense reasoning over text']]",[],[],[],[],natural_language_inference,14,16
research-problem,"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .","[('introduce', (5, 6)), ('for', (17, 18)), ('in the style of', (21, 25))]","[('COmmonsense Dataset Adversarially - authored by Humans ( CODAH )', (7, 17)), ('commonsense question answering', (18, 21)), ('SWAG multiple choice sentence completion', (25, 30))]","[['COmmonsense Dataset Adversarially - authored by Humans ( CODAH )', 'for', 'commonsense question answering'], ['commonsense question answering', 'in the style of', 'SWAG multiple choice sentence completion']]",[],"[['Dataset', 'introduce', 'COmmonsense Dataset Adversarially - authored by Humans ( CODAH )']]",[],"[['Contribution', 'has research problem', 'commonsense question answering']]",[],[],[],[],natural_language_inference,14,24
model,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .","[('propose', (1, 2)), ('for', (5, 6)), ('in which', (9, 11)), ('educated on', (14, 16)), ('of', (18, 19)), ('asked to', (33, 35)), ('adversarially target', (38, 40))]","[('novel method', (3, 5)), ('question generation', (6, 8)), ('human annotators', (11, 13)), ('workings', (17, 18)), ('state - of - the - art question answering model', (20, 30)), ('submit', (35, 36)), ('questions', (36, 37)), ('weaknesses', (41, 42))]","[['novel method', 'in which', 'human annotators'], ['human annotators', 'educated on', 'workings'], ['workings', 'of', 'state - of - the - art question answering model'], ['human annotators', 'asked to', 'submit'], ['questions', 'adversarially target', 'weaknesses'], ['novel method', 'for', 'question generation']]","[['submit', 'has', 'questions']]","[['Model', 'propose', 'novel method']]",[],[],[],[],[],[],natural_language_inference,14,25
model,"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .","[('rewarded for', (2, 4)), ('in which', (5, 7)), ('to identify', (10, 12)), ('before and after', (17, 20)), ('on', (23, 24))]","[('Annotators', (0, 1)), ('submissions', (4, 5)), ('model', (8, 9)), ('fails', (9, 10)), ('correct sentence completion', (13, 16)), ('fine - tuning', (20, 23)), ('sample of the submitted questions', (25, 30))]","[['Annotators', 'rewarded for', 'submissions'], ['submissions', 'in which', 'model'], ['fails', 'to identify', 'correct sentence completion'], ['correct sentence completion', 'before and after', 'fine - tuning'], ['fine - tuning', 'on', 'sample of the submitted questions']]","[['model', 'has', 'fails']]",[],"[['Model', 'has', 'Annotators']]",[],[],[],[],[],natural_language_inference,14,26
code,The full dataset is available at https://github.com/Websail-NU /CODAH .,[],"[('https://github.com/Websail-NU /CODAH', (6, 8))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/Websail-NU /CODAH']]",[],[],[],[],natural_language_inference,14,51
hyperparameters,"Also , when training the initial SWAG model we use the hyperparameters recommended in the BERT paper , namely a batch size of 16 , learning rate of 2 e - 5 , and 3 epochs .",[],[],"[['initial SWAG model', 'use', 'hyperparameters'], ['hyperparameters', 'namely', 'learning rate'], ['learning rate', 'of', '2 e - 5'], ['hyperparameters', 'namely', 'batch size'], ['batch size', 'of', '16'], ['hyperparameters', 'namely', 'epochs'], ['hyperparameters', 'recommended in', 'BERT paper']]","[['epochs', 'has', '3']]","[['Hyperparameters', 'when training', 'initial SWAG model']]",[],[],[],[],[],[],natural_language_inference,14,114
hyperparameters,"In our initial experiments , we found that a lower learning rate and more training epochs produced higher accuracy on CODAH , so we replaced the 5e - 5 learning rate in the original grid search with 1 e - 5 , and we added a 6 - epoch setting .","[('replaced', (24, 25)), ('in', (31, 32)), ('with', (36, 37)), ('added', (44, 45))]","[('5e - 5 learning rate', (26, 31)), ('original grid search', (33, 36)), ('1 e - 5', (37, 41)), ('6 - epoch setting', (46, 50))]","[['5e - 5 learning rate', 'with', '1 e - 5'], ['5e - 5 learning rate', 'in', 'original grid search']]",[],"[['Hyperparameters', 'added', '6 - epoch setting'], ['Hyperparameters', 'replaced', '5e - 5 learning rate']]",[],[],[],[],[],[],natural_language_inference,14,115
hyperparameters,The final hyperparameter grid is as follows :,[],"[('final hyperparameter grid', (1, 4))]",[],[],[],"[['Hyperparameters', 'has', 'final hyperparameter grid']]",[],[],[],[],"[['final hyperparameter grid', 'has', 'Number of epochs'], ['final hyperparameter grid', 'has', 'Learning rate'], ['final hyperparameter grid', 'has', 'Batch size']]",natural_language_inference,14,116
hyperparameters,"Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .",[],"[('Batch size', (0, 2)), ('16 , 32', (3, 6)), ('Learning rate', (6, 8)), ('1 e - 5 , 2 e - 5 , 3 e - 5', (9, 23)), ('Number of epochs', (23, 26)), ('3 , 4 , 6', (27, 32))]",[],"[['Number of epochs', 'has', '3 , 4 , 6'], ['Learning rate', 'has', '1 e - 5 , 2 e - 5 , 3 e - 5'], ['Batch size', 'has', '16 , 32']]",[],[],[],[],[],[],[],natural_language_inference,14,117
results,"As a baseline , we evaluate both models on the full SWAG training and validation sets , providing an accuracy of 84.2 % on BERT and 80.2 % on GPT .",[],[],"[['baseline', 'evaluate', 'both models'], ['both models', 'providing', 'accuracy'], ['accuracy', 'of', '84.2 %'], ['84.2 %', 'on', 'BERT'], ['accuracy', 'of', '80.2 %'], ['80.2 %', 'on', 'GPT'], ['both models', 'on', 'full SWAG training and validation sets']]",[],"[['Results', 'As', 'baseline']]",[],[],[],[],[],[],natural_language_inference,14,122
results,"To adjust for the difference in size between our dataset and SWAG , we also train the models on a sample of 2,241 SWAG questions ( the size of the training set in each of CODAH 's crossvalidation folds ) and evaluate them on the full SWAG validation set .",[],[],"[['evaluate', 'on', 'full SWAG validation set'], ['models', 'on', 'sample'], ['sample', 'of', '2,241 SWAG questions']]","[['models', 'has', 'evaluate']]","[['Results', 'train', 'models']]",[],[],[],[],[],[],natural_language_inference,14,123
results,This produces an accuracy of 75.2 % for BERT ( using the cross-validation grid search ) and 63.6 % for GPT . :,[],[],"[['accuracy', 'of', '63.6 %'], ['63.6 %', 'for', 'GPT'], ['accuracy', 'of', '75.2 %'], ['75.2 %', 'for', 'BERT']]",[],[],[],[],"[['models', 'produces', 'accuracy']]",[],[],[],natural_language_inference,14,124
research-problem,Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,[],"[('Semantic Sentence Matching', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Sentence Matching']]",[],[],[],[],natural_language_inference,15,2
research-problem,"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .",[],"[('Sentence matching', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence matching']]",[],[],[],[],natural_language_inference,15,4
model,"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .","[('propose', (6, 7)), ('where', (13, 14)), ('retained to', (19, 21))]","[('densely - connected recurrent network', (8, 13)), ('recurrent hidden features', (15, 18)), ('uppermost layer', (22, 24))]","[['densely - connected recurrent network', 'where', 'recurrent hidden features'], ['recurrent hidden features', 'retained to', 'uppermost layer']]",[],"[['Model', 'propose', 'densely - connected recurrent network']]",[],[],[],[],[],[],natural_language_inference,15,31
model,"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .","[('instead of', (3, 5)), ('used', (14, 15)), ('in combination with', (15, 18)), ('to preserve', (21, 23))]","[('conventional summation operation', (6, 9)), ('concatenation operation', (11, 13)), ('attention mechanism', (19, 21)), ('co-attentive information', (23, 25)), ('better', (25, 26))]","[['conventional summation operation', 'used', 'concatenation operation'], ['concatenation operation', 'in combination with', 'attention mechanism'], ['concatenation operation', 'to preserve', 'co-attentive information']]","[['co-attentive information', 'has', 'better']]","[['Model', 'instead of', 'conventional summation operation']]",[],[],[],[],[],[],natural_language_inference,15,32
model,The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,"[('called', (6, 7)), ('abbreviation for', (11, 13))]","[('DRCN', (7, 8)), ('Densely - connected Recurrent and Co -attentive neural Network', (13, 22))]","[['DRCN', 'abbreviation for', 'Densely - connected Recurrent and Co -attentive neural Network']]",[],"[['Model', 'called', 'DRCN']]",[],[],[],[],[],[],natural_language_inference,15,33
model,The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,"[('proposed', (1, 2)), ('utilize', (4, 5)), ('of', (9, 10))]","[('DRCN', (2, 3)), ('increased representational power', (6, 9)), ('deeper recurrent networks', (10, 13)), ('attentive information', (14, 16))]","[['DRCN', 'utilize', 'increased representational power'], ['increased representational power', 'of', 'deeper recurrent networks'], ['increased representational power', 'of', 'attentive information']]",[],"[['Model', 'proposed', 'DRCN']]",[],[],[],[],[],[],natural_language_inference,15,34
model,"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .","[('to', (2, 3)), ('adopted', (19, 20)), ('forwarded', (23, 24))]","[('autoencoder', (21, 22)), ('fixed length vector', (25, 28)), ('higher layer recurrent module', (30, 34))]","[['fixed length vector', 'to', 'higher layer recurrent module']]",[],"[['Model', 'adopted', 'autoencoder'], ['Model', 'forwarded', 'fixed length vector']]",[],[],[],[],[],[],natural_language_inference,15,35
hyperparameters,"We initialized word embedding with 300d Glo Ve vectors pre-trained from the 840B Common Crawl corpus ( Pennington , Socher , and Manning 2014 ) , while the word embeddings for the out - of - vocabulary words were initialized randomly .",[],[],"[['word embedding', 'with', '300d Glo Ve vectors'], ['word embeddings', 'for', 'out - of - vocabulary words'], ['word embeddings', 'initialized', 'randomly']]",[],"[['Hyperparameters', 'initialized', 'word embedding']]","[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],natural_language_inference,15,113
hyperparameters,We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network .,[],[],"[['character embedding', 'with', '16d vector'], ['32d character representation', 'with', 'convolutional network']]",[],"[['Hyperparameters', 'randomly initialized', 'character embedding'], ['Hyperparameters', 'extracted', '32d character representation']]",[],[],[],[],[],[],natural_language_inference,15,114
hyperparameters,"For the densely - connected recurrent layers , we stacked 5 layers each of which have 100 hidden units .","[('For', (0, 1)), ('stacked', (9, 10)), ('each of which have', (12, 16))]","[('densely - connected recurrent layers', (2, 7)), ('5 layers', (10, 12)), ('100 hidden units', (16, 19))]","[['densely - connected recurrent layers', 'stacked', '5 layers'], ['5 layers', 'each of which have', '100 hidden units']]",[],"[['Hyperparameters', 'For', 'densely - connected recurrent layers']]",[],[],[],[],[],[],natural_language_inference,15,115
hyperparameters,We set 1000 hidden units with respect to the fullyconnected layers .,"[('set', (1, 2)), ('with respect to', (5, 8))]","[('1000 hidden units', (2, 5)), ('fullyconnected layers', (9, 11))]","[['1000 hidden units', 'with respect to', 'fullyconnected layers']]",[],"[['Hyperparameters', 'set', '1000 hidden units']]",[],[],[],[],[],[],natural_language_inference,15,116
hyperparameters,The dropout was applied after the word and character embedding layers with a keep rate of 0.5 .,"[('applied after', (3, 5)), ('with', (11, 12)), ('of', (15, 16))]","[('dropout', (1, 2)), ('word and character embedding layers', (6, 11)), ('keep rate', (13, 15)), ('0.5', (16, 17))]","[['dropout', 'applied after', 'word and character embedding layers'], ['word and character embedding layers', 'with', 'keep rate'], ['keep rate', 'of', '0.5']]",[],[],"[['Hyperparameters', 'has', 'dropout']]",[],[],[],[],[],natural_language_inference,15,117
hyperparameters,It was also applied before the fully - connected layers with a keep rate of 0.8 .,"[('applied before', (3, 5)), ('with', (10, 11)), ('of', (14, 15))]","[('fully - connected layers', (6, 10)), ('keep rate', (12, 14)), ('0.8', (15, 16))]","[['fully - connected layers', 'with', 'keep rate'], ['keep rate', 'of', '0.8']]",[],[],[],[],"[['dropout', 'applied before', 'fully - connected layers']]",[],[],[],natural_language_inference,15,118
hyperparameters,"For the bottleneck component , we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2 .",[],[],"[['bottleneck component', 'set', '200 hidden units'], ['200 hidden units', 'with', 'dropout rate'], ['dropout rate', 'of', '0.2'], ['200 hidden units', 'as', 'encoded features'], ['encoded features', 'of', 'autoencoder']]",[],[],"[['Hyperparameters', 'For', 'bottleneck component']]",[],[],[],[],[],natural_language_inference,15,119
hyperparameters,"The batch normalization was applied on the fully - connected layers , only for the one - way type datasets .","[('applied on', (4, 6)), ('for', (13, 14))]","[('batch normalization', (1, 3)), ('fully - connected layers', (7, 11)), ('one - way type datasets', (15, 20))]","[['batch normalization', 'applied on', 'fully - connected layers'], ['batch normalization', 'for', 'one - way type datasets']]",[],[],"[['Hyperparameters', 'has', 'batch normalization']]",[],[],[],[],[],natural_language_inference,15,120
hyperparameters,The RMSProp optimizer with an initial learning rate of 0.001 was applied .,"[('with', (3, 4)), ('of', (8, 9))]","[('RMSProp optimizer', (1, 3)), ('initial learning rate', (5, 8)), ('0.001', (9, 10))]","[['RMSProp optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]",[],[],"[['Hyperparameters', 'has', 'RMSProp optimizer']]",[],[],[],[],[],natural_language_inference,15,121
hyperparameters,The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve .,"[('decreased by', (4, 6)), ('of', (8, 9)), ('when', (10, 11))]","[('learning rate', (1, 3)), ('factor', (7, 8)), ('0.85', (9, 10)), ('dev accuracy', (12, 14)), ('does not improve', (14, 17))]","[['learning rate', 'decreased by', 'factor'], ['factor', 'of', '0.85'], ['factor', 'when', 'dev accuracy']]","[['dev accuracy', 'has', 'does not improve']]",[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],natural_language_inference,15,122
hyperparameters,All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 .,"[('except', (2, 3)), ('constrained by', (6, 8)), ('with', (10, 11))]","[('weights', (1, 2)), ('embedding matrices', (3, 5)), ('L2 regularization', (8, 10)), ('regularization constant ? = 10 ?6', (12, 18))]","[['weights', 'constrained by', 'L2 regularization'], ['L2 regularization', 'with', 'regularization constant ? = 10 ?6'], ['weights', 'except', 'embedding matrices']]",[],[],"[['Hyperparameters', 'has', 'weights']]",[],[],[],[],[],natural_language_inference,15,123
hyperparameters,"The sequence lengths of the sentence are all different for each dataset : 35 for SNLI , 55 for MultiNLI , 25 for Quora question pair and 50 for TrecQA .",[],[],"[['sequence lengths', 'are', 'all different'], ['all different', 'for', 'each dataset'], ['55', 'for', 'MultiNLI'], ['35', 'for', 'SNLI'], ['25', 'for', 'Quora question pair'], ['50', 'for', 'TrecQA'], ['sequence lengths', 'of', 'sentence']]","[['each dataset', 'has', '55'], ['each dataset', 'has', '35'], ['each dataset', 'has', '25'], ['each dataset', 'has', '50']]",[],"[['Hyperparameters', 'has', 'sequence lengths']]",[],[],[],[],[],natural_language_inference,15,124
results,The proposed DRCN obtains an accuracy of 88.9 % which is a competitive score although we do not use any external knowledge like ESIM + ELMo and LM - Transformer .,"[('obtains', (3, 4)), ('of', (6, 7)), ('is', (10, 11))]","[('proposed DRCN', (1, 3)), ('accuracy', (5, 6)), ('88.9 %', (7, 9)), ('competitive score', (12, 14))]","[['proposed DRCN', 'obtains', 'accuracy'], ['accuracy', 'of', '88.9 %'], ['88.9 %', 'is', 'competitive score']]",[],[],"[['Results', 'has', 'proposed DRCN']]",[],[],[],[],[],natural_language_inference,15,133
results,"The ensemble model achieves an accuracy of 90.1 % , which sets the new state - of the - art performance .","[('achieves', (3, 4)), ('of', (6, 7)), ('sets', (11, 12))]","[('ensemble model', (1, 3)), ('accuracy', (5, 6)), ('90.1 %', (7, 9)), ('new state - of the - art performance', (13, 21))]","[['ensemble model', 'achieves', 'accuracy'], ['accuracy', 'sets', 'new state - of the - art performance'], ['accuracy', 'of', '90.1 %']]",[],[],"[['Results', 'has', 'ensemble model']]",[],[],[],[],[],natural_language_inference,15,134
results,Our ensemble model with 53 m parameters ( 6.7 m 8 ) outperforms the LM - Transformer whose the number of parameters is 85 m .,"[('with', (3, 4)), ('outperforms', (12, 13)), ('whose', (17, 18)), ('is', (22, 23))]","[('53 m parameters ( 6.7 m 8 )', (4, 12)), ('LM - Transformer', (14, 17)), ('number of parameters', (19, 22)), ('85 m', (23, 25))]","[['53 m parameters ( 6.7 m 8 )', 'outperforms', 'LM - Transformer'], ['LM - Transformer', 'whose', 'number of parameters'], ['number of parameters', 'is', '85 m']]",[],[],[],[],"[['ensemble model', 'with', '53 m parameters ( 6.7 m 8 )']]",[],[],[],natural_language_inference,15,135
results,"Furthermore , in case of the encoding - based method , we obtain the best performance of 86.5 % without the co-attention and exact match flag .","[('in case of', (2, 5)), ('obtain', (12, 13)), ('of', (16, 17)), ('without', (19, 20))]","[('encoding - based method', (6, 10)), ('best performance', (14, 16)), ('86.5 %', (17, 19)), ('co-attention and exact match flag', (21, 26))]","[['encoding - based method', 'obtain', 'best performance'], ['best performance', 'of', '86.5 %'], ['best performance', 'without', 'co-attention and exact match flag']]",[],"[['Results', 'in case of', 'encoding - based method']]",[],[],[],[],[],[],natural_language_inference,15,136
results,shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset .,"[('of', (8, 9))]","[('MultiNLI dataset', (9, 11))]",[],[],"[['Results', 'of', 'MultiNLI dataset']]",[],[],[],[],[],"[['MultiNLI dataset', 'has', 'Our plain DRCN']]",natural_language_inference,15,137
results,Our plain DRCN has a competitive performance without any contextualized knowledge .,"[('without', (7, 8))]","[('Our plain DRCN', (0, 3)), ('competitive performance', (5, 7)), ('contextualized knowledge', (9, 11))]","[['competitive performance', 'without', 'contextualized knowledge']]","[['Our plain DRCN', 'has', 'competitive performance']]",[],[],[],[],[],[],[],natural_language_inference,15,138
results,"And , by combining DRCN with the ELMo , one of the contextualized embeddings from language models , our model outperforms the LM - Transformer which has 85 m parameters with fewer parameters of 61 m .",[],[],"[['DRCN', 'with', 'ELMo'], ['our model', 'with', 'fewer parameters'], ['fewer parameters', 'of', '61 m'], ['our model', 'outperforms', 'LM - Transformer'], ['LM - Transformer', 'which has', '85 m parameters']]","[['DRCN', 'has', 'our model']]",[],[],[],"[['MultiNLI dataset', 'by combining', 'DRCN']]",[],[],[],natural_language_inference,15,139
results,Pair shows our results on the Quora question pair dataset .,"[('on', (4, 5))]","[('Quora question pair dataset', (6, 10))]",[],[],"[['Results', 'on', 'Quora question pair dataset']]",[],[],[],[],[],[],natural_language_inference,15,145
results,"We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .",[],[],"[['accuracies', 'of', '90.15 % and 91.30 %'], ['90.15 % and 91.30 %', 'in', 'single and ensemble methods'], ['90.15 % and 91.30 %', 'surpassing', 'previous state - of - the - art model'], ['previous state - of - the - art model', 'of', 'DIIN']]",[],[],[],[],"[['Quora question pair dataset', 'obtained', 'accuracies']]",[],[],[],natural_language_inference,15,147
results,TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .,[],"[('TrecQA and SelQA datasets', (10, 14))]",[],[],[],"[['Results', 'on', 'TrecQA and SelQA datasets']]",[],[],[],[],"[['TrecQA and SelQA datasets', 'has', 'proposed DRCN']]",natural_language_inference,15,148
results,"However , the proposed DRCN using collective attentions over multiple layers , achieves the new state - of the - art performance , exceeding the current state - of - the - art performance significantly on both datasets .","[('using', (5, 6)), ('over', (8, 9)), ('achieves', (12, 13)), ('exceeding', (23, 24))]","[('proposed DRCN', (3, 5)), ('collective attentions', (6, 8)), ('multiple layers', (9, 11)), ('new state - of the - art performance', (14, 22)), ('current state - of - the - art performance', (25, 34)), ('significantly', (34, 35))]","[['proposed DRCN', 'using', 'collective attentions'], ['collective attentions', 'over', 'multiple layers'], ['proposed DRCN', 'achieves', 'new state - of the - art performance'], ['new state - of the - art performance', 'exceeding', 'current state - of - the - art performance']]","[['current state - of - the - art performance', 'has', 'significantly']]",[],[],[],[],[],[],[],natural_language_inference,15,150
ablation-analysis,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was rather higher because of the regularization effect .","[('could see', (15, 17)), ('was', (20, 21)), ('because of', (23, 25))]","[('performance', (19, 20)), ('rather higher', (21, 23)), ('regularization effect', (26, 28))]","[['performance', 'because of', 'regularization effect'], ['performance', 'was', 'rather higher']]",[],"[['Ablation analysis', 'could see', 'performance']]",[],[],[],[],[],[],natural_language_inference,15,156
ablation-analysis,The result shows that the dense connections over attentive features are more effective .,"[('shows', (2, 3)), ('over', (7, 8)), ('are', (10, 11))]","[('result', (1, 2)), ('dense connections', (5, 7)), ('attentive features', (8, 10)), ('more effective', (11, 13))]","[['result', 'shows', 'dense connections'], ['dense connections', 'over', 'attentive features'], ['dense connections', 'are', 'more effective']]",[],[],"[['Ablation analysis', 'has', 'result']]",[],[],[],[],[],natural_language_inference,15,162
ablation-analysis,"In , we removed dense connections over both co-attentive and recurrent features , and the performance degraded to 88.5 % .","[('removed', (3, 4)), ('over', (6, 7)), ('to', (17, 18))]","[('dense connections', (4, 6)), ('both co-attentive and recurrent features', (7, 12)), ('performance', (15, 16)), ('degraded', (16, 17)), ('88.5 %', (18, 20))]","[['dense connections', 'over', 'both co-attentive and recurrent features'], ['degraded', 'to', '88.5 %']]","[['dense connections', 'has', 'performance'], ['performance', 'has', 'degraded']]","[['Ablation analysis', 'removed', 'dense connections']]",[],[],[],[],[],[],natural_language_inference,15,163
ablation-analysis,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .","[('demonstrate', (8, 9)), ('using', (13, 14)), ('over', (16, 17)), ('retaining', (24, 25)), ('to learn', (27, 29))]","[('dense connection', (11, 13)), ('concatenation operation', (14, 16)), ('deeper layers', (17, 19)), ('more powerful capability', (21, 24)), ('collective knowledge', (25, 27)), ('textual semantics', (29, 31))]","[['dense connection', 'using', 'concatenation operation'], ['concatenation operation', 'over', 'deeper layers'], ['more powerful capability', 'retaining', 'collective knowledge'], ['collective knowledge', 'to learn', 'textual semantics']]","[['dense connection', 'has', 'more powerful capability']]","[['Ablation analysis', 'demonstrate', 'dense connection']]",[],[],[],[],[],[],natural_language_inference,15,167
ablation-analysis,The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,"[('shows', (6, 7)), ('among', (10, 11)), ('important to help', (14, 17))]","[('connections', (9, 10)), ('layers', (12, 13)), ('gradient flow', (17, 19))]","[['connections', 'among', 'layers'], ['connections', 'important to help', 'gradient flow']]",[],"[['Ablation analysis', 'shows', 'connections']]",[],[],[],[],[],[],natural_language_inference,15,169
ablation-analysis,"And , the result of ( 11 ) shows that the attentive information functioning as a soft - alignment is significantly effective in semantic sentence matching .","[('functioning as', (13, 15)), ('is', (19, 20)), ('in', (22, 23))]","[('attentive information', (11, 13)), ('soft - alignment', (16, 19)), ('significantly effective', (20, 22)), ('semantic sentence matching', (23, 26))]","[['attentive information', 'functioning as', 'soft - alignment'], ['attentive information', 'is', 'significantly effective'], ['significantly effective', 'in', 'semantic sentence matching']]",[],[],"[['Ablation analysis', 'shows', 'attentive information']]",[],[],[],[],[],natural_language_inference,15,170
ablation-analysis,"The models ( 5 - 9 ) which have connections between layers , are more robust to the increased depth of network , however , the performances of ( 10 - 11 ) tend to degrade as layers get deeper .",[],[],"[['models', 'which have', 'connections'], ['connections', 'are', 'more robust'], ['more robust', 'to', 'increased depth of network'], ['connections', 'between', 'layers'], ['performances', 'tend to', 'degrade'], ['degrade', 'as', 'layers'], ['layers', 'get', 'deeper']]","[['models', 'has', 'performances']]",[],"[['Ablation analysis', 'has', 'models']]",[],[],[],[],[],natural_language_inference,15,172
ablation-analysis,"In addition , the models with dense connections rather than residual connections , have higher performance in general .","[('with', (5, 6)), ('rather than', (8, 10)), ('have', (13, 14))]","[('dense connections', (6, 8)), ('residual connections', (10, 12)), ('higher performance', (14, 16))]","[['dense connections', 'have', 'higher performance'], ['dense connections', 'rather than', 'residual connections']]",[],[],[],[],"[['models', 'with', 'dense connections']]",[],[],[],natural_language_inference,15,173
ablation-analysis,"shows that the connection between layers is essential , especially in deep models , endowing more representational power , and the dense connection is more effective than the residual connection .",[],[],"[['connection', 'endowing', 'more representational power'], ['connection', 'is', 'essential'], ['dense connection', 'is', 'more effective'], ['more effective', 'than', 'residual connection'], ['connection', 'between', 'layers']]","[['connection', 'has', 'dense connection']]",[],"[['Ablation analysis', 'shows', 'connection']]",[],[],[],[],[],natural_language_inference,15,174
research-problem,Natural language sentence matching is a fundamental technology for a variety of tasks .,[],"[('Natural language sentence matching', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language sentence matching']]",[],[],[],[],natural_language_inference,16,4
research-problem,Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .,[],"[('Natural language sentence matching ( NLSM )', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language sentence matching ( NLSM )']]",[],[],[],[],natural_language_inference,16,15
research-problem,"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .",[],"[('NLSM', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLSM']]",[],[],[],[],natural_language_inference,16,17
model,"In this paper , to tackle these limitations , we propose a bilateral multi-perspective matching ( BiMPM ) model for NLSM tasks .","[('propose', (10, 11)), ('for', (19, 20))]","[('bilateral multi-perspective matching ( BiMPM ) model', (12, 19)), ('NLSM tasks', (20, 22))]","[['bilateral multi-perspective matching ( BiMPM ) model', 'for', 'NLSM tasks']]",[],"[['Model', 'propose', 'bilateral multi-perspective matching ( BiMPM ) model']]",[],[],[],[],[],[],natural_language_inference,16,32
model,"Our model essentially belongs to the "" matching aggregation "" framework .","[('belongs to', (3, 5))]","[('"" matching aggregation "" framework', (6, 11))]",[],[],"[['Model', 'belongs to', '"" matching aggregation "" framework']]",[],[],[],[],[],[],natural_language_inference,16,33
model,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .","[('utilized to', (6, 8)), ('into', (12, 13))]","[('BiLSTM layer', (3, 5)), ('aggregate', (8, 9)), ('matching results', (10, 12)), ('fixed - length matching vector', (14, 19))]","[['BiLSTM layer', 'utilized to', 'aggregate'], ['matching results', 'into', 'fixed - length matching vector']]","[['aggregate', 'has', 'matching results']]",[],"[['Model', 'has', 'BiLSTM layer']]",[],[],[],[],[],natural_language_inference,16,39
model,"Finally , based on the matching vector , a decision is made through a fully connected layer .","[('based on', (2, 4)), ('made through', (11, 13))]","[('matching vector', (5, 7)), ('decision', (9, 10)), ('fully connected layer', (14, 17))]","[['decision', 'made through', 'fully connected layer']]","[['matching vector', 'has', 'decision']]","[['Model', 'based on', 'matching vector']]",[],[],[],[],[],[],natural_language_inference,16,40
hyperparameters,We initialize word embeddings in the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,"[('initialize', (1, 2)), ('in', (4, 5)), ('with', (9, 10)), ('pretrained from', (17, 19))]","[('word embeddings', (2, 4)), ('word representation layer', (6, 9)), ('300 - dimensional GloVe word vectors', (11, 17)), ('840B Common Crawl corpus', (20, 24))]","[['word embeddings', 'with', '300 - dimensional GloVe word vectors'], ['300 - dimensional GloVe word vectors', 'pretrained from', '840B Common Crawl corpus'], ['word embeddings', 'in', 'word representation layer']]",[],"[['Hyperparameters', 'initialize', 'word embeddings']]",[],[],[],[],[],[],natural_language_inference,16,130
hyperparameters,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","[('For', (0, 1)), ('initialize', (13, 14))]","[('out - of - vocabulary ( OOV ) words', (2, 11)), ('word embeddings', (15, 17)), ('randomly', (17, 18))]","[['out - of - vocabulary ( OOV ) words', 'initialize', 'word embeddings']]","[['word embeddings', 'has', 'randomly']]","[['Hyperparameters', 'For', 'out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],[],natural_language_inference,16,131
hyperparameters,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .","[('initialize', (6, 7)), ('as', (9, 10)), ('compose', (17, 18)), ('into', (20, 21)), ('with', (25, 26))]","[('charactercomposed embeddings', (2, 4)), ('each character', (7, 9)), ('20 - dimensional vector', (11, 15)), ('each word', (18, 20)), ('50 dimensional vector', (22, 25)), ('LSTM layer', (27, 29))]","[['charactercomposed embeddings', 'compose', 'each word'], ['each word', 'into', '50 dimensional vector'], ['50 dimensional vector', 'with', 'LSTM layer'], ['charactercomposed embeddings', 'initialize', 'each character'], ['each character', 'as', '20 - dimensional vector']]",[],[],"[['Hyperparameters', 'For', 'charactercomposed embeddings']]",[],[],[],[],[],natural_language_inference,16,132
hyperparameters,We set the hidden size as 100 for all BiLSTM layers .,"[('set', (1, 2)), ('as', (5, 6)), ('for', (7, 8))]","[('hidden size', (3, 5)), ('100', (6, 7)), ('all BiLSTM layers', (8, 11))]","[['hidden size', 'as', '100'], ['hidden size', 'for', 'all BiLSTM layers']]",[],"[['Hyperparameters', 'set', 'hidden size']]",[],[],[],[],[],[],natural_language_inference,16,133
hyperparameters,"We apply dropout to every layers in , and set the dropout ratio as 0.1 .","[('apply', (1, 2)), ('to', (3, 4)), ('set', (9, 10)), ('as', (13, 14))]","[('dropout', (2, 3)), ('every layers', (4, 6)), ('dropout ratio', (11, 13)), ('0.1', (14, 15))]","[['dropout', 'set', 'dropout ratio'], ['dropout ratio', 'as', '0.1'], ['dropout', 'to', 'every layers']]",[],"[['Hyperparameters', 'apply', 'dropout']]",[],[],[],[],[],[],natural_language_inference,16,134
hyperparameters,"To train the model , we minimize the cross entropy of the training set , and use the ADAM optimizer [ Kingma and Ba , 2014 ] to update parameters .","[('minimize', (6, 7)), ('of', (10, 11)), ('use', (16, 17)), ('to update', (27, 29))]","[('cross entropy', (8, 10)), ('training set', (12, 14)), ('ADAM optimizer', (18, 20)), ('parameters', (29, 30))]","[['cross entropy', 'of', 'training set'], ['ADAM optimizer', 'to update', 'parameters']]",[],"[['Hyperparameters', 'minimize', 'cross entropy'], ['Hyperparameters', 'use', 'ADAM optimizer']]",[],[],[],[],[],[],natural_language_inference,16,135
hyperparameters,We set the learning rate as 0.001 .,"[('as', (5, 6))]","[('learning rate', (3, 5)), ('0.001', (6, 7))]","[['learning rate', 'as', '0.001']]",[],[],"[['Hyperparameters', 'set', 'learning rate']]",[],[],[],[],[],natural_language_inference,16,136
hyperparameters,"During training , we do not update the pre-trained word embeddings .","[('During', (0, 1)), ('do not update', (4, 7))]","[('training', (1, 2)), ('pre-trained word embeddings', (8, 11))]","[['training', 'do not update', 'pre-trained word embeddings']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,16,137
experiments,"In this Sub-section , we compare our model with state - of - theart models on the paraphrase identification task .",[],"[('paraphrase identification task', (17, 20))]",[],[],[],[],[],[],[],[],"[['paraphrase identification task', 'has', 'Results']]",natural_language_inference,16,163
experiments,"We still experiment on the "" Quora Question Pairs "" dataset , and use the same dataset partition as Sub-section 4.2 .","[('experiment on', (2, 4))]","[('"" Quora Question Pairs "" dataset', (5, 11))]",[],[],[],[],[],"[['Results', 'experiment on', '"" Quora Question Pairs "" dataset']]",[],[],[],natural_language_inference,16,164
experiments,"First , under the Siamese framework , we implement two baseline models : "" Siamese - CNN "" and "" Siamese - LSTM "" .","[('under', (2, 3)), ('implement', (8, 9))]","[('Siamese framework', (4, 6)), ('two baseline models', (9, 12)), ('Siamese - CNN', (14, 17)), ('Siamese - LSTM', (20, 23))]","[['Siamese framework', 'implement', 'two baseline models']]","[['two baseline models', 'name', 'Siamese - CNN'], ['two baseline models', 'name', 'Siamese - LSTM']]",[],[],[],"[['Baselines', 'under', 'Siamese framework']]",[],[],[],natural_language_inference,16,167
experiments,"Second , based on the two baseline models , we implement two more baseline models "" Multi - Perspective - CNN "" and "" Multi - Perspective - LSTM "" .",[],[],[],"[['two more baseline models', 'name', 'Multi - Perspective - CNN'], ['two more baseline models', 'name', 'Multi - Perspective - LSTM']]",[],[],[],"[['Baselines', 'implement', 'two more baseline models']]",[],[],[],natural_language_inference,16,171
experiments,"Third , we re-implement the "" L.D.C. "" model proposed by , which is a model under the "" matchingaggregation "" framework and acquires the state - of - the - art performance on several tasks .",[],[],"[['"" L.D.C. "" model', 'is', 'model'], ['model', 'under', '"" matchingaggregation "" framework']]",[],[],[],[],"[['Baselines', 're-implement', '"" L.D.C. "" model']]",[],[],[],natural_language_inference,16,173
experiments,"We can see that "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) works much better than "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .","[('see that', (2, 4)), ('works', (20, 21)), ('than', (23, 24))]","[('"" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )', (4, 20)), ('much better', (21, 23)), ('"" Siamese - CNN "" ( or "" Siamese - LSTM "" )', (24, 37))]","[['"" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )', 'works', 'much better'], ['much better', 'than', '"" Siamese - CNN "" ( or "" Siamese - LSTM "" )']]",[],[],[],[],"[['Results', 'see that', '"" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )']]",[],[],[],natural_language_inference,16,175
experiments,"Our "" BiMPM "" model outperforms the "" L.D.C. "" model by more than two percent .","[('by', (11, 12))]","[('Our "" BiMPM "" model', (0, 5)), ('outperforms', (5, 6)), ('"" L.D.C. "" model', (7, 11)), ('more than two percent', (12, 16))]","[['outperforms', 'by', 'more than two percent']]","[['Our "" BiMPM "" model', 'has', 'outperforms'], ['outperforms', 'has', '"" L.D.C. "" model']]",[],[],[],[],[],"[['Results', 'has', 'Our "" BiMPM "" model']]",[],natural_language_inference,16,176
experiments,"In this Sub-section , we evaluate our model on the natural language inference task over the SNLI dataset .","[('over', (14, 15))]","[('natural language inference task', (10, 14)), ('SNLI dataset', (16, 18))]","[['natural language inference task', 'over', 'SNLI dataset']]",[],[],[],[],[],[],"[['Tasks', 'on', 'natural language inference task']]","[['SNLI dataset', 'has', 'Results']]",natural_language_inference,16,179
experiments,"First , we can see that "" Only P ? Q "" works significantly better than "" Only P ? Q "" , which tells us that , for natural language inference , matching the hypothesis against the premise is more effective than the other way around .",[],[],"[['works significantly better', 'tells', 'matching the hypothesis'], ['matching the hypothesis', 'against', 'premise'], ['matching the hypothesis', 'is', 'more effective'], ['more effective', 'than', 'other way around'], ['works significantly better', 'than', 'Only P ? Q']]","[['Only P ? Q', 'has', 'works significantly better']]",[],[],[],"[['Results', 'see that', 'Only P ? Q']]",[],[],[],natural_language_inference,16,183
experiments,"Second , our "" BiMPM "" model works much better than "" Only P ? Q "" , which reveals that matching premise against the hypothesis can also bring some benefits .","[('works', (7, 8)), ('than', (10, 11))]","[('our "" BiMPM "" model', (2, 7)), ('much better', (8, 10)), ('Only P ? Q', (12, 16))]","[['our "" BiMPM "" model', 'works', 'much better'], ['much better', 'than', 'Only P ? Q']]",[],[],[],[],[],[],"[['Results', 'has', 'our "" BiMPM "" model']]",[],natural_language_inference,16,184
experiments,"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works much better than "" ( Ensemble ) "" .","[('observe that', (19, 21)), ('on par with', (28, 31)), ('works', (50, 51)), ('than', (53, 54))]","[('our single model "" BiMPM ""', (21, 27)), ('state - of - the - art single models', (32, 41)), (""our ' BiMPM ( Ensemble )"", (43, 49)), ('much better', (51, 53)), ('Ensemble', (56, 57))]","[[""our ' BiMPM ( Ensemble )"", 'works', 'much better'], ['much better', 'than', 'Ensemble'], ['our single model "" BiMPM ""', 'on par with', 'state - of - the - art single models']]",[],[],[],[],"[['Results', 'observe that', ""our ' BiMPM ( Ensemble )""], ['Results', 'observe that', 'our single model "" BiMPM ""']]",[],[],[],natural_language_inference,16,185
experiments,"Therefore , our models achieve the state - of - the - art performance in both single and ensemble scenarios for the natural language inference task .","[('achieve', (4, 5)), ('in both', (14, 16)), ('for', (20, 21))]","[('our models', (2, 4)), ('state - of - the - art performance', (6, 14)), ('single and ensemble scenarios', (16, 20)), ('natural language inference task', (22, 26))]","[['our models', 'achieve', 'state - of - the - art performance'], ['state - of - the - art performance', 'in both', 'single and ensemble scenarios'], ['state - of - the - art performance', 'for', 'natural language inference task']]",[],[],[],[],[],[],"[['Results', 'has', 'our models']]",[],natural_language_inference,16,186
experiments,"In this Sub-section , we study the effectiveness of our model for answer sentence selection tasks .","[('for', (11, 12))]","[('answer sentence selection tasks', (12, 16))]",[],[],[],[],[],"[['Tasks', 'for', 'answer sentence selection tasks']]",[],[],[],natural_language_inference,16,188
experiments,We experiment on two datasets : TREC - QA and WikiQA .,"[('experiment on', (1, 3))]","[('two datasets', (3, 5)), ('TREC - QA', (6, 9)), ('WikiQA', (10, 11))]",[],"[['two datasets', 'name', 'TREC - QA'], ['two datasets', 'name', 'WikiQA']]",[],[],[],"[['answer sentence selection tasks', 'experiment on', 'two datasets']]",[],[],[],natural_language_inference,16,190
experiments,We can see that the performance from our model is on par with the state - of - the - art models .,"[('see that', (2, 4)), ('from', (6, 7)), ('is', (9, 10)), ('with', (12, 13))]","[('performance', (5, 6)), ('our model', (7, 9)), ('on par', (10, 12)), ('state - of - the - art models', (14, 22))]","[['performance', 'from', 'our model'], ['performance', 'is', 'on par'], ['on par', 'with', 'state - of - the - art models']]",[],[],[],[],"[['answer sentence selection tasks', 'see that', 'performance']]",[],[],[],natural_language_inference,16,192
research-problem,Reinforced Mnemonic Reader for Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,17,2
model,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .","[('present', (7, 8)), ('that', (11, 12)), ('to refine', (19, 21)), ('in', (23, 24))]","[('reattention mechanism', (9, 11)), ('temporally memorizes', (12, 14)), ('past attentions', (14, 16)), ('current attentions', (21, 23)), ('multi-round alignment architecture', (25, 28))]","[['reattention mechanism', 'that', 'temporally memorizes'], ['past attentions', 'to refine', 'current attentions'], ['current attentions', 'in', 'multi-round alignment architecture']]","[['temporally memorizes', 'has', 'past attentions']]","[['Model', 'present', 'reattention mechanism']]",[],[],[],[],[],[],natural_language_inference,17,27
model,"The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .","[('based on', (3, 5)), ('share', (11, 12)), ('if', (14, 15)), ('about', (17, 18)), ('are', (20, 21)), ('be', (25, 26))]","[('computation', (1, 2)), ('two words', (8, 10)), ('similar semantics', (12, 14)), ('attentions', (16, 17)), ('same texts', (18, 20)), ('highly overlapped', (21, 23)), ('less similar', (26, 28))]","[['computation', 'based on', 'two words'], ['two words', 'share', 'similar semantics'], ['two words', 'if', 'attentions'], ['attentions', 'about', 'same texts'], ['same texts', 'be', 'less similar'], ['same texts', 'are', 'highly overlapped']]",[],[],"[['Model', 'has', 'computation']]",[],[],[],[],[],natural_language_inference,17,28
model,"Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .",[],[],"[['reattention', 'be', 'more concentrated'], ['more concentrated', 'if', 'past attentions'], ['past attentions', 'focus on', 'same parts'], ['same parts', 'of', 'input'], ['reattention', 'be', 'relatively more distracted'], ['relatively more distracted', 'to focus on', 'new regions'], ['new regions', 'if', 'past attentions'], ['past attentions', 'are', 'not overlapped at all']]",[],[],"[['Model', 'has', 'reattention']]",[],[],[],[],[],natural_language_inference,17,29
model,"As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .","[('extend', (7, 8)), ('with', (12, 13)), ('called', (16, 17))]","[('traditional training method', (9, 12)), ('novel approach', (14, 16)), ('dynamic - critical reinforcement learning', (17, 22))]","[['traditional training method', 'with', 'novel approach'], ['novel approach', 'called', 'dynamic - critical reinforcement learning']]",[],"[['Model', 'extend', 'traditional training method']]",[],[],[],[],[],[],natural_language_inference,17,30
model,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .","[('dynamically decides', (17, 19)), ('according to', (24, 26))]","[('reward and the baseline', (20, 24)), ('two sampling strategies', (26, 29))]","[['reward and the baseline', 'according to', 'two sampling strategies']]",[],[],[],[],"[['dynamic - critical reinforcement learning', 'dynamically decides', 'reward and the baseline']]",[],[],[],natural_language_inference,17,31
hyperparameters,"We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .","[('use', (1, 2)), ('for', (12, 13))]","[('Adam optimizer', (3, 5)), ('ML and DCRL training', (14, 18))]","[['Adam optimizer', 'for', 'ML and DCRL training']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],[],[],[],natural_language_inference,17,190
hyperparameters,"The initial learning rates are 0.0008 and 0.0001 respectively , and are halved whenever meeting a bad iteration .",[],"[('initial learning rates', (1, 4))]",[],[],[],"[['Hyperparameters', 'has', 'initial learning rates']]",[],[],[],[],"[['initial learning rates', 'are', '0.0008 and 0.0001'], ['initial learning rates', 'are', 'halved']]",natural_language_inference,17,191
hyperparameters,The batch size is 48 and a dropout rate of 0.3 is used to prevent overfitting .,"[('is', (3, 4)), ('of', (9, 10)), ('to prevent', (13, 15))]","[('batch size', (1, 3)), ('48', (4, 5)), ('dropout rate', (7, 9)), ('0.3', (10, 11)), ('overfitting', (15, 16))]","[['batch size', 'is', '48'], ['dropout rate', 'of', '0.3']]","[['overfitting', 'has', 'batch size'], ['overfitting', 'has', 'dropout rate']]","[['Hyperparameters', 'to prevent', 'overfitting']]",[],[],[],[],[],[],natural_language_inference,17,192
hyperparameters,Word embeddings remain fixed during training .,"[('remain', (2, 3)), ('during', (4, 5))]","[('Word embeddings', (0, 2)), ('fixed', (3, 4)), ('training', (5, 6))]","[['Word embeddings', 'remain', 'fixed'], ['fixed', 'during', 'training']]",[],[],"[['Hyperparameters', 'has', 'Word embeddings']]",[],[],[],[],[],natural_language_inference,17,193
hyperparameters,"For out of vocabulary words , we set the embeddings from Gaussian distributions and keep them trainable .","[('For', (0, 1)), ('set', (7, 8)), ('from', (10, 11)), ('keep them', (14, 16))]","[('out of vocabulary words', (1, 5)), ('embeddings', (9, 10)), ('Gaussian distributions', (11, 13)), ('trainable', (16, 17))]","[['out of vocabulary words', 'set', 'embeddings'], ['embeddings', 'keep them', 'trainable'], ['embeddings', 'from', 'Gaussian distributions']]",[],"[['Hyperparameters', 'For', 'out of vocabulary words']]",[],[],[],[],[],[],natural_language_inference,17,194
hyperparameters,"The size of character embedding and corresponding LSTMs is 50 , the main hidden size is 100 , and the hyperparameter ? is 3 .",[],[],"[['hyperparameter', 'is', '3'], ['size', 'of', 'character embedding and corresponding LSTMs'], ['character embedding and corresponding LSTMs', 'is', '50'], ['main hidden size', 'is', '100']]",[],[],"[['Hyperparameters', 'has', 'hyperparameter'], ['Hyperparameters', 'has', 'size'], ['Hyperparameters', 'has', 'main hidden size']]",[],[],[],[],[],natural_language_inference,17,195
results,We submitted our model on the hidden test set of SQuAD for evaluation .,"[('on', (4, 5)), ('of', (9, 10))]","[('hidden test set', (6, 9)), ('SQuAD', (10, 11))]","[['hidden test set', 'of', 'SQuAD']]",[],"[['Results', 'on', 'hidden test set']]",[],[],[],[],[],"[['SQuAD', 'has', 'R.M - Reader']]",natural_language_inference,17,198
results,"As shown in , R.M - Reader achieves an EM score of 79.5 % and F1 score of 86.6 % .",[],[],"[['R.M - Reader', 'achieves', 'EM score'], ['EM score', 'of', '79.5 %'], ['R.M - Reader', 'achieves', 'F1 score'], ['F1 score', 'of', '86.6 %']]",[],[],[],[],[],[],[],[],natural_language_inference,17,200
results,"Our ensemble model improves the metrics to 82.3 % and 88.5 % respectively 2 . shows the performance comparison on two adversarial datasets , Add Sent and Add OneSent .","[('improves', (3, 4)), ('to', (6, 7))]","[('Our ensemble model', (0, 3)), ('metrics', (5, 6)), ('82.3 % and 88.5 %', (7, 12))]","[['Our ensemble model', 'improves', 'metrics'], ['metrics', 'to', '82.3 % and 88.5 %']]",[],[],[],[],[],[],"[['SQuAD', 'has', 'Our ensemble model']]",[],natural_language_inference,17,202
results,"As we can see , R.M - Reader comfortably outperforms all previous models by more than 6 % in both EM and F 1 scores , indicating that our model is more robust against adversarial attacks .","[('by', (13, 14)), ('in', (18, 19))]","[('comfortably outperforms', (8, 10)), ('previous models', (11, 13)), ('more than 6 %', (14, 18)), ('EM and F 1 scores', (20, 25))]","[['previous models', 'by', 'more than 6 %'], ['more than 6 %', 'in', 'EM and F 1 scores']]","[['comfortably outperforms', 'has', 'previous models']]",[],[],[],[],[],"[['R.M - Reader', 'has', 'comfortably outperforms']]",[],natural_language_inference,17,204
ablation-analysis,"We notice that reattention has more influences on EM score while DCRL contributes more to F1 metric , and removing both of them results in huge drops on both metrics .",[],[],"[['more influences', 'on', 'EM score'], ['DCRL', 'contributes more to', 'F1 metric'], ['removing both', 'results in', 'huge drops'], ['huge drops', 'on', 'both metrics']]","[['reattention', 'has', 'more influences']]","[['Ablation analysis', 'notice', 'reattention'], ['Ablation analysis', 'notice', 'DCRL'], ['Ablation analysis', 'notice', 'removing both']]",[],[],[],[],[],[],natural_language_inference,17,208
ablation-analysis,Replacing DCRL with SCST also causes a marginal decline of performance on both metrics .,"[('Replacing', (0, 1)), ('with', (2, 3)), ('causes', (5, 6)), ('of', (9, 10)), ('on', (11, 12))]","[('DCRL', (1, 2)), ('SCST', (3, 4)), ('marginal decline', (7, 9)), ('performance', (10, 11)), ('both metrics', (12, 14))]","[['DCRL', 'with', 'SCST'], ['DCRL', 'causes', 'marginal decline'], ['marginal decline', 'of', 'performance'], ['performance', 'on', 'both metrics']]",[],"[['Ablation analysis', 'Replacing', 'DCRL']]",[],[],[],[],[],[],natural_language_inference,17,209
ablation-analysis,"Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .","[('relace', (3, 4)), ('with', (8, 9)), ('suffer from', (29, 31))]","[('default attention function', (5, 8)), ('dot product : f ( u , v ) = u v ( 5 )', (10, 25)), ('both metrics', (27, 29)), ('degradations', (31, 32))]","[['default attention function', 'with', 'dot product : f ( u , v ) = u v ( 5 )'], ['both metrics', 'suffer from', 'degradations']]","[['default attention function', 'has', 'both metrics']]","[['Ablation analysis', 'relace', 'default attention function']]",[],[],[],[],[],[],natural_language_inference,17,210
ablation-analysis,"Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .","[('Removing', (0, 1)), ('leads to', (6, 8)), ('is', (15, 16)), ('than', (18, 19))]","[('any of the two heuristics', (1, 6)), ('some performance declines', (8, 11)), ('heuristic subtraction', (13, 15)), ('more effective', (16, 18)), ('multiplication', (19, 20))]","[['any of the two heuristics', 'leads to', 'some performance declines'], ['heuristic subtraction', 'is', 'more effective'], ['more effective', 'than', 'multiplication']]","[['any of the two heuristics', 'has', 'heuristic subtraction']]","[['Ablation analysis', 'Removing', 'any of the two heuristics']]",[],[],[],[],[],[],natural_language_inference,17,212
ablation-analysis,In both cases the highway - like function has outperformed its simpler variants .,[],"[('highway - like function', (4, 8)), ('outperformed', (9, 10)), ('simpler variants', (11, 13))]",[],"[['highway - like function', 'has', 'outperformed'], ['outperformed', 'has', 'simpler variants']]",[],"[['Ablation analysis', 'has', 'highway - like function']]",[],[],[],[],[],natural_language_inference,17,214
ablation-analysis,"We notice that using 2 blocks causes a slight performance drop , while increasing to 4 blocks barely affects the SoTA result .","[('using', (3, 4)), ('causes', (6, 7)), ('increasing to', (13, 15)), ('barely affects', (17, 19))]","[('2 blocks', (4, 6)), ('slight performance drop', (8, 11)), ('4 blocks', (15, 17)), ('SoTA result', (20, 22))]","[['2 blocks', 'causes', 'slight performance drop'], ['4 blocks', 'barely affects', 'SoTA result']]",[],"[['Ablation analysis', 'using', '2 blocks'], ['Ablation analysis', 'increasing to', '4 blocks']]",[],[],[],[],[],[],natural_language_inference,17,216
ablation-analysis,"Interestingly , a very deep alignment with 5 blocks results in a significant performance decline .","[('with', (6, 7)), ('results in', (9, 11))]","[('very deep alignment', (3, 6)), ('5 blocks', (7, 9)), ('significant performance decline', (12, 15))]","[['very deep alignment', 'with', '5 blocks'], ['5 blocks', 'results in', 'significant performance decline']]",[],[],"[['Ablation analysis', 'has', 'very deep alignment']]",[],[],[],[],[],natural_language_inference,17,217
research-problem,Neural Variational Inference for Text Processing Phil Blunsom 12,[],"[('Neural Variational Inference', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Variational Inference']]",[],[],[],[],natural_language_inference,18,2
model,"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .","[('introduces', (2, 3)), ('for', (7, 8)), ('of', (10, 11)), ('inspired by', (13, 15))]","[('neural variational framework', (4, 7)), ('generative models', (8, 10)), ('text', (11, 12)), ('variational autoencoder', (16, 18))]","[['neural variational framework', 'for', 'generative models'], ['generative models', 'of', 'text'], ['neural variational framework', 'inspired by', 'variational autoencoder']]",[],"[['Model', 'introduces', 'neural variational framework']]",[],[],[],[],[],[],natural_language_inference,18,22
model,"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .","[('build', (5, 6)), ('implemented by', (10, 12)), ('conditioned on', (16, 18)), ('to approximate', (20, 22)), ('over', (25, 26))]","[('inference network', (7, 9)), ('deep neural network', (13, 16)), ('text', (18, 19)), ('intractable distributions', (23, 25)), ('latent variables', (27, 29))]","[['inference network', 'implemented by', 'deep neural network'], ['deep neural network', 'conditioned on', 'text'], ['inference network', 'to approximate', 'intractable distributions'], ['intractable distributions', 'over', 'latent variables']]",[],"[['Model', 'build', 'inference network']]",[],[],[],[],[],[],natural_language_inference,18,23
model,"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .","[('learns to model', (16, 19))]","[('neural variational inference', (13, 16)), ('posterior probability', (20, 22))]","[['neural variational inference', 'learns to model', 'posterior probability']]",[],[],"[['Model', 'has', 'neural variational inference']]",[],[],[],[],[],natural_language_inference,18,24
model,"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .","[('using', (1, 2)), ('trained through', (11, 13)), ('w.r.t.', (21, 22))]","[('reparameteris ation method', (3, 6)), ('inference network', (8, 10)), ('back - propagating', (13, 16)), ('unbiased and low variance gradients', (16, 21)), ('latent variables', (23, 25))]","[['inference network', 'trained through', 'back - propagating'], ['unbiased and low variance gradients', 'w.r.t.', 'latent variables']]","[['reparameteris ation method', 'has', 'inference network'], ['back - propagating', 'has', 'unbiased and low variance gradients']]","[['Model', 'using', 'reparameteris ation method']]",[],[],[],[],[],[],natural_language_inference,18,27
model,"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .",[],[],"[['Neural Answer Selection Model ( NASM )', 'for', 'question answering'], ['Neural Variational Document Model ( NVDM )', 'for', 'document modelling']]",[],"[['Model', 'propose', 'Neural Answer Selection Model ( NASM )'], ['Model', 'propose', 'Neural Variational Document Model ( NVDM )']]",[],[],[],[],[],[],natural_language_inference,18,28
model,A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,"[('of', (3, 4)), ('is', (5, 6)), ('generated directly from', (10, 13)), ('instead of', (18, 20))]","[('primary feature', (1, 3)), ('NVDM', (4, 5)), ('each word', (7, 9)), ('dense continuous document representation', (14, 18)), ('more common binary semantic vector', (21, 26))]","[['primary feature', 'of', 'NVDM'], ['NVDM', 'is', 'each word'], ['each word', 'generated directly from', 'dense continuous document representation'], ['dense continuous document representation', 'instead of', 'more common binary semantic vector']]",[],[],"[['Model', 'has', 'primary feature']]",[],[],[],[],[],natural_language_inference,18,31
model,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,"[('is', (3, 4)), ('imbues', (9, 10)), ('with', (11, 12)), ('to model', (17, 19)), ('of', (21, 22))]","[('NASM', (1, 2)), ('supervised conditional model', (5, 8)), ('LSTMs', (10, 11)), ('latent stochastic attention mechanism', (13, 17)), ('semantics', (20, 21)), ('question - answer pairs', (22, 26))]","[['NASM', 'is', 'supervised conditional model'], ['supervised conditional model', 'imbues', 'LSTMs'], ['LSTMs', 'with', 'latent stochastic attention mechanism'], ['LSTMs', 'to model', 'semantics'], ['semantics', 'of', 'question - answer pairs']]",[],[],"[['Model', 'has', 'NASM']]",[],[],[],[],[],natural_language_inference,18,33
model,The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,"[('designed to', (4, 6)), ('on', (7, 8)), ('of', (10, 11)), ('strongly connected to', (15, 18)), ('modelled by', (23, 25))]","[('attention model', (1, 3)), ('focus', (6, 7)), ('phrases', (9, 10)), ('answer', (12, 13)), ('question semantics', (19, 21)), ('latent distribution', (26, 28))]","[['attention model', 'designed to', 'focus'], ['focus', 'on', 'phrases'], ['phrases', 'of', 'answer'], ['attention model', 'modelled by', 'latent distribution'], ['attention model', 'strongly connected to', 'question semantics']]",[],[],[],[],[],[],"[['NASM', 'has', 'attention model']]",[],natural_language_inference,18,34
model,"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .","[('provides', (2, 3)), ('against', (6, 7))]","[('Bayesian inference', (0, 2)), ('natural safeguard', (4, 6)), ('overfitting', (7, 8))]","[['Bayesian inference', 'provides', 'natural safeguard'], ['natural safeguard', 'against', 'overfitting']]",[],[],[],[],[],[],"[['NASM', 'has', 'Bayesian inference']]",[],natural_language_inference,18,36
experiments,Experiments on Document Modelling,"[('on', (1, 2))]","[('Document Modelling', (2, 4))]",[],[],"[['Experiments', 'on', 'Document Modelling']]",[],[],[],[],[],"[['Document Modelling', 'has', 'Results']]",natural_language_inference,18,141
experiments,The experimental results indicate that NVDM achieves the best performance on both datasets .,"[('indicate', (3, 4)), ('achieves', (6, 7)), ('on', (10, 11))]","[('NVDM', (5, 6)), ('best performance', (8, 10)), ('both datasets', (11, 13))]","[['NVDM', 'achieves', 'best performance'], ['best performance', 'on', 'both datasets']]",[],[],[],[],"[['Results', 'indicate', 'NVDM']]",[],[],[],natural_language_inference,18,145
experiments,"For the experiments on RCV1 - v2 dataset , the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension .",[],[],"[['experiments', 'on', 'RCV1 - v2 dataset'], ['NVDM', 'with', 'latent variable'], ['latent variable', 'performs', 'even better'], ['even better', 'than', 'fDARN'], ['fDARN', 'with', '200 dimension'], ['latent variable', 'of', '50 dimension']]","[['RCV1 - v2 dataset', 'has', 'NVDM']]",[],[],[],"[['Results', 'For', 'experiments']]",[],[],[],natural_language_inference,18,146
experiments,Dataset & Setup for Answer Sentence Selection,"[('for', (3, 4))]","[('Answer Sentence Selection', (4, 7))]",[],[],"[['Experiments', 'for', 'Answer Sentence Selection']]",[],[],[],[],[],"[['Answer Sentence Selection', 'has', 'Hyperparameters']]",natural_language_inference,18,155
experiments,The word embeddings ( K = 50 ) are obtained by running the word2vec tool on the English Wikipedia dump and the AQUAINT 5 corpus .,"[('are', (8, 9)), ('by running', (10, 12)), ('on', (15, 16))]","[('word embeddings ( K = 50 )', (1, 8)), ('obtained', (9, 10)), ('word2vec tool', (13, 15)), ('English Wikipedia dump', (17, 20)), ('AQUAINT 5 corpus', (22, 25))]","[['word embeddings ( K = 50 )', 'are', 'obtained'], ['obtained', 'by running', 'word2vec tool'], ['word2vec tool', 'on', 'English Wikipedia dump'], ['word2vec tool', 'on', 'AQUAINT 5 corpus']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'word embeddings ( K = 50 )']]",[],natural_language_inference,18,173
experiments,"We use LSTMs with 3 layers and 50 hidden units , and apply 40 % dropout after the embedding layer .","[('use', (1, 2)), ('with', (3, 4)), ('apply', (12, 13)), ('after', (16, 17))]","[('LSTMs', (2, 3)), ('3 layers', (4, 6)), ('50 hidden units', (7, 10)), ('40 % dropout', (13, 16)), ('embedding layer', (18, 20))]","[['LSTMs', 'with', '3 layers'], ['LSTMs', 'with', '50 hidden units'], ['LSTMs', 'apply', '40 % dropout'], ['40 % dropout', 'after', 'embedding layer']]",[],[],[],[],"[['Hyperparameters', 'use', 'LSTMs']]",[],[],[],natural_language_inference,18,174
experiments,"For the construction of the inference network , we use an MLP ( Eq. 10 ) with 2 layers and tanh units of 50 dimension , and an MLP ( Eq. 17 ) with 2 layers and tanh units of 150 dimension for modelling the joint representation .",[],[],"[['inference network', 'use', 'MLP'], ['MLP', 'with', '2 layers'], ['inference network', 'use', 'tanh units'], ['tanh units', 'of', '50 dimension'], ['MLP', 'with', '2 layers'], ['tanh units', 'of', '150 dimension']]","[['joint representation', 'has', 'MLP'], ['joint representation', 'has', 'tanh units']]",[],[],[],"[['Hyperparameters', 'For the construction of', 'inference network'], ['Hyperparameters', 'for modelling', 'joint representation']]",[],[],[],natural_language_inference,18,175
experiments,"During training we carry out stochastic estimation by taking one sample for computing the gradients , while in prediction we use 20 samples to calculate the expectation of the lower bound .","[('During', (0, 1)), ('carry out', (3, 5)), ('by taking', (7, 9)), ('for computing', (11, 13)), ('in', (17, 18)), ('use', (20, 21)), ('to calculate', (23, 25)), ('of', (27, 28))]","[('training', (1, 2)), ('stochastic estimation', (5, 7)), ('one sample', (9, 11)), ('gradients', (14, 15)), ('prediction', (18, 19)), ('20 samples', (21, 23)), ('expectation', (26, 27)), ('lower bound', (29, 31))]","[['prediction', 'use', '20 samples'], ['20 samples', 'to calculate', 'expectation'], ['expectation', 'of', 'lower bound'], ['training', 'carry out', 'stochastic estimation'], ['stochastic estimation', 'by taking', 'one sample'], ['one sample', 'for computing', 'gradients']]",[],[],[],[],"[['Hyperparameters', 'in', 'prediction'], ['Hyperparameters', 'During', 'training']]",[],[],[],natural_language_inference,18,176
experiments,"The LSTM + Att performs slightly better than the vanilla LSTM model , and our NASM improves the results further .","[('performs', (4, 5)), ('than', (7, 8)), ('improves', (16, 17))]","[('LSTM + Att', (1, 4)), ('slightly better', (5, 7)), ('vanilla LSTM model', (9, 12)), ('our NASM', (14, 16)), ('results', (18, 19))]","[['LSTM + Att', 'performs', 'slightly better'], ['slightly better', 'than', 'vanilla LSTM model'], ['our NASM', 'improves', 'results']]","[['slightly better', 'has', 'our NASM']]",[],[],[],[],[],"[['Results', 'has', 'LSTM + Att']]",[],natural_language_inference,18,187
experiments,"Since the QASent dataset is biased towards lexical overlapping features , after combining with a co-occurrence word count feature , our best model NASM outperforms all the previous models , including both neural network based models and classifiers with a set of hand - crafted features ( e.g. LCLR ) .","[('after combining with', (11, 14)), ('outperforms', (24, 25)), ('including', (30, 31)), ('with', (38, 39)), ('of', (41, 42))]","[('co-occurrence word count feature', (15, 19)), ('our best model', (20, 23)), ('all the previous models', (25, 29)), ('neural network based models', (32, 36)), ('classifiers', (37, 38)), ('set', (40, 41)), ('hand - crafted features', (42, 46))]","[['our best model', 'after combining with', 'co-occurrence word count feature'], ['our best model', 'outperforms', 'all the previous models'], ['all the previous models', 'including', 'neural network based models'], ['all the previous models', 'including', 'classifiers'], ['classifiers', 'with', 'set'], ['set', 'of', 'hand - crafted features']]",[],[],[],[],[],[],"[['Results', 'has', 'our best model']]",[],natural_language_inference,18,188
experiments,"Similarly , on the Wik - iQA dataset , all of our models outperform the previous distributional models by a large margin .","[('on', (2, 3)), ('outperform', (13, 14)), ('by', (18, 19))]","[('Wik - iQA dataset', (4, 8)), ('all of our models', (9, 13)), ('previous distributional models', (15, 18)), ('large margin', (20, 22))]","[['all of our models', 'outperform', 'previous distributional models'], ['previous distributional models', 'by', 'large margin']]","[['Wik - iQA dataset', 'has', 'all of our models']]",[],[],[],"[['Results', 'on', 'Wik - iQA dataset']]",[],[],[],natural_language_inference,18,189
experiments,"By including a word count feature , our models improve further and achieve the state - of - the - art .","[('including', (1, 2)), ('achieve', (12, 13))]","[('word count feature', (3, 6)), ('our models', (7, 9)), ('improve further', (9, 11)), ('state - of - the - art', (14, 21))]","[['our models', 'achieve', 'state - of - the - art']]","[['word count feature', 'has', 'our models'], ['our models', 'has', 'improve further']]",[],[],[],"[['Results', 'including', 'word count feature']]",[],[],[],natural_language_inference,18,190
research-problem,Distance - based Self - Attention Network for Natural Language Inference,[],"[('Natural Language Inference', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,19,2
research-problem,"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data .",[],"[('NLI', (6, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,19,10
research-problem,"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc .",[],"[('Natural Language Inference ( NLI )', (15, 21))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference ( NLI )']]",[],[],[],[],natural_language_inference,19,18
model,"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the relative distance between words .","[('propose', (6, 7)), ('introduces', (13, 14)), ('models', (18, 19)), ('between', (22, 23))]","[('Distancebased Self - Attention Network', (7, 12)), ('distance mask', (15, 17)), ('relative distance', (20, 22)), ('words', (23, 24))]","[['Distancebased Self - Attention Network', 'introduces', 'distance mask'], ['distance mask', 'models', 'relative distance'], ['relative distance', 'between', 'words']]",[],"[['Model', 'propose', 'Distancebased Self - Attention Network']]",[],[],[],[],[],[],natural_language_inference,19,32
model,"In conjunction with a directional mask , the distance mask allows us to incorporate complete positional information of words in our model .","[('In conjunction with', (0, 3)), ('allows us to incorporate', (10, 14)), ('in', (19, 20))]","[('directional mask', (4, 6)), ('distance mask', (8, 10)), ('complete positional information of words', (14, 19)), ('our model', (20, 22))]","[['distance mask', 'allows us to incorporate', 'complete positional information of words'], ['complete positional information of words', 'in', 'our model']]","[['directional mask', 'has', 'distance mask']]","[['Model', 'In conjunction with', 'directional mask']]",[],[],[],[],[],[],natural_language_inference,19,33
experimental-setup,We used the Glove 840B 300D 1 ( d e = 300 ) for the pre-trained word embedding without any finetuning .,"[('used', (1, 2)), ('for', (13, 14)), ('without', (18, 19))]","[('Glove 840B 300D 1 ( d e = 300 )', (3, 13)), ('pre-trained word embedding', (15, 18)), ('finetuning', (20, 21))]","[['Glove 840B 300D 1 ( d e = 300 )', 'for', 'pre-trained word embedding'], ['pre-trained word embedding', 'without', 'finetuning']]",[],"[['Experimental setup', 'used', 'Glove 840B 300D 1 ( d e = 300 )']]",[],[],[],[],[],[],natural_language_inference,19,165
experimental-setup,This is to train the more universally usable sentence encoder .,"[('train', (3, 4))]","[('more universally usable sentence encoder', (5, 10))]",[],[],[],[],[],"[['Glove 840B 300D 1 ( d e = 300 )', 'train', 'more universally usable sentence encoder']]",[],[],[],natural_language_inference,19,166
experimental-setup,"Layer normalization was applied to all linear projections of masked multihead attention , fusion gate , and multi-dimensional attention .","[('applied to', (3, 5)), ('of', (8, 9))]","[('Layer normalization', (0, 2)), ('all linear projections', (5, 8)), ('masked multihead attention', (9, 12)), ('fusion gate', (13, 15)), ('multi-dimensional attention', (17, 19))]","[['Layer normalization', 'applied to', 'all linear projections'], ['all linear projections', 'of', 'masked multihead attention'], ['all linear projections', 'of', 'fusion gate'], ['all linear projections', 'of', 'multi-dimensional attention']]",[],[],"[['Experimental setup', 'has', 'Layer normalization']]",[],[],[],[],[],natural_language_inference,19,167
experimental-setup,"We applied residual dropout as used in , with dropout to the output of masked multi-head attention and SF +H F +b F of fusion gate .","[('applied', (1, 2)), ('with', (8, 9)), ('to', (10, 11)), ('of', (13, 14))]","[('residual dropout', (2, 4)), ('dropout', (9, 10)), ('output', (12, 13)), ('masked multi-head attention', (14, 17)), ('SF +H F +b F of fusion gate', (18, 26))]","[['residual dropout', 'with', 'dropout'], ['dropout', 'to', 'output'], ['output', 'of', 'masked multi-head attention'], ['output', 'of', 'SF +H F +b F of fusion gate']]",[],"[['Experimental setup', 'applied', 'residual dropout']]",[],[],[],[],[],[],natural_language_inference,19,168
experimental-setup,"We set h = 5 , ? = 1.5 in the masked multi-head attention , and the dropout probability was set to 0.1 .",[],[],"[['h = 5 , ? = 1.5', 'in', 'masked multi-head attention']]",[],"[['Experimental setup', 'set', 'h = 5 , ? = 1.5']]",[],[],[],[],[],[],natural_language_inference,19,170
experimental-setup,"Batch size was 64 , and the model was trained with Adam optimizer , with a learning rate of 0.001 .","[('was', (2, 3)), ('trained with', (9, 11)), ('with', (14, 15)), ('of', (18, 19))]","[('Batch size', (0, 2)), ('64', (3, 4)), ('model', (7, 8)), ('Adam optimizer', (11, 13)), ('learning rate', (16, 18)), ('0.001', (19, 20))]","[['Batch size', 'was', '64'], ['model', 'trained with', 'Adam optimizer'], ['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.001']]",[],[],"[['Experimental setup', 'has', 'Batch size'], ['Experimental setup', 'has', 'model']]",[],[],[],[],[],natural_language_inference,19,171
experimental-setup,All models were implemented via Tensorflow on single Nvidia Geforce GTX 1080 Ti GPU .,"[('implemented via', (3, 5)), ('on', (6, 7))]","[('Tensorflow', (5, 6)), ('single Nvidia Geforce GTX 1080 Ti GPU', (7, 14))]","[['Tensorflow', 'on', 'single Nvidia Geforce GTX 1080 Ti GPU']]",[],"[['Experimental setup', 'implemented via', 'Tensorflow']]",[],[],[],[],[],[],natural_language_inference,19,172
results,Experimental results of SNLI data compared with the existing models on the SNLI leader - board 2 are shown in .,"[('of', (2, 3))]","[('SNLI data', (3, 5))]",[],[],"[['Results', 'of', 'SNLI data']]",[],[],[],[],[],[],natural_language_inference,19,174
results,"Compared with the existing state - of - the - art model , the number of parameters and the training time increased , but our results show the new state - of - theart record .","[('Compared with', (0, 2)), ('show', (26, 27))]","[('existing state - of - the - art model', (3, 12)), ('our results', (24, 26)), ('new state - of - theart record', (28, 35))]","[['our results', 'show', 'new state - of - theart record']]","[['existing state - of - the - art model', 'has', 'our results']]",[],[],[],"[['SNLI data', 'Compared with', 'existing state - of - the - art model']]",[],[],[],natural_language_inference,19,175
results,Results show that the addition of the distance mask improved the performance without significantly affecting the training time or increasing the number of parameters . 49.4 50.4 + Unigram and bigram features 99.7 78.2 Sentence encoding - based models 100D LSTM encoders 220 k 84.8 77.6 300D LSTM encoders 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3.0 m 86.2 84.6 600D,"[('show', (1, 2)), ('of', (5, 6)), ('improved', (9, 10)), ('without', (12, 13))]","[('addition', (4, 5)), ('distance mask', (7, 9)), ('performance', (11, 12)), ('significantly affecting', (13, 15)), ('training time', (16, 18)), ('increasing', (19, 20)), ('parameters', (23, 24))]","[['addition', 'of', 'distance mask'], ['distance mask', 'improved', 'performance'], ['performance', 'without', 'significantly affecting'], ['performance', 'without', 'increasing']]","[['significantly affecting', 'has', 'training time'], ['increasing', 'has', 'parameters']]",[],[],[],"[['SNLI data', 'show', 'addition']]",[],[],[],natural_language_inference,19,177
results,"2 The improvement of the test accuracy by introducing the distance mask is only by 0.3 % point , potentially because SNLI data mostly consist of short sentences .","[('of', (3, 4)), ('by introducing', (7, 9)), ('is only by', (12, 15))]","[('improvement', (2, 3)), ('test accuracy', (5, 7)), ('distance mask', (10, 12)), ('0.3 % point', (15, 18))]","[['improvement', 'by introducing', 'distance mask'], ['improvement', 'of', 'test accuracy'], ['test accuracy', 'is only by', '0.3 % point']]",[],[],[],[],[],[],"[['SNLI data', 'has', 'improvement']]",[],natural_language_inference,19,180
results,The results of applying SNLI best model to MultiNLI dataset without additional parameter tuning are presented in .,"[('applying', (3, 4)), ('to', (7, 8))]","[('SNLI best model', (4, 7)), ('MultiNLI dataset', (8, 10))]","[['SNLI best model', 'to', 'MultiNLI dataset']]",[],"[['Results', 'applying', 'SNLI best model']]",[],[],[],[],[],[],natural_language_inference,19,186
results,"Compared with the result of RepEVAL 2017 , we can see that the Distance - based Self - Attention Network performs well .","[('Compared with', (0, 2)), ('see that', (10, 12)), ('performs', (20, 21))]","[('result of RepEVAL 2017', (3, 7)), ('Distance - based Self - Attention Network', (13, 20)), ('well', (21, 22))]","[['result of RepEVAL 2017', 'see that', 'Distance - based Self - Attention Network'], ['Distance - based Self - Attention Network', 'performs', 'well']]",[],[],[],[],"[['MultiNLI dataset', 'Compared with', 'result of RepEVAL 2017']]",[],[],[],natural_language_inference,19,188
results,"When compared with the model of , our model showed similar average test accuracy with much lower number of parameters .","[('with', (2, 3)), ('showed', (9, 10))]","[('our model', (7, 9)), ('similar average test accuracy', (10, 14)), ('much lower number of parameters', (15, 20))]","[['our model', 'showed', 'similar average test accuracy'], ['similar average test accuracy', 'with', 'much lower number of parameters']]",[],[],[],[],[],[],"[['MultiNLI dataset', 'has', 'our model']]",[],natural_language_inference,19,189
research-problem,A Question - Focused Multi- Factor Attention Network for Question Answering,[],"[('Question Answering', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering']]",[],[],[],[],natural_language_inference,2,2
research-problem,Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .,[],"[('question answering ( QA )', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,2,4
research-problem,They also do not explicitly focus on the question and answer type which often plays a critical role in QA .,[],"[('QA', (19, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,2,6
research-problem,"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .",[],"[('machine comprehension - based ( MC ) question answering ( QA )', (1, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension - based ( MC ) question answering ( QA )']]",[],[],[],[],natural_language_inference,2,13
model,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .","[('propose', (5, 6)), ('for', (18, 19)), ('learns to', (29, 31)), ('distributed across', (33, 35)), ('identifies', (38, 39)), ('to help', (43, 45))]","[('end - to - end question - focused multi-factor attention network', (7, 18)), ('document - based question answering', (19, 24)), ('AMANDA', (25, 26)), ('aggregate', (31, 32)), ('evidence', (32, 33)), ('multiple sentences', (35, 37)), ('important question words', (40, 43)), ('extract', (45, 46)), ('answer', (47, 48))]","[['end - to - end question - focused multi-factor attention network', 'learns to', 'aggregate'], ['evidence', 'distributed across', 'multiple sentences'], ['end - to - end question - focused multi-factor attention network', 'identifies', 'important question words'], ['important question words', 'to help', 'extract'], ['end - to - end question - focused multi-factor attention network', 'for', 'document - based question answering']]","[['aggregate', 'has', 'evidence'], ['extract', 'has', 'answer'], ['end - to - end question - focused multi-factor attention network', 'name', 'AMANDA']]","[['Model', 'propose', 'end - to - end question - focused multi-factor attention network']]",[],[],[],[],[],[],natural_language_inference,2,33
model,"Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .","[('extracts', (3, 4)), ('by synthesizing', (8, 10)), ('from', (12, 13)), ('by implicitly determining', (17, 20)), ('during', (24, 25))]","[('AMANDA', (2, 3)), ('answer', (5, 6)), ('relevant facts', (10, 12)), ('passage', (14, 15)), ('suitable answer type', (21, 24)), ('prediction', (25, 26))]","[['AMANDA', 'extracts', 'answer'], ['answer', 'by implicitly determining', 'suitable answer type'], ['suitable answer type', 'during', 'prediction'], ['answer', 'by synthesizing', 'relevant facts'], ['relevant facts', 'from', 'passage']]",[],[],"[['Model', 'has', 'AMANDA']]",[],[],[],[],[],natural_language_inference,2,34
experimental-setup,We tokenize the corpora with NLTK 2 .,"[('tokenize', (1, 2)), ('with', (4, 5))]","[('corpora', (3, 4)), ('NLTK', (5, 6))]","[['corpora', 'with', 'NLTK']]",[],"[['Experimental setup', 'tokenize', 'corpora']]",[],[],[],[],[],[],natural_language_inference,2,174
experimental-setup,"We use the 300 dimension pre-trained word vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and we do not update them during training .","[('use', (1, 2)), ('from', (8, 9))]","[('300 dimension', (3, 5)), ('pre-trained word vectors', (5, 8)), ('GloVe', (9, 10))]","[['pre-trained word vectors', 'from', 'GloVe']]","[['300 dimension', 'has', 'pre-trained word vectors']]","[['Experimental setup', 'use', '300 dimension']]",[],[],[],[],[],[],natural_language_inference,2,175
experimental-setup,The out - of - vocabulary words are initialized with zero vectors .,"[('initialized with', (8, 10))]","[('out - of - vocabulary words', (1, 7)), ('zero vectors', (10, 12))]","[['out - of - vocabulary words', 'initialized with', 'zero vectors']]",[],[],"[['Experimental setup', 'has', 'out - of - vocabulary words']]",[],[],[],[],[],natural_language_inference,2,176
experimental-setup,We use 50 - dimension character - level embedding vectors .,[],"[('50 - dimension', (2, 5)), ('character - level embedding vectors', (5, 10))]",[],"[['50 - dimension', 'has', 'character - level embedding vectors']]",[],"[['Experimental setup', 'use', '50 - dimension']]",[],[],[],[],[],natural_language_inference,2,177
experimental-setup,The number of hidden units in all the LSTMs is 150 .,"[('in', (5, 6)), ('is', (9, 10))]","[('number of hidden units', (1, 5)), ('all the LSTMs', (6, 9)), ('150', (10, 11))]","[['number of hidden units', 'in', 'all the LSTMs'], ['number of hidden units', 'is', '150']]",[],[],"[['Experimental setup', 'has', 'number of hidden units']]",[],[],[],[],[],natural_language_inference,2,178
experimental-setup,We use dropout ) with probability 0.3 for every learnable layer .,"[('with', (4, 5)), ('for', (7, 8))]","[('dropout', (2, 3)), ('probability 0.3', (5, 7)), ('every learnable layer', (8, 11))]","[['dropout', 'with', 'probability 0.3'], ['probability 0.3', 'for', 'every learnable layer']]",[],[],"[['Experimental setup', 'use', 'dropout']]",[],[],[],[],[],natural_language_inference,2,179
experimental-setup,"For multi-factor attentive encoding , we choose 4 factors ( m ) based on our experimental findings ( refer to ) .","[('For', (0, 1)), ('choose', (6, 7))]","[('multi-factor attentive encoding', (1, 4)), ('4 factors', (7, 9))]","[['multi-factor attentive encoding', 'choose', '4 factors']]",[],"[['Experimental setup', 'For', 'multi-factor attentive encoding']]",[],[],[],[],[],[],natural_language_inference,2,180
experimental-setup,"During training , the minibatch size is fixed at 60 .","[('During', (0, 1)), ('fixed at', (7, 9))]","[('training', (1, 2)), ('minibatch size', (4, 6)), ('60', (9, 10))]","[['minibatch size', 'fixed at', '60']]","[['training', 'has', 'minibatch size']]","[['Experimental setup', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,2,181
experimental-setup,We use the Adam optimizer with learning rate of 0.001 and clipnorm of 5 .,[],[],"[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.001'], ['Adam optimizer', 'with', 'clipnorm'], ['clipnorm', 'of', '5']]",[],[],"[['Experimental setup', 'use', 'Adam optimizer']]",[],[],[],[],[],natural_language_inference,2,182
results,shows that AMANDA outperforms all the stateof - the - art models by a significant margin on the New s QA dataset .,"[('shows that', (0, 2)), ('outperforms', (3, 4)), ('by', (12, 13)), ('on', (16, 17))]","[('AMANDA', (2, 3)), ('all the stateof - the - art models', (4, 12)), ('significant margin', (14, 16)), ('New s QA dataset', (18, 22))]","[['AMANDA', 'outperforms', 'all the stateof - the - art models'], ['all the stateof - the - art models', 'by', 'significant margin'], ['all the stateof - the - art models', 'on', 'New s QA dataset']]",[],"[['Results', 'shows that', 'AMANDA']]",[],[],[],[],[],[],natural_language_inference,2,185
results,shows the results on the TriviaQA dataset .,"[('shows', (0, 1)), ('on', (3, 4))]","[('results', (2, 3)), ('TriviaQA dataset', (5, 7))]","[['results', 'on', 'TriviaQA dataset']]",[],"[['Results', 'shows', 'results']]",[],[],[],"[['TriviaQA dataset', 'shows', 'AMANDA']]",[],[],natural_language_inference,2,186
results,shows that AMANDA achieves state - of the - art results in both Wikipedia and Web domain on distantly supervised and verified data .,[],[],"[['AMANDA', 'achieves', 'state - of the - art results'], ['state - of the - art results', 'in', 'Wikipedia and Web domain'], ['Wikipedia and Web domain', 'on', 'distantly supervised and verified data']]",[],[],[],[],[],[],[],[],natural_language_inference,2,191
results,Results on the Search QA dataset are shown in .,"[('on', (1, 2))]","[('Search QA dataset', (3, 6))]",[],[],"[['Results', 'on', 'Search QA dataset']]",[],[],[],[],[],"[['Search QA dataset', 'has', 'AMANDA']]",natural_language_inference,2,193
results,"AMANDA outperforms both systems , especially for multi-word - answer questions by a huge margin .","[('outperforms', (1, 2)), ('especially for', (5, 7)), ('by', (11, 12))]","[('AMANDA', (0, 1)), ('both systems', (2, 4)), ('multi-word - answer questions', (7, 11)), ('huge margin', (13, 15))]","[['AMANDA', 'outperforms', 'both systems'], ['both systems', 'especially for', 'multi-word - answer questions'], ['multi-word - answer questions', 'by', 'huge margin']]",[],[],[],[],[],[],[],[],natural_language_inference,2,200
results,"shows that AMANDA performs better than any of the ablated models which include the ablation of multifactor attentive encoding , max - attentional question aggregation ( q ma ) , and question type representation ( q f ) .","[('performs', (3, 4)), ('than', (5, 6)), ('include', (12, 13)), ('of', (15, 16))]","[('better', (4, 5)), ('any of the ablated models', (6, 11)), ('ablation', (14, 15)), ('multifactor attentive encoding', (16, 19)), ('max - attentional question aggregation ( q ma )', (20, 29)), ('question type representation ( q f )', (31, 38))]","[['better', 'than', 'any of the ablated models'], ['any of the ablated models', 'include', 'ablation'], ['ablation', 'of', 'multifactor attentive encoding'], ['ablation', 'of', 'max - attentional question aggregation ( q ma )'], ['ablation', 'of', 'question type representation ( q f )']]",[],[],[],[],"[['AMANDA', 'performs', 'better']]",[],[],[],natural_language_inference,2,202
research-problem,Sentence Embeddings in NLI with Iterative Refinement Encoders,[],"[('Sentence Embeddings', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Embeddings']]",[],[],[],[],natural_language_inference,20,2
research-problem,Sentence - level representations are necessary for various NLP tasks .,[],"[('Sentence - level representations', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence - level representations']]",[],[],[],[],natural_language_inference,20,4
model,"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .","[('opt for', (14, 16))]","[('sentence encoding approach', (17, 20))]",[],[],"[['Model', 'opt for', 'sentence encoding approach']]",[],[],[],[],[],[],natural_language_inference,20,20
model,Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,[],[],"[['InferSent architecture', 'with', 'hierarchylike structure'], ['hierarchylike structure', 'of', 'bidirectional LSTM ( BiLSTM ) layers'], ['bidirectional LSTM ( BiLSTM ) layers', 'with', 'max pooling']]",[],"[['Model', 'extend', 'InferSent architecture']]",[],[],[],[],[],[],natural_language_inference,20,21
experimental-setup,The architecture was implemented using PyTorch .,"[('implemented using', (3, 5))]","[('architecture', (1, 2)), ('PyTorch', (5, 6))]","[['architecture', 'implemented using', 'PyTorch']]",[],[],"[['Experimental setup', 'has', 'architecture']]",[],[],[],[],[],natural_language_inference,20,71
code,We have published our code in GitHub : https://github.com/Helsinki-NLP/HBMP.,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,20,72
experimental-setup,"For all of our models we used a gradient descent optimization algorithm based on the Adam update rule , which is pre-implemented in PyTorch .","[('used', (6, 7)), ('based on', (12, 14)), ('pre-implemented in', (21, 23))]","[('gradient descent optimization algorithm', (8, 12)), ('Adam update rule', (15, 18)), ('PyTorch', (23, 24))]","[['gradient descent optimization algorithm', 'based on', 'Adam update rule'], ['gradient descent optimization algorithm', 'pre-implemented in', 'PyTorch']]",[],"[['Experimental setup', 'used', 'gradient descent optimization algorithm']]",[],[],[],[],[],[],natural_language_inference,20,73
experimental-setup,We used a learning rate of 5e - 4 for all our models .,"[('of', (5, 6))]","[('learning rate', (3, 5)), ('5e - 4', (6, 9))]","[['learning rate', 'of', '5e - 4']]",[],[],"[['Experimental setup', 'used', 'learning rate']]",[],[],[],[],[],natural_language_inference,20,74
experimental-setup,The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve .,"[('decreased by', (4, 6)), ('of', (8, 9)), ('after', (10, 11)), ('if', (13, 14)), ('not', (17, 18))]","[('factor', (7, 8)), ('0.2', (9, 10)), ('each epoch', (11, 13)), ('model', (15, 16)), ('improve', (18, 19))]","[['factor', 'of', '0.2'], ['factor', 'after', 'each epoch'], ['each epoch', 'if', 'model'], ['model', 'not', 'improve']]",[],[],[],[],"[['learning rate', 'decreased by', 'factor']]",[],[],[],natural_language_inference,20,75
experimental-setup,We used a batch size of 64 .,"[('of', (5, 6))]","[('batch size', (3, 5)), ('64', (6, 7))]","[['batch size', 'of', '64']]",[],[],"[['Experimental setup', 'used', 'batch size']]",[],[],[],[],[],natural_language_inference,20,76
experimental-setup,"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .","[('use', (1, 2)), ('of', (7, 8))]","[('pre-trained Glo Ve word embeddings', (2, 7)), ('size', (8, 9)), ('300 dimensions', (9, 11))]","[['pre-trained Glo Ve word embeddings', 'of', 'size']]","[['size', 'has', '300 dimensions']]","[['Experimental setup', 'use', 'pre-trained Glo Ve word embeddings']]",[],[],[],[],[],[],natural_language_inference,20,79
experimental-setup,"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .",[],[],"[['3 - layer multilayer perceptron ( MLP )', 'have', 'size'], ['size', 'of', '600 dimensions'], ['sentence embeddings', 'have', 'hidden size'], ['hidden size', 'of', '600'], ['600', 'for', 'both direction']]",[],[],"[['Experimental setup', 'has', '3 - layer multilayer perceptron ( MLP )'], ['Experimental setup', 'has', 'sentence embeddings']]",[],[],[],[],[],natural_language_inference,20,80
experimental-setup,We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,"[('of', (4, 5)), ('between', (6, 7))]","[('dropout', (3, 4)), ('0.1', (5, 6)), ('MLP layers', (8, 10))]","[['dropout', 'of', '0.1'], ['0.1', 'between', 'MLP layers']]",[],[],"[['Experimental setup', 'use', 'dropout']]",[],[],[],[],[],natural_language_inference,20,81
experimental-setup,Our models were trained using one NVIDIA Tesla P100 GPU .,"[('trained using', (3, 5))]","[('one NVIDIA Tesla P100 GPU', (5, 10))]",[],[],"[['Experimental setup', 'trained using', 'one NVIDIA Tesla P100 GPU']]",[],[],[],[],[],[],natural_language_inference,20,82
results,It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,"[('in', (10, 11)), ('fares', (14, 15)), ('in comparison to', (16, 19))]","[('clearly outperforms', (1, 3)), ('non-hierarchical BiLSTM models', (6, 9)), ('well', (15, 16)), ('other state of the art architectures', (19, 25)), ('sentence encoding category', (27, 30))]","[['well', 'in comparison to', 'other state of the art architectures'], ['other state of the art architectures', 'in', 'sentence encoding category']]","[['clearly outperforms', 'has', 'non-hierarchical BiLSTM models']]","[['Results', 'fares', 'well']]","[['Results', 'has', 'clearly outperforms']]",[],[],[],[],[],natural_language_inference,20,124
results,"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .","[('close to', (6, 8)), ('on', (14, 15)), ('strong on', (20, 22)), ('of', (30, 31))]","[('current state of the art', (9, 14)), ('SNLI', (15, 16)), ('matched and mismatched test sets', (25, 30)), ('MultiNLI', (31, 32))]","[['current state of the art', 'on', 'SNLI'], ['matched and mismatched test sets', 'of', 'MultiNLI']]",[],"[['Results', 'close to', 'current state of the art'], ['Results', 'strong on', 'matched and mismatched test sets']]",[],[],[],[],[],[],natural_language_inference,20,125
results,"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .","[('on', (2, 3)), ('achieve', (6, 7)), ('with', (13, 14)), ('of', (16, 17))]","[('SciTail', (3, 4)), ('new state of the art', (8, 13)), ('accuracy', (15, 16)), ('86.0 %', (17, 19))]","[['SciTail', 'achieve', 'new state of the art'], ['new state of the art', 'with', 'accuracy'], ['accuracy', 'of', '86.0 %']]",[],"[['Results', 'on', 'SciTail']]",[],[],[],[],[],[],natural_language_inference,20,126
results,"For the SNLI dataset , our model provides the test accuracy of 86.6 % after 4 epochs of training .",[],[],"[['our model', 'provides', 'test accuracy'], ['test accuracy', 'of', '86.6 %'], ['test accuracy', 'after', '4 epochs'], ['4 epochs', 'of', 'training'], ['4 epochs', 'of', 'training']]","[['SNLI dataset', 'has', 'our model']]","[['Results', 'For', 'SNLI dataset']]",[],[],[],[],[],[],natural_language_inference,20,130
results,"For the MultiNLI matched test set ( MultiNLI - m ) our model achieves a test accuracy of 73.7 % after 3 epochs of training , which is 0.8 % points lower than the state of the art 74.5 % by .",[],[],"[['our model', 'achieves', 'test accuracy'], ['test accuracy', 'of', '73.7 %'], ['73.7 %', 'after', '3 epochs'], ['3 epochs', 'of', 'training'], ['73.7 %', 'which is', '0.8 % points lower'], ['0.8 % points lower', 'than', 'state of the art 74.5 %'], ['our model', 'achieves', 'test accuracy'], ['3 epochs', 'of', 'training']]","[['MultiNLI matched test set ( MultiNLI - m )', 'has', 'our model']]",[],"[['Results', 'For', 'MultiNLI matched test set ( MultiNLI - m )']]",[],[],[],[],[],natural_language_inference,20,133
results,"For the mismatched test set ( MultiNLI - mm ) our model achieves a test accuracy of 73.0 % after 3 epochs of training , which is 0.6 % points lower than the state of the art 73.6 % by Chen , Zhu , Ling , Wei , Jiang , and Inkpen ( 2017 b ) .",[],[],"[['test accuracy', 'of', '73.0 %'], ['73.0 %', 'after', '3 epochs'], ['73.0 %', 'which is', '0.6 % points lower'], ['0.6 % points lower', 'than', 'state of the art 73.6 %']]","[['mismatched test set ( MultiNLI - mm )', 'has', 'our model']]",[],"[['Results', 'For', 'mismatched test set ( MultiNLI - mm )']]",[],[],[],[],[],natural_language_inference,20,134
results,"On the SciTail dataset we compared our model also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .","[('On', (0, 1)), ('compared', (5, 6)), ('against', (9, 10))]","[('SciTail dataset', (2, 4)), ('our model', (6, 8)), ('non-sentence embedding - based models', (10, 15))]","[['SciTail dataset', 'compared', 'our model'], ['our model', 'against', 'non-sentence embedding - based models']]",[],"[['Results', 'On', 'SciTail dataset']]",[],[],[],[],[],[],natural_language_inference,20,138
results,"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .",[],[],"[['score', 'of', '86.0 %'], ['86.0 %', 'is', '2.7 % points absolute improvement'], ['2.7 % points absolute improvement', 'on', 'previous published state of the art'], ['86.0 %', 'after', '4 epochs']]",[],[],[],[],"[['SciTail dataset', 'obtain', 'score']]",[],[],[],natural_language_inference,20,139
results,Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,"[('outperforms', (3, 4)), ('which', (8, 9)), ('accuracy', (11, 12))]","[('Our model', (0, 2)), ('In - fer Sent', (4, 8)), ('achieves', (9, 10)), ('85.1 %', (13, 15))]","[['Our model', 'outperforms', 'In - fer Sent'], ['In - fer Sent', 'which', 'achieves'], ['achieves', 'accuracy', '85.1 %']]",[],[],[],[],[],[],"[['SciTail dataset', 'has', 'Our model']]",[],natural_language_inference,20,140
results,The results achieved by our proposed model are significantly higher than the previously published results .,"[('achieved by', (2, 4)), ('are', (7, 8)), ('than', (10, 11))]","[('results', (1, 2)), ('proposed model', (5, 7)), ('significantly higher', (8, 10)), ('previously published results', (12, 15))]","[['results', 'achieved by', 'proposed model'], ['proposed model', 'are', 'significantly higher'], ['significantly higher', 'than', 'previously published results']]",[],[],[],[],[],[],"[['SciTail dataset', 'has', 'results']]",[],natural_language_inference,20,142
research-problem,DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,[],"[('RNN / CNN - Free Language Understanding', (8, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'RNN / CNN - Free Language Understanding']]",[],[],[],[],natural_language_inference,21,2
model,"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .","[('propose', (1, 2)), ('differs from', (7, 9)), ('in', (11, 12))]","[('novel attention mechanism', (3, 6)), ('previous ones', (9, 11)), ('multi-dimensional', (17, 18)), ('directional', (50, 51))]","[['novel attention mechanism', 'differs from', 'previous ones'], ['previous ones', 'in', 'multi-dimensional'], ['previous ones', 'in', 'directional']]",[],"[['Model', 'propose', 'novel attention mechanism']]",[],[],[],[],[],[],natural_language_inference,21,31
model,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .","[('compute', (1, 2)), ('since', (6, 7)), ('in', (9, 10)), ('represented by', (14, 16))]","[('feature - wise attention', (2, 6)), ('each element', (7, 9)), ('sequence', (11, 12)), ('vector', (17, 18))]","[['feature - wise attention', 'since', 'each element'], ['each element', 'represented by', 'vector'], ['each element', 'in', 'sequence']]",[],"[['Model', 'compute', 'feature - wise attention']]",[],[],[],[],[],[],natural_language_inference,21,32
model,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,"[('apply', (1, 2)), ('to', (4, 5)), ('easily encode', (10, 12)), ('such as', (15, 17))]","[('positional masks', (2, 4)), ('attention distribution', (5, 7)), ('prior structure knowledge', (12, 15)), ('temporal order', (17, 19)), ('dependency parsing', (20, 22))]","[['positional masks', 'easily encode', 'prior structure knowledge'], ['prior structure knowledge', 'such as', 'temporal order'], ['prior structure knowledge', 'such as', 'dependency parsing'], ['positional masks', 'to', 'attention distribution']]",[],"[['Model', 'apply', 'positional masks']]",[],[],[],[],[],[],natural_language_inference,21,33
model,"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .","[('build', (2, 3)), ('for', (27, 28))]","[('light - weight and RNN / CNN - free neural network', (4, 15)), ('Directional Self - Attention Network ( DiSAN )', (17, 25)), ('sentence encoding', (28, 30))]","[['light - weight and RNN / CNN - free neural network', 'for', 'sentence encoding']]","[['light - weight and RNN / CNN - free neural network', 'name', 'Directional Self - Attention Network ( DiSAN )']]","[['Model', 'build', 'light - weight and RNN / CNN - free neural network']]",[],[],[],[],[],[],natural_language_inference,21,35
model,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .","[('In', (0, 1)), ('processed by', (7, 9)), ('to model', (18, 20)), ('produce', (23, 24)), ('for', (28, 29))]","[('DiSAN', (1, 2)), ('input sequence', (4, 6)), ('directional ( forward and backward ) self - attentions', (9, 18)), ('context dependency', (20, 22)), ('context - aware representations', (24, 28)), ('all tokens', (29, 31))]","[['input sequence', 'processed by', 'directional ( forward and backward ) self - attentions'], ['directional ( forward and backward ) self - attentions', 'produce', 'context - aware representations'], ['context - aware representations', 'for', 'all tokens'], ['directional ( forward and backward ) self - attentions', 'to model', 'context dependency']]","[['DiSAN', 'has', 'input sequence']]","[['Model', 'In', 'DiSAN']]",[],[],[],[],[],[],natural_language_inference,21,37
model,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .","[('computes', (5, 6)), ('of', (9, 10)), ('passed into', (17, 19)), ('to compute', (24, 26))]","[('multi-dimensional attention', (3, 5)), ('vector representation', (7, 9)), ('entire sequence', (11, 13)), ('classification / regression module', (20, 24)), ('final prediction for a particular task', (27, 33))]","[['multi-dimensional attention', 'computes', 'vector representation'], ['vector representation', 'passed into', 'classification / regression module'], ['classification / regression module', 'to compute', 'final prediction for a particular task'], ['vector representation', 'of', 'entire sequence']]",[],[],[],[],[],[],"[['DiSAN', 'has', 'multi-dimensional attention']]",[],natural_language_inference,21,38
experimental-setup,We use cross-entropy loss plus L2 regularization penalty as optimization objective .,"[('use', (1, 2)), ('plus', (4, 5)), ('as', (8, 9))]","[('cross-entropy loss', (2, 4)), ('L2 regularization penalty', (5, 8)), ('optimization objective', (9, 11))]","[['optimization objective', 'use', 'cross-entropy loss'], ['cross-entropy loss', 'plus', 'L2 regularization penalty']]",[],"[['Experimental setup', 'as', 'optimization objective']]",[],[],[],[],[],"[['cross-entropy loss', 'has', 'minimize']]",natural_language_inference,21,180
experimental-setup,We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 .,"[('by', (3, 4)), ('of', (9, 10)), ('with', (15, 16))]","[('minimize', (1, 2)), ('Adadelta', (4, 5)), ('batch size', (16, 18)), ('64', (19, 20))]","[['minimize', 'with', 'batch size'], ['batch size', 'of', '64'], ['minimize', 'by', 'Adadelta']]",[],[],[],[],[],[],[],[],natural_language_inference,21,181
experimental-setup,Initial learning rate is set to 0.5 .,"[('set to', (4, 6))]","[('Initial learning rate', (0, 3)), ('0.5', (6, 7))]","[['Initial learning rate', 'set to', '0.5']]",[],[],"[['Experimental setup', 'has', 'Initial learning rate']]",[],[],[],[],[],natural_language_inference,21,183
experimental-setup,"All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 .","[('initialized by', (4, 6)), ('initialized with', (13, 15))]","[('weight matrices', (1, 3)), ('Glorot Initialization', (6, 8)), ('biases', (11, 12)), ('0', (15, 16))]","[['biases', 'initialized with', '0'], ['weight matrices', 'initialized by', 'Glorot Initialization']]",[],[],"[['Experimental setup', 'has', 'biases'], ['Experimental setup', 'has', 'weight matrices']]",[],[],[],[],[],natural_language_inference,21,184
experimental-setup,We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors .,"[('initialize', (1, 2)), ('in', (5, 6)), ('by', (7, 8))]","[('word embedding', (3, 5)), ('x', (6, 7)), ('300D Glo Ve 6B pre-trained vectors', (8, 14))]","[['word embedding', 'in', 'x'], ['word embedding', 'by', '300D Glo Ve 6B pre-trained vectors']]",[],"[['Experimental setup', 'initialize', 'word embedding']]",[],[],[],[],[],[],natural_language_inference,21,185
experimental-setup,"The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) .","[('in', (7, 8)), ('by', (13, 14)), ('between', (16, 17))]","[('Out - of - Vocabulary words', (1, 7)), ('training set', (8, 10)), ('randomly initialized', (11, 13)), ('uniform distribution', (14, 16)), ('( ? 0.05 , 0.05 )', (17, 23))]","[['Out - of - Vocabulary words', 'in', 'training set'], ['randomly initialized', 'by', 'uniform distribution'], ['uniform distribution', 'between', '( ? 0.05 , 0.05 )']]","[['Out - of - Vocabulary words', 'has', 'randomly initialized']]",[],"[['Experimental setup', 'has', 'Out - of - Vocabulary words']]",[],[],[],[],[],natural_language_inference,21,186
experimental-setup,We use Dropout ) with keep probability 0.75 for language inference and 0.8 for sentiment analysis .,[],[],"[['Dropout', 'with', 'keep probability'], ['0.75', 'for', 'language inference'], ['0.8', 'for', 'sentiment analysis']]","[['keep probability', 'has', '0.75'], ['keep probability', 'has', '0.8']]","[['Experimental setup', 'use', 'Dropout']]",[],[],[],[],[],[],natural_language_inference,21,188
experimental-setup,"The L2 regularization decay factors ? are 5 10 ?5 and 10 ? 4 for language inference and sentiment analysis , respectively .","[('are', (6, 7)), ('for', (14, 15))]","[('L2 regularization decay factors', (1, 5)), ('5 10 ?5 and 10 ? 4', (7, 14)), ('language inference and sentiment analysis', (15, 20))]","[['L2 regularization decay factors', 'are', '5 10 ?5 and 10 ? 4'], ['5 10 ?5 and 10 ? 4', 'for', 'language inference and sentiment analysis']]",[],[],"[['Experimental setup', 'has', 'L2 regularization decay factors']]",[],[],[],[],[],natural_language_inference,21,189
experimental-setup,Hidden units number d h is set to 300 .,"[('set to', (6, 8))]","[('Hidden units number', (0, 3)), ('d h', (3, 5)), ('300', (8, 9))]","[['d h', 'set to', '300']]","[['Hidden units number', 'has', 'd h']]",[],"[['Experimental setup', 'has', 'Hidden units number']]",[],[],[],[],[],natural_language_inference,21,192
experimental-setup,"Activation functions ? ( ) are ELU ( exponential linear unit ) ( Clevert , Unterthiner , and Hochreiter 2016 ) if not specified .","[('are', (5, 6))]","[('Activation functions', (0, 2)), ('ELU ( exponential linear unit )', (6, 12))]","[['Activation functions', 'are', 'ELU ( exponential linear unit )']]",[],[],"[['Experimental setup', 'has', 'Activation functions']]",[],[],[],[],[],natural_language_inference,21,193
experimental-setup,All models are implemented with TensorFlow 2 and run on sin - 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3 gle Nvidia GTX 1080 Ti graphic card .,"[('implemented with', (3, 5)), ('run on', (8, 10))]","[('TensorFlow 2', (5, 7)), ('Nvidia GTX 1080 Ti graphic card', (73, 79))]",[],[],"[['Experimental setup', 'implemented with', 'TensorFlow 2'], ['Experimental setup', 'run on', 'Nvidia GTX 1080 Ti graphic card']]",[],[],[],[],[],[],natural_language_inference,21,194
results,Natural Language Inference,[],"[('Natural Language Inference', (0, 3))]",[],[],[],"[['Results', 'has', 'Natural Language Inference']]",[],[],[],[],[],natural_language_inference,21,195
results,"Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .","[('Compared to', (0, 2)), ('from', (4, 5)), ('outperforms', (13, 14)), ('improves', (17, 18)), ('achieved by', (24, 26)), ('by', (34, 35)), ('of', (38, 39))]","[('results', (3, 4)), ('official leaderboard of SNLI', (6, 10)), ('DiSAN', (12, 13)), ('previous works', (14, 16)), ('best latest test accuracy', (19, 23)), ('memory - based NSE encoder network', (27, 33)), ('remarkable margin', (36, 38)), ('1.02 %', (39, 41))]","[['results', 'from', 'official leaderboard of SNLI'], ['DiSAN', 'improves', 'best latest test accuracy'], ['best latest test accuracy', 'achieved by', 'memory - based NSE encoder network'], ['best latest test accuracy', 'by', 'remarkable margin'], ['remarkable margin', 'of', '1.02 %'], ['DiSAN', 'outperforms', 'previous works']]","[['official leaderboard of SNLI', 'has', 'DiSAN']]",[],[],[],"[['Natural Language Inference', 'Compared to', 'results']]",[],[],[],natural_language_inference,21,215
results,"DiSAN surpasses the RNN / CNN based models with more complicated architecture and more parameters by large margins , e.g. , + 2.32 % to Bi - LSTM , + 1.42 % to Bi - LSTM with additive attention .","[('surpasses', (1, 2)), ('with', (8, 9)), ('by', (15, 16))]","[('DiSAN', (0, 1)), ('RNN / CNN based models', (3, 8)), ('more complicated architecture', (9, 12)), ('more parameters', (13, 15)), ('large margins', (16, 18))]","[['DiSAN', 'surpasses', 'RNN / CNN based models'], ['RNN / CNN based models', 'with', 'more complicated architecture'], ['RNN / CNN based models', 'with', 'more parameters'], ['RNN / CNN based models', 'by', 'large margins']]",[],[],[],[],[],[],"[['Natural Language Inference', 'has', 'DiSAN']]",[],natural_language_inference,21,216
results,"It even outperforms models with the assistance of a semantic parsing tree , e.g. , + 3.52 % to Tree - based CNN , + 2.42 % to SPINN - PI .","[('outperforms', (2, 3)), ('with', (4, 5)), ('of', (7, 8))]","[('models', (3, 4)), ('assistance', (6, 7)), ('semantic parsing tree', (9, 12))]","[['models', 'with', 'assistance'], ['assistance', 'of', 'semantic parsing tree']]",[],[],[],[],"[['DiSAN', 'outperforms', 'models']]",[],[],[],natural_language_inference,21,217
results,"First , a comparison between the first two models shows that changing token - wise attention to multi-dimensional / feature - wise attention leads to 3.31 % improvement on a word embedding based model .","[('shows', (9, 10)), ('to', (16, 17)), ('leads to', (23, 25)), ('on', (28, 29))]","[('changing', (11, 12)), ('token - wise attention', (12, 16)), ('multi-dimensional / feature - wise attention', (17, 23)), ('3.31 % improvement', (25, 28)), ('word embedding based model', (30, 34))]","[['token - wise attention', 'leads to', '3.31 % improvement'], ['3.31 % improvement', 'on', 'word embedding based model'], ['token - wise attention', 'to', 'multi-dimensional / feature - wise attention']]","[['changing', 'has', 'token - wise attention']]",[],[],[],"[['Natural Language Inference', 'shows', 'changing']]",[],[],[],natural_language_inference,21,219
results,"Also , a comparison between the third baseline and DiSAN shows that DiSAN can substantially outperform multi-head attention by 1.45 % .","[('between', (4, 5)), ('shows', (10, 11)), ('by', (18, 19))]","[('comparison', (3, 4)), ('third baseline and DiSAN', (6, 10)), ('DiSAN', (12, 13)), ('substantially outperform', (14, 16)), ('multi-head attention', (16, 18)), ('1.45 %', (19, 21))]","[['comparison', 'between', 'third baseline and DiSAN'], ['third baseline and DiSAN', 'shows', 'DiSAN'], ['multi-head attention', 'by', '1.45 %']]","[['DiSAN', 'has', 'substantially outperform'], ['substantially outperform', 'has', 'multi-head attention']]",[],[],[],[],[],"[['Natural Language Inference', 'has', 'comparison']]",[],natural_language_inference,21,220
results,"Moreover , a comparison between the forth baseline and DiSAN shows that the DiSA block can even outperform Bi - LSTM layer in context encoding , improving test accuracy by 0.64 % .","[('shows', (10, 11)), ('in', (22, 23)), ('improving', (26, 27)), ('by', (29, 30))]","[('forth baseline and DiSAN', (6, 10)), ('DiSA block', (13, 15)), ('outperform', (17, 18)), ('Bi - LSTM layer', (18, 22)), ('context encoding', (23, 25)), ('test accuracy', (27, 29)), ('0.64 %', (30, 32))]","[['forth baseline and DiSAN', 'shows', 'DiSA block'], ['outperform', 'improving', 'test accuracy'], ['test accuracy', 'by', '0.64 %'], ['Bi - LSTM layer', 'in', 'context encoding']]","[['DiSA block', 'has', 'outperform'], ['outperform', 'has', 'Bi - LSTM layer']]",[],[],[],[],[],"[['comparison', 'between', 'forth baseline and DiSAN']]",[],natural_language_inference,21,221
results,A comparison between the fifth baseline and DiSAN shows that directional self - attention with forward and backward masks ( with temporal order encoded ) can bring 0.96 % improvement .,"[('shows', (8, 9)), ('with', (14, 15)), ('bring', (26, 27))]","[('fifth baseline and DiSAN', (4, 8)), ('directional self - attention', (10, 14)), ('forward and backward masks ( with temporal order encoded )', (15, 25)), ('0.96 % improvement', (27, 30))]","[['fifth baseline and DiSAN', 'shows', 'directional self - attention'], ['directional self - attention', 'with', 'forward and backward masks ( with temporal order encoded )'], ['directional self - attention', 'bring', '0.96 % improvement']]",[],[],[],[],[],[],"[['comparison', 'between', 'fifth baseline and DiSAN']]",[],natural_language_inference,21,222
results,Sentiment Analysis,[],"[('Sentiment Analysis', (0, 2))]",[],[],[],"[['Results', 'has', 'Sentiment Analysis']]",[],[],[],[],"[['Sentiment Analysis', 'has', 'DiSAN']]",natural_language_inference,21,226
results,"To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .","[('improves', (8, 9)), ('given by', (14, 16)), ('by', (20, 21))]","[('DiSAN', (7, 8)), ('last best accuracy', (10, 13)), ('CNN - Tensor', (16, 19)), ('0.52 %', (21, 23))]","[['DiSAN', 'improves', 'last best accuracy'], ['last best accuracy', 'by', '0.52 %'], ['last best accuracy', 'given by', 'CNN - Tensor']]",[],[],[],[],[],[],[],[],natural_language_inference,21,235
results,"Compared to tree - based models with heavy use of the prior structure , e.g. , MV - RNN , RNTN and Tree - LSTM , DiSAN outperforms them by 7.32 % , 6.02 % and 0.72 % , respectively .","[('Compared to', (0, 2)), ('with', (6, 7)), ('of', (9, 10)), ('e.g.', (14, 15)), ('by', (29, 30))]","[('tree - based models', (2, 6)), ('heavy use', (7, 9)), ('prior structure', (11, 13)), ('MV - RNN', (16, 19)), ('RNTN', (20, 21)), ('Tree - LSTM', (22, 25)), ('DiSAN', (26, 27)), ('outperforms', (27, 28)), ('7.32 % , 6.02 % and 0.72 %', (30, 38))]","[['tree - based models', 'with', 'heavy use'], ['heavy use', 'of', 'prior structure'], ['outperforms', 'by', '7.32 % , 6.02 % and 0.72 %'], ['tree - based models', 'e.g.', 'MV - RNN'], ['tree - based models', 'e.g.', 'RNTN'], ['tree - based models', 'e.g.', 'Tree - LSTM']]","[['tree - based models', 'has', 'DiSAN']]",[],[],[],"[['Sentiment Analysis', 'Compared to', 'tree - based models']]",[],[],[],natural_language_inference,21,236
results,"Additionally , DiSAN achieves better performance than CNN - based models .","[('achieves', (3, 4)), ('than', (6, 7))]","[('better performance', (4, 6)), ('CNN - based models', (7, 11))]","[['better performance', 'than', 'CNN - based models']]",[],[],[],[],"[['DiSAN', 'achieves', 'better performance']]",[],[],[],natural_language_inference,21,237
results,"Nonetheless , DiSAN still outperforms these fancy models , such as NCSL ( + 0.62 % ) and LR- Bi- LSTM ( + 1.12 % ) . :","[('such as', (9, 11))]","[('outperforms', (4, 5)), ('NCSL', (11, 12)), ('+ 0.62 %', (13, 16)), ('LR- Bi- LSTM', (18, 21)), ('+ 1.12 %', (22, 25))]","[['outperforms', 'such as', 'LR- Bi- LSTM'], ['outperforms', 'such as', 'NCSL']]","[['LR- Bi- LSTM', 'has', '+ 1.12 %'], ['NCSL', 'has', '+ 0.62 %']]",[],[],[],[],[],"[['DiSAN', 'has', 'outperforms'], ['DiSAN', 'has', 'outperforms']]",[],natural_language_inference,21,240
research-problem,"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context .",[],"[('machine comprehension ( MC )', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension ( MC )']]",[],[],[],[],natural_language_inference,22,4
code,1 The latest results are listed at https://rajpurkar.github.io/SQuAD -explorer/,[],"[('https://rajpurkar.github.io/SQuAD -explorer/', (7, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https://rajpurkar.github.io/SQuAD -explorer/']]",[],[],[],[],natural_language_inference,22,10
research-problem,Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .,[],"[('question answering ( QA )', (11, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,22,14
model,"We propose an extension of BIDAF , called Ruminating Reader , which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer .",[],[],"[['extension', 'called', 'Ruminating Reader'], ['extension', 'of', 'BIDAF'], ['extension', 'uses', 'second pass'], ['second pass', 'of', 'reading and reasoning'], ['reading and reasoning', 'to avoid', 'mistakes'], ['reading and reasoning', 'to ensure', 'effectively use'], ['full context', 'when selecting', 'answer']]","[['effectively use', 'has', 'full context']]","[['Model', 'propose', 'extension']]",[],[],[],[],[],[],natural_language_inference,22,23
model,"In addition to adding a second pass , we also introduce two novel layer types , the ruminate layers , which use gating mechanisms to fuse the obtained from the first and second passes .","[('introduce', (10, 11)), ('use', (21, 22)), ('to fuse', (24, 26))]","[('two novel layer types', (11, 15)), ('ruminate layers', (17, 19)), ('gating mechanisms', (22, 24)), ('first and second passes', (30, 34))]","[['two novel layer types', 'use', 'gating mechanisms'], ['gating mechanisms', 'to fuse', 'first and second passes']]","[['two novel layer types', 'name', 'ruminate layers']]","[['Model', 'introduce', 'two novel layer types']]",[],[],[],[],[],[],natural_language_inference,22,24
model,"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .","[('to penalize', (9, 11)), ('between', (12, 13))]","[('answer-question similarity loss', (6, 9)), ('overlap', (11, 12)), ('question and predicted answer', (13, 17))]","[['answer-question similarity loss', 'to penalize', 'overlap'], ['overlap', 'between', 'question and predicted answer']]",[],[],"[['Model', 'introduce', 'answer-question similarity loss']]",[],[],[],[],[],natural_language_inference,22,26
experimental-setup,"In the character encoding layer , we use 100 filters of width 5 .","[('In', (0, 1)), ('use', (7, 8)), ('of', (10, 11))]","[('character encoding layer', (2, 5)), ('100 filters', (8, 10)), ('width 5', (11, 13))]","[['character encoding layer', 'use', '100 filters'], ['100 filters', 'of', 'width 5']]",[],"[['Experimental setup', 'In', 'character encoding layer']]",[],[],[],[],[],[],natural_language_inference,22,148
experimental-setup,"In the remainder of the model , we set the hidden layer dimension ( d ) to 100 .","[('set', (8, 9)), ('to', (16, 17))]","[('hidden layer dimension ( d )', (10, 16)), ('100', (17, 18))]","[['hidden layer dimension ( d )', 'to', '100']]",[],"[['Experimental setup', 'set', 'hidden layer dimension ( d )']]",[],[],[],[],[],[],natural_language_inference,22,149
experimental-setup,We use pretrained 100D Glo Ve vectors ( 6B - token version ) as word embeddings .,"[('use', (1, 2)), ('as', (13, 14))]","[('pretrained 100D Glo Ve vectors ( 6B - token version )', (2, 13)), ('word embeddings', (14, 16))]","[['pretrained 100D Glo Ve vectors ( 6B - token version )', 'as', 'word embeddings']]",[],"[['Experimental setup', 'use', 'pretrained 100D Glo Ve vectors ( 6B - token version )']]",[],[],[],[],[],[],natural_language_inference,22,150
experimental-setup,"Out - of - vocobulary tokens are represented by an UNK symbol in the word embedding layer , but treated normally by the character embedding layer .","[('represented by', (7, 9)), ('in', (12, 13)), ('treated', (19, 20)), ('by', (21, 22))]","[('Out - of - vocobulary tokens', (0, 6)), ('UNK symbol', (10, 12)), ('word embedding layer', (14, 17)), ('normally', (20, 21)), ('character embedding layer', (23, 26))]","[['Out - of - vocobulary tokens', 'represented by', 'UNK symbol'], ['UNK symbol', 'in', 'word embedding layer'], ['Out - of - vocobulary tokens', 'treated', 'normally'], ['normally', 'by', 'character embedding layer']]",[],[],"[['Experimental setup', 'has', 'Out - of - vocobulary tokens']]",[],[],[],[],[],natural_language_inference,22,151
experimental-setup,"We use the AdaDelta optimizer ( Zeiler , 2012 ) for optimization .","[('for', (10, 11))]","[('AdaDelta optimizer', (3, 5)), ('optimization', (11, 12))]","[['AdaDelta optimizer', 'for', 'optimization']]",[],[],"[['Experimental setup', 'use', 'AdaDelta optimizer']]",[],[],[],[],[],natural_language_inference,22,153
experimental-setup,We selected hyperparameter values through random search .,"[('selected', (1, 2)), ('through', (4, 5))]","[('hyperparameter values', (2, 4)), ('random search', (5, 7))]","[['hyperparameter values', 'through', 'random search']]",[],"[['Experimental setup', 'selected', 'hyperparameter values']]",[],[],[],[],[],[],natural_language_inference,22,154
experimental-setup,Batch size is 30 .,"[('is', (2, 3))]","[('Batch size', (0, 2)), ('30', (3, 4))]","[['Batch size', 'is', '30']]",[],[],"[['Experimental setup', 'has', 'Batch size']]",[],[],[],[],[],natural_language_inference,22,155
experimental-setup,"Learning rate starts at 0.5 , and decreases to 0.2 once the model stops improving .","[('starts at', (2, 4)), ('decreases to', (7, 9))]","[('Learning rate', (0, 2)), ('0.5', (4, 5)), ('0.2', (9, 10)), ('model', (12, 13)), ('stops improving', (13, 15))]","[['Learning rate', 'starts at', '0.5'], ['Learning rate', 'decreases to', '0.2']]","[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]",[],"[['Experimental setup', 'has', 'Learning rate']]",[],[],[],[],[],natural_language_inference,22,156
experimental-setup,"The L2-regularization weight is 1 e - 4 , AQSL weight is 1 and dropout with a drop rate of 0.2 is A typical model run converges in about 40 k steps .",[],[],"[['converges', 'in about', '40 k steps'], ['L2-regularization weight', 'is', '1 e - 4'], ['dropout', 'with', 'drop rate'], ['drop rate', 'of', '0.2'], ['AQSL weight', 'is', '1']]","[['typical model run', 'has', 'converges']]",[],"[['Experimental setup', 'has', 'typical model run'], ['Experimental setup', 'has', 'L2-regularization weight'], ['Experimental setup', 'has', 'dropout'], ['Experimental setup', 'has', 'AQSL weight']]",[],[],[],[],[],natural_language_inference,22,157
experimental-setup,This takes two days using Tensorflow and a single NVIDIA K80 GPU . provide an official evaluation script that allows us to measure F 1 score and EM score by comparing the prediction and ground truth answers .,"[('using', (4, 5))]","[('Tensorflow', (5, 6)), ('single NVIDIA K80 GPU', (8, 12))]",[],[],"[['Experimental setup', 'using', 'Tensorflow'], ['Experimental setup', 'using', 'single NVIDIA K80 GPU']]",[],[],[],[],[],[],natural_language_inference,22,158
results,"At the time of submission , our model is tied in accuracy on the hidden test set with the bestperforming published single model .","[('is', (8, 9)), ('in', (10, 11)), ('on', (12, 13)), ('with', (17, 18))]","[('model', (7, 8)), ('tied', (9, 10)), ('accuracy', (11, 12)), ('hidden test set', (14, 17)), ('bestperforming published single model', (19, 23))]","[['model', 'is', 'tied'], ['tied', 'with', 'bestperforming published single model'], ['tied', 'in', 'accuracy'], ['tied', 'on', 'hidden test set']]",[],[],"[['Results', 'has', 'model']]",[],[],[],[],[],natural_language_inference,22,169
results,We achieve an F 1 score of 79.5 and EM score of 70.6 .,[],[],"[['F 1 score', 'of', '79.5'], ['EM score', 'of', '70.6']]",[],"[['Results', 'achieve', 'F 1 score'], ['Results', 'achieve', 'EM score']]",[],[],[],[],[],[],natural_language_inference,22,170
ablation-analysis,Experiments 3 and 4 show that the two ruminate layers are both important and helpful in contributing performance .,"[('show', (4, 5)), ('are', (10, 11)), ('in contributing', (15, 17))]","[('two ruminate layers', (7, 10)), ('important and helpful', (12, 15)), ('performance', (17, 18))]","[['two ruminate layers', 'are', 'important and helpful'], ['important and helpful', 'in contributing', 'performance']]",[],"[['Ablation analysis', 'show', 'two ruminate layers']]",[],[],[],[],[],[],natural_language_inference,22,180
ablation-analysis,It is worth noting that the BiLSTM in the context ruminate layer contributes substantially to model performance .,"[('worth noting', (2, 4)), ('in', (7, 8)), ('contributes', (12, 13)), ('to', (14, 15))]","[('BiLSTM', (6, 7)), ('context ruminate layer', (9, 12)), ('substantially', (13, 14)), ('model performance', (15, 17))]","[['BiLSTM', 'in', 'context ruminate layer'], ['BiLSTM', 'contributes', 'substantially'], ['substantially', 'to', 'model performance']]",[],"[['Ablation analysis', 'worth noting', 'BiLSTM']]",[],[],[],[],[],[],natural_language_inference,22,181
research-problem,FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION,[],"[('MACHINE COMPREHENSION', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'MACHINE COMPREHENSION']]",[],[],[],[],natural_language_inference,23,2
research-problem,"Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence .",[],"[('Teaching machines to read , process and comprehend text and then answer questions', (0, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Teaching machines to read , process and comprehend text and then answer questions']]",[],[],[],[],natural_language_inference,23,24
research-problem,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,[],"[('machine reading comprehension ( MRC )', (18, 24))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading comprehension ( MRC )']]",[],[],[],[],natural_language_inference,23,29
research-problem,We argue that this hypothesis also holds in language understanding and MRC .,[],"[('language understanding', (8, 10)), ('MRC', (11, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'language understanding'], ['Contribution', 'has research problem', 'MRC']]",[],[],[],[],natural_language_inference,23,35
model,"To alleviate this challenge , we identify an attention scoring function utilizing all layers of representation with less training burden .","[('identify', (6, 7)), ('utilizing', (11, 12)), ('of', (14, 15)), ('with', (16, 17))]","[('attention scoring function', (8, 11)), ('all layers', (12, 14)), ('representation', (15, 16)), ('less training burden', (17, 20))]","[['attention scoring function', 'utilizing', 'all layers'], ['all layers', 'with', 'less training burden'], ['all layers', 'of', 'representation']]",[],"[['Model', 'identify', 'attention scoring function']]",[],[],[],[],[],[],natural_language_inference,23,39
model,This leads to an attention that thoroughly captures the complete information between the question and the context .,"[('leads to', (1, 3)), ('thoroughly captures', (6, 8)), ('between', (11, 12))]","[('attention', (4, 5)), ('complete information', (9, 11)), ('question and the context', (13, 17))]","[['attention', 'thoroughly captures', 'complete information'], ['complete information', 'between', 'question and the context']]",[],"[['Model', 'leads to', 'attention']]",[],[],[],[],[],[],natural_language_inference,23,40
model,"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .","[('With', (0, 1)), ('put forward', (8, 10)), ('to understand', (15, 17)), ('in', (19, 20)), ('exploit it', (24, 26)), ('on', (29, 30))]","[('fully - aware attention', (2, 6)), ('multi -level attention mechanism', (11, 15)), ('information', (18, 19)), ('question', (21, 22)), ('layer by layer', (26, 29)), ('context side', (31, 33))]","[['fully - aware attention', 'put forward', 'multi -level attention mechanism'], ['multi -level attention mechanism', 'to understand', 'information'], ['information', 'in', 'question'], ['information', 'exploit it', 'layer by layer'], ['layer by layer', 'on', 'context side']]",[],"[['Model', 'With', 'fully - aware attention']]",[],[],[],[],[],[],natural_language_inference,23,41
model,"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .","[('integrated into', (5, 7)), ('called', (15, 16))]","[('innovations', (3, 4)), ('new end - to - end structure', (8, 15)), ('FusionNet', (16, 17))]","[['innovations', 'integrated into', 'new end - to - end structure'], ['new end - to - end structure', 'called', 'FusionNet']]",[],[],"[['Model', 'has', 'innovations']]",[],[],[],[],[],natural_language_inference,23,42
results,"From the results , we can see that our models not only perform well on the original SQuAD dataset , but also outperform all previous models by more than 5 % in EM score on the adversarial datasets .",[],[],"[['our models', 'not only perform', 'well'], ['well', 'on', 'original SQuAD dataset'], ['our models', 'also', 'outperform'], ['all previous models', 'by', 'more than 5 %'], ['more than 5 %', 'in', 'EM score'], ['more than 5 %', 'on', 'adversarial datasets']]","[['outperform', 'has', 'all previous models']]","[['Results', 'see that', 'our models']]",[],[],[],[],[],[],natural_language_inference,23,238
results,This shows that FusionNet is better at language understanding of both the context and question .,"[('shows', (1, 2)), ('is', (4, 5)), ('at', (6, 7)), ('of both', (9, 11))]","[('FusionNet', (3, 4)), ('better', (5, 6)), ('language understanding', (7, 9)), ('context and question', (12, 15))]","[['FusionNet', 'is', 'better'], ['better', 'at', 'language understanding'], ['language understanding', 'of both', 'context and question']]",[],"[['Results', 'shows', 'FusionNet']]",[],[],[],[],[],[],natural_language_inference,23,239
research-problem,Stochastic Answer Networks for Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,24,2
research-problem,Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .,[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine reading comprehension ( MRC )']]",[],[],[],[],natural_language_inference,24,8
research-problem,It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .,[],"[('MRC', (6, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'MRC']]",[],[],[],[],natural_language_inference,24,10
model,"In this work , we derive an alternative multi-step reasoning neural network for MRC .","[('derive', (5, 6)), ('for', (12, 13))]","[('alternative multi-step reasoning neural network', (7, 12)), ('MRC', (13, 14))]","[['alternative multi-step reasoning neural network', 'for', 'MRC']]",[],"[['Model', 'derive', 'alternative multi-step reasoning neural network']]",[],[],[],[],[],[],natural_language_inference,24,18
model,"During training , we fix the number of reasoning steps , but perform stochastic dropout on the answer module ( final layer predictions ) .","[('During', (0, 1)), ('fix', (4, 5)), ('perform', (12, 13)), ('on', (15, 16))]","[('training', (1, 2)), ('number of reasoning steps', (6, 10)), ('stochastic dropout', (13, 15)), ('answer module ( final layer predictions )', (17, 24))]","[['training', 'fix', 'number of reasoning steps'], ['training', 'perform', 'stochastic dropout'], ['stochastic dropout', 'on', 'answer module ( final layer predictions )']]",[],"[['Model', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,24,19
model,"During decoding , we generate answers based on the average of predictions in all steps , rather than the final step .","[('generate', (4, 5)), ('based on', (6, 8)), ('of', (10, 11)), ('in', (12, 13)), ('rather than', (16, 18))]","[('decoding', (1, 2)), ('answers', (5, 6)), ('average', (9, 10)), ('predictions', (11, 12)), ('all steps', (13, 15)), ('final step', (19, 21))]","[['decoding', 'generate', 'answers'], ['answers', 'based on', 'average'], ['average', 'of', 'predictions'], ['predictions', 'in', 'all steps'], ['all steps', 'rather than', 'final step']]",[],[],"[['Model', 'During', 'decoding']]",[],[],[],[],[],natural_language_inference,24,20
model,"We call this a stochastic answer network ( SAN ) because the stochastic dropout is applied to the answer module ; albeit simple , this technique significantly improves the robustness and over all accuracy of the model .","[('call', (1, 2))]","[('stochastic answer network ( SAN )', (4, 10))]",[],[],"[['Model', 'call', 'stochastic answer network ( SAN )']]",[],[],[],[],[],[],natural_language_inference,24,21
experimental-setup,"The spaCy tool 2 is used to tokenize the both passages and questions , and generate lemma , part - of - speech and named entity tags .","[('used to', (5, 7)), ('both', (9, 10))]","[('spaCy tool', (1, 3)), ('tokenize', (7, 8)), ('passages and questions', (10, 13)), ('generate', (15, 16)), ('lemma', (16, 17)), ('part - of - speech', (18, 23)), ('named entity tags', (24, 27))]","[['spaCy tool', 'used to', 'generate'], ['spaCy tool', 'used to', 'tokenize'], ['tokenize', 'both', 'passages and questions']]","[['generate', 'has', 'lemma'], ['generate', 'has', 'part - of - speech'], ['generate', 'has', 'named entity tags']]",[],"[['Experimental setup', 'has', 'spaCy tool']]",[],[],[],[],[],natural_language_inference,24,127
experimental-setup,We use 2 - layer BiLSTM with d = 128 hidden units for both passage and question encoding .,"[('use', (1, 2)), ('with', (6, 7)), ('for', (12, 13))]","[('2 - layer BiLSTM', (2, 6)), ('d = 128 hidden units', (7, 12)), ('both passage and question encoding', (13, 18))]","[['2 - layer BiLSTM', 'with', 'd = 128 hidden units'], ['d = 128 hidden units', 'for', 'both passage and question encoding']]",[],"[['Experimental setup', 'use', '2 - layer BiLSTM']]",[],[],[],[],[],[],natural_language_inference,24,128
experimental-setup,The mini-batch size is set to 32 and Adamax is used as our optimizer .,"[('set to', (4, 6)), ('used as', (10, 12))]","[('mini-batch size', (1, 3)), ('32', (6, 7)), ('Adamax', (8, 9)), ('our optimizer', (12, 14))]","[['mini-batch size', 'set to', '32'], ['Adamax', 'used as', 'our optimizer']]",[],[],"[['Experimental setup', 'has', 'mini-batch size'], ['Experimental setup', 'has', 'Adamax']]",[],[],[],[],[],natural_language_inference,24,129
experimental-setup,The learning rate is set to 0.002 at first and decreased by half after every 10 epochs .,"[('set to', (4, 6)), ('at', (7, 8)), ('by', (11, 12)), ('after', (13, 14))]","[('learning rate', (1, 3)), ('0.002', (6, 7)), ('first', (8, 9)), ('decreased', (10, 11)), ('half', (12, 13)), ('every 10 epochs', (14, 17))]","[['learning rate', 'set to', '0.002'], ['0.002', 'at', 'first'], ['decreased', 'by', 'half'], ['half', 'after', 'every 10 epochs']]","[['0.002', 'has', 'decreased']]",[],"[['Experimental setup', 'has', 'learning rate']]",[],[],[],[],[],natural_language_inference,24,130
experimental-setup,"We set the dropout rate for all the hidden units of LSTM , and the answer module output layer to 0.4 .","[('set', (1, 2)), ('for', (5, 6)), ('of', (10, 11)), ('to', (19, 20))]","[('dropout rate', (3, 5)), ('all the hidden units', (6, 10)), ('LSTM', (11, 12)), ('answer module output layer', (15, 19)), ('0.4', (20, 21))]","[['dropout rate', 'to', '0.4'], ['0.4', 'for', 'all the hidden units'], ['all the hidden units', 'of', 'LSTM'], ['all the hidden units', 'of', 'answer module output layer']]",[],"[['Experimental setup', 'set', 'dropout rate']]",[],[],[],[],[],[],natural_language_inference,24,131
experimental-setup,"To prevent degenerate output , we ensure that at least one step in the answer module is active during training .","[('To prevent', (0, 2)), ('ensure that', (6, 8)), ('in', (12, 13)), ('is', (16, 17)), ('during', (18, 19))]","[('degenerate output', (2, 4)), ('at least one step', (8, 12)), ('answer module', (14, 16)), ('active', (17, 18)), ('training', (19, 20))]","[['degenerate output', 'ensure that', 'at least one step'], ['at least one step', 'in', 'answer module'], ['at least one step', 'is', 'active'], ['active', 'during', 'training']]",[],"[['Experimental setup', 'To prevent', 'degenerate output']]",[],[],[],[],[],[],natural_language_inference,24,132
results,"We observe that SAN achieves 76.235 EM and 84.056 F1 , outperforming all other models .","[('observe', (1, 2)), ('achieves', (4, 5))]","[('SAN', (3, 4)), ('76.235 EM', (5, 7)), ('84.056 F1', (8, 10)), ('outperforming', (11, 12)), ('all other models', (12, 15))]","[['SAN', 'achieves', '76.235 EM'], ['SAN', 'achieves', '84.056 F1']]","[['SAN', 'has', 'outperforming'], ['outperforming', 'has', 'all other models']]","[['Results', 'observe', 'SAN']]",[],[],[],[],[],[],natural_language_inference,24,146
results,Standard 1 - step model only achieves 75.139 EM and dynamic steps ( via ReasoNet ) achieves only 75.355 EM .,[],[],"[['Standard 1 - step model', 'achieves', '75.139 EM'], ['dynamic steps ( via ReasoNet )', 'achieves', 'only 75.355 EM']]",[],[],"[['Results', 'has', 'Standard 1 - step model'], ['Results', 'has', 'dynamic steps ( via ReasoNet )']]",[],[],[],[],[],natural_language_inference,24,147
results,"SAN also outperforms a 5 - step memory net with averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .","[('with', (9, 10))]","[('SAN', (0, 1)), ('outperforms', (2, 3)), ('5 - step memory net', (4, 9)), ('averaging', (10, 11))]","[['5 - step memory net', 'with', 'averaging']]","[['SAN', 'has', 'outperforms'], ['outperforms', 'has', '5 - step memory net']]",[],"[['Results', 'has', 'SAN']]",[],[],[],[],[],natural_language_inference,24,148
results,SAN also outperforms the other models in terms of K- best oracle scores .,"[('other', (4, 5)), ('in terms of', (6, 9))]","[('models', (5, 6)), ('K- best oracle scores', (9, 13))]","[['models', 'in terms of', 'K- best oracle scores']]",[],[],[],[],"[['outperforms', 'other', 'models']]",[],[],[],natural_language_inference,24,152
results,We see that SAN is very competitive in both single and ensemble settings ( ranked in second ) despite its simplicity .,"[('see that', (1, 3)), ('is', (4, 5)), ('in', (7, 8)), ('ranked', (14, 15))]","[('SAN', (3, 4)), ('very competitive', (5, 7)), ('both single and ensemble settings', (8, 13)), ('second', (16, 17))]","[['SAN', 'is', 'very competitive'], ['very competitive', 'in', 'both single and ensemble settings'], ['very competitive', 'ranked', 'second']]",[],"[['Results', 'see that', 'SAN']]",[],[],[],[],[],[],natural_language_inference,24,156
research-problem,Contextualized Word Representations for Reading Comprehension,[],"[('Reading Comprehension', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],natural_language_inference,25,2
research-problem,Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .,[],"[('Reading comprehension ( RC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading comprehension ( RC )']]",[],[],[],[],natural_language_inference,25,8
research-problem,"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures .",[],"[('RC', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],natural_language_inference,25,9
model,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .","[('take', (6, 7)), ('carries out', (10, 12)), ('prepend to', (19, 21)), ('that produces', (24, 26)), ('by', (28, 29)), ('between', (31, 32))]","[('model', (8, 9)), ('only basic question - document interaction', (12, 18)), ('module', (23, 24)), ('token embeddings', (26, 28)), ('explicitly gating', (29, 31)), ('contextual and non-contextual representations', (32, 36))]","[['model', 'carries out', 'only basic question - document interaction'], ['only basic question - document interaction', 'prepend to', 'module'], ['module', 'that produces', 'token embeddings'], ['token embeddings', 'by', 'explicitly gating'], ['explicitly gating', 'between', 'contextual and non-contextual representations']]",[],"[['Model', 'take', 'model']]",[],[],[],[],[],[],natural_language_inference,25,14
model,"Motivated by these findings , we turn to a semisupervised setting in which we leverage a language model , pre-trained on large amounts of data , as a sequence encoder which forcibly facilitates context utilization .","[('turn to', (6, 8)), ('leverage', (14, 15)), ('pre-trained on', (19, 21)), ('as', (26, 27)), ('forcibly facilitates', (31, 33))]","[('semisupervised setting', (9, 11)), ('language model', (16, 18)), ('large amounts of data', (21, 25)), ('sequence encoder', (28, 30)), ('context utilization', (33, 35))]","[['semisupervised setting', 'leverage', 'language model'], ['language model', 'pre-trained on', 'large amounts of data'], ['large amounts of data', 'as', 'sequence encoder'], ['sequence encoder', 'forcibly facilitates', 'context utilization']]",[],"[['Model', 'turn to', 'semisupervised setting']]",[],[],[],[],[],[],natural_language_inference,25,16
results,"In we compare these two variants over the development set and observe superior performance by the contextual one , illustrating the benefit of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .","[('observe', (11, 12)), ('by', (14, 15)), ('illustrating', (19, 20)), ('of', (22, 23))]","[('superior performance', (12, 14)), ('contextual one', (16, 18)), ('benefit', (21, 22)), ('contextualization', (23, 24))]","[['superior performance', 'illustrating', 'benefit'], ['benefit', 'of', 'contextualization'], ['superior performance', 'by', 'contextual one']]",[],"[['Results', 'observe', 'superior performance']]",[],[],[],[],[],[],natural_language_inference,25,68
results,"On average , the less frequent a word - type is , the smaller are its gate activations , i.e. , the reembedded representation of a rare word places less weight on its fixed word - embedding and more on its contextual representation , compared to a common word .","[('is', (10, 11)), ('are', (14, 15))]","[('less frequent', (4, 6)), ('word - type', (7, 10)), ('smaller', (13, 14)), ('gate activations', (16, 18))]","[['less frequent', 'is', 'word - type'], ['gate activations', 'are', 'smaller']]","[['word - type', 'has', 'gate activations']]",[],"[['Results', 'has', 'less frequent']]",[],[],[],[],[],natural_language_inference,25,76
results,Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective .,[],[],"[['calculation', 'of', 'token reembeddings'], ['token reembeddings', 'with', 'hidden states'], ['hidden states', 'of', 'strong language model'], ['token reembeddings', 'proves to be', 'highly effective']]",[],"[['Results', 'Supplementing', 'calculation']]",[],[],[],[],[],[],natural_language_inference,25,80
results,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a large language model .","[('observe', (3, 4)), ('with', (7, 8)), ('showing', (13, 14)), ('of', (16, 17)), ('in', (21, 22))]","[('significant improvement', (5, 7)), ('benefit', (15, 16)), ('training', (17, 18)), ('QA model', (19, 21)), ('semisupervised fashion', (23, 25)), ('large language model', (27, 30))]","[['benefit', 'of', 'training'], ['training', 'in', 'semisupervised fashion'], ['semisupervised fashion', 'with', 'large language model'], ['training', 'observe', 'significant improvement']]","[['training', 'has', 'QA model']]","[['Results', 'showing', 'benefit']]",[],[],[],[],[],[],natural_language_inference,25,83
results,"Besides a crosscutting boost in results , we note that the performance due to utilizing the LM hidden states of the first LSTM layer significantly surpasses the other two variants .","[('due to', (12, 14)), ('of', (19, 20))]","[('performance', (11, 12)), ('utilizing', (14, 15)), ('LM hidden states', (16, 19)), ('first LSTM layer', (21, 24)), ('significantly surpasses', (24, 26)), ('other two variants', (27, 30))]","[['performance', 'due to', 'utilizing'], ['LM hidden states', 'of', 'first LSTM layer']]","[['utilizing', 'has', 'LM hidden states'], ['LM hidden states', 'has', 'significantly surpasses'], ['significantly surpasses', 'has', 'other two variants']]",[],"[['Results', 'has', 'performance']]",[],[],[],[],[],natural_language_inference,25,84
hyperparameters,We use pre-trained GloVe embeddings of dimension d w = 300 and produce character - based word representations via dc = 100 convolutional filters over character embeddings as in .,"[('use', (1, 2)), ('of dimension d', (5, 8)), ('produce', (12, 13)), ('via', (18, 19)), ('over', (24, 25))]","[('pre-trained GloVe embeddings', (2, 5)), ('w = 300', (8, 11)), ('character - based word representations', (13, 18)), ('dc = 100', (19, 22)), ('convolutional filters', (22, 24)), ('character embeddings', (25, 27))]","[['pre-trained GloVe embeddings', 'of dimension d', 'w = 300'], ['character - based word representations', 'via', 'dc = 100'], ['convolutional filters', 'over', 'character embeddings']]","[['dc = 100', 'has', 'convolutional filters']]","[['Hyperparameters', 'use', 'pre-trained GloVe embeddings'], ['Hyperparameters', 'produce', 'character - based word representations']]",[],[],[],[],[],[],natural_language_inference,25,90
research-problem,"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks .",[],"[('language representations', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'language representations']]",[],[],[],[],natural_language_inference,26,4
research-problem,"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks .",[],"[('learning universal language representations', (14, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'learning universal language representations']]",[],[],[],[],natural_language_inference,26,12
model,"Thus we are motivated to enrich the sentence contextual semantics in multiple predicate - specific argument sequences by presenting SemBERT : Semantics - aware BERT , which is a fine - tuned BERT with explicit contextual semantic clues .","[('enrich', (5, 6)), ('in', (10, 11)), ('by presenting', (17, 19)), ('is', (27, 28)), ('with', (33, 34))]","[('sentence contextual semantics', (7, 10)), ('multiple predicate - specific argument sequences', (11, 17)), ('SemBERT : Semantics - aware BERT', (19, 25)), ('fine - tuned BERT', (29, 33)), ('explicit contextual semantic clues', (34, 38))]","[['sentence contextual semantics', 'in', 'multiple predicate - specific argument sequences'], ['multiple predicate - specific argument sequences', 'by presenting', 'SemBERT : Semantics - aware BERT'], ['SemBERT : Semantics - aware BERT', 'is', 'fine - tuned BERT'], ['fine - tuned BERT', 'with', 'explicit contextual semantic clues']]",[],"[['Model', 'enrich', 'sentence contextual semantics']]",[],[],[],[],[],[],natural_language_inference,26,26
model,The proposed SemBERT learns the representation in a fine - grained manner and takes both strengths of BERT on plain context representation and explicit semantics for deeper meaning representation .,"[('learns', (3, 4)), ('in', (6, 7)), ('takes', (13, 14)), ('of', (16, 17)), ('on', (18, 19)), ('for', (25, 26))]","[('proposed SemBERT', (1, 3)), ('representation', (5, 6)), ('fine - grained manner', (8, 12)), ('strengths', (15, 16)), ('BERT', (17, 18)), ('plain context representation', (19, 22)), ('explicit semantics', (23, 25)), ('deeper meaning representation', (26, 29))]","[['proposed SemBERT', 'learns', 'representation'], ['representation', 'in', 'fine - grained manner'], ['proposed SemBERT', 'takes', 'strengths'], ['strengths', 'of', 'BERT'], ['BERT', 'on', 'plain context representation'], ['proposed SemBERT', 'takes', 'explicit semantics'], ['explicit semantics', 'for', 'deeper meaning representation']]",[],[],"[['Model', 'has', 'proposed SemBERT']]",[],[],[],[],[],natural_language_inference,26,27
model,Our model consists of three components :,"[('consists of', (2, 4))]","[('three components', (4, 6))]",[],[],"[['Model', 'consists of', 'three components']]",[],[],[],[],[],"[['three components', 'has', 'sequence encoder'], ['three components', 'has', 'an out - ofshelf semantic role labeler'], ['three components', 'has', 'semantic integration component']]",natural_language_inference,26,28
model,1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .,[],[],"[['sequence encoder', 'where', 'pre-trained language model'], ['pre-trained language model', 'used to build', 'representation'], ['representation', 'for input', 'raw texts'], ['sequence encoder', 'where', 'semantic role labels'], ['semantic role labels', 'mapped to', 'embedding'], ['embedding', 'in', 'parallel'], ['an out - ofshelf semantic role labeler', 'to annotate', 'input sentences'], ['input sentences', 'with', 'variety of semantic role labels'], ['semantic integration component', 'to integrate', 'text representation'], ['text representation', 'with', 'contextual explicit semantic embedding'], ['contextual explicit semantic embedding', 'to obtain', 'joint representation'], ['joint representation', 'for', 'downstream tasks']]",[],[],[],[],[],[],[],[],natural_language_inference,26,29
experimental-setup,Our implementation is based on the PyTorch implementation of BERT 6 .,"[('based on', (3, 5)), ('of', (8, 9))]","[('Our implementation', (0, 2)), ('PyTorch implementation', (6, 8)), ('BERT', (9, 10))]","[['Our implementation', 'based on', 'PyTorch implementation'], ['PyTorch implementation', 'of', 'BERT']]",[],[],"[['Experimental setup', 'has', 'Our implementation']]",[],[],[],[],[],natural_language_inference,26,124
experimental-setup,"We use the pre-trained weights of BERT and follow the same fine - tuning procedure as BERT without any modification , and all the layers are tuned with moderate model size increasing , as the extra SRL embedding volume is less than 15 % of the original encoder size .",[],[],"[['pre-trained weights', 'of', 'BERT'], ['same fine - tuning procedure', 'as', 'BERT'], ['all the layers', 'tuned with', 'moderate model size'], ['moderate model size', 'as', 'extra SRL embedding volume'], ['extra SRL embedding volume', 'is', 'less than 15 %'], ['less than 15 %', 'of', 'original encoder size']]","[['BERT', 'has', 'all the layers'], ['moderate model size', 'has', 'increasing']]","[['Experimental setup', 'use', 'pre-trained weights'], ['Experimental setup', 'follow', 'same fine - tuning procedure']]",[],[],[],[],[],[],natural_language_inference,26,125
experimental-setup,"We set the initial learning rate in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .",[],[],"[['initial learning rate', 'with', 'warm - up rate'], ['warm - up rate', 'of', '0.1'], ['initial learning rate', 'with', 'L2 weight decay'], ['L2 weight decay', 'of', '0.01'], ['initial learning rate', 'in', '{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }']]",[],"[['Experimental setup', 'set', 'initial learning rate']]",[],[],[],[],[],[],natural_language_inference,26,126
experimental-setup,"The batch size is selected in { 16 , 24 , 32 } .","[('selected in', (4, 6))]","[('batch size', (1, 3)), ('{ 16 , 24 , 32 }', (6, 13))]","[['batch size', 'selected in', '{ 16 , 24 , 32 }']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,26,127
experimental-setup,"The maximum number of epochs is set in [ 2 , 5 ] depending on tasks .","[('set in', (6, 8)), ('depending on', (13, 15))]","[('maximum number of epochs', (1, 5)), ('[ 2 , 5 ]', (8, 13)), ('tasks', (15, 16))]","[['maximum number of epochs', 'set in', '[ 2 , 5 ]'], ['maximum number of epochs', 'depending on', 'tasks']]",[],[],"[['Experimental setup', 'has', 'maximum number of epochs']]",[],[],[],[],[],natural_language_inference,26,128
experimental-setup,"Texts are tokenized using wordpieces , with maximum length of 384 for SQuAD and 128 or 200 for other tasks .",[],[],"[['Texts', 'with', 'maximum length'], ['maximum length', 'of', '128 or 200'], ['128 or 200', 'for', 'other tasks'], ['maximum length', 'of', '384'], ['384', 'for', 'SQuAD'], ['Texts', 'tokenized using', 'wordpieces']]",[],[],"[['Experimental setup', 'has', 'Texts']]",[],[],[],[],[],natural_language_inference,26,129
experimental-setup,The dimension of SRL embedding is set to 10 .,"[('of', (2, 3)), ('set to', (6, 8))]","[('dimension', (1, 2)), ('SRL embedding', (3, 5)), ('10', (8, 9))]","[['dimension', 'of', 'SRL embedding'], ['SRL embedding', 'set to', '10']]",[],[],"[['Experimental setup', 'has', 'dimension']]",[],[],[],[],[],natural_language_inference,26,130
experimental-setup,The default maximum number of predicateargument structures m is set to 3 .,"[('of', (4, 5)), ('set to', (9, 11))]","[('default maximum number', (1, 4)), ('predicateargument structures m', (5, 8)), ('3', (11, 12))]","[['default maximum number', 'of', 'predicateargument structures m'], ['predicateargument structures m', 'set to', '3']]",[],[],"[['Experimental setup', 'has', 'default maximum number']]",[],[],[],[],[],natural_language_inference,26,131
ablation-analysis,"From the results , we observe that the concatenation would yield an improvement , verifying that integrating contextual semantics would be quite useful for language understanding .","[('observe that', (5, 7)), ('yield', (10, 11)), ('verifying that', (14, 16)), ('be', (20, 21)), ('for', (23, 24))]","[('concatenation', (8, 9)), ('improvement', (12, 13)), ('integrating', (16, 17)), ('contextual semantics', (17, 19)), ('quite useful', (21, 23)), ('language understanding', (24, 26))]","[['concatenation', 'yield', 'improvement'], ['improvement', 'verifying that', 'integrating'], ['integrating', 'be', 'quite useful'], ['quite useful', 'for', 'language understanding']]","[['integrating', 'has', 'contextual semantics']]","[['Ablation analysis', 'observe that', 'concatenation']]",[],[],[],[],[],[],natural_language_inference,26,171
ablation-analysis,"However , SemBERT still outperforms the simple BERT + SRL model just like the latter outperforms the original BERT by a large performance margin , which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time .",[],[],"[['SemBERT', 'shows that', 'SemBERT'], ['SemBERT', 'works', 'more effectively'], ['more effectively', 'for', 'integrating'], ['integrating', 'both', 'plain contextual representation'], ['integrating', 'both', 'contextual semantics']]","[['SemBERT', 'has', 'outperforms'], ['outperforms', 'has', 'simple BERT + SRL model']]",[],"[['Ablation analysis', 'has', 'SemBERT']]",[],[],[],[],[],natural_language_inference,26,172
research-problem,COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION,[],"[('READING COMPRE - HENSION', (9, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'READING COMPRE - HENSION']]",[],[],[],[],natural_language_inference,27,3
research-problem,Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,[],"[('machine reading and question answering ( Q&A )', (6, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading and question answering ( Q&A )']]",[],[],[],[],natural_language_inference,27,5
research-problem,There is growing interest in the tasks of machine reading comprehension and automated question answering .,[],"[('machine reading comprehension', (8, 11)), ('automated question answering', (12, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading comprehension'], ['Contribution', 'has research problem', 'automated question answering']]",[],[],[],[],natural_language_inference,27,14
research-problem,"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .","[('to make', (5, 7)), ('remove', (15, 16))]","[('machine comprehension', (8, 10)), ('fast', (10, 11)), ('recurrent nature of these models', (17, 22))]","[['fast', 'remove', 'recurrent nature of these models']]","[['machine comprehension', 'has', 'fast']]","[['Model', 'to make', 'machine comprehension']]",[],"[['Contribution', 'has research problem', 'machine comprehension']]",[],[],[],[],natural_language_inference,27,21
model,We instead exclusively use convolutions and self - attentions as the building blocks of encoders that separately encodes the query and context .,"[('use', (3, 4)), ('as', (9, 10)), ('of', (13, 14)), ('separately encodes', (16, 18))]","[('convolutions and self - attentions', (4, 9)), ('building blocks', (11, 13)), ('encoders', (14, 15)), ('query and context', (19, 22))]","[['convolutions and self - attentions', 'as', 'building blocks'], ['building blocks', 'separately encodes', 'query and context'], ['building blocks', 'of', 'encoders']]",[],"[['Model', 'use', 'convolutions and self - attentions']]",[],[],[],[],[],[],natural_language_inference,27,22
model,Then we learn the interactions between context and question by standard attentions .,"[('learn', (2, 3)), ('between', (5, 6)), ('by', (9, 10))]","[('interactions', (4, 5)), ('context and question', (6, 9)), ('standard attentions', (10, 12))]","[['interactions', 'by', 'standard attentions'], ['interactions', 'between', 'context and question']]",[],"[['Model', 'learn', 'interactions']]",[],[],[],[],[],[],natural_language_inference,27,23
model,The resulting representation is encoded again with our recurrency - free encoder before finally decoding to the probability of each position being the start or end of the answer span .,[],[],"[['resulting representation', 'is', 'encoded again'], ['encoded again', 'with', 'our recurrency - free encoder'], ['encoded again', 'before', 'decoding'], ['decoding', 'to', 'probability'], ['probability', 'of', 'each position'], ['each position', 'being', 'start or end'], ['start or end', 'of', 'answer span']]",[],[],"[['Model', 'has', 'resulting representation']]",[],[],[],[],[],natural_language_inference,27,24
model,"We call this architecture QANet , which is shown in .","[('call', (1, 2))]","[('architecture QANet', (3, 5))]",[],[],"[['Model', 'call', 'architecture QANet']]",[],[],[],[],[],[],natural_language_inference,27,25
experiments,EXPERIMENTS ON SQUAD,"[('ON', (1, 2))]","[('SQUAD', (2, 3))]",[],[],"[['Experiments', 'ON', 'SQUAD']]",[],[],[],[],[],"[['SQUAD', 'has', 'Experimental setup']]",natural_language_inference,27,182
experiments,We employ two types of standard regularizations .,"[('employ', (1, 2)), ('of', (4, 5))]","[('two types', (2, 4)), ('standard regularizations', (5, 7))]","[['two types', 'of', 'standard regularizations']]",[],[],[],[],"[['Experimental setup', 'employ', 'two types']]",[],[],[],natural_language_inference,27,201
experiments,"First , we use L2 weight decay on all the trainable variables , with parameter ? = 3 10 ?7 .","[('use', (3, 4)), ('on', (7, 8)), ('with parameter', (13, 15))]","[('L2 weight decay', (4, 7)), ('all the trainable variables', (8, 12)), ('? = 3 10 ?7', (15, 20))]","[['L2 weight decay', 'with parameter', '? = 3 10 ?7'], ['L2 weight decay', 'on', 'all the trainable variables']]",[],[],[],[],"[['Experimental setup', 'use', 'L2 weight decay']]",[],[],[],natural_language_inference,27,202
experiments,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .","[('on', (4, 5)), ('where', (13, 14)), ('are', (20, 21)), ('between', (30, 31)), ('is', (34, 35))]","[('dropout', (3, 4)), ('word', (5, 6)), ('character embeddings', (7, 9)), ('between layers', (10, 12)), ('word and character dropout rates', (15, 20)), ('0.1 and 0.05', (21, 24)), ('dropout rate', (28, 30)), ('every two layers', (31, 34)), ('0.1', (35, 36))]","[['dropout', 'where', 'dropout rate'], ['dropout rate', 'between', 'every two layers'], ['every two layers', 'is', '0.1'], ['dropout', 'where', 'word and character dropout rates'], ['word and character dropout rates', 'are', '0.1 and 0.05'], ['dropout', 'on', 'word'], ['dropout', 'on', 'character embeddings'], ['dropout', 'on', 'between layers']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'dropout']]",[],natural_language_inference,27,203
experiments,"We also adopt the stochastic depth method ( layer dropout ) within each embedding or model encoder layer , where sublayer l has survival probability pl = 1 ? l L ( 1 ? p L ) where L is the last layer and p L = 0.9 .",[],[],"[['stochastic depth method ( layer dropout )', 'within', 'each embedding or model encoder layer'], ['each embedding or model encoder layer', 'where', 'sublayer l'], ['survival probability pl = 1 ? l L ( 1 ? p L )', 'where', 'L'], ['L', 'is', 'last layer'], ['survival probability pl = 1 ? l L ( 1 ? p L )', 'where', 'p L'], ['p L', '=', '0.9']]","[['sublayer l', 'has', 'survival probability pl = 1 ? l L ( 1 ? p L )']]",[],[],[],"[['Experimental setup', 'adopt', 'stochastic depth method ( layer dropout )']]",[],[],[],natural_language_inference,27,204
experiments,"The hidden size and the convolution filter number are all 128 , the batch size is 32 , training steps are 150 K for original data , 250 K for "" data augmentation 2 "" , and 340 K for "" data augmentation 3 "" .",[],[],"[['batch size', 'is', '32'], ['hidden size and the convolution filter number', 'are', '128'], ['training steps', 'are', '250 K'], ['250 K', 'for', 'data augmentation 2'], ['training steps', 'are', '150 K'], ['150 K', 'for', 'original data'], ['training steps', 'are', '340 K'], ['340 K', 'for', 'data augmentation 3']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'batch size'], ['Experimental setup', 'has', 'hidden size and the convolution filter number'], ['Experimental setup', 'has', 'training steps']]",[],natural_language_inference,27,205
experiments,"The numbers of convolution layers in the embedding and modeling encoder are 4 and 2 , kernel sizes are 7 and 5 , and the block numbers for the encoders are 1 and 7 , respectively .",[],[],"[['kernel sizes', 'are', '7 and 5'], ['numbers of convolution layers', 'in', 'embedding and modeling encoder'], ['embedding and modeling encoder', 'are', '4 and 2'], ['block numbers', 'for', 'encoders'], ['encoders', 'are', '1 and 7']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'kernel sizes'], ['Experimental setup', 'has', 'numbers of convolution layers'], ['Experimental setup', 'has', 'block numbers']]",[],natural_language_inference,27,206
experiments,"We use the ADAM optimizer ( Kingma & Ba , 2014 ) with ? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7 .","[('with', (12, 13))]","[('ADAM optimizer', (3, 5)), ('? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7', (13, 26))]","[['ADAM optimizer', 'with', '? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'ADAM optimizer']]",[],natural_language_inference,27,207
experiments,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .","[('with', (9, 10)), ('from', (14, 15)), ('to', (16, 17)), ('in', (18, 19)), ('then', (25, 26)), ('for', (31, 32))]","[('learning rate warm - up scheme', (3, 9)), ('inverse exponential', (11, 13)), ('increase', (13, 14)), ('0.0', (15, 16)), ('0.001', (17, 18)), ('first 1000 steps', (20, 23)), ('maintain', (26, 27)), ('constant learning rate', (28, 31)), ('remainder of training', (33, 36))]","[['learning rate warm - up scheme', 'with', 'inverse exponential'], ['increase', 'in', 'first 1000 steps'], ['increase', 'from', '0.0'], ['0.0', 'to', '0.001'], ['increase', 'then', 'maintain'], ['constant learning rate', 'for', 'remainder of training']]","[['inverse exponential', 'has', 'increase'], ['maintain', 'has', 'constant learning rate']]",[],[],[],[],[],"[['Experimental setup', 'use', 'learning rate warm - up scheme']]",[],natural_language_inference,27,208
experiments,Exponential moving average is applied on all trainable variables with a decay rate 0.9999 .,"[('applied on', (4, 6)), ('with', (9, 10))]","[('Exponential moving average', (0, 3)), ('all trainable variables', (6, 9)), ('decay rate 0.9999', (11, 14))]","[['Exponential moving average', 'applied on', 'all trainable variables'], ['all trainable variables', 'with', 'decay rate 0.9999']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'Exponential moving average']]",[],natural_language_inference,27,209
experiments,"Finally , we implement our model in Python using Tensorflow and carry out our experiments on an NVIDIA p 100 GPU .","[('implement', (3, 4)), ('in', (6, 7)), ('using', (8, 9)), ('carry out', (11, 13)), ('on', (15, 16))]","[('our model', (4, 6)), ('Python', (7, 8)), ('Tensorflow', (9, 10)), ('our experiments', (13, 15)), ('NVIDIA p 100 GPU', (17, 21))]","[['our experiments', 'on', 'NVIDIA p 100 GPU'], ['our model', 'using', 'Tensorflow'], ['our model', 'in', 'Python']]",[],[],[],[],"[['Experimental setup', 'carry out', 'our experiments'], ['Experimental setup', 'implement', 'our model']]",[],[],[],natural_language_inference,27,210
experiments,"As can be seen from the table , the accuracy ( EM / F1 ) performance of our model is on par with the state - of - the - art models .","[('of', (16, 17)), ('is', (19, 20)), ('with', (22, 23))]","[('accuracy ( EM / F1 ) performance', (9, 16)), ('our model', (17, 19)), ('on par', (20, 22)), ('state - of - the - art models', (24, 32))]","[['accuracy ( EM / F1 ) performance', 'of', 'our model'], ['accuracy ( EM / F1 ) performance', 'is', 'on par'], ['on par', 'with', 'state - of - the - art models']]",[],[],[],[],[],[],"[['Results', 'has', 'accuracy ( EM / F1 ) performance']]",[],natural_language_inference,27,219
experiments,"In particular , our model trained on the original dataset outperforms all the documented results in the literature , in terms of both EM and F1 scores ( see second column of ) .","[('trained on', (5, 7)), ('in terms of', (19, 22))]","[('our model', (3, 5)), ('original dataset', (8, 10)), ('outperforms', (10, 11)), ('all the documented results in the literature', (11, 18)), ('EM and F1 scores', (23, 27))]","[['our model', 'trained on', 'original dataset'], ['outperforms', 'in terms of', 'EM and F1 scores']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'all the documented results in the literature']]",[],[],[],[],[],"[['Results', 'has', 'our model']]",[],natural_language_inference,27,220
experiments,"When trained with the augmented data with proper sampling scheme , our model can get significant gain 1.5/1.1 on EM / F1 .","[('trained with', (1, 3)), ('with', (6, 7)), ('can get', (13, 15)), ('on', (18, 19))]","[('augmented data', (4, 6)), ('proper sampling scheme', (7, 10)), ('our model', (11, 13)), ('significant gain 1.5/1.1', (15, 18)), ('EM / F1', (19, 22))]","[['augmented data', 'with', 'proper sampling scheme'], ['our model', 'can get', 'significant gain 1.5/1.1'], ['significant gain 1.5/1.1', 'on', 'EM / F1']]","[['augmented data', 'has', 'our model']]",[],[],[],"[['Results', 'trained with', 'augmented data']]",[],[],[],natural_language_inference,27,221
experiments,"Finally , our result on the official test set is 76.2/84.6 , which significantly outperforms the best documented result 73.2/81.8 .","[('on', (4, 5)), ('is', (9, 10)), ('which', (12, 13))]","[('official test set', (6, 9)), ('76.2/84.6', (10, 11)), ('significantly outperforms', (13, 15)), ('best documented result 73.2/81.8', (16, 20))]","[['official test set', 'is', '76.2/84.6'], ['76.2/84.6', 'which', 'significantly outperforms']]","[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]",[],[],[],"[['Results', 'on', 'official test set']]",[],[],[],natural_language_inference,27,222
ablation-analysis,"As can be seen from the table , the use of convolutions in the encoders is crucial : both F1 and EM drop drastically by almost 3 percent if it is removed .","[('use of', (9, 11)), ('in', (12, 13)), ('is', (15, 16)), ('by', (24, 25)), ('if', (28, 29))]","[('convolutions', (11, 12)), ('encoders', (14, 15)), ('crucial', (16, 17)), ('both F1 and EM', (18, 22)), ('drop drastically', (22, 24)), ('almost 3 percent', (25, 28)), ('removed', (31, 32))]","[['convolutions', 'in', 'encoders'], ['convolutions', 'is', 'crucial'], ['drop drastically', 'by', 'almost 3 percent'], ['almost 3 percent', 'if', 'removed']]","[['crucial', 'has', 'both F1 and EM'], ['both F1 and EM', 'has', 'drop drastically']]","[['Ablation analysis', 'use of', 'convolutions']]",[],[],[],[],[],[],natural_language_inference,27,247
ablation-analysis,Self- attention in the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,"[('in', (2, 3)), ('is', (5, 6)), ('that contributes', (10, 12)), ('to', (18, 19))]","[('Self- attention', (0, 2)), ('encoders', (4, 5)), ('necessary component', (8, 10)), ('1.4/1.3 gain of EM / F1', (12, 18)), ('ultimate performance', (20, 22))]","[['Self- attention', 'in', 'encoders'], ['encoders', 'is', 'necessary component'], ['necessary component', 'that contributes', '1.4/1.3 gain of EM / F1'], ['1.4/1.3 gain of EM / F1', 'to', 'ultimate performance']]",[],[],"[['Ablation analysis', 'has', 'Self- attention']]",[],[],[],[],[],natural_language_inference,27,248
ablation-analysis,"As the last block of rows in the table shows , data augmentation proves to be helpful in further boosting performance .","[('in', (6, 7)), ('proves to be', (13, 16))]","[('data augmentation', (11, 13)), ('helpful', (16, 17)), ('further boosting performance', (18, 21))]","[['data augmentation', 'proves to be', 'helpful'], ['helpful', 'in', 'further boosting performance']]",[],[],"[['Ablation analysis', 'has', 'data augmentation']]",[],[],[],[],[],natural_language_inference,27,254
ablation-analysis,"Making the training data twice as large by adding the En - Fr - En data only ( ratio 1:1 between original training data and augmented data , as indicated by row "" data augmentation 2 ( 1:1:0 ) "" ) yields an increase in the F1 by 0.5 percent .","[('Making', (0, 1)), ('by adding', (7, 9)), ('by', (30, 31)), ('yields', (41, 42)), ('in', (44, 45))]","[('training data', (2, 4)), ('twice as large', (4, 7)), ('En - Fr - En data only', (10, 17)), ('increase', (43, 44)), ('F1', (46, 47)), ('0.5 percent', (48, 50))]","[['twice as large', 'by adding', 'En - Fr - En data only'], ['twice as large', 'yields', 'increase'], ['increase', 'in', 'F1'], ['F1', 'by', '0.5 percent']]","[['training data', 'has', 'twice as large']]","[['Ablation analysis', 'Making', 'training data']]",[],[],[],[],[],[],natural_language_inference,27,255
ablation-analysis,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during training can further boost the model performance .","[('observe that', (13, 15)), ('between', (19, 20)), ('during', (25, 26)), ('can further', (27, 29))]","[('good sampling ratio', (16, 19)), ('original and augmented data', (21, 25)), ('training', (26, 27)), ('boost', (29, 30)), ('model performance', (31, 33))]","[['good sampling ratio', 'between', 'original and augmented data'], ['original and augmented data', 'can further', 'boost'], ['original and augmented data', 'during', 'training']]","[['boost', 'has', 'model performance']]","[['Ablation analysis', 'observe that', 'good sampling ratio']]",[],[],[],[],[],[],natural_language_inference,27,259
ablation-analysis,"In particular , when we increase the sampling weight of augmented data from ( 1:1:1 ) to ( 1:2:1 ) , the EM / F1 performance drops by 0.5/0.3 .","[('increase', (5, 6)), ('of', (9, 10)), ('from', (12, 13)), ('to', (16, 17)), ('by', (27, 28))]","[('sampling weight', (7, 9)), ('augmented data', (10, 12)), ('( 1:1:1 )', (13, 16)), ('( 1:2:1 )', (17, 20)), ('EM / F1 performance', (22, 26)), ('drops', (26, 27)), ('0.5/0.3', (28, 29))]","[['sampling weight', 'of', 'augmented data'], ['sampling weight', 'from', '( 1:1:1 )'], ['( 1:1:1 )', 'to', '( 1:2:1 )'], ['drops', 'by', '0.5/0.3']]","[['sampling weight', 'has', 'EM / F1 performance'], ['EM / F1 performance', 'has', 'drops']]","[['Ablation analysis', 'increase', 'sampling weight']]",[],[],[],[],[],[],natural_language_inference,27,260
ablation-analysis,"Empirically , the ratio ( 3:1:1 ) yields the best performance , with 1.5/1.1 gain over the base model on EM / F1 .","[('yields', (7, 8)), ('with', (12, 13)), ('over', (15, 16)), ('on', (19, 20))]","[('ratio ( 3:1:1 )', (3, 7)), ('best performance', (9, 11)), ('1.5/1.1 gain', (13, 15)), ('base model', (17, 19)), ('EM / F1', (20, 23))]","[['ratio ( 3:1:1 )', 'yields', 'best performance'], ['best performance', 'with', '1.5/1.1 gain'], ['1.5/1.1 gain', 'over', 'base model'], ['base model', 'on', 'EM / F1']]",[],[],"[['Ablation analysis', 'has', 'ratio ( 3:1:1 )']]",[],[],[],[],[],natural_language_inference,27,264
research-problem,The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,[],"[('Recurrent Neural Networks ( RNN )', (14, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'Recurrent Neural Networks ( RNN )']]",[],[],[],[],natural_language_inference,28,4
research-problem,"However , RNN still have a limited capacity to manipulate long - term memory .",[],"[('RNN', (2, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'RNN']]",[],[],[],[],natural_language_inference,28,5
model,"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .","[('propose', (3, 4)), ('of', (13, 14))]","[('novel RNN cell', (5, 8)), ('resolves simultaneously', (9, 11)), ('weaknesses', (12, 13)), ('basic RNN', (14, 16))]","[['weaknesses', 'of', 'basic RNN']]","[['novel RNN cell', 'has', 'resolves simultaneously'], ['resolves simultaneously', 'has', 'weaknesses']]","[['Model', 'propose', 'novel RNN cell']]",[],[],[],[],[],[],natural_language_inference,28,27
model,The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,"[('is', (5, 6)), ('whose', (10, 11)), ('acts as', (13, 15))]","[('Rotational Unit of Memory', (1, 5)), ('modified gated model', (7, 10)), ('rotational operation', (11, 13)), ('associative memory', (15, 17)), ('orthogonal matrix', (21, 23))]","[['Rotational Unit of Memory', 'is', 'modified gated model'], ['modified gated model', 'whose', 'rotational operation'], ['rotational operation', 'acts as', 'associative memory'], ['Rotational Unit of Memory', 'is', 'orthogonal matrix']]",[],[],"[['Model', 'has', 'Rotational Unit of Memory']]",[],[],[],[],[],natural_language_inference,28,28
experiments,COPYING MEMORY TASK,[],"[('COPYING MEMORY TASK', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'COPYING MEMORY TASK']]","[['COPYING MEMORY TASK', 'has', 'RUM']]",natural_language_inference,28,146
experiments,"1 . RUM utilizes a different representation of memory that outperforms those of LSTM and GRU ; 2 . RUM solves the task completely , despite its update gate , which does not allow all of the information encoded in the hidden stay to pass through .","[('utilizes', (3, 4)), ('that', (9, 10)), ('those of', (11, 13)), ('solves', (20, 21))]","[('RUM', (2, 3)), ('different representation of memory', (5, 9)), ('outperforms', (10, 11)), ('LSTM and GRU', (13, 16)), ('task', (22, 23)), ('completely', (23, 24))]","[['RUM', 'solves', 'task'], ['RUM', 'utilizes', 'different representation of memory'], ['different representation of memory', 'that', 'outperforms'], ['outperforms', 'those of', 'LSTM and GRU']]","[['task', 'has', 'completely']]",[],[],[],[],[],[],[],natural_language_inference,28,151
experiments,ASSOCIATIVE RECALL TASK,[],"[('ASSOCIATIVE RECALL TASK', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'ASSOCIATIVE RECALL TASK']]","[['ASSOCIATIVE RECALL TASK', 'has', 'Hyperparameters']]",natural_language_inference,28,165
experiments,All the models have the same hidden state N h = 50 for different lengths T .,"[('have', (3, 4)), ('for', (12, 13))]","[('same hidden state N h = 50', (5, 12)), ('different lengths T', (13, 16))]","[['same hidden state N h = 50', 'for', 'different lengths T']]",[],[],[],[],"[['Hyperparameters', 'have', 'same hidden state N h = 50']]",[],[],[],natural_language_inference,28,175
experiments,We use a batch size 128 .,"[('use', (1, 2))]","[('batch size', (3, 5)), ('128', (5, 6))]",[],"[['batch size', 'has', '128']]",[],[],[],"[['Hyperparameters', 'use', 'batch size']]",[],[],[],natural_language_inference,28,176
experiments,The optimizer is RMSProp with a learning rate 0.001 .,"[('is', (2, 3)), ('with', (4, 5))]","[('optimizer', (1, 2)), ('RMSProp', (3, 4)), ('learning rate', (6, 8)), ('0.001', (8, 9))]","[['optimizer', 'is', 'RMSProp'], ['RMSProp', 'with', 'learning rate']]","[['learning rate', 'of', '0.001']]",[],[],[],[],[],"[['Hyperparameters', 'has', 'optimizer']]",[],natural_language_inference,28,177
experiments,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .",[],[],"[['fails', 'to learn', 'task']]","[['LSTM', 'has', 'fails']]",[],[],[],"[['Results', 'find that', 'LSTM']]",[],[],[],natural_language_inference,28,178
experiments,"NTM and Fast - weight RNN fail longer tasks , which means they can not learn to manipulate their memory efficiently .",[],"[('NTM and Fast - weight RNN', (0, 6)), ('fail', (6, 7)), ('longer tasks', (7, 9))]",[],"[['NTM and Fast - weight RNN', 'has', 'fail'], ['fail', 'has', 'longer tasks']]",[],[],[],[],[],"[['Results', 'has', 'NTM and Fast - weight RNN']]",[],natural_language_inference,28,179
experiments,QUESTION ANSWERING,[],"[('QUESTION ANSWERING', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'QUESTION ANSWERING']]","[['QUESTION ANSWERING', 'has', 'Baselines']]",natural_language_inference,28,180
experiments,"We compare our model with several baselines : a simple LSTM , an End - to - end Memory Network ) and a GORU .",[],"[('simple LSTM', (9, 11)), ('End - to - end Memory Network', (13, 20)), ('GORU', (23, 24))]",[],[],[],[],[],[],[],"[['Baselines', 'name', 'simple LSTM'], ['Baselines', 'name', 'End - to - end Memory Network'], ['Baselines', 'name', 'GORU']]",[],natural_language_inference,28,187
experiments,"We find that RUM outperforms significantly LSTM and GORU and achieves competitive result with those of MemN2N , which has an attention mechanism .","[('find that', (1, 3)), ('achieves', (10, 11)), ('with', (13, 14))]","[('RUM', (3, 4)), ('outperforms significantly', (4, 6)), ('LSTM and GORU', (6, 9)), ('competitive result', (11, 13)), ('those of MemN2N', (14, 17)), ('attention mechanism', (21, 23))]","[['RUM', 'achieves', 'competitive result'], ['competitive result', 'with', 'those of MemN2N']]","[['those of MemN2N', 'has', 'attention mechanism'], ['RUM', 'has', 'outperforms significantly'], ['outperforms significantly', 'has', 'LSTM and GORU']]",[],[],[],"[['Results', 'find that', 'RUM']]",[],[],[],natural_language_inference,28,188
experiments,CHARACTER LEVEL LANGUAGE MODELING,[],"[('CHARACTER LEVEL LANGUAGE MODELING', (0, 4))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'CHARACTER LEVEL LANGUAGE MODELING']]","[['CHARACTER LEVEL LANGUAGE MODELING', 'has', 'PENN TREEBANK CORPUS DATA SET']]",natural_language_inference,28,193
experiments,PENN TREEBANK CORPUS DATA SET,[],"[('PENN TREEBANK CORPUS DATA SET', (0, 5))]",[],[],[],[],[],[],[],[],"[['PENN TREEBANK CORPUS DATA SET', 'has', 'Results']]",natural_language_inference,28,196
experiments,"FS - RUM - 2 generalizes better than other gated models , such as GRU and LSTM , because it learns efficient patterns for activation in its kernels .","[('than', (7, 8)), ('such as', (12, 14))]","[('FS - RUM - 2', (0, 5)), ('generalizes better', (5, 7)), ('other gated models', (8, 11)), ('GRU and LSTM', (14, 17))]","[['generalizes better', 'than', 'other gated models'], ['other gated models', 'such as', 'GRU and LSTM']]","[['FS - RUM - 2', 'has', 'generalizes better']]",[],[],[],[],[],"[['Results', 'has', 'FS - RUM - 2']]",[],natural_language_inference,28,215
research-problem,Product - Aware Answer Generation in E - Commerce Question - Answering,[],"[('Product - Aware Answer Generation', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Product - Aware Answer Generation']]",[],[],[],[],natural_language_inference,29,2
research-problem,"In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results .",[],"[('question - answering ( QA )', (8, 14)), ('reading comprehension', (19, 21))]",[],[],[],[],"[['Contribution', 'has research problem', 'question - answering ( QA )'], ['Contribution', 'has research problem', 'reading comprehension']]",[],[],[],[],natural_language_inference,29,14
model,"In this paper , we propose the product - aware answer generator ( PAAG ) , a product related question answering model which incorporates customer reviews with product attributes .","[('propose', (5, 6)), ('incorporates', (23, 24)), ('with', (26, 27))]","[('product - aware answer generator ( PAAG )', (7, 15)), ('product related question answering model', (17, 22)), ('customer reviews', (24, 26)), ('product attributes', (27, 29))]","[['product related question answering model', 'incorporates', 'customer reviews'], ['customer reviews', 'with', 'product attributes']]","[['product - aware answer generator ( PAAG )', 'has', 'product related question answering model']]","[['Model', 'propose', 'product - aware answer generator ( PAAG )']]",[],[],[],[],[],[],natural_language_inference,29,38
model,"Specifically , at the beginning we employ an attention mechanism to model interactions between a question and reviews .","[('employ', (6, 7)), ('to model', (10, 12)), ('between', (13, 14))]","[('attention mechanism', (8, 10)), ('interactions', (12, 13)), ('question and reviews', (15, 18))]","[['attention mechanism', 'to model', 'interactions'], ['interactions', 'between', 'question and reviews']]",[],"[['Model', 'employ', 'attention mechanism']]",[],[],[],[],[],[],natural_language_inference,29,39
model,"Simultaneously , we employ a key - value memory network to store the product attributes and extract the relevance values according to the question .","[('to store', (10, 12)), ('extract', (16, 17)), ('according to', (20, 22))]","[('key - value memory network', (5, 10)), ('product attributes', (13, 15)), ('relevance values', (18, 20)), ('question', (23, 24))]","[['key - value memory network', 'extract', 'relevance values'], ['relevance values', 'according to', 'question'], ['key - value memory network', 'to store', 'product attributes']]",[],[],"[['Model', 'employ', 'key - value memory network']]",[],[],[],[],[],natural_language_inference,29,40
model,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines product - aware review representation and attributes to generate the answer .","[('combines', (15, 16)), ('to generate', (23, 25))]","[('recurrent neural network ( RNN ) based decoder', (5, 13)), ('product - aware review representation and attributes', (16, 23)), ('answer', (26, 27))]","[['recurrent neural network ( RNN ) based decoder', 'combines', 'product - aware review representation and attributes'], ['product - aware review representation and attributes', 'to generate', 'answer']]",[],[],"[['Model', 'propose', 'recurrent neural network ( RNN ) based decoder']]",[],[],[],[],[],natural_language_inference,29,41
model,"More importantly , to tackle the problem of meaningless answers , we propose an adversarial learning mechanism in the loss calculation for optimizing parameters .","[('to tackle', (3, 5)), ('propose', (12, 13)), ('in', (17, 18)), ('for optimizing', (21, 23))]","[('meaningless answers', (8, 10)), ('adversarial learning mechanism', (14, 17)), ('loss calculation', (19, 21)), ('parameters', (23, 24))]","[['meaningless answers', 'propose', 'adversarial learning mechanism'], ['adversarial learning mechanism', 'for optimizing', 'parameters'], ['adversarial learning mechanism', 'in', 'loss calculation']]",[],"[['Model', 'to tackle', 'meaningless answers']]",[],[],[],[],[],[],natural_language_inference,29,42
baselines,( 1 ) S2SA : Sequence - to - sequence framework has been proposed for language generation task .,"[('proposed for', (13, 15))]","[('S2SA', (3, 4)), ('Sequence - to - sequence framework', (5, 11)), ('language generation task', (15, 18))]","[['S2SA', 'proposed for', 'language generation task']]","[['S2SA', 'name', 'Sequence - to - sequence framework']]",[],"[['Baselines', 'has', 'S2SA']]",[],[],[],[],[],natural_language_inference,29,269
baselines,( 2 ) S2SAR : We implement a simple method which can incorporate the review information when generating the answer .,"[('implement', (6, 7)), ('incorporate', (12, 13)), ('when generating', (16, 18))]","[('S2SAR', (3, 4)), ('simple method', (8, 10)), ('review information', (14, 16)), ('answer', (19, 20))]","[['S2SAR', 'implement', 'simple method'], ['simple method', 'incorporate', 'review information'], ['review information', 'when generating', 'answer']]",[],[],"[['Baselines', 'has', 'S2SAR']]",[],[],[],[],[],natural_language_inference,29,272
baselines,( 3 ) SNet : S- Net is a two - stage state - of - the - art model which extracts some text spans from multiple documents context and synthesis the answer from those spans .,[],[],"[['SNet', 'is', 'two - stage state - of - the - art model'], ['two - stage state - of - the - art model', 'synthesis', 'answer'], ['answer', 'from', 'spans'], ['two - stage state - of - the - art model', 'which extracts', 'some text spans'], ['some text spans', 'from', 'multiple documents context']]",[],[],"[['Baselines', 'has', 'SNet']]",[],[],[],[],[],natural_language_inference,29,274
baselines,( 4 ) QS : We implement the query - based summarization model proposed by Hasselqvist et al ..,"[('implement', (6, 7))]","[('QS', (3, 4)), ('query - based summarization model', (8, 13))]","[['QS', 'implement', 'query - based summarization model']]",[],[],"[['Baselines', 'has', 'QS']]",[],[],[],[],[],natural_language_inference,29,277
baselines,( 5 ) BM25 : BM25 is a bag - of - words retrieval function that ranks a set of reviews based on the question terms appearing in each review .,"[('is', (6, 7)), ('ranks', (16, 17)), ('based on', (21, 23)), ('appearing in', (26, 28))]","[('BM25', (3, 4)), ('bag - of - words retrieval function', (8, 15)), ('set of reviews', (18, 21)), ('question terms', (24, 26)), ('each review', (28, 30))]","[['BM25', 'is', 'bag - of - words retrieval function'], ['bag - of - words retrieval function', 'ranks', 'set of reviews'], ['set of reviews', 'based on', 'question terms'], ['question terms', 'appearing in', 'each review']]",[],[],"[['Baselines', 'has', 'BM25']]",[],[],[],[],[],natural_language_inference,29,279
baselines,( 6 ) TF - IDF : Term Frequency - Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review .,"[('is', (13, 14)), ('to reflect', (20, 22)), ('to', (28, 29))]","[('TF - IDF', (3, 6)), ('Term Frequency - Inverse Document Frequency', (7, 13)), ('numerical statistic', (15, 17)), ('how important', (22, 24)), ('question word', (25, 27)), ('review', (30, 31))]","[['TF - IDF', 'is', 'numerical statistic'], ['numerical statistic', 'to reflect', 'how important'], ['question word', 'to', 'review']]","[['TF - IDF', 'name', 'Term Frequency - Inverse Document Frequency'], ['how important', 'has', 'question word']]",[],"[['Baselines', 'has', 'TF - IDF']]",[],[],[],[],[],natural_language_inference,29,281
experimental-setup,"Without using pre-trained embeddings , we randomly initialize the network parameters at the beginning of our experiments .","[('Without using', (0, 2)), ('at', (11, 12)), ('of', (14, 15))]","[('pre-trained embeddings', (2, 4)), ('randomly initialize', (6, 8)), ('network parameters', (9, 11)), ('beginning', (13, 14)), ('experiments', (16, 17))]","[['network parameters', 'at', 'beginning'], ['beginning', 'of', 'experiments'], ['network parameters', 'Without using', 'pre-trained embeddings']]","[['randomly initialize', 'has', 'network parameters']]",[],"[['Experimental setup', 'has', 'randomly initialize']]",[],[],[],[],[],natural_language_inference,29,284
experimental-setup,All the RNN networks have 512 hidden units and the dimension of word embedding is 256 .,"[('have', (4, 5)), ('of', (11, 12)), ('is', (14, 15))]","[('All the RNN networks', (0, 4)), ('512 hidden units', (5, 8)), ('dimension', (10, 11)), ('word embedding', (12, 14)), ('256', (15, 16))]","[['All the RNN networks', 'have', '512 hidden units'], ['dimension', 'of', 'word embedding'], ['word embedding', 'is', '256']]",[],[],"[['Experimental setup', 'has', 'All the RNN networks'], ['Experimental setup', 'has', 'dimension']]",[],[],[],[],[],natural_language_inference,29,285
experimental-setup,"To produce better answers , we use beam search with beam size","[('To produce', (0, 2)), ('use', (6, 7)), ('with', (9, 10))]","[('better answers', (2, 4)), ('beam search', (7, 9)), ('beam size', (10, 12))]","[['better answers', 'use', 'beam search'], ['beam search', 'with', 'beam size']]",[],"[['Experimental setup', 'To produce', 'better answers']]",[],[],[],[],[],"[['beam size', 'has', '4']]",natural_language_inference,29,286
experimental-setup,4 .,[],"[('4', (0, 1))]",[],[],[],[],[],[],[],[],[],natural_language_inference,29,287
experimental-setup,Adagrad with learning rate 0.1 is used to optimize the parameters and batch size is 64 .,"[('with', (1, 2)), ('is', (5, 6)), ('to optimize', (7, 9))]","[('Adagrad', (0, 1)), ('learning rate', (2, 4)), ('0.1', (4, 5)), ('parameters', (10, 11)), ('batch size', (12, 14)), ('64', (15, 16))]","[['Adagrad', 'with', 'learning rate'], ['0.1', 'to optimize', 'parameters'], ['Adagrad', 'with', 'batch size'], ['batch size', 'is', '64']]","[['learning rate', 'has', '0.1']]",[],"[['Experimental setup', 'has', 'Adagrad']]",[],[],[],[],[],natural_language_inference,29,288
experimental-setup,We implement our model using TensorFlow framework and train our model and all baseline models on NVIDIA Tesla P40 GPU .,"[('implement', (1, 2)), ('using', (4, 5)), ('train', (8, 9)), ('on', (15, 16))]","[('our model', (2, 4)), ('TensorFlow framework', (5, 7)), ('our model and all baseline models', (9, 15)), ('NVIDIA Tesla P40 GPU', (16, 20))]","[['our model', 'using', 'TensorFlow framework'], ['our model and all baseline models', 'on', 'NVIDIA Tesla P40 GPU']]",[],"[['Experimental setup', 'implement', 'our model'], ['Experimental setup', 'train', 'our model and all baseline models']]",[],[],[],[],[],[],natural_language_inference,29,289
results,"In these experimental results , we see that PAAG achieves a 111 % , 8 % and 62.73 % increment over the stateof - the - art baseline SNet in terms of BLEU , embedding greedy and consistency score , respectively .","[('see that', (6, 8)), ('achieves', (9, 10)), ('over', (20, 21)), ('in terms of', (29, 32))]","[('PAAG', (8, 9)), ('111 %', (11, 13)), ('8 %', (14, 16)), ('62.73 % increment', (17, 20)), ('stateof - the - art baseline SNet', (22, 29)), ('BLEU', (32, 33)), ('embedding greedy', (34, 36)), ('consistency score', (37, 39))]","[['PAAG', 'over', 'stateof - the - art baseline SNet'], ['stateof - the - art baseline SNet', 'achieves', '111 %'], ['stateof - the - art baseline SNet', 'achieves', '8 %'], ['stateof - the - art baseline SNet', 'achieves', '62.73 % increment'], ['stateof - the - art baseline SNet', 'in terms of', 'BLEU'], ['stateof - the - art baseline SNet', 'in terms of', 'embedding greedy'], ['stateof - the - art baseline SNet', 'in terms of', 'consistency score']]",[],"[['Results', 'see that', 'PAAG']]",[],[],[],[],[],[],natural_language_inference,29,294
results,"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .","[('in', (12, 13)), ('with respect to', (15, 18))]","[('outperforms', (7, 8)), ('all the baseline', (8, 11)), ('significantly', (11, 12)), ('semantic distance', (13, 15)), ('ground truth', (19, 21))]","[['all the baseline', 'in', 'semantic distance'], ['semantic distance', 'with respect to', 'ground truth']]","[['outperforms', 'has', 'all the baseline'], ['all the baseline', 'has', 'significantly']]",[],[],[],[],[],"[['PAAG', 'has', 'outperforms']]",[],natural_language_inference,29,295
results,"In , we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts .","[('in', (11, 12)), ('with', (17, 18))]","[('other baseline models', (8, 11)), ('both sentence fluency and consistency', (12, 17)), ('facts', (19, 20))]","[['other baseline models', 'in', 'both sentence fluency and consistency'], ['both sentence fluency and consistency', 'with', 'facts']]",[],[],[],[],[],[],"[['outperforms', 'has', 'other baseline models']]",[],natural_language_inference,29,299
results,"Although there is a small increment of S2 SAR with respect to S2SA in all metrics , we still find a noticeable gap between S2SAR and PAAG .","[('of', (6, 7)), ('with respect to', (9, 12)), ('in', (13, 14)), ('find', (19, 20)), ('between', (23, 24))]","[('small increment', (4, 6)), ('S2 SAR', (7, 9)), ('S2SA', (12, 13)), ('all metrics', (14, 16)), ('noticeable gap', (21, 23)), ('S2SAR and PAAG', (24, 27))]","[['noticeable gap', 'between', 'S2SAR and PAAG'], ['small increment', 'of', 'S2 SAR'], ['S2 SAR', 'with respect to', 'S2SA'], ['S2SA', 'in', 'all metrics']]",[],"[['Results', 'find', 'noticeable gap']]","[['Results', 'has', 'small increment']]",[],[],[],[],[],natural_language_inference,29,305
ablation-analysis,"There is a slight increment from RAGF to RAGFD , which demonstrates the effectiveness of discriminator .","[('from', (5, 6)), ('to', (7, 8)), ('demonstrates', (11, 12)), ('of', (14, 15))]","[('slight increment', (3, 5)), ('RAGF', (6, 7)), ('RAGFD', (8, 9)), ('effectiveness', (13, 14)), ('discriminator', (15, 16))]","[['slight increment', 'demonstrates', 'effectiveness'], ['effectiveness', 'of', 'discriminator'], ['slight increment', 'from', 'RAGF'], ['RAGF', 'to', 'RAGFD']]",[],[],"[['Ablation analysis', 'has', 'slight increment']]",[],[],[],[],[],natural_language_inference,29,323
ablation-analysis,"From , we find that RAGFWD achieves a 4.3 % improvement over RAGFD in terms of BLEU , and PAAG outperforms RAGFWD 4.1 % in terms of BLEU .",[],[],"[['4.1 %', 'in terms of', 'BLEU'], ['RAGFWD', 'achieves', '4.3 % improvement'], ['4.3 % improvement', 'over', 'RAGFD'], ['4.3 % improvement', 'in terms of', 'BLEU']]","[['PAAG', 'has', 'outperforms'], ['outperforms', 'has', 'RAGFWD'], ['RAGFWD', 'has', '4.1 %']]","[['Ablation analysis', 'find that', 'PAAG'], ['Ablation analysis', 'find that', 'RAGFWD']]",[],[],[],[],[],[],natural_language_inference,29,324
ablation-analysis,"Accordingly , we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty .","[('conclude that', (3, 5)), ('of', (7, 8)), ('benefits from', (9, 11)), ('using', (11, 12)), ('with', (17, 18))]","[('performance', (6, 7)), ('PAAG', (8, 9)), ('Wasserstein distance based adversarial learning', (12, 17)), ('gradient penalty', (18, 20))]","[['performance', 'of', 'PAAG'], ['PAAG', 'benefits from', 'Wasserstein distance based adversarial learning'], ['Wasserstein distance based adversarial learning', 'with', 'gradient penalty']]",[],"[['Ablation analysis', 'conclude that', 'performance']]",[],[],[],[],[],[],natural_language_inference,29,325
ablation-analysis,This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture .,"[('can help', (2, 4)), ('to achieve', (6, 8)), ('than', (11, 12))]","[('our model', (4, 6)), ('better performance', (9, 11)), ('model', (13, 14)), ('vanilla GAN architecture', (16, 19))]","[['our model', 'to achieve', 'better performance'], ['better performance', 'than', 'model']]","[['model', 'using', 'vanilla GAN architecture']]",[],[],[],"[['Wasserstein distance based adversarial learning', 'can help', 'our model']]",[],[],[],natural_language_inference,29,326
research-problem,Modelling Interaction of Sentence Pair with Coupled- LSTMs,[],"[('Modelling Interaction of Sentence Pair', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Modelling Interaction of Sentence Pair']]",[],[],[],[],natural_language_inference,3,2
research-problem,"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .",[],"[('modelling the interactions of two sentences', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'modelling the interactions of two sentences']]",[],[],[],[],natural_language_inference,3,4
research-problem,"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .",[],"[('modelling the relevance / similarity of the sentence pair', (8, 17)), ('text semantic matching', (22, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'modelling the relevance / similarity of the sentence pair'], ['Contribution', 'has research problem', 'text semantic matching']]",[],[],[],[],natural_language_inference,3,12
model,"In this paper , we propose a new deep neural network architecture to model the strong interactions of two sentences .","[('propose', (5, 6)), ('to model', (12, 14)), ('of', (17, 18))]","[('new deep neural network architecture', (7, 12)), ('strong interactions', (15, 17)), ('two sentences', (18, 20))]","[['new deep neural network architecture', 'to model', 'strong interactions'], ['strong interactions', 'of', 'two sentences']]",[],"[['Model', 'propose', 'new deep neural network architecture']]",[],[],[],[],[],[],natural_language_inference,3,28
model,"Different with modelling two sentences with separated LSTMs , we utilize two interdependent LSTMs , called coupled - LSTMs , to fully affect each other at different time steps .","[('utilize', (10, 11)), ('called', (15, 16)), ('to fully affect', (20, 23)), ('at', (25, 26))]","[('two interdependent LSTMs', (11, 14)), ('coupled - LSTMs', (16, 19)), ('each other', (23, 25)), ('different time steps', (26, 29))]","[['two interdependent LSTMs', 'called', 'coupled - LSTMs'], ['two interdependent LSTMs', 'to fully affect', 'each other'], ['each other', 'at', 'different time steps']]",[],"[['Model', 'utilize', 'two interdependent LSTMs']]",[],[],[],[],[],[],natural_language_inference,3,29
model,The output of coupled - LSTMs at each step depends on both sentences .,"[('of', (2, 3)), ('at', (6, 7)), ('depends on', (9, 11))]","[('output', (1, 2)), ('coupled - LSTMs', (3, 6)), ('each step', (7, 9)), ('both sentences', (11, 13))]","[['output', 'of', 'coupled - LSTMs'], ['coupled - LSTMs', 'at', 'each step'], ['coupled - LSTMs', 'depends on', 'both sentences']]",[],[],"[['Model', 'has', 'output']]",[],[],[],[],[],natural_language_inference,3,30
model,"Specifically , we propose two interdependent ways for the coupled - LSTMs : loosely coupled model ( LC - LSTMs ) and tightly coupled model ( TC - LSTMs ) .","[('for', (7, 8))]","[('two interdependent ways', (4, 7)), ('coupled - LSTMs', (9, 12)), ('loosely coupled model ( LC - LSTMs )', (13, 21)), ('tightly coupled model ( TC - LSTMs )', (22, 30))]","[['two interdependent ways', 'for', 'coupled - LSTMs']]","[['coupled - LSTMs', 'name', 'loosely coupled model ( LC - LSTMs )'], ['coupled - LSTMs', 'name', 'tightly coupled model ( TC - LSTMs )']]",[],"[['Model', 'propose', 'two interdependent ways']]",[],[],[],[],[],natural_language_inference,3,31
model,"To utilize all the information of four directions of coupled - LSTMs , we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals .",[],[],"[['all the information', 'adopt', 'dynamic pooling strategy'], ['dynamic pooling strategy', 'to automatically select', 'most informative interaction signals'], ['all the information', 'of', 'four directions'], ['four directions', 'of', 'coupled - LSTMs']]","[['all the information', 'has', 'aggregate']]",[],"[['Model', 'utilize', 'all the information']]",[],[],[],[],[],natural_language_inference,3,33
model,"Finally , we feed them into a fully connected layer , followed by an output layer to compute the matching score .","[('feed them into', (3, 6)), ('followed by', (11, 13)), ('to compute', (16, 18))]","[('fully connected layer', (7, 10)), ('output layer', (14, 16)), ('matching score', (19, 21))]","[['fully connected layer', 'followed by', 'output layer'], ['output layer', 'to compute', 'matching score']]",[],[],[],[],"[['all the information', 'feed them into', 'fully connected layer']]",[],[],[],natural_language_inference,3,34
hyperparameters,"The word embeddings for all of the models are initialized with the 100d GloVe vectors ( 840B token version , ) and fine - tuned during training to improve the performance .","[('for', (3, 4)), ('initialized with', (9, 11)), ('fine - tuned during', (22, 26)), ('to improve', (27, 29))]","[('word embeddings', (1, 3)), ('all of the models', (4, 8)), ('100d GloVe vectors', (12, 15)), ('training', (26, 27)), ('performance', (30, 31))]","[['word embeddings', 'for', 'all of the models'], ['word embeddings', 'fine - tuned during', 'training'], ['training', 'to improve', 'performance'], ['word embeddings', 'initialized with', '100d GloVe vectors']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],natural_language_inference,3,151
hyperparameters,"The other parameters are initialized by randomly sampling from uniform distribution in [ ? 0.1 , 0.1 ] .","[('initialized by', (4, 6)), ('from', (8, 9)), ('in', (11, 12))]","[('other parameters', (1, 3)), ('randomly sampling', (6, 8)), ('uniform distribution', (9, 11)), ('[ ? 0.1 , 0.1 ]', (12, 18))]","[['other parameters', 'initialized by', 'randomly sampling'], ['randomly sampling', 'from', 'uniform distribution'], ['uniform distribution', 'in', '[ ? 0.1 , 0.1 ]']]",[],[],"[['Hyperparameters', 'has', 'other parameters']]",[],[],[],[],[],natural_language_inference,3,152
hyperparameters,"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value","[('For', (0, 1)), ('take', (5, 6)), ('achieve', (9, 10)), ('on', (13, 14)), ('via', (17, 18)), ('over', (22, 23)), ('of', (24, 25))]","[('each task', (1, 3)), ('hyperparameters', (7, 8)), ('best performance', (11, 13)), ('development set', (15, 17)), ('small grid search', (19, 22)), ('combinations', (23, 24)), ('initial learning rate [ 0.05 , 0.0005 , 0.0001 ]', (26, 36)), ('l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]', (37, 53)), ('threshold value', (55, 57))]","[['each task', 'take', 'hyperparameters'], ['hyperparameters', 'achieve', 'best performance'], ['best performance', 'on', 'development set'], ['best performance', 'via', 'small grid search'], ['small grid search', 'over', 'combinations'], ['combinations', 'of', 'initial learning rate [ 0.05 , 0.0005 , 0.0001 ]'], ['combinations', 'of', 'l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]'], ['combinations', 'of', 'threshold value']]",[],"[['Hyperparameters', 'For', 'each task']]",[],[],[],[],[],[],natural_language_inference,3,153
baselines,Neural bag - of - words ( NBOW ) :,[],"[('Neural bag - of - words ( NBOW )', (0, 9))]",[],[],[],"[['Baselines', 'has', 'Neural bag - of - words ( NBOW )']]",[],[],[],[],"[['Neural bag - of - words ( NBOW )', 'has', 'Each sequence']]",natural_language_inference,3,155
baselines,"Each sequence as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","[('sum of', (4, 6)), ('of', (8, 9)), ('to', (20, 21))]","[('Each sequence', (0, 2)), ('embeddings', (7, 8)), ('words it contains', (10, 13)), ('concatenated and fed', (17, 20)), ('MLP', (22, 23))]","[['concatenated and fed', 'to', 'MLP'], ['concatenated and fed', 'to', 'MLP'], ['Each sequence', 'sum of', 'embeddings'], ['embeddings', 'of', 'words it contains']]","[['Each sequence', 'has', 'concatenated and fed']]",[],[],[],[],[],[],[],natural_language_inference,3,156
baselines,"Single LSTM : A single LSTM to encode the two sequences , which is used in .","[('encode', (7, 8))]","[('Single LSTM', (0, 2)), ('two sequences', (9, 11))]","[['Single LSTM', 'encode', 'two sequences']]",[],[],"[['Baselines', 'has', 'Single LSTM']]",[],[],[],[],[],natural_language_inference,3,157
baselines,"Parallel LSTMs : Two sequences are encoded by two LSTMs separately , then they are concatenated and fed to a MLP .",[],[],"[['Two sequences', 'encoded by', 'two LSTMs separately'], ['Two sequences', 'are', 'concatenated and fed']]","[['Parallel LSTMs', 'has', 'Two sequences']]",[],"[['Baselines', 'has', 'Parallel LSTMs']]",[],[],[],[],[],natural_language_inference,3,158
baselines,"Attention LSTMs : An attentive LSTM to encode two sentences into a semantic space , which used in .","[('to encode', (6, 8)), ('into', (10, 11))]","[('Attention LSTMs', (0, 2)), ('attentive LSTM', (4, 6)), ('two sentences', (8, 10)), ('semantic space', (12, 14))]","[['attentive LSTM', 'to encode', 'two sentences'], ['two sentences', 'into', 'semantic space']]","[['Attention LSTMs', 'has', 'attentive LSTM']]",[],"[['Baselines', 'has', 'Attention LSTMs']]",[],[],[],[],[],natural_language_inference,3,159
results,Experiment - I : Recognizing Textual Entailment,[],"[('Experiment - I : Recognizing Textual Entailment', (0, 7))]",[],[],[],"[['Results', 'has', 'Experiment - I : Recognizing Textual Entailment']]",[],[],[],[],"[['Experiment - I : Recognizing Textual Entailment', 'has', 'proposed two C - LSTMs models with four stacked blocks']]",natural_language_inference,3,160
results,"Our proposed two C - LSTMs models with four stacked blocks outperform all the competitor models , which indicates that our thinner and deeper network does work effectively .","[('outperform', (11, 12)), ('indicates that', (18, 20))]","[('proposed two C - LSTMs models with four stacked blocks', (1, 11)), ('all the competitor models', (12, 16)), ('our thinner and deeper network', (20, 25)), ('does work effectively', (25, 28))]","[['proposed two C - LSTMs models with four stacked blocks', 'outperform', 'all the competitor models'], ['proposed two C - LSTMs models with four stacked blocks', 'indicates that', 'our thinner and deeper network']]","[['our thinner and deeper network', 'has', 'does work effectively']]",[],[],[],[],[],[],[],natural_language_inference,3,169
results,"Compared with attention LSTMs , our two models achieve comparable results to them using much fewer parameters ( nearly 1 / 5 ) .","[('Compared with', (0, 2)), ('achieve', (8, 9)), ('using', (13, 14))]","[('attention LSTMs', (2, 4)), ('our two models', (5, 8)), ('comparable results', (9, 11)), ('much fewer parameters ( nearly 1 / 5 )', (14, 23))]","[['our two models', 'using', 'much fewer parameters ( nearly 1 / 5 )'], ['our two models', 'achieve', 'comparable results']]","[['attention LSTMs', 'has', 'our two models']]",[],[],[],"[['Experiment - I : Recognizing Textual Entailment', 'Compared with', 'attention LSTMs']]",[],[],[],natural_language_inference,3,172
results,"By stacking C - LSTMs , the performance of them are improved significantly , and the four stacked TC - LSTMs achieve 85.1 % accuracy on this dataset .","[('By stacking', (0, 2)), ('improved', (11, 12))]","[('C - LSTMs', (2, 5)), ('performance', (7, 8)), ('significantly', (12, 13))]","[['performance', 'improved', 'significantly']]","[['C - LSTMs', 'has', 'performance']]",[],[],[],"[['Experiment - I : Recognizing Textual Entailment', 'By stacking', 'C - LSTMs']]",[],[],[],natural_language_inference,3,173
research-problem,Open Question Answering with Weakly Supervised Embedding Models,[],"[('Open Question Answering', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Open Question Answering']]",[],[],[],[],natural_language_inference,30,2
research-problem,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",[],"[('open - domain question answering', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'open - domain question answering']]",[],[],[],[],natural_language_inference,30,12
research-problem,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,[],"[('Question answering', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question answering']]",[],[],[],[],natural_language_inference,30,16
approach,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",[],[],"[['approach', 'of', 'converting questions'], ['converting questions', 'to', '( uninterpretable ) vectorial representations'], ['( uninterpretable ) vectorial representations', 'can', 'query'], ['query', 'any KB', 'independent'], ['independent', 'of', 'schema'], ['( uninterpretable ) vectorial representations', 'require no', 'pre-defined grammars or lexicons']]",[],"[['Approach', 'take', 'approach']]",[],[],[],[],[],[],natural_language_inference,30,23
approach,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .","[('focus on', (3, 5)), ('on', (9, 10))]","[('answering', (5, 6)), ('simple factual questions', (6, 9)), ('broad range of topics', (11, 15))]","[['simple factual questions', 'on', 'broad range of topics']]","[['answering', 'has', 'simple factual questions']]","[['Approach', 'focus on', 'answering']]",[],[],[],[],[],[],natural_language_inference,30,24
approach,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,[],[],"[['low - dimensional vector embeddings', 'so that', 'representations'], ['representations', 'end up', 'similar'], ['similar', 'in', 'embedding space'], ['representations', 'of', 'questions and corresponding answers'], ['low - dimensional vector embeddings', 'of', 'words'], ['low - dimensional vector embeddings', 'of', 'KB triples']]","[['learning', 'has', 'low - dimensional vector embeddings']]","[['Approach', 'based on', 'learning']]",[],[],[],[],[],[],natural_language_inference,30,31
approach,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .","[('make use of', (21, 24))]","[('weak supervision', (24, 26))]",[],[],"[['Approach', 'make use of', 'weak supervision']]",[],[],[],[],[],[],natural_language_inference,30,33
approach,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,"[('able to', (7, 9)), ('of', (11, 12)), ('by', (16, 17)), ('from', (23, 24)), ('treating this as', (27, 30)), ('with', (39, 40)), ('collaboratively marked as', (45, 48)), ('with no', (50, 52))]","[('model', (5, 6)), ('take advantage', (9, 11)), ('noisy and indirect supervision', (12, 16)), ('automatically generating', (20, 22)), ('questions', (22, 23)), ('KB triples', (24, 26)), ('training data', (30, 32)), ('supplementing', (37, 38)), ('data set of questions', (41, 45)), ('paraphrases', (48, 49)), ('associated answers', (52, 54))]","[['model', 'able to', 'take advantage'], ['take advantage', 'of', 'noisy and indirect supervision'], ['noisy and indirect supervision', 'by', 'automatically generating'], ['questions', 'treating this as', 'training data'], ['questions', 'from', 'KB triples'], ['noisy and indirect supervision', 'by', 'supplementing'], ['supplementing', 'with', 'data set of questions'], ['data set of questions', 'collaboratively marked as', 'paraphrases'], ['data set of questions', 'with no', 'associated answers']]","[['automatically generating', 'has', 'questions']]",[],"[['Approach', 'has', 'model']]",[],[],[],[],[],natural_language_inference,30,34
approach,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,"[('end up learning', (1, 4)), ('for', (7, 8)), ('involving up to', (9, 12)), ('of', (18, 19)), ('with', (24, 25))]","[('meaningful vectorial representations', (4, 7)), ('questions', (8, 9)), ('800 k words', (12, 15)), ('triples', (17, 18)), ('mostly automatically created KB', (20, 24)), ('2.4 M entities', (25, 28)), ('600 k relationships', (29, 32))]","[['meaningful vectorial representations', 'for', 'triples'], ['triples', 'of', 'mostly automatically created KB'], ['mostly automatically created KB', 'with', '2.4 M entities'], ['mostly automatically created KB', 'with', '600 k relationships'], ['meaningful vectorial representations', 'for', 'questions'], ['questions', 'involving up to', '800 k words']]",[],"[['Approach', 'end up learning', 'meaningful vectorial representations']]",[],[],[],[],[],[],natural_language_inference,30,35
results,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .","[('see that', (4, 6)), ('with', (7, 8)), ('is', (10, 11)), ('improves', (14, 15)), ('from', (16, 17)), ('to', (18, 19))]","[('multitasking', (6, 7)), ('paraphrase data', (8, 10)), ('essential', (11, 12)), ('F1', (15, 16)), ('0.60', (17, 18)), ('0.68', (19, 20))]","[['multitasking', 'with', 'paraphrase data'], ['paraphrase data', 'improves', 'F1'], ['F1', 'from', '0.60'], ['0.60', 'to', '0.68'], ['paraphrase data', 'is', 'essential']]",[],"[['Results', 'see that', 'multitasking']]",[],[],[],[],[],[],natural_language_inference,30,201
results,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,[],[],"[['embedding model', 'grants', 'bump'], ['bump', 'of', '5 points'], ['5 points', 'of', 'F1'], ['embedding model', 'is', 'very beneficial'], ['very beneficial', 'to optimize', 'top of the list']]","[['Fine - tuning', 'has', 'embedding model']]",[],"[['Results', 'has', 'Fine - tuning']]",[],[],[],[],[],natural_language_inference,30,207
results,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .","[('improves', (14, 15)), ('by', (19, 20))]","[('All versions of our system', (0, 5)), ('greatly outperform', (5, 7)), ('paralex', (7, 8)), ('fine - tuned model', (10, 14)), ('F1 - score', (16, 19)), ('almost 20 points', (20, 23))]","[['fine - tuned model', 'improves', 'F1 - score'], ['F1 - score', 'by', 'almost 20 points']]","[['All versions of our system', 'has', 'greatly outperform'], ['greatly outperform', 'has', 'paralex'], ['All versions of our system', 'has', 'fine - tuned model']]",[],"[['Results', 'has', 'All versions of our system']]",[],[],[],[],[],natural_language_inference,30,208
results,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .","[('both', (9, 10))]","[('string matching', (3, 5)), ('greatly improves', (5, 7)), ('results', (7, 8)), ('precision and recall', (11, 14)), ('significantly reduces', (17, 19)), ('evaluation time', (19, 21))]","[['greatly improves', 'both', 'precision and recall']]","[['string matching', 'has', 'significantly reduces'], ['significantly reduces', 'has', 'evaluation time'], ['string matching', 'has', 'greatly improves'], ['greatly improves', 'has', 'results']]",[],"[['Results', 'has', 'string matching']]",[],[],[],[],[],natural_language_inference,30,231
results,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .","[('obtained by', (3, 5)), ('is', (10, 11)), ('then', (13, 14)), ('of', (16, 17)), ('in', (18, 19))]","[('final F1', (1, 3)), ('our fine - tuned model', (5, 10)), ('even better', (11, 13)), ('result', (15, 16)), ('paralex', (17, 18)), ('reranking', (19, 20))]","[['final F1', 'obtained by', 'our fine - tuned model'], ['our fine - tuned model', 'is', 'even better'], ['even better', 'in', 'reranking'], ['even better', 'then', 'result'], ['result', 'of', 'paralex']]",[],[],"[['Results', 'has', 'final F1']]",[],[],[],[],[],natural_language_inference,30,232
research-problem,Simple and Effective Text Matching with Richer Alignment Features,[],"[('Text Matching', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text Matching']]",[],[],[],[],natural_language_inference,31,2
model,"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .","[('presents', (2, 3)), ('with', (11, 12)), ('for', (15, 16))]","[('RE2', (3, 4)), ('fast and strong neural architecture', (6, 11)), ('multiple alignment processes', (12, 15)), ('general purpose text matching', (16, 20))]","[['fast and strong neural architecture', 'with', 'multiple alignment processes'], ['fast and strong neural architecture', 'for', 'general purpose text matching']]","[['RE2', 'has', 'fast and strong neural architecture']]","[['Model', 'presents', 'RE2']]",[],[],[],[],[],[],natural_language_inference,31,21
model,"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",[],"[('components', (1, 2)), ('previous aligned features', (11, 14)), ('Residual vectors', (15, 17)), ('original point - wise features', (19, 24)), ('Embedding vectors', (25, 27)), ('contextual features', (30, 32)), ('Encoded vectors', (33, 35))]",[],"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]",[],"[['Model', 'has', 'components']]",[],[],[],[],[],natural_language_inference,31,25
model,An embedding layer first embeds discrete tokens .,"[('embeds', (4, 5))]","[('embedding layer', (1, 3)), ('discrete tokens', (5, 7))]","[['embedding layer', 'embeds', 'discrete tokens']]",[],[],"[['Model', 'has', 'embedding layer']]",[],[],[],[],[],natural_language_inference,31,28
model,"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .","[('consisting of', (5, 7)), ('process', (14, 15))]","[('same - structured blocks', (1, 5)), ('encoding', (7, 8)), ('alignment', (9, 10)), ('fusion layers', (11, 13)), ('sequences', (16, 17)), ('consecutively', (17, 18))]","[['same - structured blocks', 'process', 'sequences'], ['same - structured blocks', 'consisting of', 'encoding'], ['same - structured blocks', 'consisting of', 'alignment'], ['same - structured blocks', 'consisting of', 'fusion layers']]","[['sequences', 'has', 'consecutively']]",[],"[['Model', 'has', 'same - structured blocks']]",[],[],[],[],[],natural_language_inference,31,29
model,These blocks are connected by an augmented version of residual connections ( see section 2.1 ) .,"[('connected by', (3, 5))]","[('augmented version of residual connections', (6, 11))]",[],[],[],[],[],"[['same - structured blocks', 'connected by', 'augmented version of residual connections']]",[],[],[],natural_language_inference,31,30
model,A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction .,"[('aggregates', (3, 4)), ('into', (6, 7)), ('processed by', (11, 13)), ('to give', (16, 18))]","[('pooling layer', (1, 3)), ('sequential representations', (4, 6)), ('vectors', (7, 8)), ('prediction layer', (14, 16)), ('final prediction', (19, 21))]","[['pooling layer', 'aggregates', 'sequential representations'], ['sequential representations', 'into', 'vectors'], ['vectors', 'processed by', 'prediction layer'], ['prediction layer', 'to give', 'final prediction']]",[],[],"[['Model', 'has', 'pooling layer']]",[],[],[],[],[],natural_language_inference,31,31
model,"The implementation of each layer is kept as simple as possible , and the whole model , as a well - organized combination , is quite powerful and lightweight at the same time .","[('of', (2, 3)), ('kept', (6, 7))]","[('implementation', (1, 2)), ('each layer', (3, 5)), ('as simple as possible', (7, 11))]","[['implementation', 'of', 'each layer'], ['each layer', 'kept', 'as simple as possible']]",[],[],"[['Model', 'has', 'implementation']]",[],[],[],[],[],natural_language_inference,31,32
experimental-setup,We implement our model with TensorFlow and train on Nvidia P100 GPUs .,"[('implement', (1, 2)), ('with', (4, 5)), ('train on', (7, 9))]","[('model', (3, 4)), ('TensorFlow', (5, 6)), ('Nvidia P100 GPUs', (9, 12))]","[['model', 'with', 'TensorFlow']]",[],[],[],[],"[['Experimental Setup', 'train on', 'Nvidia P100 GPUs'], ['Experimental Setup', 'implement', 'model']]",[],[],[],natural_language_inference,31,127
experimental-setup,"We tokenize sentences with the NLTK toolkit , convert them to lower cases and remove all punctuations .","[('tokenize', (1, 2)), ('with', (3, 4)), ('convert', (8, 9)), ('remove', (14, 15))]","[('sentences', (2, 3)), ('NLTK toolkit', (5, 7)), ('lower cases', (11, 13)), ('all punctuations', (15, 17))]","[['sentences', 'with', 'NLTK toolkit'], ['sentences', 'convert', 'lower cases'], ['sentences', 'remove', 'all punctuations']]",[],[],[],[],"[['Experimental Setup', 'tokenize', 'sentences']]",[],[],[],natural_language_inference,31,128
experimental-setup,Word embeddings are initialized with 840B - 300d,"[('initialized with', (3, 5))]","[('Word embeddings', (0, 2)), ('840B - 300d', (5, 8))]","[['Word embeddings', 'initialized with', '840B - 300d']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Word embeddings']]",[],natural_language_inference,31,130
experimental-setup,Glo Ve word vectors and fixed during training .,"[('fixed during', (5, 7))]","[('Glo Ve word vectors', (0, 4)), ('training', (7, 8))]","[['Glo Ve word vectors', 'fixed during', 'training']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Glo Ve word vectors']]",[],natural_language_inference,31,131
experimental-setup,Embeddings of out - ofvocabulary words are initialized to zeros and fixed as well .,"[('of', (1, 2)), ('initialized to', (7, 9))]","[('Embeddings', (0, 1)), ('out - ofvocabulary words', (2, 6)), ('zeros', (9, 10))]","[['Embeddings', 'of', 'out - ofvocabulary words'], ['Embeddings', 'initialized to', 'zeros']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Embeddings']]",[],natural_language_inference,31,132
experimental-setup,All other parameters are initialized with He initialization and normalized by weight normalization .,"[('are', (3, 4)), ('with', (5, 6)), ('by', (10, 11))]","[('All other parameters', (0, 3)), ('initialized', (4, 5)), ('He initialization', (6, 8)), ('normalized', (9, 10)), ('weight normalization', (11, 13))]","[['All other parameters', 'are', 'normalized'], ['normalized', 'by', 'weight normalization'], ['All other parameters', 'are', 'initialized'], ['initialized', 'with', 'He initialization']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'All other parameters']]",[],natural_language_inference,31,133
experimental-setup,Dropout with a keep probability of 0.8 is applied before every fully - connected or convolutional layer .,"[('with', (1, 2)), ('of', (5, 6)), ('applied before', (8, 10))]","[('Dropout', (0, 1)), ('keep probability', (3, 5)), ('0.8', (6, 7)), ('fully - connected', (11, 14)), ('convolutional layer', (15, 17))]","[['Dropout', 'with', 'keep probability'], ['keep probability', 'applied before', 'fully - connected'], ['keep probability', 'applied before', 'convolutional layer'], ['keep probability', 'of', '0.8']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Dropout']]",[],natural_language_inference,31,134
experimental-setup,The kernel size of the convolutional encoder is set to 3 .,"[('of', (3, 4)), ('set to', (8, 10))]","[('kernel size', (1, 3)), ('convolutional encoder', (5, 7)), ('3', (10, 11))]","[['kernel size', 'of', 'convolutional encoder'], ['kernel size', 'set to', '3']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'kernel size']]",[],natural_language_inference,31,135
experimental-setup,The prediction layer is a two - layer feed - forward network .,"[('is', (3, 4))]","[('prediction layer', (1, 3)), ('two - layer feed - forward network', (5, 12))]","[['prediction layer', 'is', 'two - layer feed - forward network']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'prediction layer']]",[],natural_language_inference,31,136
experimental-setup,The hidden size is set to 150 in all experiments .,"[('set to', (4, 6))]","[('hidden size', (1, 3)), ('150', (6, 7))]","[['hidden size', 'set to', '150']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'hidden size']]",[],natural_language_inference,31,137
experimental-setup,"Activations in all feed - forward networks are GeLU activations , and we use ?","[('in', (1, 2)), ('are', (7, 8))]","[('Activations', (0, 1)), ('all feed - forward networks', (2, 7)), ('GeLU activations', (8, 10))]","[['Activations', 'in', 'all feed - forward networks'], ['Activations', 'are', 'GeLU activations']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Activations']]",[],natural_language_inference,31,138
experimental-setup,We scale the summation in augmented residual connections by 1 / ? 2 when n ? 3 to preserve the variance under the assumption that the two addends have the same variance .,"[('scale', (1, 2)), ('in', (4, 5)), ('by', (8, 9)), ('when', (13, 14)), ('to preserve', (17, 19)), ('under', (21, 22)), ('that', (24, 25)), ('have', (28, 29))]","[('summation', (3, 4)), ('augmented residual connections', (5, 8)), ('1 / ? 2', (9, 13)), ('n ? 3', (14, 17)), ('variance', (20, 21)), ('assumption', (23, 24)), ('two addends', (26, 28)), ('same variance', (30, 32))]","[['summation', 'in', 'augmented residual connections'], ['summation', 'by', '1 / ? 2'], ['1 / ? 2', 'when', 'n ? 3'], ['1 / ? 2', 'to preserve', 'variance'], ['variance', 'under', 'assumption'], ['assumption', 'that', 'two addends'], ['two addends', 'have', 'same variance']]",[],[],[],[],"[['Experimental Setup', 'scale', 'summation']]",[],[],[],natural_language_inference,31,140
experimental-setup,The number of blocks is tuned in a range from 1 to 3 .,"[('tuned in', (5, 7)), ('from', (9, 10))]","[('number of blocks', (1, 4)), ('range', (8, 9)), ('1 to 3', (10, 13))]","[['number of blocks', 'tuned in', 'range'], ['range', 'from', '1 to 3']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'number of blocks']]",[],natural_language_inference,31,141
experimental-setup,The number of layers of the convolutional encoder is tuned from 1 to 3 .,"[('of', (4, 5)), ('tuned from', (9, 11)), ('to', (12, 13))]","[('number of layers', (1, 4)), ('convolutional encoder', (6, 8)), ('1', (11, 12)), ('3', (13, 14))]","[['number of layers', 'of', 'convolutional encoder'], ['convolutional encoder', 'tuned from', '1'], ['1', 'to', '3']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'number of layers']]",[],natural_language_inference,31,142
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2015 ) and an exponentially decaying learning rate with a linear warmup .","[('use', (1, 2)), ('with', (18, 19))]","[('Adam optimizer ( Kingma and Ba , 2015 )', (3, 12)), ('exponentially decaying learning rate', (14, 18)), ('linear warmup', (20, 22))]","[['exponentially decaying learning rate', 'with', 'linear warmup']]",[],[],[],[],"[['Experimental Setup', 'use', 'Adam optimizer ( Kingma and Ba , 2015 )'], ['Experimental Setup', 'use', 'exponentially decaying learning rate']]",[],[],[],natural_language_inference,31,144
experimental-setup,The initial learning rate is tuned from 0.0001 to 0.003 .,"[('tuned from', (5, 7))]","[('initial learning rate', (1, 4)), ('0.0001 to 0.003', (7, 10))]","[['initial learning rate', 'tuned from', '0.0001 to 0.003']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'initial learning rate']]",[],natural_language_inference,31,145
experimental-setup,The batch size is tuned from 64 to 512 .,"[('tuned from', (4, 6))]","[('batch size', (1, 3)), ('64 to 512', (6, 9))]","[['batch size', 'tuned from', '64 to 512']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'batch size']]",[],natural_language_inference,31,146
experimental-setup,The threshold for gradient clipping is set to 5 .,"[('for', (2, 3)), ('set to', (6, 8))]","[('threshold', (1, 2)), ('gradient clipping', (3, 5)), ('5', (8, 9))]","[['threshold', 'for', 'gradient clipping'], ['gradient clipping', 'set to', '5']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'threshold']]",[],natural_language_inference,31,147
results,Results on WikiQA dataset are listed in .,"[('on', (1, 2))]","[('WikiQA dataset', (2, 4))]",[],[],"[['Results', 'on', 'WikiQA dataset']]",[],[],[],[],[],[],natural_language_inference,31,160
results,We obtain a result on par with the state - of - the - art reported on this dataset .,"[('obtain', (1, 2)), ('on par with', (4, 7))]","[('result', (3, 4)), ('state - of - the - art', (8, 15))]","[['result', 'on par with', 'state - of - the - art']]",[],[],[],[],"[['WikiQA dataset', 'obtain', 'result']]",[],[],[],natural_language_inference,31,163
results,Our method can perform well in the answer selection task without any taskspecific modifications .,"[('perform', (3, 4)), ('in', (5, 6)), ('without', (10, 11))]","[('method', (1, 2)), ('well', (4, 5)), ('answer selection task', (7, 10)), ('any taskspecific modifications', (11, 14))]","[['method', 'perform', 'well'], ['well', 'in', 'answer selection task'], ['well', 'without', 'any taskspecific modifications']]",[],[],"[['Results', 'has', 'method']]",[],[],[],[],[],natural_language_inference,31,165
ablation-analysis,"The first ablation baseline shows that without richer features as the alignment input , the performance on all datasets degrades significantly .","[('shows', (4, 5)), ('as', (9, 10)), ('on', (16, 17))]","[('first ablation baseline', (1, 4)), ('without richer features', (6, 9)), ('alignment input', (11, 13)), ('performance', (15, 16)), ('all datasets', (17, 19)), ('degrades significantly', (19, 21))]","[['first ablation baseline', 'shows', 'without richer features'], ['without richer features', 'as', 'alignment input'], ['performance', 'on', 'all datasets']]","[['without richer features', 'has', 'performance'], ['performance', 'has', 'degrades significantly']]",[],"[['Ablation analysis', 'has', 'first ablation baseline']]",[],[],[],[],[],natural_language_inference,31,198
ablation-analysis,The results of the second baseline show that vanilla residual connections without direct access to the original pointwise features are not enough to model the relations in many text matching tasks .,"[('show', (6, 7)), ('without direct access to', (11, 15)), ('are', (19, 20)), ('to model', (22, 24)), ('in', (26, 27))]","[('second baseline', (4, 6)), ('vanilla residual connections', (8, 11)), ('original pointwise features', (16, 19)), ('not enough', (20, 22)), ('relations', (25, 26)), ('many text matching tasks', (27, 31))]","[['second baseline', 'show', 'vanilla residual connections'], ['vanilla residual connections', 'without direct access to', 'original pointwise features'], ['vanilla residual connections', 'are', 'not enough'], ['not enough', 'to model', 'relations'], ['relations', 'in', 'many text matching tasks']]",[],[],"[['Ablation analysis', 'has', 'second baseline']]",[],[],[],[],[],natural_language_inference,31,200
ablation-analysis,"The simpler implementation of the fusion layer leads to evidently worse performance , indicating that the fu- sion layer can not be further simplified .","[('of', (3, 4)), ('leads to', (7, 9))]","[('simpler implementation', (1, 3)), ('fusion layer', (5, 7)), ('evidently worse performance', (9, 12))]","[['simpler implementation', 'leads to', 'evidently worse performance'], ['simpler implementation', 'of', 'fusion layer']]",[],[],"[['Ablation analysis', 'has', 'simpler implementation']]",[],[],[],[],[],natural_language_inference,31,201
ablation-analysis,"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the preference for deeper models over wider ones .","[('see that', (8, 10)), ('perform', (12, 13)), ('than', (14, 15)), ('supports', (19, 20)), ('for', (22, 23)), ('over', (25, 26))]","[('parallel blocks', (10, 12)), ('worse', (13, 14)), ('stacked blocks', (15, 17)), ('preference', (21, 22)), ('deeper models', (23, 25)), ('wider ones', (26, 28))]","[['parallel blocks', 'perform', 'worse'], ['worse', 'than', 'stacked blocks'], ['worse', 'supports', 'preference'], ['preference', 'for', 'deeper models'], ['deeper models', 'over', 'wider ones']]",[],"[['Ablation analysis', 'see that', 'parallel blocks']]",[],[],[],[],[],[],natural_language_inference,31,203
research-problem,FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,[],"[('CONVERSATIONAL MACHINE COMPREHENSION', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'CONVERSATIONAL MACHINE COMPREHENSION']]",[],[],[],[],natural_language_inference,32,2
model,"We present FLOWQA , a model designed for conversational machine comprehension .","[('present', (1, 2)), ('designed for', (6, 8))]","[('FLOWQA', (2, 3)), ('conversational machine comprehension', (8, 11))]","[['FLOWQA', 'designed for', 'conversational machine comprehension']]",[],"[['Model', 'present', 'FLOWQA']]",[],[],[],[],[],[],natural_language_inference,32,13
model,FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,"[('consists of', (1, 3)), ('for', (11, 12)), ('encodes', (21, 22))]","[('FLOWQA', (0, 1)), ('two main components', (3, 6)), ('base neural model', (8, 11)), ('single - turn MC', (12, 16)), ('FLOW mechanism', (18, 20)), ('conversation history', (23, 25))]","[['FLOWQA', 'consists of', 'two main components'], ['base neural model', 'for', 'single - turn MC'], ['FLOW mechanism', 'encodes', 'conversation history']]","[['two main components', 'has', 'base neural model'], ['two main components', 'has', 'FLOW mechanism']]",[],"[['Model', 'has', 'FLOWQA']]",[],[],[],[],[],natural_language_inference,32,14
model,"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .","[('of', (1, 2)), ('feed', (15, 16)), ('generated during', (23, 25))]","[('entire hidden representations', (20, 23)), ('process', (26, 27)), ('answering previous questions', (28, 31))]","[['entire hidden representations', 'generated during', 'process'], ['process', 'of', 'answering previous questions']]",[],"[['Model', 'feed', 'entire hidden representations']]",[],[],[],[],[],[],natural_language_inference,32,15
model,"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .","[('is', (3, 4)), ('at tracking', (7, 9)), ('for', (12, 13))]","[('FLOW mechanism', (1, 3)), ('remarkably effective', (5, 7)), ('world states', (10, 12)), ('sequential instruction understanding', (13, 16))]","[['FLOW mechanism', 'is', 'remarkably effective'], ['remarkably effective', 'at tracking', 'world states'], ['world states', 'for', 'sequential instruction understanding']]",[],[],"[['Model', 'has', 'FLOW mechanism']]",[],[],[],[],[],natural_language_inference,32,17
model,"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .",[],[],"[['viewed', 'as', 'stacking single - turn QA models'], ['stacking single - turn QA models', 'along', 'dialog progression'], ['stacking single - turn QA models', 'building', 'information flow'], ['information flow', 'along', 'dialog']]",[],[],[],[],"[['FLOW mechanism', 'can be', 'viewed']]",[],[],[],natural_language_inference,32,18
model,"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .","[('happens for', (3, 5)), ('allowing', (9, 10)), ('in', (12, 13)), ('to', (16, 17))]","[('information transfer', (1, 3)), ('each context word', (5, 8)), ('rich information', (10, 12)), ('reasoning process', (14, 16)), ('flow', (17, 18))]","[['information transfer', 'happens for', 'each context word'], ['information transfer', 'allowing', 'rich information'], ['rich information', 'in', 'reasoning process'], ['rich information', 'to', 'flow']]",[],[],"[['Model', 'has', 'information transfer']]",[],[],[],[],[],natural_language_inference,32,19
model,"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .","[('propose', (6, 7)), ('which', (13, 14)), ('between', (15, 16)), ('in parallel of', (20, 23))]","[('alternating parallel processing structure', (8, 12)), ('alternates', (14, 15)), ('sequentially processing', (16, 18)), ('one dimension', (18, 20)), ('other dimension', (24, 26)), ('speeds up', (29, 31)), ('training', (31, 32)), ('significantly', (32, 33))]","[['alternating parallel processing structure', 'which', 'alternates'], ['alternates', 'between', 'sequentially processing'], ['one dimension', 'in parallel of', 'other dimension']]","[['sequentially processing', 'has', 'one dimension'], ['sequentially processing', 'has', 'speeds up'], ['speeds up', 'has', 'training'], ['training', 'has', 'significantly']]","[['Model', 'propose', 'alternating parallel processing structure']]",[],[],[],[],[],[],natural_language_inference,32,22
research-problem,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,[],"[('conversational machine comprehension ( MC )', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'conversational machine comprehension ( MC )']]",[],[],[],[],natural_language_inference,32,29
code,Our code can be found in https://github.com/momohuang/FlowQA.,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,32,37
baselines,"applied BiDAF ++ , a strong extractive QA model to QuAC dataset .","[('applied', (0, 1)), ('to', (9, 10))]","[('BiDAF ++', (1, 3)), ('strong extractive QA model', (5, 9)), ('QuAC dataset', (10, 12))]","[['strong extractive QA model', 'to', 'QuAC dataset']]","[['BiDAF ++', 'has', 'strong extractive QA model']]","[['Baselines', 'applied', 'BiDAF ++']]",[],[],[],[],[],[],natural_language_inference,32,151
baselines,"Here we briefly describe the ablated systems : "" - FLOW "" removes the flow component from IF layer ( Eq. 2 in Section 3.2 ) , "" - QHIER - RNN "" removes the hierarchical LSTM layers on final question vectors ( Eq. 7 in Section 3.3 ) .",[],[],"[['- QHIER - RNN', 'removes', 'hierarchical LSTM layers'], ['hierarchical LSTM layers', 'on', 'final question vectors'], ['- FLOW', 'removes', 'flow component'], ['flow component', 'from', 'IF layer']]",[],[],"[['Baselines', 'has', '- QHIER - RNN'], ['Baselines', 'has', '- FLOW']]",[],[],[],[],[],natural_language_inference,32,156
results,"FLOWQA yields substantial improvement over existing models on both datasets ( + 7.2 % F 1 on CoQA , + 4.0 % F 1 on QuAC ) .",[],[],"[['FLOWQA', 'yields', 'substantial improvement'], ['substantial improvement', 'over', 'existing models'], ['+ 4.0 % F 1', 'on', 'QuAC'], ['+ 7.2 % F 1', 'on', 'CoQA'], ['substantial improvement', 'on', 'both datasets']]","[['substantial improvement', 'has', '+ 4.0 % F 1'], ['substantial improvement', 'has', '+ 7.2 % F 1']]",[],"[['Results', 'has', 'FLOWQA']]",[],[],[],[],[],natural_language_inference,32,158
results,We find that FLOW is a critical component .,"[('find that', (1, 3)), ('is', (4, 5))]","[('FLOW', (3, 4)), ('critical component', (6, 8))]","[['FLOW', 'is', 'critical component']]",[],"[['Results', 'find that', 'FLOW']]",[],[],[],[],[],[],natural_language_inference,32,161
results,"Removing QHier - RNN has a minor impact ( 0.1 % on both datasets ) , while removing FLOW results in a substantial performance drop , with or without using QHierRNN ( 2 - 3 % on QuAC , 4.1 % on CoQA ) .",[],[],"[['minor impact', 'on', 'both datasets'], ['FLOW', 'results in', 'substantial performance drop'], ['substantial performance drop', 'with or without using', 'QHierRNN'], ['2 - 3 %', 'on', 'QuAC'], ['4.1 %', 'on', 'CoQA']]","[['QHier - RNN', 'has', 'minor impact'], ['minor impact', 'has', '0.1 %'], ['substantial performance drop', 'has', '2 - 3 %'], ['substantial performance drop', 'has', '4.1 %']]","[['Results', 'Removing', 'QHier - RNN'], ['Results', 'removing', 'FLOW']]",[],[],[],[],[],[],natural_language_inference,32,162
results,"By comparing 0 - Ans and 1 - Ans on two datasets , we can see that providing gold answers is more crucial for QuAC .","[('comparing', (1, 2)), ('on', (9, 10)), ('providing', (17, 18)), ('is', (20, 21)), ('for', (23, 24))]","[('0 - Ans and 1 - Ans', (2, 9)), ('two datasets', (10, 12)), ('gold answers', (18, 20)), ('more crucial', (21, 23)), ('QuAC', (24, 25))]","[['0 - Ans and 1 - Ans', 'providing', 'gold answers'], ['gold answers', 'is', 'more crucial'], ['more crucial', 'for', 'QuAC'], ['0 - Ans and 1 - Ans', 'on', 'two datasets']]",[],"[['Results', 'comparing', '0 - Ans and 1 - Ans']]",[],[],[],[],[],[],natural_language_inference,32,166
results,"Based on the training time each epoch takes ( i.e. , time needed for passing through the data once ) , the speedup is 8.1x on CoQA and 4.2 x on QuAC .",[],[],"[['speedup', 'is', '4.2 x'], ['4.2 x', 'on', 'QuAC'], ['speedup', 'is', '8.1x'], ['8.1x', 'on', 'CoQA'], ['training time', 'takes', 'each epoch']]","[['training time', 'has', 'speedup']]","[['Results', 'Based on', 'training time']]",[],[],[],[],[],[],natural_language_inference,32,171
research-problem,Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,[],"[('LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS', (8, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS']]",[],[],[],[],natural_language_inference,33,2
research-problem,A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,[],"[('distributed vector representations of words', (17, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'distributed vector representations of words']]",[],[],[],[],natural_language_inference,33,4
research-problem,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",[],"[('learning representations of sequences of words', (6, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'learning representations of sequences of words']]",[],[],[],[],natural_language_inference,33,6
research-problem,Some recent work has addressed this by learning general - purpose sentence representations .,[],"[('learning general - purpose sentence representations', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'learning general - purpose sentence representations']]",[],[],[],[],natural_language_inference,33,18
approach,"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .","[('in', (4, 5)), ('of', (7, 8)), ('wherein', (23, 24)), ('shared across', (30, 32))]","[('context', (6, 7)), ('simple one - to - many multi -task learning ( MTL ) framework', (9, 22)), ('single recurrent sentence encoder', (25, 29)), ('multiple tasks', (32, 34))]","[['context', 'of', 'simple one - to - many multi -task learning ( MTL ) framework'], ['simple one - to - many multi -task learning ( MTL ) framework', 'wherein', 'single recurrent sentence encoder'], ['single recurrent sentence encoder', 'shared across', 'multiple tasks']]",[],"[['Approach', 'in', 'context']]",[],[],[],[],[],[],natural_language_inference,33,26
approach,"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .","[('aims at', (3, 5))]","[('learning', (5, 6)), ('fixed - length distributed sentence representations', (6, 12))]",[],"[['learning', 'has', 'fixed - length distributed sentence representations']]","[['Approach', 'aims at', 'learning']]",[],[],[],[],[],[],natural_language_inference,33,29
approach,The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,"[('of', (3, 4)), ('combine', (8, 9)), ('into', (18, 19))]","[('benefits', (10, 11)), ('diverse sentence - representation learning objectives', (12, 18)), ('single multi-task framework', (20, 23))]","[['benefits', 'of', 'diverse sentence - representation learning objectives'], ['diverse sentence - representation learning objectives', 'into', 'single multi-task framework']]",[],"[['Approach', 'combine', 'benefits']]",[],[],[],[],[],[],natural_language_inference,33,31
results,It is evident from that adding more tasks improves the transfer performance of our model .,"[('adding', (5, 6)), ('improves', (8, 9)), ('of', (12, 13))]","[('more tasks', (6, 8)), ('transfer performance', (10, 12)), ('our model', (13, 15))]","[['more tasks', 'improves', 'transfer performance'], ['transfer performance', 'of', 'our model']]",[],"[['Results', 'adding', 'more tasks']]",[],[],[],[],[],[],natural_language_inference,33,136
results,Increasing the capacity our sentence encoder with more hidden units ( + L ) as well as an additional layer ( + 2L ) also lead to improved transfer performance .,"[('Increasing', (0, 1)), ('with', (6, 7)), ('as well as', (14, 17)), ('lead to', (25, 27))]","[('capacity', (2, 3)), ('our sentence encoder', (3, 6)), ('more hidden units ( + L )', (7, 14)), ('additional layer ( + 2L )', (18, 24)), ('improved transfer performance', (27, 30))]","[['our sentence encoder', 'lead to', 'improved transfer performance'], ['our sentence encoder', 'with', 'more hidden units ( + L )'], ['our sentence encoder', 'as well as', 'additional layer ( + 2L )']]","[['capacity', 'has', 'our sentence encoder']]","[['Results', 'Increasing', 'capacity']]",[],[],[],[],[],[],natural_language_inference,33,137
results,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .","[('observe', (1, 2)), ('of', (3, 4)), ('on', (8, 9)), ('over', (22, 23))]","[('gains', (2, 3)), ('1.1 - 2.0 %', (4, 8)), ('sentiment classification tasks', (10, 13)), ('MR', (14, 15)), ('CR', (16, 17)), ('SUBJ', (18, 19)), ('MPQA', (20, 21)), ('Infersent', (23, 24))]","[['gains', 'of', '1.1 - 2.0 %'], ['1.1 - 2.0 %', 'over', 'Infersent'], ['1.1 - 2.0 %', 'on', 'sentiment classification tasks']]","[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","[['Results', 'observe', 'gains']]",[],[],[],[],[],[],natural_language_inference,33,138
results,"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .",[],[],"[['roughly 2 %', 'over', 'CNN - LSTM'], ['6 %', 'over', 'Infersent'], ['substantial gains', 'on', 'TREC']]","[['substantial gains', 'has', 'roughly 2 %'], ['substantial gains', 'has', 'outperforming'], ['outperforming', 'has', 'competitive supervised baseline'], ['substantial gains', 'has', '6 %']]","[['Results', 'demonstrate', 'substantial gains']]",[],[],[],[],[],[],natural_language_inference,33,139
results,"We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .",[],[],"[['similar gains', 'closing', 'gap'], ['gap', 'on', 'supervised approaches'], ['supervised approaches', 'trained from', 'scratch'], ['similar gains', 'on', 'paraphrase identification ( MPRC )']]","[['similar gains', 'has', '2.3 %']]","[['Results', 'see', 'similar gains']]",[],[],[],[],[],[],natural_language_inference,33,140
results,The addition of constituency parsing improves performance on sentence relatedness ( SICK - R ) and entailment ( SICK - E ) consistent with observations made by .,"[('addition of', (1, 3)), ('on', (7, 8))]","[('constituency parsing', (3, 5)), ('improves', (5, 6)), ('performance', (6, 7)), ('sentence relatedness ( SICK - R )', (8, 15)), ('entailment ( SICK - E )', (16, 22))]","[['performance', 'on', 'sentence relatedness ( SICK - R )'], ['performance', 'on', 'entailment ( SICK - E )']]","[['constituency parsing', 'has', 'improves'], ['improves', 'has', 'performance']]","[['Results', 'addition of', 'constituency parsing']]",[],[],[],[],[],[],natural_language_inference,33,141
results,"In , we show that simply training an MLP on top of our fixed sentence representations outperforms several strong & complex supervised approaches that use attention mechanisms , even on this fairly large dataset .","[('show that', (3, 5)), ('on top of', (9, 12)), ('that use', (23, 25))]","[('training', (6, 7)), ('MLP', (8, 9)), ('our fixed sentence representations', (12, 16)), ('outperforms', (16, 17)), ('several strong & complex supervised approaches', (17, 23)), ('attention mechanisms', (25, 27))]","[['MLP', 'on top of', 'our fixed sentence representations'], ['several strong & complex supervised approaches', 'that use', 'attention mechanisms']]","[['training', 'has', 'MLP'], ['our fixed sentence representations', 'has', 'outperforms'], ['outperforms', 'has', 'several strong & complex supervised approaches']]","[['Results', 'show that', 'training']]",[],[],[],[],[],[],natural_language_inference,33,142
results,"For example , we observe a 0.2-0.5 % improvement over the decomposable attention model .","[('over', (9, 10))]","[('0.2-0.5 % improvement', (6, 9)), ('decomposable attention model', (11, 14))]","[['0.2-0.5 % improvement', 'over', 'decomposable attention model']]",[],[],"[['Results', 'observe', '0.2-0.5 % improvement']]",[],[],[],[],[],natural_language_inference,33,143
results,"When using only a small fraction of the training data , indicated by the columns 1 k - 25 k , we are able to outperform the Siamese and Multi - Perspective CNN using roughly 6 % of the available training set .",[],[],"[['small fraction', 'able to', 'outperform'], ['Siamese and Multi - Perspective CNN', 'using', 'roughly 6 %'], ['roughly 6 %', 'of', 'available training set'], ['small fraction', 'of', 'training data']]","[['outperform', 'has', 'Siamese and Multi - Perspective CNN']]","[['Results', 'When using', 'small fraction']]",[],[],[],[],[],[],natural_language_inference,33,144
results,We also outperform the Deconv LVM model proposed by in this low - resource setting .,[],"[('outperform', (2, 3)), ('Deconv LVM model', (4, 7))]",[],"[['outperform', 'has', 'Deconv LVM model']]",[],"[['Results', 'has', 'outperform']]",[],[],[],[],[],natural_language_inference,33,145
results,"Somewhat surprisingly , in we observe that the learned word embeddings are competitive with popular methods such as GloVe , word2vec , and fasttext on the benchmarks presented by and .","[('observe that', (5, 7)), ('are', (11, 12)), ('with', (13, 14)), ('such as', (16, 18))]","[('learned word embeddings', (8, 11)), ('competitive', (12, 13)), ('popular methods', (14, 16)), ('GloVe', (18, 19)), ('word2vec', (20, 21)), ('fasttext', (23, 24))]","[['learned word embeddings', 'are', 'competitive'], ['competitive', 'with', 'popular methods'], ['popular methods', 'such as', 'GloVe'], ['popular methods', 'such as', 'word2vec'], ['popular methods', 'such as', 'fasttext']]",[],"[['Results', 'observe that', 'learned word embeddings']]",[],[],[],[],[],[],natural_language_inference,33,147
results,Representations learned solely from NLI do appear to encode syntax but incorporation into our multi-task framework does not amplify this signal .,"[('learned solely from', (1, 4)), ('appear to encode', (6, 9)), ('incorporation into', (11, 13)), ('does not', (16, 18))]","[('Representations', (0, 1)), ('NLI', (4, 5)), ('syntax', (9, 10)), ('our multi-task framework', (13, 16)), ('amplify', (18, 19)), ('signal', (20, 21))]","[['Representations', 'learned solely from', 'NLI'], ['NLI', 'appear to encode', 'syntax'], ['Representations', 'incorporation into', 'our multi-task framework'], ['our multi-task framework', 'does not', 'amplify']]","[['amplify', 'has', 'signal']]",[],"[['Results', 'has', 'Representations']]",[],[],[],[],[],natural_language_inference,33,150
results,"Similarly , we observe that sentence characteristics such as length and word order are better encoded with the addition of parsing .","[('such as', (7, 9)), ('are', (13, 14)), ('addition of', (18, 20))]","[('sentence characteristics', (5, 7)), ('length and word order', (9, 13)), ('better encoded', (14, 16)), ('parsing', (20, 21))]","[['sentence characteristics', 'such as', 'length and word order'], ['length and word order', 'are', 'better encoded'], ['better encoded', 'addition of', 'parsing']]",[],[],"[['Results', 'observe that', 'sentence characteristics']]",[],[],[],[],[],natural_language_inference,33,151
research-problem,Dynamically Fused Graph Network for Multi-hop Reasoning,[],"[('Multi-hop Reasoning', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi-hop Reasoning']]",[],[],[],[],natural_language_inference,34,2
research-problem,Text - based question answering ( TBQA ) has been studied extensively in recent years .,[],"[('Text - based question answering ( TBQA )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text - based question answering ( TBQA )']]",[],[],[],[],natural_language_inference,34,4
research-problem,Question answering ( QA ) has been a popular topic in natural language processing .,[],"[('Question answering ( QA )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question answering ( QA )']]",[],[],[],[],natural_language_inference,34,13
research-problem,QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,[],"[('QA', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,34,14
model,"In this paper , we propose Dynamically Fused Graph Network ( DFGN ) , a novel method to address the aforementioned concerns for multi-hop text - based QA .","[('propose', (5, 6))]","[('Dynamically Fused Graph Network ( DFGN )', (6, 13)), ('novel method', (15, 17))]",[],"[['Dynamically Fused Graph Network ( DFGN )', 'has', 'novel method']]","[['Model', 'propose', 'Dynamically Fused Graph Network ( DFGN )']]",[],[],[],[],[],[],natural_language_inference,34,50
model,"For the first challenge , DFGN constructs a dynamic entity graph based on entity mentions in the query and documents .","[('constructs', (6, 7)), ('based on', (11, 13)), ('in', (15, 16))]","[('DFGN', (5, 6)), ('dynamic entity graph', (8, 11)), ('entity mentions', (13, 15)), ('query and documents', (17, 20))]","[['DFGN', 'constructs', 'dynamic entity graph'], ['dynamic entity graph', 'based on', 'entity mentions'], ['entity mentions', 'in', 'query and documents']]",[],[],"[['Model', 'has', 'DFGN']]",[],[],[],[],"[['dynamic entity graph', 'has', 'process']]",natural_language_inference,34,51
model,This process iterates in multiple rounds to achieve multihop reasoning .,"[('iterates in', (2, 4)), ('to achieve', (6, 8))]","[('process', (1, 2)), ('multiple rounds', (4, 6)), ('multihop reasoning', (8, 10))]","[['process', 'iterates in', 'multiple rounds'], ['multiple rounds', 'to achieve', 'multihop reasoning']]",[],[],[],[],[],[],[],[],natural_language_inference,34,52
model,"In each round , DFGN generates and reasons on a dynamic graph , where irrelevant entities are masked out while only reasoning sources are preserved , via a mask prediction module .",[],[],"[['generates and reasons', 'on', 'dynamic graph'], ['irrelevant entities', 'are', 'masked out'], ['masked out', 'while', 'reasoning sources'], ['reasoning sources', 'are', 'preserved'], ['preserved', 'via', 'mask prediction module']]","[['each round', 'has', 'DFGN'], ['DFGN', 'has', 'generates and reasons'], ['dynamic graph', 'has', 'irrelevant entities']]","[['Model', 'In', 'each round']]",[],[],[],[],[],[],natural_language_inference,34,53
model,"To solve the second challenge , we propose a fusion process in DFGN to solve the unrestricted QA challenge .","[('in', (11, 12)), ('to solve', (13, 15))]","[('fusion process', (9, 11)), ('DFGN', (12, 13)), ('unrestricted QA challenge', (16, 19))]","[['fusion process', 'in', 'DFGN'], ['DFGN', 'to solve', 'unrestricted QA challenge']]",[],[],"[['Model', 'propose', 'fusion process']]",[],[],[],[],[],natural_language_inference,34,56
model,"We not only aggregate information from documents to the entity graph ( doc2 graph ) , but also propagate the information of the entity graph back to document representations ( graph2doc ) .",[],[],"[['entity graph', 'back to', 'document representations'], ['information', 'from', 'documents'], ['documents', 'to', 'entity graph']]","[['documents', 'name', 'doc2 graph']]","[['Model', 'propagate', 'information'], ['Model', 'aggregate', 'information']]",[],[],[],[],[],[],natural_language_inference,34,57
model,"The fusion process is iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .","[('is', (3, 4)), ('at', (6, 7)), ('through', (9, 10)), ('obtained from', (23, 25))]","[('fusion process', (1, 3)), ('iteratively performed', (4, 6)), ('each hop', (7, 9)), ('document tokens and entities', (11, 15)), ('final resulting answer', (18, 21)), ('document tokens', (25, 27))]","[['fusion process', 'is', 'iteratively performed'], ['iteratively performed', 'through', 'document tokens and entities'], ['iteratively performed', 'at', 'each hop'], ['final resulting answer', 'obtained from', 'document tokens']]","[['iteratively performed', 'has', 'final resulting answer']]",[],"[['Model', 'has', 'fusion process']]",[],[],[],[],[],natural_language_inference,34,58
model,"The fusion process of doc2 graph and graph2doc along with the dynamic entity graph jointly improve the interaction between the information of documents and the entity graph , leading to a less noisy entity graph and thus more accurate answers .",[],[],"[['information', 'of', 'entity graph'], ['doc2 graph and graph2doc', 'along with', 'dynamic entity graph'], ['interaction', 'leading to', 'less noisy entity graph'], ['interaction', 'between', 'information'], ['information', 'of', 'documents'], ['information', 'of', 'entity graph']]","[['doc2 graph and graph2doc', 'has', 'jointly improve'], ['jointly improve', 'has', 'interaction'], ['less noisy entity graph', 'has', 'more accurate answers']]",[],[],[],"[['fusion process', 'of', 'doc2 graph and graph2doc']]",[],[],[],natural_language_inference,34,59
hyperparameters,"In paragraph selection stage , we use the uncased version of BERT Tokenizer to tokenize all passages and questions .","[('In', (0, 1)), ('use', (6, 7)), ('to tokenize', (13, 15))]","[('paragraph selection stage', (1, 4)), ('uncased version of BERT Tokenizer', (8, 13)), ('all passages and questions', (15, 19))]","[['paragraph selection stage', 'use', 'uncased version of BERT Tokenizer'], ['uncased version of BERT Tokenizer', 'to tokenize', 'all passages and questions']]",[],"[['Hyperparameters', 'In', 'paragraph selection stage']]",[],[],[],[],[],[],natural_language_inference,34,209
hyperparameters,The encoding vectors of sentence pairs are generated from a pre-trained BERT model .,"[('of', (3, 4)), ('generated from', (7, 9))]","[('encoding vectors', (1, 3)), ('sentence pairs', (4, 6)), ('pre-trained BERT model', (10, 13))]","[['encoding vectors', 'of', 'sentence pairs'], ['encoding vectors', 'generated from', 'pre-trained BERT model']]",[],[],"[['Hyperparameters', 'has', 'encoding vectors']]",[],[],[],[],[],natural_language_inference,34,210
hyperparameters,We set a relatively low threshold during selection to keep a high recall ( 97 % ) and a reasonable precision ( 69 % ) on supporting facts .,"[('set', (1, 2)), ('during', (6, 7)), ('to keep', (8, 10)), ('on', (25, 26))]","[('relatively low threshold', (3, 6)), ('selection', (7, 8)), ('high recall ( 97 % )', (11, 17)), ('reasonable precision ( 69 % )', (19, 25)), ('supporting facts', (26, 28))]","[['relatively low threshold', 'during', 'selection'], ['selection', 'on', 'supporting facts'], ['supporting facts', 'to keep', 'high recall ( 97 % )'], ['supporting facts', 'to keep', 'reasonable precision ( 69 % )']]",[],"[['Hyperparameters', 'set', 'relatively low threshold']]",[],[],[],[],[],[],natural_language_inference,34,211
hyperparameters,"In graph construction stage , we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities .","[('use', (6, 7)), ('from', (11, 12)), ('to extract', (16, 18))]","[('graph construction stage', (1, 4)), ('pretrained NER model', (8, 11)), ('Stanford CoreNLP Toolkits', (12, 15)), ('named entities', (18, 20))]","[['graph construction stage', 'use', 'pretrained NER model'], ['pretrained NER model', 'to extract', 'named entities'], ['pretrained NER model', 'from', 'Stanford CoreNLP Toolkits']]",[],[],"[['Hyperparameters', 'In', 'graph construction stage']]",[],[],[],[],[],natural_language_inference,34,212
hyperparameters,The maximum number of entities in a graph is set to be 40 .,"[('in', (5, 6)), ('set to', (9, 11))]","[('maximum number of entities', (1, 5)), ('graph', (7, 8)), ('40', (12, 13))]","[['maximum number of entities', 'in', 'graph'], ['maximum number of entities', 'set to', '40']]",[],[],"[['Hyperparameters', 'has', 'maximum number of entities']]",[],[],[],[],[],natural_language_inference,34,213
hyperparameters,Each entity node in the entity graphs has an average degree of 3.52 .,"[('in', (3, 4)), ('of', (11, 12))]","[('Each entity node', (0, 3)), ('entity graphs', (5, 7)), ('average degree', (9, 11)), ('3.52', (12, 13))]","[['Each entity node', 'in', 'entity graphs'], ['average degree', 'of', '3.52']]","[['Each entity node', 'has', 'average degree']]",[],"[['Hyperparameters', 'has', 'Each entity node']]",[],[],[],[],[],natural_language_inference,34,214
hyperparameters,"In the encoding stage , we also use a pre-trained BERT model as the encoder , thus d 1 is 768 .","[('use', (7, 8)), ('as', (12, 13)), ('is', (19, 20))]","[('encoding stage', (2, 4)), ('pre-trained BERT model', (9, 12)), ('encoder', (14, 15)), ('d 1', (17, 19)), ('768', (20, 21))]","[['encoding stage', 'use', 'pre-trained BERT model'], ['pre-trained BERT model', 'as', 'encoder'], ['d 1', 'is', '768']]","[['pre-trained BERT model', 'has', 'd 1']]",[],"[['Hyperparameters', 'In', 'encoding stage']]",[],[],[],[],[],natural_language_inference,34,215
hyperparameters,All the hidden state dimensions d 2 are set to 300 .,"[('set to', (8, 10))]","[('hidden state dimensions d 2', (2, 7)), ('300', (10, 11))]","[['hidden state dimensions d 2', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'hidden state dimensions d 2']]",[],[],[],[],[],natural_language_inference,34,216
hyperparameters,We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively .,"[('for', (5, 6)), ('of', (9, 10)), ('to', (15, 16))]","[('dropout rate', (3, 5)), ('all hidden units', (6, 9)), ('LSTM and dynamic graph attention', (10, 15)), ('0.3 and 0.5', (16, 19))]","[['dropout rate', 'for', 'all hidden units'], ['all hidden units', 'of', 'LSTM and dynamic graph attention'], ['LSTM and dynamic graph attention', 'to', '0.3 and 0.5']]",[],[],"[['Hyperparameters', 'set', 'dropout rate']]",[],[],[],[],[],natural_language_inference,34,217
hyperparameters,"For optimization , we use Adam Optimizer with an initial learning rate of 1 e ?4 .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('of', (12, 13))]","[('optimization', (1, 2)), ('Adam Optimizer', (5, 7)), ('initial learning rate', (9, 12)), ('1 e ?4', (13, 16))]","[['optimization', 'use', 'Adam Optimizer'], ['Adam Optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '1 e ?4']]",[],"[['Hyperparameters', 'For', 'optimization']]",[],[],[],[],[],[],natural_language_inference,34,218
results,We first present a comparison between baseline models and our DFGN 2 . shows the performance of different models in the private test set of Hotpot QA .,"[('of', (16, 17)), ('in', (19, 20))]","[('private test set', (21, 24)), ('Hotpot QA', (25, 27))]","[['private test set', 'of', 'Hotpot QA']]",[],"[['Results', 'in', 'private test set']]",[],[],[],[],[],[],natural_language_inference,34,222
results,From the table we can see that our model achieves the second best result on the leaderboard now 3 ( on March 1st ) .,"[('see that', (5, 7)), ('achieves', (9, 10)), ('on', (14, 15))]","[('our model', (7, 9)), ('second best result', (11, 14)), ('leaderboard', (16, 17)), ('March 1st', (21, 23))]","[['our model', 'achieves', 'second best result'], ['second best result', 'on', 'leaderboard'], ['second best result', 'on', 'March 1st']]",[],[],[],[],"[['Hotpot QA', 'see that', 'our model']]",[],[],[],natural_language_inference,34,223
results,"Besides , the answer performance and the joint performance of our model are competitive against state - of - the - art unpublished models .","[('of', (9, 10)), ('are', (12, 13)), ('against', (14, 15))]","[('joint performance', (7, 9)), ('our model', (10, 12)), ('competitive', (13, 14)), ('state - of - the - art unpublished models', (15, 24))]","[['joint performance', 'of', 'our model'], ['our model', 'are', 'competitive'], ['competitive', 'against', 'state - of - the - art unpublished models']]",[],[],"[['Results', 'has', 'joint performance']]",[],[],[],[],[],natural_language_inference,34,224
results,The results show that our model achieves a 1.5 % gain in the joint F1 - score with the entity graph built from a better entity recognizer .,"[('show', (2, 3)), ('achieves', (6, 7)), ('in', (11, 12)), ('with', (17, 18)), ('built from', (21, 23))]","[('our model', (4, 6)), ('1.5 % gain', (8, 11)), ('joint F1 - score', (13, 17)), ('entity graph', (19, 21)), ('better entity recognizer', (24, 27))]","[['our model', 'achieves', '1.5 % gain'], ['1.5 % gain', 'with', 'entity graph'], ['entity graph', 'built from', 'better entity recognizer'], ['1.5 % gain', 'in', 'joint F1 - score']]",[],"[['Results', 'show', 'our model']]",[],[],[],[],[],[],natural_language_inference,34,227
ablation-analysis,The ablation results of QA performances in the development set of Hotpot QA are shown in .,[],[],"[['QA performances', 'in', 'development set'], ['development set', 'of', 'Hotpot QA']]",[],"[['Ablation analysis', 'of', 'QA performances']]",[],[],[],[],[],[],natural_language_inference,34,230
ablation-analysis,From the table we can see that each of our model components can provide from 1 % to 2 % relative gain over the QA performance .,"[('see that', (5, 7)), ('provide', (13, 14)), ('over', (22, 23))]","[('each of our model components', (7, 12)), ('from 1 % to 2 %', (14, 20)), ('relative gain', (20, 22)), ('QA performance', (24, 26))]","[['each of our model components', 'provide', 'from 1 % to 2 %'], ['relative gain', 'over', 'QA performance']]","[['from 1 % to 2 %', 'has', 'relative gain']]",[],[],[],"[['development set', 'see that', 'each of our model components']]",[],[],[],natural_language_inference,34,231
ablation-analysis,"Particularly , using a 1 - layer fusion block leads to an obvious performance loss , which implies the significance of performing multi-hop reasoning in Hotpot QA .","[('using', (2, 3)), ('leads to', (9, 11)), ('implies', (17, 18)), ('of', (20, 21)), ('in', (24, 25))]","[('1 - layer fusion block', (4, 9)), ('obvious performance loss', (12, 15)), ('significance', (19, 20)), ('performing', (21, 22)), ('multi-hop reasoning', (22, 24)), ('Hotpot QA', (25, 27))]","[['1 - layer fusion block', 'leads to', 'obvious performance loss'], ['obvious performance loss', 'implies', 'significance'], ['significance', 'of', 'performing'], ['multi-hop reasoning', 'in', 'Hotpot QA']]","[['performing', 'has', 'multi-hop reasoning']]","[['Ablation analysis', 'using', '1 - layer fusion block']]",[],[],[],[],[],[],natural_language_inference,34,232
ablation-analysis,"Besides , the dataset abla-tion results show that our model is not very sensitive to the noisy paragraphs comparing with the baseline model which can achieve a more than 5 % performance gain in the "" gold paragraphs only "" and "" supporting facts only "" settings .","[('show', (6, 7)), ('to', (14, 15)), ('comparing with', (18, 20)), ('can achieve', (24, 26)), ('in', (33, 34))]","[('dataset abla-tion results', (3, 6)), ('our model', (8, 10)), ('not very sensitive', (11, 14)), ('noisy paragraphs', (16, 18)), ('baseline model', (21, 23)), ('more than 5 % performance gain', (27, 33)), ('gold paragraphs only', (36, 39)), ('supporting facts only', (42, 45))]","[['dataset abla-tion results', 'show', 'our model'], ['not very sensitive', 'can achieve', 'more than 5 % performance gain'], ['more than 5 % performance gain', 'in', 'gold paragraphs only'], ['more than 5 % performance gain', 'in', 'supporting facts only'], ['not very sensitive', 'to', 'noisy paragraphs'], ['not very sensitive', 'comparing with', 'baseline model']]","[['our model', 'has', 'not very sensitive']]",[],"[['Ablation analysis', 'has', 'dataset abla-tion results']]",[],[],[],[],[],natural_language_inference,34,233
research-problem,Multi - Style Generative Reading Comprehension,[],"[('Generative Reading Comprehension', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generative Reading Comprehension']]",[],[],[],[],natural_language_inference,35,2
research-problem,"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .",[],"[('generative reading comprehension ( RC )', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'generative reading comprehension ( RC )']]",[],[],[],[],natural_language_inference,35,4
research-problem,"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .",[],"[('reading comprehension ( RC )', (2, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'reading comprehension ( RC )']]",[],[],[],[],natural_language_inference,35,15
research-problem,"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .",[],"[('RC', (5, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],natural_language_inference,35,16
research-problem,"In this study , we propose Masque , a generative model for multi-passage RC .","[('propose', (5, 6)), ('for', (11, 12))]","[('Masque', (6, 7)), ('generative model', (9, 11)), ('multi-passage RC', (12, 14))]","[['generative model', 'for', 'multi-passage RC']]","[['Masque', 'has', 'generative model']]","[['Model', 'propose', 'Masque']]",[],"[['Contribution', 'has research problem', 'multi-passage RC']]",[],[],[],[],natural_language_inference,35,24
model,"We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .","[('introduce', (1, 2)), ('for generating', (7, 9)), ('from', (12, 13)), ('covers', (20, 21))]","[('pointer - generator mechanism', (3, 7)), ('abstractive answer', (10, 12)), ('question', (14, 15)), ('multiple passages', (16, 18)), ('various answer styles', (21, 24))]","[['pointer - generator mechanism', 'for generating', 'abstractive answer'], ['abstractive answer', 'from', 'question'], ['abstractive answer', 'from', 'multiple passages'], ['multiple passages', 'covers', 'various answer styles']]",[],"[['Model', 'introduce', 'pointer - generator mechanism']]",[],[],[],[],[],[],natural_language_inference,35,28
model,We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .,"[('extend', (1, 2)), ('to', (4, 5)), ('that allows', (9, 11)), ('to be generated from', (12, 16)), ('to be copied from', (19, 23))]","[('mechanism', (3, 4)), ('Transformer based one', (6, 9)), ('words', (11, 12)), ('vocabulary', (17, 18)), ('question', (24, 25)), ('passages', (26, 27))]","[['mechanism', 'to', 'Transformer based one'], ['Transformer based one', 'that allows', 'words'], ['words', 'to be generated from', 'vocabulary'], ['words', 'to be copied from', 'question'], ['words', 'to be copied from', 'passages']]",[],"[['Model', 'extend', 'mechanism']]",[],[],[],[],[],[],natural_language_inference,35,29
model,We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .,"[('that enables', (4, 6)), ('to control', (8, 10)), ('for', (15, 16))]","[('multi-style learning', (2, 4)), ('our model', (6, 8)), ('answer styles', (10, 12)), ('improves', (13, 14)), ('RC', (14, 15)), ('all styles involved', (16, 19))]","[['multi-style learning', 'that enables', 'our model'], ['our model', 'to control', 'answer styles'], ['RC', 'for', 'all styles involved']]","[['our model', 'has', 'improves'], ['improves', 'has', 'RC']]",[],"[['Model', 'introduce', 'multi-style learning']]",[],[],[],[],[],natural_language_inference,35,31
model,"We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .","[('to', (7, 8)), ('introducing', (12, 13)), ('corresponding to', (16, 18))]","[('pointer - generator', (4, 7)), ('conditional decoder', (9, 11)), ('artificial token', (14, 16)), ('each style', (18, 20))]","[['pointer - generator', 'introducing', 'artificial token'], ['artificial token', 'corresponding to', 'each style'], ['pointer - generator', 'to', 'conditional decoder']]",[],[],"[['Model', 'extend', 'pointer - generator']]",[],[],[],[],[],natural_language_inference,35,32
model,"For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .","[('For', (0, 1)), ('controls', (6, 7)), ('over', (10, 11))]","[('each decoding step', (1, 4)), ('mixture weights', (8, 10)), ('three distributions', (11, 13))]","[['each decoding step', 'controls', 'mixture weights'], ['mixture weights', 'over', 'three distributions']]",[],"[['Model', 'For', 'each decoding step']]",[],[],[],[],[],[],natural_language_inference,35,33
results,"shows that our single model , trained with two styles and controlled with the NQA style , pushed forward the state - of - the - art by a significant margin .",[],[],"[['our single model', 'pushed forward', 'state - of - the - art'], ['state - of - the - art', 'by', 'significant margin'], ['our single model', 'controlled with', 'NQA style'], ['our single model', 'trained with', 'two styles']]",[],[],"[['Results', 'has', 'our single model']]",[],[],[],[],[],natural_language_inference,35,182
results,The evaluation scores of the model controlled with the NLG style were low because the two styles are different .,"[('of', (3, 4)), ('controlled with', (6, 8)), ('were', (11, 12))]","[('evaluation scores', (1, 3)), ('model', (5, 6)), ('NLG style', (9, 11)), ('low', (12, 13))]","[['evaluation scores', 'of', 'model'], ['model', 'were', 'low'], ['model', 'controlled with', 'NLG style']]",[],[],"[['Results', 'has', 'evaluation scores']]",[],[],[],[],[],natural_language_inference,35,183
results,"Also , our model without multi-style learning ( trained with only the NQA style ) outperformed the baselines in terms of ROUGE - L .","[('without', (4, 5)), ('in terms of', (18, 21))]","[('our model', (2, 4)), ('multi-style learning', (5, 7)), ('outperformed', (15, 16)), ('baselines', (17, 18)), ('ROUGE - L', (21, 24))]","[['outperformed', 'in terms of', 'ROUGE - L'], ['our model', 'without', 'multi-style learning']]","[['our model', 'has', 'outperformed'], ['outperformed', 'has', 'baselines']]",[],"[['Results', 'has', 'our model']]",[],[],[],[],[],natural_language_inference,35,184
,Experiments on NarrativeQA,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,35,186
research-problem,Explicit Contextual Semantics for Text Comprehension,[],"[('Text Comprehension', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text Comprehension']]",[],[],[],[],natural_language_inference,36,2
research-problem,"This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) .",[],"[('text comprehension ( TC )', (6, 11)), ('machine reading comprehension ( MRC )', (13, 19)), ('textual entailment ( TE )', (20, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'text comprehension ( TC )'], ['Contribution', 'has research problem', 'machine reading comprehension ( MRC )'], ['Contribution', 'has research problem', 'textual entailment ( TE )']]",[],[],[],[],natural_language_inference,36,12
model,"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore integrative models for finer - grained text comprehension and inference .","[('explore', (17, 18)), ('for', (20, 21))]","[('integrative models', (18, 20)), ('finer - grained text comprehension and inference', (21, 28))]","[['integrative models', 'for', 'finer - grained text comprehension and inference']]",[],"[['Model', 'explore', 'integrative models']]",[],[],[],[],[],[],natural_language_inference,36,26
model,"In this work , we propose a semantics enhancement framework for TC tasks , which boosts the strong baselines effectively .","[('propose', (5, 6)), ('for', (10, 11))]","[('semantics enhancement framework', (7, 10)), ('TC tasks', (11, 13))]","[['semantics enhancement framework', 'for', 'TC tasks']]",[],"[['Model', 'propose', 'semantics enhancement framework']]",[],[],[],[],[],[],natural_language_inference,36,27
model,We implement an easy and feasible scheme to integrate semantic signals in downstream neural models in end - to - end manner to boost strong baselines effectively .,[],[],"[['easy and feasible scheme', 'to integrate', 'semantic signals'], ['semantic signals', 'in', 'downstream neural models'], ['downstream neural models', 'in', 'end - to - end manner'], ['downstream neural models', 'to boost', 'effectively']]","[['effectively', 'has', 'strong baselines']]","[['Model', 'implement', 'easy and feasible scheme']]",[],[],[],[],[],[],natural_language_inference,36,28
experiments,Textual Entailment,[],"[('Textual Entailment', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Textual Entailment']]","[['Textual Entailment', 'has', 'Results']]",natural_language_inference,36,168
experiments,Results in show that SRL embedding can boost the ESIM + ELMo model by + 0.7 % improvement .,"[('show', (2, 3)), ('by', (13, 14))]","[('SRL embedding', (4, 6)), ('boost', (7, 8)), ('ESIM + ELMo model', (9, 13)), ('+ 0.7 % improvement', (14, 18))]","[['ESIM + ELMo model', 'by', '+ 0.7 % improvement']]","[['SRL embedding', 'has', 'boost'], ['boost', 'has', 'ESIM + ELMo model']]",[],[],[],"[['Results', 'show', 'SRL embedding']]",[],[],[],natural_language_inference,36,172
experiments,"With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .","[('With', (0, 1)), ('yields', (10, 11)), ('achieves', (21, 22)), ('in', (36, 37))]","[('semantic cues', (2, 4)), ('simple sequential encoding model', (6, 10)), ('substantial gains', (11, 13)), ('our single BERT LARGE model', (15, 20)), ('new stateof - the - art', (23, 29)), ('outperforms', (31, 32)), ('all the ensemble models', (32, 36)), ('leaderboard', (38, 39))]","[['simple sequential encoding model', 'yields', 'substantial gains'], ['our single BERT LARGE model', 'achieves', 'new stateof - the - art'], ['all the ensemble models', 'in', 'leaderboard']]","[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]",[],[],[],"[['Results', 'With', 'semantic cues']]",[],[],[],natural_language_inference,36,173
experiments,Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Machine Reading Comprehension']]","[['Machine Reading Comprehension', 'has', 'Baselines']]",natural_language_inference,36,181
experiments,"Our baseline includes MQAN for single task and multi-task with SRL , BiDAF + ELMo , R.M. Reader and BERT .","[('includes', (2, 3)), ('for', (4, 5)), ('with', (9, 10))]","[('MQAN', (3, 4)), ('single task and multi-task', (5, 9)), ('SRL', (10, 11)), ('BiDAF + ELMo', (12, 15)), ('R.M. Reader', (16, 18)), ('BERT', (19, 20))]","[['MQAN', 'for', 'single task and multi-task'], ['single task and multi-task', 'with', 'SRL'], ['single task and multi-task', 'with', 'BiDAF + ELMo'], ['single task and multi-task', 'with', 'R.M. Reader'], ['single task and multi-task', 'with', 'BERT']]",[],[],[],[],"[['Baselines', 'includes', 'MQAN']]",[],[],[],natural_language_inference,36,186
experiments,"9 . The SRL embeddings give substantial performance gains over all the strong baselines , showing it is also quite effective for more complex document and question encoding .","[('give', (5, 6)), ('over', (9, 10)), ('showing', (15, 16)), ('for', (21, 22))]","[('SRL embeddings', (3, 5)), ('substantial performance gains', (6, 9)), ('all the strong baselines', (10, 14)), ('quite effective', (19, 21)), ('more complex document and question encoding', (22, 28))]","[['SRL embeddings', 'give', 'substantial performance gains'], ['substantial performance gains', 'over', 'all the strong baselines'], ['SRL embeddings', 'showing', 'quite effective'], ['quite effective', 'for', 'more complex document and question encoding']]",[],[],[],[],[],[],"[['Results', 'has', 'SRL embeddings']]",[],natural_language_inference,36,188
research-problem,Simple and Effective Multi - Paragraph Reading Comprehension,[],"[('Multi - Paragraph Reading Comprehension', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi - Paragraph Reading Comprehension']]",[],[],[],[],natural_language_inference,37,2
research-problem,We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .,[],"[('neural paragraph - level question answering', (6, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural paragraph - level question answering']]",[],[],[],[],natural_language_inference,37,4
research-problem,The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .,[],"[('answering questions given a related paragraph', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'answering questions given a related paragraph']]",[],[],[],[],natural_language_inference,37,14
model,In this paper we start by proposing an improved pipelined method which achieves state - of - the - art results .,"[('proposing', (6, 7))]","[('improved pipelined method', (8, 11))]",[],[],"[['Model', 'proposing', 'improved pipelined method']]",[],[],[],[],[],[],natural_language_inference,37,21
model,"Then we introduce a method for training models to produce accurate per-paragraph confidence scores , and we show how combining this method with multiple paragraph selection further increases performance .","[('introduce', (2, 3)), ('for training', (5, 7)), ('to produce', (8, 10))]","[('method', (4, 5)), ('models', (7, 8)), ('accurate per-paragraph confidence scores', (10, 14))]","[['method', 'for training', 'models'], ['models', 'to produce', 'accurate per-paragraph confidence scores']]",[],"[['Model', 'introduce', 'method']]",[],[],[],[],[],[],natural_language_inference,37,22
model,We propose a TF - IDF heuristic to select which paragraphs to train and test on .,"[('propose', (1, 2)), ('to select', (7, 9)), ('to', (11, 12))]","[('TF - IDF heuristic', (3, 7)), ('paragraphs', (10, 11)), ('train and test on', (12, 16))]","[['TF - IDF heuristic', 'to select', 'paragraphs'], ['paragraphs', 'to', 'train and test on']]",[],"[['Model', 'propose', 'TF - IDF heuristic']]",[],[],[],[],[],[],natural_language_inference,37,24
model,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over all locations the answer text occurs .","[('use', (8, 9)), ('that marginalizes', (13, 15)), ('over', (19, 20)), ('occurs', (25, 26))]","[('summed objective function', (10, 13)), (""model 's output"", (16, 19)), ('all locations', (20, 22)), ('answer text', (23, 25))]","[['summed objective function', 'that marginalizes', ""model 's output""], [""model 's output"", 'over', 'all locations'], ['all locations', 'occurs', 'answer text']]",[],"[['Model', 'use', 'summed objective function']]",[],[],[],[],[],[],natural_language_inference,37,26
model,"We resolve these problems by sampling paragraphs from the context documents , including paragraphs that do not contain an answer , to train on .",[],[],"[['paragraphs', 'including', 'paragraphs'], ['paragraphs', 'do not contain', 'answer'], ['answer', 'to', 'train on'], ['paragraphs', 'from', 'context documents']]",[],"[['Model', 'sampling', 'paragraphs']]",[],[],[],[],[],[],natural_language_inference,37,31
model,"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .","[('where', (8, 9)), ('processed', (11, 12)), ('of', (17, 18)), ('is', (21, 22)), ('over', (23, 24)), ('sampled from', (26, 28))]","[('shared - normalization objective', (4, 8)), ('paragraphs', (9, 10)), ('independently', (12, 13)), ('probability', (16, 17)), ('answer candidate', (19, 21)), ('marginalized', (22, 23)), ('all paragraphs', (24, 26)), ('same document', (29, 31))]","[['shared - normalization objective', 'where', 'paragraphs'], ['paragraphs', 'processed', 'independently'], ['probability', 'of', 'answer candidate'], ['probability', 'is', 'marginalized'], ['marginalized', 'over', 'all paragraphs'], ['all paragraphs', 'sampled from', 'same document']]","[['paragraphs', 'has', 'probability']]",[],"[['Model', 'use', 'shared - normalization objective']]",[],[],[],[],[],natural_language_inference,37,32
hyperparameters,"We train the model with the Adadelta optimizer ( Zeiler , 2012 ) with a batch size 60 for Triv - ia QA and 45 for SQuAD .",[],[],"[['model', 'with', 'Adadelta optimizer ( Zeiler , 2012 )'], ['Adadelta optimizer ( Zeiler , 2012 )', 'with', 'batch size'], ['45', 'for', 'SQuAD'], ['60', 'for', 'Triv - ia QA']]","[['batch size', 'has', '45'], ['batch size', 'has', '60']]","[['Hyperparameters', 'train', 'model']]",[],[],[],[],[],[],natural_language_inference,37,164
hyperparameters,The Glo Ve 300 dimensional word vectors released by are used for word embeddings .,"[('used for', (10, 12))]","[('Glo Ve 300 dimensional word vectors', (1, 7)), ('word embeddings', (12, 14))]","[['Glo Ve 300 dimensional word vectors', 'used for', 'word embeddings']]",[],[],"[['Hyperparameters', 'has', 'Glo Ve 300 dimensional word vectors']]",[],[],[],[],[],natural_language_inference,37,168
hyperparameters,"On SQuAD , we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism .",[],[],"[['SQuAD', 'use', 'dimensionality'], ['dimensionality', 'of size', '100'], ['100', 'for', 'GRUs'], ['dimensionality', 'of size', '200'], ['200', 'for', 'linear layers'], ['linear layers', 'employed after', 'each attention mechanism']]",[],"[['Hyperparameters', 'On', 'SQuAD']]",[],[],[],[],[],[],natural_language_inference,37,169
hyperparameters,"We find for TriviaQA , likely because there is more data , using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial .",[],[],"[['TriviaQA', 'using', 'larger dimensionality'], ['larger dimensionality', 'of', '280'], ['280', 'for', 'linear layers'], ['linear layers', 'is', 'beneficial'], ['larger dimensionality', 'of', '140'], ['140', 'for', 'each GRU']]",[],"[['Hyperparameters', 'for', 'TriviaQA']]",[],[],[],[],[],[],natural_language_inference,37,170
hyperparameters,"During training , we maintain an exponential moving average of the weights with a decay rate of 0.999 .",[],[],"[['training', 'maintain', 'exponential moving average'], ['exponential moving average', 'with', 'decay rate'], ['decay rate', 'of', '0.999'], ['exponential moving average', 'of', 'weights']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,37,171
results,Trivia QA Web,[],"[('Trivia QA Web', (0, 3))]",[],[],[],"[['Results', 'has', 'Trivia QA Web']]",[],[],[],[],[],natural_language_inference,37,174
results,We find both TF - IDF ranking and the sum objective to be effective ; even without changing the model we achieve state - of - the - art results .,"[('find', (1, 2)), ('to be', (11, 13))]","[('TF - IDF ranking', (3, 7)), ('sum objective', (9, 11)), ('effective', (13, 14))]","[['effective', 'to be', 'TF - IDF ranking'], ['effective', 'to be', 'sum objective']]",[],[],[],[],"[['Trivia QA Web', 'find', 'effective']]",[],[],[],natural_language_inference,37,180
results,Using our refined model increases the gain by another 4 points .,"[('Using', (0, 1)), ('by', (7, 8))]","[('refined model', (2, 4)), ('increases', (4, 5)), ('gain', (6, 7)), ('another 4 points', (8, 11))]","[['gain', 'by', 'another 4 points']]","[['refined model', 'has', 'increases'], ['increases', 'has', 'gain']]",[],[],[],"[['Trivia QA Web', 'Using', 'refined model']]",[],[],[],natural_language_inference,37,181
results,"The shared - norm , merge , and no-answer training methods improve the model 's ability to utilize more text , with the shared - norm method being significantly ahead of the others on the verified set and tied with the merge approach on the general set .",[],[],"[['improve', 'with', 'shared - norm method'], ['shared - norm method', 'being', 'tied'], ['tied', 'with', 'merge approach'], ['merge approach', 'on', 'general set'], ['shared - norm method', 'being', 'significantly ahead'], ['significantly ahead', 'of', 'others'], ['others', 'on', 'verified set'], [""model 's ability"", 'to utilize', 'more text']]","[['shared - norm , merge , and no-answer training methods', 'has', 'improve'], ['improve', 'has', ""model 's ability""]]",[],[],[],[],[],"[['Trivia QA Web', 'has', 'shared - norm , merge , and no-answer training methods']]",[],natural_language_inference,37,186
results,Trivia QA Unfiltered,[],"[('Trivia QA Unfiltered', (0, 3))]",[],[],[],"[['Results', 'has', 'Trivia QA Unfiltered']]",[],[],[],[],"[['Trivia QA Unfiltered', 'has', 'base model']]",natural_language_inference,37,187
results,"Note the base model starts to lose performance as more paragraphs are used , showing that errors are being caused by the model being overly confident in incorrect extractions . :","[('starts to', (4, 6)), ('as', (8, 9)), ('are', (11, 12))]","[('base model', (2, 4)), ('lose', (6, 7)), ('performance', (7, 8)), ('more paragraphs', (9, 11)), ('used', (12, 13))]","[['base model', 'starts to', 'lose'], ['lose', 'as', 'more paragraphs'], ['more paragraphs', 'are', 'used']]","[['lose', 'has', 'performance']]",[],[],[],[],[],[],[],natural_language_inference,37,192
results,SQuAD,[],"[('SQuAD', (0, 1))]",[],[],[],"[['Results', 'has', 'SQuAD']]",[],[],[],[],"[['SQuAD', 'has', 'shared - norm model'], ['SQuAD', 'has', 'all our approaches']]",natural_language_inference,37,196
results,"While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .","[('is', (13, 14)), ('as', (26, 27)), ('are', (31, 32))]","[('all our approaches', (1, 4)), ('some benefit', (5, 7)), ('shared - norm model', (9, 13)), ('strongest', (15, 16)), ('not lose', (23, 25)), ('performance', (25, 26)), ('large numbers of paragraphs', (27, 31)), ('used', (32, 33))]","[['shared - norm model', 'is', 'strongest'], ['performance', 'as', 'large numbers of paragraphs'], ['large numbers of paragraphs', 'are', 'used']]","[['shared - norm model', 'has', 'not lose'], ['not lose', 'has', 'performance'], ['all our approaches', 'has', 'some benefit']]",[],[],[],[],[],[],[],natural_language_inference,37,210
results,"Our paragraph - level model is competitive on this task , and our variations to handle the multi-paragraph setting only cause a minor loss of performance .","[('is', (5, 6)), ('to handle', (14, 16)), ('cause', (20, 21)), ('of', (24, 25))]","[('Our paragraph - level model', (0, 5)), ('competitive', (6, 7)), ('our variations', (12, 14)), ('multi-paragraph setting', (17, 19)), ('minor loss', (22, 24)), ('performance', (25, 26))]","[['our variations', 'to handle', 'multi-paragraph setting'], ['multi-paragraph setting', 'cause', 'minor loss'], ['minor loss', 'of', 'performance'], ['Our paragraph - level model', 'is', 'competitive']]",[],[],[],[],[],[],"[['SQuAD', 'has', 'our variations'], ['SQuAD', 'has', 'Our paragraph - level model']]",[],natural_language_inference,37,216
results,The base model starts to drop in performance once more than two paragraphs are used .,"[('starts to', (3, 5)), ('in', (6, 7)), ('once', (8, 9)), ('are', (13, 14))]","[('base model', (1, 3)), ('drop', (5, 6)), ('performance', (7, 8)), ('more than two paragraphs', (9, 13)), ('used', (14, 15))]","[['base model', 'starts to', 'drop'], ['drop', 'in', 'performance'], ['drop', 'once', 'more than two paragraphs'], ['more than two paragraphs', 'are', 'used']]",[],[],[],[],[],[],"[['SQuAD', 'has', 'base model']]",[],natural_language_inference,37,219
results,"However , the shared - norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs .","[('able to', (8, 10)), ('of', (14, 15)), ('given', (20, 21))]","[('shared - norm approach', (3, 7)), ('reach', (10, 11)), ('peak performance', (12, 14)), ('72.37 F1', (15, 17)), ('64.08 EM', (18, 20)), ('15 paragraphs', (21, 23))]","[['shared - norm approach', 'able to', 'reach'], ['peak performance', 'given', '15 paragraphs'], ['peak performance', 'of', '72.37 F1'], ['peak performance', 'of', '64.08 EM']]","[['reach', 'has', 'peak performance']]",[],[],[],[],[],"[['SQuAD', 'has', 'shared - norm approach']]",[],natural_language_inference,37,220
research-problem,MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension,[],"[('Machine Comprehension', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,38,2
research-problem,Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .,[],"[('Machine comprehension ( MC ) style question answering', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine comprehension ( MC ) style question answering']]",[],[],[],[],natural_language_inference,38,4
research-problem,Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .,[],"[('Machine comprehension ( MC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine comprehension ( MC )']]",[],[],[],[],natural_language_inference,38,12
model,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .","[('introduce', (5, 6)), ('for', (26, 27))]","[('Multi - layer Embedding with Memory Networks ( MEMEN )', (7, 17)), ('end - to - end neural network', (19, 26)), ('machine comprehension task', (27, 30))]","[['end - to - end neural network', 'for', 'machine comprehension task']]","[['Multi - layer Embedding with Memory Networks ( MEMEN )', 'has', 'end - to - end neural network']]","[['Model', 'introduce', 'Multi - layer Embedding with Memory Networks ( MEMEN )']]",[],[],[],[],[],[],natural_language_inference,38,27
model,Our model consists of three parts :,"[('consists of', (2, 4))]","[('three parts', (4, 6))]",[],[],"[['Model', 'consists of', 'three parts']]",[],[],[],[],[],"[['three parts', 'has', 'high - efficiency multilayer memory network'], ['three parts', 'has', 'pointer - network based answer boundary prediction layer'], ['three parts', 'has', 'encoding']]",natural_language_inference,38,28
model,"1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .",[],[],"[['high - efficiency multilayer memory network', 'of', 'full - orientation matching'], ['full - orientation matching', 'to match', 'question and context'], ['pointer - network based answer boundary prediction layer', 'to get', 'location'], ['location', 'of', 'answer'], ['answer', 'in', 'passage'], ['encoding', 'in which', 'add'], ['useful syntactic and semantic information', 'in', 'embedding'], ['embedding', 'of', 'every word'], ['encoding', 'of', 'context and query']]","[['add', 'has', 'useful syntactic and semantic information']]",[],[],[],[],[],[],[],natural_language_inference,38,29
experimental-setup,The tokenizers we use in the step of preprocessing data are from Stanford CoreNLP .,"[('use in', (3, 5)), ('of', (7, 8)), ('from', (11, 12))]","[('tokenizers', (1, 2)), ('step', (6, 7)), ('preprocessing', (8, 9)), ('data', (9, 10)), ('Stanford CoreNLP', (12, 14))]","[['tokenizers', 'use in', 'step'], ['step', 'of', 'preprocessing'], ['tokenizers', 'from', 'Stanford CoreNLP']]","[['preprocessing', 'has', 'data']]",[],"[['Experimental setup', 'has', 'tokenizers']]",[],[],[],[],[],natural_language_inference,38,110
experimental-setup,We also use part - of - speech tagger and named - entity recognition tagger in Stanford CoreNLP utilities to transform the passage and question .,"[('use', (2, 3)), ('in', (15, 16)), ('to transform', (19, 21))]","[('part - of - speech tagger', (3, 9)), ('named - entity recognition tagger', (10, 15)), ('Stanford CoreNLP utilities', (16, 19)), ('passage and question', (22, 25))]","[['passage and question', 'in', 'Stanford CoreNLP utilities'], ['Stanford CoreNLP utilities', 'use', 'part - of - speech tagger'], ['Stanford CoreNLP utilities', 'use', 'named - entity recognition tagger']]",[],"[['Experimental setup', 'to transform', 'passage and question']]",[],[],[],[],[],[],natural_language_inference,38,111
experimental-setup,"For the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .","[('For', (0, 1)), ('refers to', (9, 11)), ('in', (15, 16)), ('set as', (27, 29))]","[('skip - gram model', (2, 6)), ('our model', (7, 9)), ('word2 vec module', (12, 15)), ('open source software library', (16, 20)), ('Tensorflow', (21, 22)), ('skip window', (24, 26)), ('2', (29, 30))]","[['our model', 'refers to', 'word2 vec module'], ['word2 vec module', 'in', 'open source software library'], ['skip window', 'set as', '2']]","[['skip - gram model', 'has', 'our model'], ['open source software library', 'name', 'Tensorflow'], ['word2 vec module', 'has', 'skip window']]","[['Experimental setup', 'For', 'skip - gram model']]",[],[],[],[],[],[],natural_language_inference,38,112
experimental-setup,"To improve the reliability and stabllity , we screen out the sentences whose length are shorter than 9 .","[('To improve', (0, 2)), ('screen out', (8, 10)), ('whose', (12, 13)), ('are', (14, 15))]","[('reliability and stabllity', (3, 6)), ('sentences', (11, 12)), ('length', (13, 14)), ('shorter than 9', (15, 18))]","[['reliability and stabllity', 'screen out', 'sentences'], ['sentences', 'whose', 'length'], ['length', 'are', 'shorter than 9']]",[],"[['Experimental setup', 'To improve', 'reliability and stabllity']]",[],[],[],[],[],[],natural_language_inference,38,114
experimental-setup,"We use 100 one dimensional filters for CNN in the character level embedding , with width of 5 for each one .",[],[],"[['filters', 'with', 'width'], ['width', 'of', '5'], ['5', 'for', 'each one'], ['filters', 'for', 'CNN'], ['CNN', 'in', 'character level embedding']]","[['100 one dimensional', 'has', 'filters']]","[['Experimental setup', 'use', '100 one dimensional']]",[],[],[],[],[],[],natural_language_inference,38,115
experimental-setup,We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0.2 .,[],[],"[['hidden size', 'as', '100'], ['100', 'for', 'all the LSTM and GRU layers'], ['dropout', 'between', 'layers'], ['layers', 'with', 'dropout ratio'], ['dropout ratio', 'as', '0.2']]",[],"[['Experimental setup', 'set', 'hidden size'], ['Experimental setup', 'apply', 'dropout']]",[],[],[],[],[],[],natural_language_inference,38,116
experimental-setup,"We use the AdaDelta ( Zeiler , 2012 ) optimizer with a initial learning rate as 0.001 .","[('with', (10, 11)), ('as', (15, 16))]","[('AdaDelta ( Zeiler , 2012 ) optimizer', (3, 10)), ('initial learning rate', (12, 15)), ('0.001', (16, 17))]","[['AdaDelta ( Zeiler , 2012 ) optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'as', '0.001']]",[],[],"[['Experimental setup', 'use', 'AdaDelta ( Zeiler , 2012 ) optimizer']]",[],[],[],[],[],natural_language_inference,38,117
experimental-setup,"For the memory networks , we set the number of layer as 3 .","[('set', (6, 7)), ('as', (11, 12))]","[('memory networks', (2, 4)), ('number of layer', (8, 11)), ('3', (12, 13))]","[['memory networks', 'set', 'number of layer'], ['number of layer', 'as', '3']]",[],[],"[['Experimental setup', 'For', 'memory networks']]",[],[],[],[],[],natural_language_inference,38,118
results,"As we can see in , our model outperforms all other baselines and achieves the state - of - the - art result on all subsets on TriviaQA .","[('see', (3, 4)), ('achieves', (13, 14)), ('on', (23, 24))]","[('our model', (6, 8)), ('outperforms', (8, 9)), ('all other baselines', (9, 12)), ('state - of - the - art result', (15, 23)), ('all subsets on TriviaQA', (24, 28))]","[['our model', 'achieves', 'state - of - the - art result'], ['state - of - the - art result', 'on', 'all subsets on TriviaQA']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'all other baselines']]","[['Results', 'see', 'our model']]",[],[],[],[],[],[],natural_language_inference,38,129
results,We also use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,"[('use', (2, 3)), ('to', (13, 14))]","[('Stanford Question Answering Dataset ( SQuAD ) v 1.1', (4, 13))]",[],[],"[['Results', 'use', 'Stanford Question Answering Dataset ( SQuAD ) v 1.1']]",[],[],[],[],[],"[['Stanford Question Answering Dataset ( SQuAD ) v 1.1', 'has', 'our model']]",natural_language_inference,38,132
results,"The results of this dataset are all exhibited on a leaderboard , and top methods are almost all ensemble models , our model achieves an exact match score of 75.37 % and an F1 score of 82 . 66 % , which is competitive to state - of - the - art method .",[],[],"[['our model', 'is', 'competitive'], ['state - of - the - art method', 'achieves', 'exact match score'], ['exact match score', 'of', '75.37 %'], ['state - of - the - art method', 'achieves', 'F1 score'], ['F1 score', 'of', '82 . 66 %']]","[['competitive', 'to', 'state - of - the - art method']]",[],[],[],[],[],[],[],natural_language_inference,38,138
ablation-analysis,We also run the ablations of our single model on SQ u AD dev set to evaluate the individual contribution .,"[('of', (5, 6)), ('on', (9, 10))]","[('single model', (7, 9)), ('SQ u AD dev set', (10, 15))]","[['single model', 'on', 'SQ u AD dev set']]",[],"[['Ablation analysis', 'of', 'single model']]",[],[],[],[],[],"[['single model', 'has', 'contribute']]",natural_language_inference,38,156
ablation-analysis,"As shows , both syntactic embeddings and semantic embeddings contribute towards the model 's performance and the POS tags seem to be more important .","[('both', (3, 4)), ('towards', (10, 11)), ('to be', (20, 22))]","[('syntactic embeddings', (4, 6)), ('semantic embeddings', (7, 9)), ('contribute', (9, 10)), (""model 's performance"", (12, 15)), ('POS tags', (17, 19)), ('more important', (22, 24))]","[['contribute', 'towards', ""model 's performance""], ['POS tags', 'to be', 'more important'], [""model 's performance"", 'both', 'syntactic embeddings'], [""model 's performance"", 'both', 'semantic embeddings']]","[[""model 's performance"", 'has', 'POS tags']]",[],[],[],[],[],[],[],natural_language_inference,38,157
ablation-analysis,"For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .","[('For ablating', (0, 2)), ('about', (9, 10)), ('shows that', (17, 19)), ('of', (22, 23)), ('for', (24, 25)), ('in', (27, 28)), ('is', (29, 30))]","[('integral query matching', (2, 5)), ('result', (7, 8)), ('drops', (8, 9)), ('2 %', (10, 12)), ('integral information', (20, 22)), ('query', (23, 24)), ('each word', (25, 27)), ('passage', (28, 29)), ('crucial', (30, 31))]","[['integral query matching', 'shows that', 'integral information'], ['integral information', 'of', 'query'], ['query', 'for', 'each word'], ['each word', 'in', 'passage'], ['each word', 'is', 'crucial'], ['drops', 'about', '2 %']]","[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","[['Ablation analysis', 'For ablating', 'integral query matching']]",[],[],[],[],[],[],natural_language_inference,38,160
ablation-analysis,"The query - based similarity matching accounts for about 10 % performance degradation , which proves the effectiveness of alignment context words against query .","[('accounts for', (6, 8)), ('proves', (15, 16)), ('of', (18, 19)), ('against', (22, 23))]","[('query - based similarity matching', (1, 6)), ('about 10 % performance degradation', (8, 13)), ('effectiveness', (17, 18)), ('alignment context words', (19, 22)), ('query', (23, 24))]","[['query - based similarity matching', 'accounts for', 'about 10 % performance degradation'], ['about 10 % performance degradation', 'proves', 'effectiveness'], ['effectiveness', 'of', 'alignment context words'], ['alignment context words', 'against', 'query']]",[],[],"[['Ablation analysis', 'has', 'query - based similarity matching']]",[],[],[],[],[],natural_language_inference,38,161
ablation-analysis,"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be contributory to the performance of full - orientation matching .","[('For', (0, 1)), ('took out', (9, 11)), ('from', (14, 15)), ('proved to be', (21, 24)), ('to', (25, 26)), ('of', (28, 29))]","[('context - based similarity matching', (1, 6)), ('M 3', (12, 14)), ('linear function', (16, 18)), ('contributory', (24, 25)), ('performance', (27, 28)), ('full - orientation matching', (29, 33))]","[['context - based similarity matching', 'took out', 'M 3'], ['M 3', 'proved to be', 'contributory'], ['contributory', 'to', 'performance'], ['performance', 'of', 'full - orientation matching'], ['M 3', 'from', 'linear function']]",[],"[['Ablation analysis', 'For', 'context - based similarity matching']]",[],[],[],[],[],[],natural_language_inference,38,162
research-problem,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,[],"[('Semantic Similarity Measurement', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Similarity Measurement']]",[],[],[],[],natural_language_inference,39,2
research-problem,"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences .",[],"[('Textual similarity measurement', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Textual similarity measurement']]",[],[],[],[],natural_language_inference,39,4
research-problem,"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation .",[],"[('semantic textual similarity ( STS )', (8, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic textual similarity ( STS )']]",[],[],[],[],natural_language_inference,39,9
approach,"In contrast , we focus on capturing fine - grained word - level information directly .","[('focus on', (4, 6))]","[('capturing', (6, 7)), ('fine - grained word - level information', (7, 14))]",[],"[['capturing', 'has', 'fine - grained word - level information']]","[['Approach', 'focus on', 'capturing']]",[],[],[],[],[],[],natural_language_inference,39,16
approach,"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .","[('instead of using', (2, 5)), ('propose', (9, 10)), ('that encourages', (14, 16)), ('across', (20, 21))]","[('sentence modeling', (5, 7)), ('pairwise word interaction modeling', (10, 14)), ('explicit word context interactions', (16, 20)), ('sentences', (21, 22))]","[['sentence modeling', 'propose', 'pairwise word interaction modeling'], ['pairwise word interaction modeling', 'that encourages', 'explicit word context interactions'], ['explicit word context interactions', 'across', 'sentences']]",[],"[['Approach', 'instead of using', 'sentence modeling']]",[],[],[],[],[],[],natural_language_inference,39,18
approach,"Second , based on the pairwise word interactions , we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement .","[('based on', (2, 4)), ('describe', (10, 11)), ('which', (16, 17)), ('selectively identify', (20, 22)), ('for', (29, 30))]","[('pairwise word interactions', (5, 8)), ('novel similarity focus layer', (12, 16)), ('helps', (17, 18)), ('model', (19, 20)), ('important word interactions', (22, 25)), ('similarity measurement', (30, 32))]","[['pairwise word interactions', 'describe', 'novel similarity focus layer'], ['novel similarity focus layer', 'which', 'helps'], ['model', 'selectively identify', 'important word interactions'], ['important word interactions', 'for', 'similarity measurement']]","[['helps', 'has', 'model']]","[['Approach', 'based on', 'pairwise word interactions']]",[],[],[],[],[],[],natural_language_inference,39,20
experimental-setup,"For the SICK and MSRVID experiments , we used 300 - dimension Glo Ve word embeddings .","[('For', (0, 1)), ('used', (8, 9))]","[('SICK and MSRVID experiments', (2, 6)), ('300 - dimension', (9, 12)), ('Glo Ve word embeddings', (12, 16))]","[['SICK and MSRVID experiments', 'used', '300 - dimension']]","[['300 - dimension', 'has', 'Glo Ve word embeddings']]","[['Experimental setup', 'For', 'SICK and MSRVID experiments']]",[],[],[],[],[],[],natural_language_inference,39,155
experimental-setup,"For the STS2014 , WikiQA , and TrecQA experiments , we used 300 dimension PARAGRAM - SL999 embeddings from and the PARAGRAM - PHRASE embeddings from , trained on word pairs from the Paraphrase Database ( PPDB ) .","[('used', (11, 12)), ('from', (18, 19)), ('trained on', (27, 29))]","[('STS2014 , WikiQA , and TrecQA experiments', (2, 9)), ('300 dimension', (12, 14)), ('PARAGRAM - SL999 embeddings', (14, 18)), ('PARAGRAM - PHRASE embeddings', (21, 25)), ('word pairs', (29, 31)), ('Paraphrase Database ( PPDB )', (33, 38))]","[['STS2014 , WikiQA , and TrecQA experiments', 'trained on', 'word pairs'], ['word pairs', 'from', 'Paraphrase Database ( PPDB )'], ['Paraphrase Database ( PPDB )', 'used', '300 dimension']]","[['300 dimension', 'has', 'PARAGRAM - SL999 embeddings'], ['300 dimension', 'has', 'PARAGRAM - PHRASE embeddings']]",[],"[['Experimental setup', 'For', 'STS2014 , WikiQA , and TrecQA experiments']]",[],[],[],[],[],natural_language_inference,39,156
experimental-setup,Our timing experiments were conducted on an Intel Xeon E5 - 2680 CPU .,"[('conducted on', (4, 6))]","[('Our timing experiments', (0, 3)), ('Intel Xeon E5 - 2680 CPU', (7, 13))]","[['Our timing experiments', 'conducted on', 'Intel Xeon E5 - 2680 CPU']]",[],[],"[['Experimental setup', 'has', 'Our timing experiments']]",[],[],[],[],[],natural_language_inference,39,161
experimental-setup,"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..",[],[],"[['sentence length variations', 'for', 'SICK and MSRVID data'], ['SICK and MSRVID data', 'padded', 'sentences'], ['sentences', 'to', '32 words'], ['sentence length variations', 'for', 'STS2014 , WikiQA , and TrecQA data'], ['STS2014 , WikiQA , and TrecQA data', 'padded', 'sentences'], ['sentences', 'to', '48 words']]",[],"[['Experimental setup', 'Due to', 'sentence length variations']]",[],[],[],[],[],[],natural_language_inference,39,162
results,Wiki QA Results .,[],"[('Wiki QA', (0, 2))]",[],[],[],"[['Results', 'has', 'Wiki QA']]",[],[],[],[],"[['Wiki QA', 'has', 'paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features']]",natural_language_inference,39,176
results,"The neural network models in the table , paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features , are mostly based on sentence modeling .",[],"[('paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features', (8, 28))]",[],[],[],[],[],[],[],[],"[['paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features', 'has', 'Our model']]",natural_language_inference,39,178
results,Our model outperforms them all .,[],"[('Our model', (0, 2)), ('outperforms', (2, 3))]",[],"[['Our model', 'has', 'outperforms']]",[],[],[],[],[],[],[],natural_language_inference,39,179
ablation-analysis,"We found large drops when removing the context modeling component , indicating that the context information provided by the Bi - LSTMs is crucial for the following components ( e.g. , interaction modeling ) .","[('found', (1, 2)), ('removing', (5, 6))]","[('large drops', (2, 4)), ('context modeling component', (7, 10))]","[['large drops', 'removing', 'context modeling component']]",[],"[['Ablation analysis', 'found', 'large drops']]",[],[],[],[],[],[],natural_language_inference,39,190
ablation-analysis,"The use of our similarity focus layer is also beneficial , especially on the WikiQA data .","[('use', (1, 2)), ('is', (7, 8)), ('on', (12, 13))]","[('similarity focus layer', (4, 7)), ('beneficial', (9, 10)), ('WikiQA data', (14, 16))]","[['similarity focus layer', 'is', 'beneficial'], ['beneficial', 'on', 'WikiQA data']]",[],"[['Ablation analysis', 'use', 'similarity focus layer']]",[],[],[],[],[],[],natural_language_inference,39,191
ablation-analysis,"When we replaced the entire similarity focus layer with a random dropout layer ( p = 0.3 ) , the dropout layer hurts accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .","[('replaced', (2, 3)), ('with', (8, 9))]","[('entire similarity focus layer', (4, 8)), ('random dropout layer ( p = 0.3 )', (10, 18)), ('hurts', (22, 23)), ('accuracy', (23, 24))]","[['entire similarity focus layer', 'with', 'random dropout layer ( p = 0.3 )']]","[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","[['Ablation analysis', 'replaced', 'entire similarity focus layer']]",[],[],[],[],[],[],natural_language_inference,39,192
research-problem,DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,[],"[('QUESTION ANSWERING', (10, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'QUESTION ANSWERING']]",[],[],[],[],natural_language_inference,4,2
model,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .","[('propose', (6, 7)), ('combines', (11, 12)), ('over', (16, 17)), ('with', (18, 19)), ('trained with', (24, 26))]","[('mixed objective', (8, 10)), ('traditional cross entropy loss', (12, 16)), ('positions', (17, 18)), ('measure of word overlap', (20, 24)), ('reinforcement learning', (26, 28))]","[['mixed objective', 'combines', 'traditional cross entropy loss'], ['traditional cross entropy loss', 'over', 'positions'], ['traditional cross entropy loss', 'with', 'measure of word overlap'], ['measure of word overlap', 'trained with', 'reinforcement learning']]",[],"[['Model', 'propose', 'mixed objective']]",[],[],[],[],[],[],natural_language_inference,4,18
model,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,"[('obtain', (1, 2)), ('using', (5, 6)), ('in which', (11, 13)), ('based on', (16, 18)), ('between', (20, 21))]","[('latter objective', (3, 5)), ('self - critical policy learning', (6, 11)), ('reward', (14, 15)), ('word overlap', (18, 20)), ('proposed answer', (22, 24)), ('ground truth answer', (26, 29))]","[['latter objective', 'using', 'self - critical policy learning'], ['self - critical policy learning', 'in which', 'reward'], ['reward', 'based on', 'word overlap'], ['word overlap', 'between', 'proposed answer'], ['word overlap', 'between', 'ground truth answer']]",[],"[['Model', 'obtain', 'latter objective']]",[],[],[],[],[],[],natural_language_inference,4,19
model,"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .","[('extend', (9, 10)), ('with', (18, 19))]","[('Dynamic Coattention Network ( DCN )', (11, 17)), ('deep residual coattention encoder', (20, 24))]","[['Dynamic Coattention Network ( DCN )', 'with', 'deep residual coattention encoder']]",[],"[['Model', 'extend', 'Dynamic Coattention Network ( DCN )']]",[],[],[],[],[],[],natural_language_inference,4,22
experimental-setup,"To preprocess the corpus , we use the reversible tokenizer from Stanford CoreNLP .","[('preprocess', (1, 2)), ('use', (6, 7)), ('from', (10, 11))]","[('corpus', (3, 4)), ('reversible tokenizer', (8, 10)), ('Stanford CoreNLP', (11, 13))]","[['corpus', 'use', 'reversible tokenizer'], ['reversible tokenizer', 'from', 'Stanford CoreNLP']]",[],"[['Experimental setup', 'preprocess', 'corpus']]",[],[],[],[],[],[],natural_language_inference,4,126
experimental-setup,"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .","[('For', (0, 1)), ('use', (5, 6)), ('pretrained on', (8, 10))]","[('word embeddings', (1, 3)), ('GloVe embeddings', (6, 8)), ('840B Common Crawl corpus', (11, 15))]","[['word embeddings', 'use', 'GloVe embeddings'], ['GloVe embeddings', 'pretrained on', '840B Common Crawl corpus']]",[],"[['Experimental setup', 'For', 'word embeddings']]",[],[],[],[],[],[],natural_language_inference,4,127
experimental-setup,"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .","[('concatenate', (4, 5)), ('with', (7, 8))]","[('these embeddings', (5, 7)), ('context vectors ( CoVe )', (8, 13))]","[['these embeddings', 'with', 'context vectors ( CoVe )']]",[],[],[],[],"[['word embeddings', 'concatenate', 'these embeddings']]",[],[],[],natural_language_inference,4,128
experimental-setup,"For out of vocabulary words , we set the embeddings and context vectors to zero .","[('set', (7, 8)), ('to', (13, 14))]","[('out of vocabulary words', (1, 5)), ('embeddings and context vectors', (9, 13)), ('zero', (14, 15))]","[['out of vocabulary words', 'set', 'embeddings and context vectors'], ['embeddings and context vectors', 'to', 'zero']]",[],[],"[['Experimental setup', 'For', 'out of vocabulary words']]",[],[],[],[],[],natural_language_inference,4,129
experimental-setup,We perform word dropout on the document which zeros a word embedding with probability 0.075 .,"[('perform', (1, 2)), ('on', (4, 5)), ('zeros', (8, 9)), ('with', (12, 13))]","[('word dropout', (2, 4)), ('document', (6, 7)), ('word embedding', (10, 12)), ('probability 0.075', (13, 15))]","[['word dropout', 'on', 'document'], ['document', 'zeros', 'word embedding'], ['word embedding', 'with', 'probability 0.075']]",[],"[['Experimental setup', 'perform', 'word dropout']]",[],[],[],[],[],[],natural_language_inference,4,130
experimental-setup,"In addition , we swap the first maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer .",[],[],"[['first maxout layer', 'with', 'sparse mixture'], ['sparse mixture', 'of', 'experts layer'], ['first maxout layer', 'of', 'highway maxout network'], ['highway maxout network', 'in', 'DCN decoder']]",[],"[['Experimental setup', 'swap', 'first maxout layer']]",[],[],[],[],[],[],natural_language_inference,4,131
results,Comparison to baseline DCN with CoVe. DCN + outperforms the baseline by 3.2 % exact match accuracy and 3.2 % F1 on the SQuAD development set .,"[('Comparison to', (0, 2)), ('outperforms', (8, 9)), ('by', (11, 12)), ('on', (21, 22))]","[('DCN +', (6, 8)), ('baseline', (10, 11)), ('3.2 % exact match accuracy', (12, 17)), ('3.2 % F1', (18, 21)), ('SQuAD development set', (23, 26))]","[['DCN +', 'outperforms', 'baseline'], ['baseline', 'on', 'SQuAD development set'], ['SQuAD development set', 'by', '3.2 % exact match accuracy'], ['SQuAD development set', 'by', '3.2 % F1']]",[],[],[],[],[],[],"[['baseline DCN with CoVe', 'has', 'DCN +']]","[['Results', 'Comparison to', 'baseline DCN with CoVe']]",natural_language_inference,4,135
results,"shows the consistent performance gain of DCN + over the baseline across question types , question lengths , and answer lengths .","[('shows', (0, 1)), ('of', (5, 6)), ('over', (8, 9)), ('across', (11, 12))]","[('consistent performance gain', (2, 5)), ('DCN +', (6, 8)), ('baseline', (10, 11)), ('question types', (12, 14)), ('question lengths', (15, 17)), ('answer lengths', (19, 21))]","[['consistent performance gain', 'over', 'baseline'], ['consistent performance gain', 'across', 'question types'], ['consistent performance gain', 'across', 'question lengths'], ['consistent performance gain', 'across', 'answer lengths'], ['consistent performance gain', 'of', 'DCN +']]",[],"[['Results', 'shows', 'consistent performance gain']]",[],[],[],[],[],[],natural_language_inference,4,136
results,"In particular , DCN + provides a significant advantage for long questions .","[('provides', (5, 6)), ('for', (9, 10))]","[('DCN +', (3, 5)), ('significant advantage', (7, 9)), ('long questions', (10, 12))]","[['DCN +', 'provides', 'significant advantage'], ['significant advantage', 'for', 'long questions']]",[],[],"[['Results', 'has', 'DCN +']]",[],[],[],[],[],natural_language_inference,4,137
ablation-analysis,"We note that the deep residual coattention yielded the highest contribution to model performance , followed by the mixed objective .","[('note', (1, 2)), ('yielded', (7, 8)), ('to', (11, 12)), ('followed by', (15, 17))]","[('deep residual coattention', (4, 7)), ('highest contribution', (9, 11)), ('model performance', (12, 14)), ('mixed objective', (18, 20))]","[['deep residual coattention', 'yielded', 'highest contribution'], ['highest contribution', 'followed by', 'mixed objective'], ['highest contribution', 'to', 'model performance']]",[],"[['Ablation analysis', 'note', 'deep residual coattention']]",[],[],[],[],[],[],natural_language_inference,4,140
ablation-analysis,The sparse mixture of experts layer in the decoder added minor improvements to the model performance . :,"[('of', (3, 4)), ('in', (6, 7)), ('added', (9, 10)), ('to', (12, 13))]","[('sparse mixture', (1, 3)), ('experts layer', (4, 6)), ('decoder', (8, 9)), ('minor improvements', (10, 12)), ('model performance', (14, 16))]","[['sparse mixture', 'of', 'experts layer'], ['experts layer', 'in', 'decoder'], ['experts layer', 'added', 'minor improvements'], ['minor improvements', 'to', 'model performance']]",[],[],"[['Ablation analysis', 'has', 'sparse mixture']]",[],[],[],[],[],natural_language_inference,4,141
research-problem,Linguistic Knowledge as Memory for Recurrent Neural Networks,[],"[('Recurrent Neural Networks', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Recurrent Neural Networks']]",[],[],[],[],natural_language_inference,40,2
research-problem,Training recurrent neural networks to model long term dependencies is difficult .,[],"[('Training recurrent neural networks to model long term dependencies', (0, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Training recurrent neural networks to model long term dependencies']]",[],[],[],[],natural_language_inference,40,4
approach,"Instead , we utilize the order inherent in the the unaugmented sequence to decompose the graph into two Directed Acyclic Graphs ( DAGs ) with a topological ordering .","[('utilize', (3, 4)), ('in', (7, 8)), ('to decompose', (12, 14)), ('into', (16, 17)), ('with', (24, 25))]","[('order inherent', (5, 7)), ('unaugmented sequence', (10, 12)), ('graph', (15, 16)), ('two Directed Acyclic Graphs ( DAGs )', (17, 24)), ('topological ordering', (26, 28))]","[['order inherent', 'in', 'unaugmented sequence'], ['order inherent', 'to decompose', 'graph'], ['graph', 'into', 'two Directed Acyclic Graphs ( DAGs )'], ['two Directed Acyclic Graphs ( DAGs )', 'with', 'topological ordering']]",[],"[['Approach', 'utilize', 'order inherent']]",[],[],[],[],[],[],natural_language_inference,40,40
approach,"We introduce the Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework to compute the representation of such graphs while touching every node only once , and implement a GRU version of it called MAGE - GRU .","[('introduce', (1, 2)), ('to compute', (15, 17)), ('of', (19, 20)), ('while', (22, 23)), ('every', (24, 25)), ('implement', (30, 31)), ('called', (36, 37))]","[('Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework', (3, 15)), ('representation', (18, 19)), ('graphs', (21, 22)), ('touching', (23, 24)), ('node', (25, 26)), ('only once', (26, 28)), ('GRU version', (32, 34)), ('MAGE - GRU', (37, 40))]","[['Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework', 'to compute', 'representation'], ['representation', 'of', 'graphs'], ['representation', 'while', 'touching'], ['touching', 'every', 'node'], ['Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework', 'implement', 'GRU version'], ['GRU version', 'called', 'MAGE - GRU']]","[['touching', 'has', 'only once']]","[['Approach', 'introduce', 'Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework']]",[],[],[],[],[],[],natural_language_inference,40,41
approach,"MAGE - RNN learns separate representations for propagation along each edge type , which leads to superior performance empirically .","[('learns', (3, 4)), ('for', (6, 7)), ('along', (8, 9)), ('leads to', (14, 16))]","[('MAGE - RNN', (0, 3)), ('separate representations', (4, 6)), ('propagation', (7, 8)), ('each edge type', (9, 12)), ('superior performance empirically', (16, 19))]","[['MAGE - RNN', 'learns', 'separate representations'], ['separate representations', 'leads to', 'superior performance empirically'], ['separate representations', 'for', 'propagation'], ['propagation', 'along', 'each edge type']]",[],[],"[['Approach', 'has', 'MAGE - RNN']]",[],[],[],[],[],natural_language_inference,40,42
approach,"We use MAGE - RNN to model coreference relations for text comprehension tasks , where answers to a query have to be extracted from a context document .","[('use', (1, 2)), ('to model', (5, 7)), ('for', (9, 10)), ('where', (14, 15)), ('have to be extracted from', (19, 24))]","[('MAGE - RNN', (2, 5)), ('coreference relations', (7, 9)), ('text comprehension tasks', (10, 13)), ('answers to a query', (15, 19)), ('context document', (25, 27))]","[['MAGE - RNN', 'to model', 'coreference relations'], ['coreference relations', 'for', 'text comprehension tasks'], ['text comprehension tasks', 'where', 'answers to a query'], ['answers to a query', 'have to be extracted from', 'context document']]",[],"[['Approach', 'use', 'MAGE - RNN']]",[],[],[],[],[],"[['coreference relations', 'has', 'Tokens']]",natural_language_inference,40,44
approach,Tokens in a document are connected by a coreference relation if they refer to the same underlying entity .,"[('in', (1, 2)), ('connected by', (5, 7)), ('if', (10, 11)), ('to', (13, 14))]","[('Tokens', (0, 1)), ('document', (3, 4)), ('coreference relation', (8, 10)), ('refer', (12, 13)), ('same underlying entity', (15, 18))]","[['Tokens', 'in', 'document'], ['Tokens', 'connected by', 'coreference relation'], ['coreference relation', 'if', 'refer'], ['refer', 'to', 'same underlying entity']]",[],[],[],[],[],[],[],[],natural_language_inference,40,45
results,Story Based,[],"[('Story Based', (0, 2))]",[],[],[],"[['Results', 'has', 'Story Based']]",[],[],[],[],"[['Story Based', 'has', 'Our model']]",natural_language_inference,40,171
results,"Our model achieves new state - of - the - art results , outperforming strong baselines such as QRNs .","[('achieves', (2, 3)), ('such as', (16, 18))]","[('Our model', (0, 2)), ('new state - of - the - art results', (3, 12)), ('outperforming', (13, 14)), ('strong baselines', (14, 16)), ('QRNs', (18, 19))]","[['Our model', 'achieves', 'new state - of - the - art results'], ['strong baselines', 'such as', 'QRNs']]","[['Our model', 'has', 'outperforming'], ['outperforming', 'has', 'strong baselines']]",[],[],[],[],[],[],[],natural_language_inference,40,182
results,"Moreover , we observe that the proposed MAGE architecture can substantially improve the performance for both bi - GRUs and GAs .","[('observe', (3, 4)), ('for', (14, 15))]","[('proposed MAGE architecture', (6, 9)), ('substantially improve', (10, 12)), ('performance', (13, 14)), ('both bi - GRUs and GAs', (15, 21))]","[['performance', 'for', 'both bi - GRUs and GAs']]","[['proposed MAGE architecture', 'has', 'substantially improve'], ['substantially improve', 'has', 'performance']]",[],[],[],"[['Story Based', 'observe', 'proposed MAGE architecture']]",[],[],[],natural_language_inference,40,183
results,"Adding the same information as one - hot features fails to improve the performance , which indicates that the inductive bias we employ on MAGE is useful .","[('Adding', (0, 1)), ('as', (4, 5)), ('fails to', (9, 11))]","[('same information', (2, 4)), ('one - hot features', (5, 9)), ('improve', (11, 12)), ('performance', (13, 14))]","[['same information', 'as', 'one - hot features'], ['one - hot features', 'fails to', 'improve']]","[['improve', 'has', 'performance']]",[],[],[],"[['Story Based', 'Adding', 'same information']]",[],[],[],natural_language_inference,40,184
results,"The DAG - RNN baseline from and the shared version of MAGE ( where edge representations are tied ) also perform worse , showing that our proposed architecture is superior .","[('perform', (20, 21)), ('showing', (23, 24)), ('is', (28, 29))]","[('DAG - RNN baseline', (1, 5)), ('shared version of MAGE', (8, 12)), ('worse', (21, 22)), ('our proposed architecture', (25, 28)), ('superior', (29, 30))]","[['our proposed architecture', 'perform', 'worse'], ['our proposed architecture', 'is', 'superior']]","[['worse', 'has', 'DAG - RNN baseline'], ['worse', 'has', 'shared version of MAGE']]",[],[],[],"[['Story Based', 'showing', 'our proposed architecture']]",[],[],[],natural_language_inference,40,185
results,"Both variants of MAGE substantially outperform QRNs , which are the current state - of - the - art models on the bAbi dataset .","[('are', (9, 10)), ('on', (20, 21))]","[('Both variants of MAGE', (0, 4)), ('substantially outperform', (4, 6)), ('QRNs', (6, 7)), ('current state - of - the - art models', (11, 20)), ('bAbi dataset', (22, 24))]","[['QRNs', 'are', 'current state - of - the - art models'], ['current state - of - the - art models', 'on', 'bAbi dataset']]","[['Both variants of MAGE', 'has', 'substantially outperform'], ['substantially outperform', 'has', 'QRNs']]",[],[],[],[],[],"[['Story Based', 'has', 'Both variants of MAGE']]",[],natural_language_inference,40,203
results,Broad Context Language Modeling :,[],"[('Broad Context Language Modeling', (0, 4))]",[],[],[],"[['Results', 'has', 'Broad Context Language Modeling']]",[],[],[],[],[],natural_language_inference,40,220
results,"For our second benchmark we pick the LAMBADA dataset from , where the task is to predict the last word in a given passage .","[('pick', (5, 6))]","[('LAMBADA dataset', (7, 9))]",[],[],[],[],[],"[['Broad Context Language Modeling', 'pick', 'LAMBADA dataset']]",[],[],"[['LAMBADA dataset', 'has', 'Our implementation of GA']]",natural_language_inference,40,221
results,"Our implementation of GA gave higher performance than that reported by , without the use of linguistic features .","[('gave', (4, 5))]","[('Our implementation of GA', (0, 4)), ('higher performance', (5, 7))]","[['Our implementation of GA', 'gave', 'higher performance']]",[],[],[],[],[],[],[],[],natural_language_inference,40,232
results,"On the simple bi - GRU architecture we see an improvement of 1.7 % by incorporating coreference edges in the graph , whereas the one - hot baseline does not lead to any improvement .","[('On', (0, 1)), ('see', (8, 9)), ('of', (11, 12)), ('by incorporating', (14, 16)), ('in', (18, 19))]","[('simple bi - GRU architecture', (2, 7)), ('improvement', (10, 11)), ('1.7 %', (12, 14)), ('coreference edges', (16, 18)), ('graph', (20, 21))]","[['simple bi - GRU architecture', 'see', 'improvement'], ['improvement', 'of', '1.7 %'], ['1.7 %', 'by incorporating', 'coreference edges'], ['coreference edges', 'in', 'graph']]",[],[],[],[],"[['LAMBADA dataset', 'On', 'simple bi - GRU architecture']]",[],[],[],natural_language_inference,40,234
results,"On the multi - layer GA architecture , the coreference edges again lead to an improvement of 2 % , setting a new state - of - theart on this dataset .","[('lead to', (12, 14)), ('of', (16, 17)), ('setting', (20, 21))]","[('multi - layer GA architecture', (2, 7)), ('coreference edges', (9, 11)), ('improvement', (15, 16)), ('2 %', (17, 19)), ('new state - of - theart', (22, 28))]","[['coreference edges', 'lead to', 'improvement'], ['improvement', 'of', '2 %'], ['2 %', 'setting', 'new state - of - theart']]","[['multi - layer GA architecture', 'has', 'coreference edges']]",[],[],[],[],[],"[['LAMBADA dataset', 'On', 'multi - layer GA architecture']]",[],natural_language_inference,40,236
results,"Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .","[('on', (11, 12)), ('of', (19, 20))]","[('Cloze - style QA', (0, 4)), ('CNN dataset', (13, 15))]","[['Cloze - style QA', 'on', 'CNN dataset']]",[],[],"[['Results', 'has', 'Cloze - style QA']]",[],[],[],[],[],natural_language_inference,40,245
results,Augmenting the bi - GRU model with MAGE leads to an improvement of 2.5 % on the test set .,"[('Augmenting', (0, 1)), ('with', (6, 7)), ('leads to', (8, 10)), ('on', (15, 16))]","[('bi - GRU model', (2, 6)), ('MAGE', (7, 8)), ('improvement', (11, 12)), ('2.5 %', (13, 15)), ('test set', (17, 19))]","[['bi - GRU model', 'with', 'MAGE'], ['MAGE', 'leads to', 'improvement'], ['improvement', 'on', 'test set']]","[['improvement', 'of', '2.5 %']]",[],[],[],"[['CNN dataset', 'Augmenting', 'bi - GRU model']]",[],[],[],natural_language_inference,40,257
results,"The previous best results for this dataset were achieved by the GA Reader , and we see that adding MAGE to it leads to a further improvement of 0.7 % , setting a new state of the art .","[('achieved by', (8, 10)), ('adding', (18, 19)), ('leads to', (22, 24)), ('of', (27, 28)), ('setting', (31, 32))]","[('previous best results', (1, 4)), ('GA Reader', (11, 13)), ('MAGE', (19, 20)), ('further improvement', (25, 27)), ('0.7 %', (28, 30)), ('new state of the art', (33, 38))]","[['previous best results', 'achieved by', 'GA Reader'], ['GA Reader', 'adding', 'MAGE'], ['MAGE', 'leads to', 'further improvement'], ['further improvement', 'of', '0.7 %'], ['further improvement', 'setting', 'new state of the art']]",[],[],[],[],[],[],"[['CNN dataset', 'has', 'previous best results']]",[],natural_language_inference,40,258
research-problem,"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension",[],"[('Sequence - to - Sequence Pre-training', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sequence - to - Sequence Pre-training']]",[],[],[],[],natural_language_inference,41,2
research-problem,"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",[],"[('pretraining sequence - to - sequence models', (8, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'pretraining sequence - to - sequence models']]",[],[],[],[],natural_language_inference,41,4
model,"In this paper , we present BART , which pre-trains a model combining Bidirectional and Auto - Regressive Transformers .","[('present', (5, 6)), ('pre-trains', (9, 10)), ('combining', (12, 13))]","[('BART', (6, 7)), ('model', (11, 12)), ('Bidirectional and Auto - Regressive Transformers', (13, 19))]","[['BART', 'pre-trains', 'model'], ['model', 'combining', 'Bidirectional and Auto - Regressive Transformers']]",[],"[['Model', 'present', 'BART']]",[],[],[],[],[],[],natural_language_inference,41,17
model,BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,"[('is', (1, 2)), ('built with', (5, 7)), ('applicable to', (16, 18))]","[('BART', (0, 1)), ('denoising autoencoder', (3, 5)), ('sequence - to - sequence model', (8, 14)), ('very wide range of end tasks', (19, 25))]","[['BART', 'is', 'denoising autoencoder'], ['denoising autoencoder', 'built with', 'sequence - to - sequence model'], ['denoising autoencoder', 'applicable to', 'very wide range of end tasks']]",[],[],"[['Model', 'has', 'BART']]",[],[],[],[],[],natural_language_inference,41,18
model,"Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .","[('is', (8, 9)), ('corrupted with', (9, 11)), ('to reconstruct', (29, 31))]","[('Pretraining', (0, 1)), ('two stages', (2, 4)), ('text', (7, 8)), ('arbitrary noising function', (12, 15)), ('sequence - to - sequence model', (21, 27)), ('learned', (28, 29)), ('original text', (32, 34))]","[['sequence - to - sequence model', 'is', 'learned'], ['learned', 'to reconstruct', 'original text'], ['text', 'corrupted with', 'arbitrary noising function']]","[['Pretraining', 'has', 'two stages'], ['two stages', 'has', 'sequence - to - sequence model'], ['two stages', 'has', 'text']]",[],"[['Model', 'has', 'Pretraining']]",[],[],[],[],[],natural_language_inference,41,19
model,"BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see .","[('uses', (1, 2)), ('seen as', (19, 21)), ('due to', (24, 26)), ('with', (33, 34))]","[('standard Tranformer - based neural machine translation architecture', (3, 11)), ('generalizing', (21, 22)), ('BERT', (22, 23)), ('bidirectional encoder', (27, 29)), ('GPT', (31, 32)), ('left - to - right decoder', (35, 41))]","[['standard Tranformer - based neural machine translation architecture', 'seen as', 'generalizing'], ['BERT', 'due to', 'bidirectional encoder'], ['GPT', 'with', 'left - to - right decoder']]","[['generalizing', 'has', 'BERT'], ['generalizing', 'has', 'GPT']]",[],[],[],"[['BART', 'uses', 'standard Tranformer - based neural machine translation architecture']]",[],[],[],natural_language_inference,41,20
experiments,"We pre-train a large model with 12 layers in each of the encoder and decoder , and a hidden size of 1024 .","[('pre-train', (1, 2)), ('with', (5, 6)), ('in', (8, 9)), ('of', (10, 11))]","[('large model', (3, 5)), ('12 layers', (6, 8)), ('encoder and decoder', (12, 15)), ('hidden size', (18, 20)), ('1024', (21, 22))]","[['large model', 'with', '12 layers'], ['12 layers', 'in', 'encoder and decoder'], ['large model', 'with', 'hidden size'], ['hidden size', 'of', '1024']]",[],[],[],[],"[['Experimental setup', 'pre-train', 'large model']]",[],[],[],natural_language_inference,41,154
experiments,"Following RoBERTa , we use a batch size of 8000 , and train the model for 500000 steps .","[('Following', (0, 1)), ('use', (4, 5)), ('of', (8, 9)), ('train', (12, 13)), ('for', (15, 16))]","[('RoBERTa', (1, 2)), ('batch size', (6, 8)), ('8000', (9, 10)), ('model', (14, 15)), ('500000 steps', (16, 18))]","[['RoBERTa', 'use', 'batch size'], ['batch size', 'of', '8000'], ['RoBERTa', 'train', 'model'], ['model', 'for', '500000 steps']]",[],[],[],[],"[['Experimental setup', 'Following', 'RoBERTa']]",[],[],[],natural_language_inference,41,155
experiments,Documents are tokenized with the same byte - pair encoding as GPT - 2 .,"[('tokenized with', (2, 4)), ('as', (10, 11))]","[('Documents', (0, 1)), ('same byte - pair encoding', (5, 10)), ('GPT - 2', (11, 14))]","[['Documents', 'tokenized with', 'same byte - pair encoding'], ['same byte - pair encoding', 'as', 'GPT - 2']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'Documents']]",[],natural_language_inference,41,156
experiments,"Based on the results in Section 4 , we use a combination of text infilling and sentence permutation .","[('use', (9, 10)), ('of', (12, 13))]","[('combination', (11, 12)), ('text infilling', (13, 15)), ('sentence permutation', (16, 18))]","[['combination', 'of', 'text infilling'], ['combination', 'of', 'sentence permutation']]",[],[],[],[],"[['Experimental setup', 'use', 'combination']]",[],[],[],natural_language_inference,41,157
experiments,"We mask 30 % of tokens in each document , and permute all sentences .","[('mask', (1, 2)), ('of', (4, 5)), ('in', (6, 7)), ('permute', (11, 12))]","[('30 %', (2, 4)), ('tokens', (5, 6)), ('each document', (7, 9)), ('all sentences', (12, 14))]","[['30 %', 'of', 'tokens'], ['tokens', 'in', 'each document']]",[],[],[],[],"[['Experimental setup', 'permute', 'all sentences'], ['Experimental setup', 'mask', '30 %']]",[],[],[],natural_language_inference,41,158
experiments,"To help the model better fit the data , we dis abled dropout for the final 10 % of training steps .","[('To help', (0, 2)), ('better fit', (4, 6)), ('dis abled', (10, 12)), ('for', (13, 14)), ('of', (18, 19))]","[('model', (3, 4)), ('data', (7, 8)), ('dropout', (12, 13)), ('final 10 %', (15, 18)), ('training steps', (19, 21))]","[['dropout', 'for', 'final 10 %'], ['final 10 %', 'of', 'training steps'], ['final 10 %', 'To help', 'model'], ['model', 'better fit', 'data']]",[],[],[],[],"[['Experimental setup', 'dis abled', 'dropout']]",[],[],[],natural_language_inference,41,160
experiments,"The most directly comparable baseline is RoBERTa , which was pre-trained with the same resources , but a different objective .","[('pre-trained with', (10, 12))]","[('RoBERTa', (6, 7)), ('same resources', (13, 15)), ('different objective', (18, 20))]","[['RoBERTa', 'pre-trained with', 'same resources'], ['RoBERTa', 'pre-trained with', 'different objective']]",[],[],[],[],[],[],"[['Baselines', 'has', 'RoBERTa']]",[],natural_language_inference,41,163
experiments,We also experiment with several text generation tasks .,"[('experiment with', (2, 4))]","[('several text generation tasks', (4, 8))]",[],[],[],[],[],"[['Tasks', 'experiment with', 'several text generation tasks']]",[],[],[],natural_language_inference,41,169
experiments,"To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .","[('present', (17, 18)), ('on', (19, 20))]","[('results', (18, 19)), ('two summarization datasets', (20, 23)), ('CNN / DailyMail', (24, 27)), ('XSum', (28, 29))]","[['results', 'on', 'two summarization datasets']]","[['two summarization datasets', 'name', 'CNN / DailyMail'], ['two summarization datasets', 'name', 'XSum']]",[],[],[],"[['several text generation tasks', 'present', 'results']]",[],[],"[['two summarization datasets', 'has', 'BART']]",natural_language_inference,41,175
experiments,"Nevertheless , BART outperforms all existing work .",[],"[('BART', (2, 3)), ('outperforms', (3, 4))]",[],"[['BART', 'has', 'outperforms'], ['BART', 'has', 'outperforms'], ['BART', 'has', 'outperforms']]",[],[],[],[],[],[],"[['outperforms', 'has', 'all existing work']]",natural_language_inference,41,178
experiments,"BART outperforms the best previous work , which leverages BERT , by roughly 6.0 points on all ROUGE metrics - representing a significant advance in performance on this problem .","[('leverages', (8, 9)), ('by', (11, 12)), ('on', (15, 16)), ('representing', (20, 21)), ('in', (24, 25))]","[('best previous work', (3, 6)), ('BERT', (9, 10)), ('roughly 6.0 points', (12, 15)), ('all ROUGE metrics', (16, 19)), ('significant advance', (22, 24)), ('performance', (25, 26))]","[['best previous work', 'by', 'roughly 6.0 points'], ['roughly 6.0 points', 'representing', 'significant advance'], ['significant advance', 'in', 'performance'], ['roughly 6.0 points', 'on', 'all ROUGE metrics'], ['best previous work', 'leverages', 'BERT']]",[],[],[],[],[],[],"[['outperforms', 'has', 'best previous work'], ['outperforms', 'has', 'best previous work']]",[],natural_language_inference,41,180
experiments,"We evaluate dialogue response generation on CONVAI2 , in which agents must generate responses conditioned on both the previous context and a textually - specified persona .","[('evaluate', (1, 2)), ('on', (5, 6))]","[('dialogue response generation', (2, 5)), ('CONVAI2', (6, 7))]","[['dialogue response generation', 'on', 'CONVAI2']]",[],[],[],[],"[['several text generation tasks', 'evaluate', 'dialogue response generation']]",[],[],"[['dialogue response generation', 'has', 'BART']]",natural_language_inference,41,183
experiments,BART outperforms previous work on two automated metrics .,"[('on', (4, 5))]","[('BART', (0, 1)), ('outperforms', (1, 2)), ('previous work', (2, 4)), ('two automated metrics', (5, 8))]","[['outperforms', 'on', 'two automated metrics']]","[['outperforms', 'has', 'previous work']]",[],[],[],[],[],[],[],natural_language_inference,41,184
experiments,We use the recently proposed ELI5 dataset to test the model 's ability to generate long freeform answers .,"[('use', (1, 2))]","[('recently proposed ELI5 dataset', (3, 7))]",[],[],[],[],[],"[['several text generation tasks', 'use', 'recently proposed ELI5 dataset']]",[],[],[],natural_language_inference,41,186
experiments,"We find BART outperforms the best previous work by 1.2 ROUGE - L , but the dataset remains a challenging , because answers are only weakly specified by the question .",[],[],"[['outperforms', 'by', '1.2 ROUGE - L']]",[],[],[],[],"[['recently proposed ELI5 dataset', 'find', 'BART']]",[],[],[],natural_language_inference,41,187
experiments,For each row we experiment on the original WMT16 Romanian - English augmented with back - translation data .,"[('experiment on', (4, 6)), ('augmented with', (12, 14))]","[('original WMT16 Romanian - English', (7, 12)), ('back - translation data', (14, 18))]","[['original WMT16 Romanian - English', 'augmented with', 'back - translation data']]",[],[],[],[],"[['several text generation tasks', 'experiment on', 'original WMT16 Romanian - English']]",[],[],"[['original WMT16 Romanian - English', 'has', 'Preliminary results']]",natural_language_inference,41,194
experiments,"1 . Preliminary results suggested that our approach was less effective without back - translation data , and prone to overfitting - future work should explore additional regularization techniques .","[('suggested', (4, 5)), ('was', (8, 9)), ('without', (11, 12)), ('prone to', (18, 20))]","[('Preliminary results', (2, 4)), ('our approach', (6, 8)), ('less effective', (9, 11)), ('back - translation data', (12, 16)), ('overfitting', (20, 21))]","[['Preliminary results', 'suggested', 'our approach'], ['our approach', 'prone to', 'overfitting'], ['our approach', 'was', 'less effective'], ['less effective', 'without', 'back - translation data']]",[],[],[],[],[],[],[],[],natural_language_inference,41,196
research-problem,A Fully Attention - Based Information Retriever,[],"[('Information Retriever', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Information Retriever']]",[],[],[],[],natural_language_inference,42,2
research-problem,Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .,[],"[('Question - answering ( QA )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question - answering ( QA )']]",[],[],[],[],natural_language_inference,42,9
research-problem,"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD .",[],"[('open - domain QA', (11, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'open - domain QA']]",[],[],[],[],natural_language_inference,42,12
research-problem,"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P .",[],"[('QA', (16, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,42,13
model,"Inspired by the positive results of Vaswani et al. in machine translation , we have applied a similar architecture to the domain of question - answering , a model that we have named Fully Attention - Based Information Retriever ( FABIR ) .","[('named', (32, 33))]","[('Fully Attention - Based Information Retriever ( FABIR )', (33, 42))]",[],[],"[['Model', 'named', 'Fully Attention - Based Information Retriever ( FABIR )']]",[],[],[],[],[],[],natural_language_inference,42,20
model,"Our goal then was to verify how much performance we can get exclusively from the attention mechanism , without combining it with several other techniques .","[('to verify', (4, 6)), ('get exclusively from', (11, 14)), ('without combining it with', (18, 22))]","[('how much performance', (6, 9)), ('attention mechanism', (15, 17)), ('several other techniques', (22, 25))]","[['how much performance', 'get exclusively from', 'attention mechanism'], ['attention mechanism', 'without combining it with', 'several other techniques']]",[],"[['Model', 'to verify', 'how much performance']]",[],[],[],[],[],[],natural_language_inference,42,21
model,"Convolutional attention : a novel attention mechanism that encodes many - to - many relationships between words , enabling richer contextual representations .","[('encodes', (8, 9)), ('between', (15, 16)), ('enabling', (18, 19))]","[('Convolutional attention', (0, 2)), ('novel attention mechanism', (4, 7)), ('many - to - many relationships', (9, 15)), ('words', (16, 17)), ('richer contextual representations', (19, 22))]","[['novel attention mechanism', 'encodes', 'many - to - many relationships'], ['many - to - many relationships', 'enabling', 'richer contextual representations'], ['many - to - many relationships', 'between', 'words']]","[['Convolutional attention', 'has', 'novel attention mechanism']]",[],"[['Model', 'has', 'Convolutional attention']]",[],[],[],[],[],natural_language_inference,42,24
model,Reduction layer : a new layer design that fits the pipeline proposed by Vaswani et al .,"[('fits', (8, 9)), ('proposed by', (11, 13))]","[('Reduction layer', (0, 2)), ('new layer design', (4, 7)), ('pipeline', (10, 11)), ('Vaswani et al .', (13, 17))]","[['new layer design', 'fits', 'pipeline'], ['pipeline', 'proposed by', 'Vaswani et al .']]","[['Reduction layer', 'has', 'new layer design']]",[],"[['Model', 'has', 'Reduction layer']]",[],[],[],[],[],natural_language_inference,42,25
model,Column - wise cross - attention : we modify the crossattention operation by and propose a new technique that is better suited to question - answering .,"[('modify', (8, 9)), ('propose', (14, 15)), ('better suited to', (20, 23))]","[('Column - wise cross - attention', (0, 6)), ('crossattention operation', (10, 12)), ('new technique', (16, 18)), ('question - answering', (23, 26))]","[['Column - wise cross - attention', 'modify', 'crossattention operation'], ['Column - wise cross - attention', 'propose', 'new technique'], ['new technique', 'better suited to', 'question - answering']]",[],[],"[['Model', 'has', 'Column - wise cross - attention']]",[],[],[],[],[],natural_language_inference,42,27
experimental-setup,We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM .,[],[],"[['FABIR model', 'with', 'batch size'], ['batch size', 'of', '75'], ['FABIR model', 'in', 'GPU NVidia Titan X'], ['GPU NVidia Titan X', 'with', '12 GB'], ['12 GB', 'of', 'RAM'], ['FABIR model', 'during', '54 epochs']]",[],"[['Experimental setup', 'trained', 'FABIR model']]",[],[],[],[],[],[],natural_language_inference,42,217
experimental-setup,We developed our model in Tensorflow and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297 / for replicability .,"[('developed', (1, 2)), ('in', (4, 5))]","[('our model', (2, 4)), ('Tensorflow', (5, 6))]","[['our model', 'in', 'Tensorflow']]",[],"[['Experimental setup', 'developed', 'our model']]",[],[],[],[],[],[],natural_language_inference,42,218
experimental-setup,We pre-processed the texts with the NLTK Tokenizer .,"[('pre-processed', (1, 2)), ('with', (4, 5))]","[('texts', (3, 4)), ('NLTK Tokenizer', (6, 8))]","[['texts', 'with', 'NLTK Tokenizer']]",[],"[['Experimental setup', 'pre-processed', 'texts']]",[],[],[],[],[],[],natural_language_inference,42,219
experimental-setup,"For regularization , we applied residual and attention dropout of 0.9 in processing layers and of 0.8 in the reduction layer .",[],[],"[['regularization', 'applied', 'residual and attention dropout'], ['residual and attention dropout', 'of', '0.8'], ['0.8', 'in', 'reduction layer'], ['residual and attention dropout', 'of', '0.9'], ['0.9', 'in', 'processing layers']]",[],"[['Experimental setup', 'For', 'regularization']]",[],[],[],[],[],[],natural_language_inference,42,221
experimental-setup,"In the character - level embedding process , a dropout of 0.75 was added before the convolution .","[('In', (0, 1)), ('of', (10, 11)), ('added before', (13, 15))]","[('character - level embedding process', (2, 7)), ('dropout', (9, 10)), ('0.75', (11, 12)), ('convolution', (16, 17))]","[['dropout', 'of', '0.75'], ['0.75', 'added before', 'convolution']]","[['character - level embedding process', 'has', 'dropout']]","[['Experimental setup', 'In', 'character - level embedding process']]",[],[],[],[],[],[],natural_language_inference,42,222
experimental-setup,"Additionally , a dropout of 0.8 was added before each convolutional layer in the answer selector .","[('of', (4, 5)), ('added before', (7, 9)), ('in', (12, 13))]","[('dropout', (3, 4)), ('0.8', (5, 6)), ('each convolutional layer', (9, 12)), ('answer selector', (14, 16))]","[['dropout', 'of', '0.8'], ['0.8', 'added before', 'each convolutional layer'], ['each convolutional layer', 'in', 'answer selector']]",[],[],"[['Experimental setup', 'has', 'dropout']]",[],[],[],[],[],natural_language_inference,42,223
experimental-setup,"We set processing layers dimension d model to 100 , the number of heads n heads in each attention sublayer to 4 , the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer .",[],[],"[['feed - forward hidden size', 'to', '200'], ['200', 'in', 'processing layers'], ['feed - forward hidden size', 'to', '400'], ['400', 'in', 'reduction layer'], ['d model', 'to', '100'], ['n heads', 'in', 'each attention sublayer'], ['each attention sublayer', 'to', '4']]","[['processing layers dimension', 'has', 'd model'], ['number of heads', 'has', 'n heads']]","[['Experimental setup', 'set', 'feed - forward hidden size'], ['Experimental setup', 'set', 'processing layers dimension'], ['Experimental setup', 'set', 'number of heads']]",[],[],[],[],[],[],natural_language_inference,42,224
ablation-analysis,"This analysis confirms the effectiveness of char- embeddings , as its addition increased the F1 and EM scores , by 2.7 % and 3.1 % , respectively .","[('confirms', (2, 3)), ('of', (5, 6)), ('addition', (11, 12)), ('increased', (12, 13)), ('by', (19, 20))]","[('effectiveness', (4, 5)), ('char- embeddings', (6, 8)), ('F1 and EM scores', (14, 18)), ('2.7 % and 3.1 %', (20, 25))]","[['effectiveness', 'of', 'char- embeddings'], ['F1 and EM scores', 'by', '2.7 % and 3.1 %']]",[],"[['Ablation analysis', 'confirms', 'effectiveness']]",[],[],"[['has', 'increased', 'F1 and EM scores']]","[['char- embeddings', 'addition', 'has']]",[],[],natural_language_inference,42,229
ablation-analysis,"Most importantly , when the convolutional attention was replaced by the standard attention mechanism proposed in , the performance dropped by 2.4 % in F1 and 2.5 % in EM .",[],[],"[['convolutional attention', 'replaced by', 'standard attention mechanism'], ['dropped', 'by', '2.5 %'], ['2.5 %', 'in', 'EM'], ['dropped', 'by', '2.4 %'], ['2.4 %', 'in', 'F1']]","[['standard attention mechanism', 'has', 'performance'], ['performance', 'has', 'dropped']]","[['Ablation analysis', 'when', 'convolutional attention']]",[],[],[],[],[],[],natural_language_inference,42,230
ablation-analysis,"Moreover , the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings .","[('indicate', (5, 6)), ('capable of producing', (11, 14)), ('when', (17, 18))]","[('reduction layer', (8, 10)), ('useful word representations', (14, 17)), ('compressing', (18, 19)), ('embeddings', (20, 21))]","[['reduction layer', 'capable of producing', 'useful word representations'], ['useful word representations', 'when', 'compressing']]","[['compressing', 'has', 'embeddings']]","[['Ablation analysis', 'indicate', 'reduction layer']]",[],[],[],[],[],[],natural_language_inference,42,232
ablation-analysis,"Indeed , when we replaced that layer by a standard feedforward layer with the same reduction ratio , there was a drop of 2.1 % and 2.5 % in the F1 and EM scores , respectively .","[('replaced', (4, 5)), ('with', (12, 13)), ('of', (22, 23)), ('in', (28, 29))]","[('standard feedforward layer', (9, 12)), ('same reduction ratio', (14, 17)), ('drop', (21, 22)), ('2.1 % and 2.5 %', (23, 28)), ('F1 and EM scores', (30, 34))]","[['standard feedforward layer', 'with', 'same reduction ratio'], ['drop', 'of', '2.1 % and 2.5 %'], ['2.1 % and 2.5 %', 'in', 'F1 and EM scores']]","[['standard feedforward layer', 'has', 'drop']]",[],[],[],"[['reduction layer', 'replaced', 'standard feedforward layer']]",[],[],[],natural_language_inference,42,233
results,"Regarding EM and F 1 scores , FABIR and BiDAF showed similar performances .","[('Regarding', (0, 1)), ('showed', (10, 11))]","[('EM and F 1 scores', (1, 6)), ('FABIR and BiDAF', (7, 10)), ('similar performances', (11, 13))]","[['FABIR and BiDAF', 'showed', 'similar performances']]","[['EM and F 1 scores', 'has', 'FABIR and BiDAF']]","[['Results', 'Regarding', 'EM and F 1 scores']]",[],[],[],[],[],[],natural_language_inference,42,245
results,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .,"[('analyze', (4, 5)), ('of', (7, 8))]","[('performance', (6, 7)), ('FABIR and BiDAF', (8, 11))]","[['performance', 'of', 'FABIR and BiDAF']]",[],"[['Results', 'analyze', 'performance']]",[],[],[],[],[],[],natural_language_inference,42,255
results,"shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .","[('shows', (0, 1)), ('are', (4, 5)), ('for', (6, 7))]","[('shorter answers', (2, 4)), ('easier', (5, 6)), ('both models', (7, 9))]","[['shorter answers', 'are', 'easier'], ['easier', 'for', 'both models']]",[],[],[],[],"[['FABIR and BiDAF', 'shows', 'shorter answers']]",[],[],[],natural_language_inference,42,256
results,"shows that both models had their best performance with "" when "" questions .","[('with', (8, 9))]","[('best performance', (6, 8)), ('"" when "" questions', (9, 13))]","[['best performance', 'with', '"" when "" questions']]",[],[],[],[],[],[],"[['FABIR and BiDAF', 'shows', 'best performance']]",[],natural_language_inference,42,262
results,"Together with "" when "" questions , "" how long "" and "" how many "" also proved easier to respond , as they possess the same property of having a smaller universe of possible answers .","[('proved', (17, 18)), ('to', (19, 20)), ('possess', (24, 25)), ('of having', (28, 30)), ('of', (33, 34))]","[('"" how long "" and "" how many ""', (7, 16)), ('easier', (18, 19)), ('respond', (20, 21)), ('same property', (26, 28)), ('smaller universe', (31, 33)), ('possible answers', (34, 36))]","[['"" how long "" and "" how many ""', 'proved', 'easier'], ['easier', 'to', 'respond'], ['"" how long "" and "" how many ""', 'possess', 'same property'], ['same property', 'of having', 'smaller universe'], ['smaller universe', 'of', 'possible answers']]",[],[],[],[],[],[],"[['FABIR and BiDAF', 'has', '"" how long "" and "" how many ""']]",[],natural_language_inference,42,264
results,"In contrast to these , "" how "" and "" why "" questions resulted in considerably lower F1 and EM scores , as they can be answered by any sentence , and hence require a deeper understanding of the text .","[('resulted in', (13, 15))]","[('"" how "" and "" why "" questions', (5, 13)), ('considerably lower F1 and EM scores', (15, 21))]","[['"" how "" and "" why "" questions', 'resulted in', 'considerably lower F1 and EM scores']]",[],[],[],[],[],[],"[['FABIR and BiDAF', 'has', '"" how "" and "" why "" questions']]",[],natural_language_inference,42,265
results,"Questions which expect a "" yes "" or a "" no "" as an answer are also difficult because it is not always possible to find those words in a snippet from the passage .","[('expect', (2, 3)), ('as', (12, 13)), ('are', (15, 16))]","[('Questions', (0, 1)), ('"" yes "" or a "" no ""', (4, 12)), ('answer', (14, 15)), ('also difficult', (16, 18))]","[['Questions', 'expect', '"" yes "" or a "" no ""'], ['"" yes "" or a "" no ""', 'as', 'answer'], ['"" yes "" or a "" no ""', 'are', 'also difficult']]",[],[],[],[],[],[],"[['FABIR and BiDAF', 'has', 'Questions']]",[],natural_language_inference,42,270
results,It is curious that shorter passages showed the worst performance for both models .,"[('showed', (6, 7)), ('for', (10, 11))]","[('shorter passages', (4, 6)), ('worst performance', (8, 10)), ('both models', (11, 13))]","[['shorter passages', 'showed', 'worst performance'], ['worst performance', 'for', 'both models']]",[],[],[],[],[],[],"[['FABIR and BiDAF', 'has', 'shorter passages']]",[],natural_language_inference,42,272
research-problem,Evaluating Semantic Parsing against a Simple Web - based Question Answering Model,[],"[('Semantic Parsing', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Parsing']]",[],[],[],[],natural_language_inference,43,2
model,"We develop a simple log - linear model , in the spirit of traditional web - based QA systems , that answers questions by querying the web and extracting the answer from returned web snippets .","[('develop', (1, 2)), ('in the spirit of', (9, 13)), ('that', (20, 21)), ('by', (23, 24)), ('extracting', (28, 29)), ('from', (31, 32))]","[('simple log - linear model', (3, 8)), ('traditional web - based QA systems', (13, 19)), ('answers questions', (21, 23)), ('querying', (24, 25)), ('web', (26, 27)), ('answer', (30, 31)), ('returned web snippets', (32, 35))]","[['simple log - linear model', 'in the spirit of', 'traditional web - based QA systems'], ['simple log - linear model', 'that', 'answers questions'], ['answers questions', 'by', 'querying'], ['simple log - linear model', 'extracting', 'answer'], ['answer', 'from', 'returned web snippets']]","[['querying', 'has', 'web']]","[['Model', 'develop', 'simple log - linear model']]",[],[],[],[],[],[],natural_language_inference,43,18
model,"Thus , our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web ( in contrast with virtual assitants for which the knowledge is specific to an application ) .","[('suitable for', (6, 8)), ('in which', (11, 13)), ('required for', (15, 17)), ('covered by', (20, 22))]","[('our evaluation scheme', (2, 5)), ('semantic parsing benchmarks', (8, 11)), ('knowledge', (14, 15)), ('answering questions', (17, 19)), ('web', (23, 24))]","[['our evaluation scheme', 'suitable for', 'semantic parsing benchmarks'], ['semantic parsing benchmarks', 'in which', 'knowledge'], ['knowledge', 'required for', 'answering questions'], ['answering questions', 'covered by', 'web']]",[],[],"[['Model', 'has', 'our evaluation scheme']]",[],[],[],[],[],natural_language_inference,43,19
experiments,"We compare our model , WEBQA , to STAGG and COMPQ , which are to the best of our knowledge the highest performing semantic parsing models on both COMPLEXQUESTIONS and WEBQUES - TIONS .","[('which are', (12, 14)), ('on', (26, 27))]","[('STAGG and COMPQ', (8, 11)), ('highest performing semantic parsing models', (21, 26)), ('COMPLEXQUESTIONS and WEBQUES - TIONS', (28, 33))]","[['STAGG and COMPQ', 'which are', 'highest performing semantic parsing models'], ['highest performing semantic parsing models', 'on', 'COMPLEXQUESTIONS and WEBQUES - TIONS']]",[],[],[],[],[],[],"[['Baselines', 'has', 'STAGG and COMPQ']]",[],natural_language_inference,43,108
experiments,"WEBQA obtained 32.6 F 1 ( 33.5 p@1 , 42.4 MRR ) compared to 40.9 F 1 of COMPQ .","[('obtained', (1, 2)), ('compared to', (12, 14)), ('of', (17, 18))]","[('WEBQA', (0, 1)), ('32.6 F 1', (2, 5)), ('40.9 F 1', (14, 17)), ('COMPQ', (18, 19))]","[['WEBQA', 'obtained', '32.6 F 1'], ['32.6 F 1', 'compared to', '40.9 F 1'], ['40.9 F 1', 'of', 'COMPQ']]",[],[],[],[],[],[],"[['Results', 'has', 'WEBQA']]",[],natural_language_inference,43,117
experiments,Our candidate extraction step finds the correct answer in the top - K candidates in 65.9 % of development examples and 62.7 % of test examples .,[],[],"[['candidate extraction step', 'finds', 'correct answer'], ['correct answer', 'in', 'top - K candidates'], ['top - K candidates', 'in', '62.7 %'], ['62.7 %', 'of', 'test examples'], ['top - K candidates', 'in', '65.9 %'], ['65.9 %', 'of', 'development examples']]",[],[],[],[],[],[],"[['Results', 'has', 'candidate extraction step']]",[],natural_language_inference,43,118
experiments,"Thus , our test F 1 on examples for which candidate extraction succeeded ( WEBQA - SUBSET ) is 51.9 ( 53.4 p@1 , 67.5 MRR ) .","[('on', (6, 7)), ('for which', (8, 10)), ('is', (18, 19))]","[('test F 1', (3, 6)), ('examples', (7, 8)), ('candidate extraction succeeded ( WEBQA - SUBSET )', (10, 18)), ('51.9 ( 53.4 p@1 , 67.5 MRR )', (19, 27))]","[['test F 1', 'on', 'examples'], ['examples', 'for which', 'candidate extraction succeeded ( WEBQA - SUBSET )'], ['candidate extraction succeeded ( WEBQA - SUBSET )', 'is', '51.9 ( 53.4 p@1 , 67.5 MRR )']]",[],[],[],[],[],[],"[['Results', 'has', 'test F 1']]",[],natural_language_inference,43,119
experiments,"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .","[('obtained', (5, 6)), ('on', (9, 10)), ('compared to', (14, 16)), ('when training on', (20, 23))]","[('COMPQ', (4, 5)), ('42.2 F 1', (6, 9)), ('test set', (11, 13)), ('40.9 F 1', (16, 19)), ('COM - PLEXQUESTIONS only', (23, 27))]","[['COMPQ', 'obtained', '42.2 F 1'], ['42.2 F 1', 'compared to', '40.9 F 1'], ['40.9 F 1', 'when training on', 'COM - PLEXQUESTIONS only'], ['42.2 F 1', 'on', 'test set']]",[],[],[],[],[],[],"[['Results', 'has', 'COMPQ']]",[],natural_language_inference,43,121
experiments,"Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .",[],[],"[['predictions', 'to', 'subset'], ['subset', 'for which', 'candidate extraction'], ['F 1', 'of', 'COMPQ - SUBSET'], ['COMPQ - SUBSET', 'is', '48.5'], ['48.5', 'is', '3.4 F 1 points lower'], ['3.4 F 1 points lower', 'than', 'WEBQA - SUBSET']]","[['candidate extraction', 'has', 'succeeded'], ['predictions', 'has', 'F 1']]",[],[],[],"[['Results', 'Restricting', 'predictions']]",[],[],[],natural_language_inference,43,122
ablation-analysis,"Note that TF - IDF is by far the most impactful feature , leading to a large drop of 12 points in performance .","[('is', (5, 6)), ('leading to', (13, 15)), ('of', (18, 19))]","[('TF - IDF', (2, 5)), ('most impactful feature', (9, 12)), ('large drop', (16, 18)), ('12 points', (19, 21))]","[['TF - IDF', 'leading to', 'large drop'], ['large drop', 'of', '12 points'], ['TF - IDF', 'is', 'most impactful feature']]",[],[],"[['Ablation analysis', 'has', 'TF - IDF']]",[],[],[],[],[],natural_language_inference,43,133
code,"Code , data , annotations , and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/.",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,43,155
research-problem,Efficient and Robust Question Answering from Minimal Context over Documents,[],"[('Question Answering', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering']]",[],[],[],[],natural_language_inference,44,2
research-problem,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,[],"[('question answering ( QA )', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,44,4
research-problem,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",[],"[('QA', (22, 23))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,44,8
model,"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .","[('develop', (7, 8)), ('scalable to', (13, 15)), ('robust to', (20, 22))]","[('QA system', (9, 11)), ('large documents', (15, 17)), ('adversarial inputs', (22, 24))]","[['QA system', 'scalable to', 'large documents'], ['QA system', 'robust to', 'adversarial inputs']]",[],"[['Model', 'develop', 'QA system']]",[],[],[],[],[],[],natural_language_inference,44,20
model,"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .","[('study', (3, 4)), ('required to', (6, 8)), ('by sampling', (11, 13)), ('in', (14, 15))]","[('context', (5, 6)), ('answer the question', (8, 11)), ('examples', (13, 14)), ('dataset', (16, 17))]","[['context', 'by sampling', 'examples'], ['examples', 'in', 'dataset'], ['context', 'required to', 'answer the question']]",[],"[['Model', 'study', 'context']]",[],[],[],[],[],[],natural_language_inference,44,21
model,"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .","[('propose', (8, 9)), ('to select', (12, 14)), ('to give', (19, 21)), ('to answer', (27, 29))]","[('sentence selector', (10, 12)), ('minimal set of sentences', (15, 19)), ('QA model', (23, 25)), ('question', (30, 31))]","[['sentence selector', 'to select', 'minimal set of sentences'], ['minimal set of sentences', 'to give', 'QA model'], ['QA model', 'to answer', 'question']]",[],"[['Model', 'propose', 'sentence selector']]",[],[],[],[],[],[],natural_language_inference,44,24
model,"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .","[('chooses', (14, 15)), ('for', (20, 21))]","[('different number of sentences', (16, 20)), ('each question', (21, 23))]","[['different number of sentences', 'for', 'each question']]",[],[],[],[],"[['sentence selector', 'chooses', 'different number of sentences']]",[],[],[],natural_language_inference,44,25
model,"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .","[('leverages', (3, 4))]","[('three simple techniques', (4, 7)), ('weight transfer', (8, 10)), ('data modification', (11, 13)), ('score normalization', (14, 16))]",[],"[['three simple techniques', 'has', 'weight transfer'], ['three simple techniques', 'has', 'data modification'], ['three simple techniques', 'has', 'score normalization']]",[],[],[],"[['sentence selector', 'leverages', 'three simple techniques']]",[],[],[],natural_language_inference,44,26
experiments,"Results shows results in the task of sentence selection on SQuAD and New s QA . First , our selector outperforms TF - IDF method and the previous state - of - the - art by large margin ( up to 2.9 % MAP ) .","[('by', (35, 36)), ('up to', (39, 41))]","[('SQuAD and New s QA', (10, 15)), ('selector', (19, 20)), ('outperforms', (20, 21)), ('TF - IDF method and the previous state - of - the - art', (21, 35)), ('large margin', (36, 38)), ('2.9 % MAP', (41, 44))]","[['TF - IDF method and the previous state - of - the - art', 'by', 'large margin'], ['large margin', 'up to', '2.9 % MAP']]","[['selector', 'has', 'outperforms'], ['outperforms', 'has', 'TF - IDF method and the previous state - of - the - art']]",[],[],[],[],[],"[['Tasks', 'has', 'SQuAD and New s QA'], ['Results', 'has', 'selector']]","[['SQuAD and New s QA', 'has', 'Results']]",natural_language_inference,44,110
experiments,"Second , our three training techniques - weight transfer , data modification and score normalization - improve performance by up to 5.6 % MAP .","[('improve', (16, 17)), ('by', (18, 19))]","[('three training techniques', (3, 6)), ('weight transfer', (7, 9)), ('data modification', (10, 12)), ('score normalization', (13, 15)), ('performance', (17, 18)), ('up to 5.6 % MAP', (19, 24))]","[['three training techniques', 'improve', 'performance'], ['performance', 'by', 'up to 5.6 % MAP']]","[['three training techniques', 'has', 'weight transfer'], ['three training techniques', 'has', 'data modification'], ['three training techniques', 'has', 'score normalization']]",[],[],[],[],[],"[['Results', 'has', 'three training techniques']]",[],natural_language_inference,44,111
experiments,"Finally , our Dyn method achieves higher accuracy with less sentences than the Top k method .","[('achieves', (5, 6)), ('with', (8, 9)), ('than', (11, 12))]","[('Dyn method', (3, 5)), ('higher accuracy', (6, 8)), ('less sentences', (9, 11)), ('Top k method', (13, 16))]","[['Dyn method', 'achieves', 'higher accuracy'], ['higher accuracy', 'with', 'less sentences'], ['less sentences', 'than', 'Top k method']]",[],[],[],[],[],[],"[['Results', 'has', 'Dyn method']]",[],natural_language_inference,44,112
experiments,"On News QA , Top 4 achieves 92.5 accuracy , whereas Dyn achieves 94.6 accuracy with 3.9 sentences per example .",[],[],"[['Dyn', 'achieves', '94.6 accuracy'], ['94.6 accuracy', 'with', '3.9 sentences per example'], ['Top 4', 'achieves', '92.5 accuracy']]","[['News QA', 'has', 'Dyn'], ['News QA', 'has', 'Top 4']]",[],[],[],"[['Results', 'On', 'News QA']]",[],[],[],natural_language_inference,44,116
experiments,"On SQuAD , S - Reader achieves 6.7 training and 3.6 inference speedup on SQuAD , and 15.0 training and 6.9 inference speedup on News QA .",[],[],"[['S - Reader', 'on', 'SQuAD'], ['SQuAD', 'achieves', 'speedup'], ['S - Reader', 'on', 'News QA']]","[['SQuAD', 'has', 'S - Reader'], ['speedup', 'has', '6.7 training'], ['speedup', 'has', '3.6 inference'], ['News QA', 'has', 'speedup'], ['speedup', 'has', '15.0 training'], ['speedup', 'has', '6.9 inference']]",[],[],[],[],[],"[['Results', 'On', 'SQuAD']]",[],natural_language_inference,44,121
experiments,Trivia QA and SQuAD - Open,[],"[('Trivia QA and SQuAD - Open', (0, 6))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Trivia QA and SQuAD - Open']]","[['Trivia QA and SQuAD - Open', 'has', 'Baselines']]",natural_language_inference,44,153
experiments,We compare with the results from the sentences selected by TF - IDF method and our selector ( Dyn ) .,"[('compare with', (1, 3))]","[('TF - IDF method', (10, 14)), ('our selector ( Dyn )', (15, 20))]",[],[],[],[],[],"[['Baselines', 'compare with', 'TF - IDF method'], ['Baselines', 'compare with', 'our selector ( Dyn )']]",[],[],"[['Baselines', 'compare with', 'published Rank1 - 3 models']]",natural_language_inference,44,171
experiments,We also compare with published Rank1 - 3 models .,[],"[('published Rank1 - 3 models', (4, 9))]",[],[],[],[],[],[],[],[],[],natural_language_inference,44,172
experiments,"Results shows results on Trivia QA ( Wikipedia ) and SQuAD - Open. First , MINI - MAL obtains higher F1 and EM over FULL , with the inference speedup of up to 13.8 .","[('obtains', (18, 19)), ('over', (23, 24))]","[('MINI - MAL', (15, 18)), ('higher F1 and EM', (19, 23)), ('FULL', (24, 25))]","[['MINI - MAL', 'obtains', 'higher F1 and EM'], ['higher F1 and EM', 'over', 'FULL']]",[],[],[],[],[],[],"[['Results', 'has', 'MINI - MAL']]",[],natural_language_inference,44,173
experiments,"Second , the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF - IDF selector .",[],[],"[['model', 'with', 'our sentence selector'], ['our sentence selector', 'with', 'Dyn'], ['our sentence selector', 'achieves', 'higher F1 and EM'], ['higher F1 and EM', 'over', 'model'], ['model', 'with', 'TF - IDF selector']]",[],[],[],[],[],[],"[['Results', 'has', 'model']]",[],natural_language_inference,44,174
experiments,"Third , we outperforms the published state - of - the - art on both dataset .","[('on', (13, 14))]","[('outperforms', (3, 4)), ('state - of - the - art', (6, 13)), ('both dataset', (14, 16))]","[['state - of - the - art', 'on', 'both dataset']]","[['outperforms', 'has', 'state - of - the - art']]",[],[],[],[],[],"[['Results', 'has', 'outperforms']]",[],natural_language_inference,44,176
experiments,SQuAD - Adversarial,[],"[('SQuAD - Adversarial', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'SQuAD - Adversarial']]","[['SQuAD - Adversarial', 'has', 'Results']]",natural_language_inference,44,177
experiments,"Results shows that MINIMAL outperforms FULL , achieving the new state - of - the - art by large margin ( + 11.1 and + 11.5 F1 on AddSent and Add OneSent , respectively ) .","[('shows', (1, 2)), ('achieving', (7, 8)), ('by', (17, 18)), ('on', (27, 28))]","[('MINIMAL', (3, 4)), ('outperforms', (4, 5)), ('FULL', (5, 6)), ('new state - of - the - art', (9, 17)), ('large margin', (18, 20)), ('+ 11.1 and + 11.5 F1', (21, 27)), ('AddSent and Add OneSent', (28, 32))]","[['outperforms', 'achieving', 'new state - of - the - art'], ['new state - of - the - art', 'by', 'large margin'], ['+ 11.1 and + 11.5 F1', 'on', 'AddSent and Add OneSent']]","[['MINIMAL', 'has', 'outperforms'], ['large margin', 'has', '+ 11.1 and + 11.5 F1'], ['outperforms', 'has', 'FULL']]",[],[],[],"[['Results', 'shows', 'MINIMAL']]",[],[],[],natural_language_inference,44,181
research-problem,Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION,[],"[('READING COMPREHENSION', (17, 19))]",[],[],[],[],"[['Contribution', 'has research problem', 'READING COMPREHENSION']]",[],[],[],[],natural_language_inference,45,2
model,"In this work , we present a fine - grained gating mechanism to combine the word - level and characterlevel representations .","[('present', (5, 6)), ('to combine', (12, 14))]","[('fine - grained gating mechanism', (7, 12)), ('word - level and characterlevel representations', (15, 21))]","[['fine - grained gating mechanism', 'to combine', 'word - level and characterlevel representations']]",[],"[['Model', 'present', 'fine - grained gating mechanism']]",[],[],[],[],[],[],natural_language_inference,45,28
model,We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,"[('compute', (1, 2)), ('as', (5, 6)), ('of', (9, 10))]","[('vector gate', (3, 5)), ('linear projection', (7, 9)), ('token features', (11, 13)), ('https://github.com/kimiyoung/fg-gating', (19, 20))]","[['vector gate', 'as', 'linear projection'], ['linear projection', 'of', 'token features']]",[],"[['Model', 'compute', 'vector gate']]",[],"[['Contribution', 'Code', 'https://github.com/kimiyoung/fg-gating']]",[],[],[],[],natural_language_inference,45,29
model,We then multiplicatively apply the gate to the character - level and wordlevel representations .,"[('apply', (3, 4)), ('to', (6, 7))]","[('multiplicatively', (2, 3)), ('gate', (5, 6)), ('character - level and wordlevel representations', (8, 14))]","[['multiplicatively', 'apply', 'gate'], ['gate', 'to', 'character - level and wordlevel representations']]",[],[],"[['Model', 'has', 'multiplicatively']]",[],[],[],[],[],natural_language_inference,45,31
model,Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,"[('of', (2, 3)), ('controls', (5, 6)), ('flowed from', (10, 12))]","[('Each dimension', (0, 2)), ('gate', (4, 5)), ('how much information', (6, 9)), ('word - level and character - level representations', (13, 21))]","[['Each dimension', 'controls', 'how much information'], ['how much information', 'flowed from', 'word - level and character - level representations'], ['Each dimension', 'of', 'gate']]",[],[],"[['Model', 'has', 'Each dimension']]",[],[],[],[],[],natural_language_inference,45,32
model,"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .","[('use', (1, 2)), ('for', (22, 23)), ('determine', (26, 27))]","[('named entity tags', (2, 5)), ('part - ofspeech tags', (6, 10)), ('document frequencies', (11, 13)), ('word - level representations', (15, 19)), ('token properties', (23, 25)), ('gate', (28, 29))]","[['token properties', 'determine', 'gate'], ['token properties', 'use', 'named entity tags'], ['token properties', 'use', 'part - ofspeech tags'], ['token properties', 'use', 'document frequencies'], ['token properties', 'use', 'word - level representations']]",[],"[['Model', 'for', 'token properties']]",[],[],[],[],[],[],natural_language_inference,45,33
model,"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .","[('used to', (11, 13)), ('multiple levels', (14, 16)), ('in', (18, 19)), ('including', (21, 22))]","[('our fine - grained gating mechanism', (3, 9)), ('model', (13, 14)), ('structure', (17, 18)), ('language', (19, 20)), ('words', (22, 23)), ('characters', (24, 25)), ('phrases', (26, 27)), ('sentences', (28, 29)), ('paragraphs', (30, 31))]","[['our fine - grained gating mechanism', 'used to', 'model'], ['model', 'multiple levels', 'structure'], ['structure', 'including', 'words'], ['structure', 'including', 'characters'], ['structure', 'including', 'phrases'], ['structure', 'including', 'sentences'], ['structure', 'including', 'paragraphs'], ['structure', 'in', 'language']]",[],[],"[['Model', 'has', 'our fine - grained gating mechanism']]",[],[],[],[],[],natural_language_inference,45,34
research-problem,The NarrativeQA Reading Comprehension Challenge,[],"[('Reading Comprehension', (2, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],natural_language_inference,46,2
research-problem,"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .",[],"[('Reading comprehension ( RC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading comprehension ( RC )']]",[],[],[],[],natural_language_inference,46,4
research-problem,"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .",[],"[('RC', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],natural_language_inference,46,5
dataset,"We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .","[('call', (10, 11))]","[('NarrativeQA', (11, 12))]",[],[],"[['Dataset', 'call', 'NarrativeQA']]",[],[],[],[],[],[],natural_language_inference,46,39
dataset,"The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .","[('consists of', (2, 4)), ('which are', (6, 8)), ('with', (13, 14)), ('based solely on', (19, 22))]","[('stories', (4, 5)), ('books and movie scripts', (8, 12)), ('human written questions and answers', (14, 19)), ('human - generated abstractive summaries', (22, 27))]","[['stories', 'which are', 'books and movie scripts'], ['books and movie scripts', 'with', 'human written questions and answers'], ['human written questions and answers', 'based solely on', 'human - generated abstractive summaries']]",[],"[['Dataset', 'consists of', 'stories']]",[],[],[],[],[],[],natural_language_inference,46,40
dataset,"For the RC tasks , questions maybe answered using just the summaries or the full story text .","[('maybe', (6, 7)), ('using', (8, 9))]","[('questions', (5, 6)), ('answered', (7, 8)), ('summaries', (11, 12)), ('full story text', (14, 17))]","[['questions', 'maybe', 'answered'], ['answered', 'using', 'summaries'], ['answered', 'using', 'full story text']]",[],[],"[['Dataset', 'has', 'questions']]",[],[],[],[],[],natural_language_inference,46,41
experiments,Reading Summaries Only,[],"[('Reading Summaries Only', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Reading Summaries Only']]","[['Reading Summaries Only', 'has', 'Results']]",natural_language_inference,46,196
experiments,"This is indeed the case , with the neural span prediction model significantly outperforming all other proposed methods .",[],"[('neural span prediction model', (8, 12)), ('significantly outperforming', (12, 14)), ('all other proposed methods', (14, 18))]",[],"[['neural span prediction model', 'has', 'significantly outperforming'], ['significantly outperforming', 'has', 'all other proposed methods']]",[],[],[],[],[],"[['Results', 'has', 'neural span prediction model']]",[],natural_language_inference,46,201
experiments,"Both the plain sequence to sequence model and the AS Reader , successfully applied to the CNN / DailyMail reading comprehension task , also perform well on this task .","[('perform', (24, 25))]","[('Both the plain sequence to sequence model and the AS Reader', (0, 11)), ('well', (25, 26))]","[['Both the plain sequence to sequence model and the AS Reader', 'perform', 'well']]",[],[],[],[],[],[],"[['Results', 'has', 'Both the plain sequence to sequence model and the AS Reader']]",[],natural_language_inference,46,203
experiments,An additional inductive bias results in higher performance for the span prediction model .,"[('results in', (4, 6)), ('for', (8, 9))]","[('additional inductive bias', (1, 4)), ('higher performance', (6, 8)), ('span prediction model', (10, 13))]","[['additional inductive bias', 'results in', 'higher performance'], ['higher performance', 'for', 'span prediction model']]",[],[],[],[],[],[],"[['Results', 'has', 'additional inductive bias']]",[],natural_language_inference,46,205
experiments,"summarizes the results on the full Narra - tive QA task , where the context documents are full stories .","[('on', (3, 4)), ('where', (12, 13)), ('are', (16, 17))]","[('full Narra - tive QA task', (5, 11)), ('context documents', (14, 16)), ('full stories', (17, 19))]","[['full Narra - tive QA task', 'where', 'context documents'], ['context documents', 'are', 'full stories']]",[],[],[],[],"[['Results', 'on', 'full Narra - tive QA task']]",[],[],[],natural_language_inference,46,208
experiments,"As expected ( and desired ) , we observe a decline in performance of the span- selection oracle IR model , compared with the results on summaries .","[('observe', (8, 9)), ('in', (11, 12)), ('of', (13, 14))]","[('decline', (10, 11)), ('performance', (12, 13)), ('span- selection oracle IR model', (15, 20))]","[['decline', 'in', 'performance'], ['performance', 'of', 'span- selection oracle IR model']]",[],[],[],[],"[['full Narra - tive QA task', 'observe', 'decline']]",[],[],[],natural_language_inference,46,209
experiments,Reading Full Stories Only,[],"[('Reading Full Stories Only', (0, 4))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Reading Full Stories Only']]","[['Reading Full Stories Only', 'has', 'Results']]",natural_language_inference,46,215
experiments,"The AS Reader , which was the better - performing model on the summaries task , underperforms the simple no -context Seq2Seq baseline ( shown in ) in terms of MRR .","[('in terms of', (27, 30))]","[('AS Reader', (1, 3)), ('underperforms', (16, 17)), ('simple no -context Seq2Seq baseline', (18, 23)), ('MRR', (30, 31))]","[['underperforms', 'in terms of', 'MRR']]","[['AS Reader', 'has', 'underperforms'], ['underperforms', 'has', 'simple no -context Seq2Seq baseline']]",[],[],[],[],[],"[['Results', 'has', 'AS Reader']]",[],natural_language_inference,46,224
experiments,"As with the AS Reader , we observed no significant differences for varying number of chunks .","[('observed', (7, 8)), ('for', (11, 12))]","[('no significant differences', (8, 11)), ('varying', (12, 13)), ('number of chunks', (13, 16))]","[['no significant differences', 'for', 'varying']]","[['varying', 'has', 'number of chunks']]",[],[],[],"[['Results', 'observed', 'no significant differences']]",[],[],[],natural_language_inference,46,229
research-problem,A large annotated corpus for learning natural language inference,[],"[('natural language inference', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference']]",[],[],[],[],natural_language_inference,47,2
research-problem,"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning .",[],"[('natural language inference ( NLI )', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,47,11
research-problem,"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks .",[],"[('NLI', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,47,12
dataset,"To address this , this paper introduces the Stanford Natural Language Inference ( SNLI ) corpus , a collection of sentence pairs labeled for entailment , contradiction , and semantic independence .","[('introduces', (6, 7)), ('of', (19, 20)), ('labeled for', (22, 24))]","[('Stanford Natural Language Inference ( SNLI ) corpus', (8, 16)), ('collection', (18, 19)), ('sentence pairs', (20, 22)), ('entailment', (24, 25)), ('contradiction', (26, 27)), ('semantic independence', (29, 31))]","[['collection', 'of', 'sentence pairs'], ['sentence pairs', 'labeled for', 'entailment'], ['sentence pairs', 'labeled for', 'contradiction'], ['sentence pairs', 'labeled for', 'semantic independence']]","[['Stanford Natural Language Inference ( SNLI ) corpus', 'has', 'collection']]","[['Dataset', 'introduces', 'Stanford Natural Language Inference ( SNLI ) corpus']]",[],[],[],[],[],[],natural_language_inference,47,19
dataset,"At 570,152 sentence pairs , SNLI is two orders of magnitude larger than all other resources of its type .","[('At', (0, 1)), ('is', (6, 7)), ('than', (12, 13)), ('of', (16, 17))]","[('570,152 sentence pairs', (1, 4)), ('SNLI', (5, 6)), ('two orders of magnitude larger', (7, 12)), ('other resources', (14, 16)), ('its type', (17, 19))]","[['SNLI', 'is', 'two orders of magnitude larger'], ['two orders of magnitude larger', 'than', 'other resources'], ['other resources', 'of', 'its type']]","[['570,152 sentence pairs', 'has', 'SNLI']]","[['Dataset', 'At', '570,152 sentence pairs']]",[],[],[],[],[],[],natural_language_inference,47,20
dataset,"And , in contrast to many such resources , all of its sentences and labels were written by humans in a grounded , naturalistic context .","[('in', (2, 3)), ('written by', (16, 18))]","[('sentences and labels', (12, 15)), ('humans', (18, 19)), ('grounded , naturalistic context', (21, 25))]","[['sentences and labels', 'written by', 'humans'], ['humans', 'in', 'grounded , naturalistic context']]",[],[],"[['Dataset', 'has', 'sentences and labels']]",[],[],[],[],[],natural_language_inference,47,21
dataset,"In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .",[],[],"[['separate validation phase', 'collected', 'four additional judgments'], ['four additional judgments', 'for', 'each label'], ['each label', 'for', '56,941'], ['56,941', 'of', 'examples']]",[],"[['Dataset', 'In', 'separate validation phase']]",[],[],[],[],[],[],natural_language_inference,47,22
dataset,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .","[('Of these', (0, 2)), ('of', (5, 6)), ('emerge with', (7, 9)), ('see', (16, 17)), ('from', (20, 21))]","[('98 %', (3, 5)), ('cases', (6, 7)), ('threeannotator consensus', (10, 12)), ('58 %', (14, 16)), ('unanimous consensus', (18, 20)), ('all five annotators', (21, 24))]","[['58 %', 'see', 'unanimous consensus'], ['unanimous consensus', 'from', 'all five annotators'], ['98 %', 'of', 'cases'], ['cases', 'emerge with', 'threeannotator consensus']]",[],[],[],[],"[['four additional judgments', 'Of these', '58 %'], ['four additional judgments', 'Of these', '98 %']]",[],[],[],natural_language_inference,47,23
results,"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .","[('performed', (5, 6)), ('than', (8, 9))]","[('sum of words model', (1, 5)), ('slightly worse', (6, 8)), ('fundamentally similar lexicalized classifier', (10, 14))]","[['sum of words model', 'performed', 'slightly worse'], ['slightly worse', 'than', 'fundamentally similar lexicalized classifier']]",[],[],"[['Results', 'has', 'sum of words model']]",[],[],[],[],[],natural_language_inference,47,163
results,"Of the two RNN models , the LSTM 's more robust ability to learn long - term dependencies serves it well , giving it a substantial advantage over the plain RNN , and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set ( LSTM performance near the stopping iteration varies by up to 0.5 % between evaluation steps ) .","[('Of', (0, 1)), ('to learn', (12, 14)), ('serves', (18, 19)), ('resulting in', (33, 35)), ('that is', (36, 38)), ('to', (40, 41)), ('on', (44, 45))]","[('two RNN models', (2, 5)), (""LSTM 's"", (7, 9)), ('more robust ability', (9, 12)), ('long - term dependencies', (14, 18)), ('well', (20, 21)), ('performance', (35, 36)), ('essentially equivalent', (38, 40)), ('lexicalized classifier', (42, 44)), ('test set', (46, 48))]","[['more robust ability', 'serves', 'well'], ['more robust ability', 'resulting in', 'performance'], ['performance', 'that is', 'essentially equivalent'], ['essentially equivalent', 'to', 'lexicalized classifier'], ['lexicalized classifier', 'on', 'test set'], ['more robust ability', 'to learn', 'long - term dependencies']]","[['two RNN models', 'has', ""LSTM 's""], [""LSTM 's"", 'has', 'more robust ability']]","[['Results', 'Of', 'two RNN models']]",[],[],[],[],[],[],natural_language_inference,47,164
results,"While the lexicalized model fits the training set almost perfectly , the gap between train and test set accuracy is relatively small for all three neural network models , suggesting that research into significantly higher capacity versions of these models would be productive .","[('between', (13, 14)), ('is', (19, 20)), ('for', (22, 23))]","[('gap', (12, 13)), ('train and test set accuracy', (14, 19)), ('relatively small', (20, 22)), ('all three neural network models', (23, 28))]","[['gap', 'between', 'train and test set accuracy'], ['train and test set accuracy', 'is', 'relatively small'], ['relatively small', 'for', 'all three neural network models']]",[],[],"[['Results', 'has', 'gap']]",[],[],[],[],[],natural_language_inference,47,165
results,"In addition , though the LSTM and the lexicalized model show similar performance when trained on the current full corpus , the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets .","[('on', (15, 16)), ('for', (25, 26)), ('hints that', (28, 30)), ('of', (37, 38)), ('give it', (41, 43)), ('over', (45, 46))]","[('LSTM', (5, 6)), ('somewhat steeper slope', (22, 25)), ('ability to learn', (31, 34)), ('arbitrarily structured representations', (34, 37)), ('sentence meaning', (38, 40)), ('advantage', (44, 45)), ('more constrained lexicalized model', (47, 51)), ('larger datasets', (53, 55))]","[['somewhat steeper slope', 'for', 'LSTM'], ['somewhat steeper slope', 'hints that', 'ability to learn'], ['arbitrarily structured representations', 'give it', 'advantage'], ['advantage', 'over', 'more constrained lexicalized model'], ['more constrained lexicalized model', 'on', 'larger datasets'], ['arbitrarily structured representations', 'of', 'sentence meaning']]","[['ability to learn', 'has', 'arbitrarily structured representations']]",[],"[['Results', 'has', 'somewhat steeper slope']]",[],[],[],[],[],natural_language_inference,47,168
research-problem,Iterative Alternating Neural Attention for Machine Reading,[],"[('Machine Reading', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading']]",[],[],[],[],natural_language_inference,48,2
research-problem,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",[],"[('machine comprehension', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension']]",[],[],[],[],natural_language_inference,48,4
model,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .","[('propose', (12, 13)), ('to perform', (22, 24))]","[('novel neural attention - based inference model', (14, 21)), ('machine reading comprehension tasks', (24, 28))]","[['novel neural attention - based inference model', 'to perform', 'machine reading comprehension tasks']]",[],"[['Model', 'propose', 'novel neural attention - based inference model']]",[],[],[],[],[],[],natural_language_inference,48,19
model,The model first reads the document and the query using a recurrent neural network .,"[('first reads', (2, 4)), ('using', (9, 10))]","[('document and the query', (5, 9)), ('recurrent neural network', (11, 14))]","[['document and the query', 'using', 'recurrent neural network']]",[],"[['Model', 'first reads', 'document and the query']]",[],[],[],[],[],[],natural_language_inference,48,20
model,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .","[('deploys', (3, 4)), ('to uncover', (8, 10)), ('between', (15, 16))]","[('iterative inference process', (5, 8)), ('inferential links', (11, 13)), ('missing query word', (17, 20)), ('query', (22, 23)), ('document', (26, 27))]","[['iterative inference process', 'to uncover', 'inferential links'], ['inferential links', 'between', 'missing query word'], ['inferential links', 'between', 'query'], ['inferential links', 'between', 'document']]",[],"[['Model', 'deploys', 'iterative inference process']]",[],[],[],[],[],[],natural_language_inference,48,21
model,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","[('involves', (2, 3)), ('first attends', (10, 12)), ('of', (15, 16)), ('then finds', (19, 21)), ('by attending', (24, 26))]","[('novel alternating attention mechanism', (4, 8)), ('some parts', (13, 15)), ('query', (17, 18)), ('corresponding matches', (22, 24)), ('document', (28, 29))]","[['novel alternating attention mechanism', 'then finds', 'corresponding matches'], ['corresponding matches', 'by attending', 'document'], ['novel alternating attention mechanism', 'first attends', 'some parts'], ['some parts', 'of', 'query']]",[],[],[],[],"[['iterative inference process', 'involves', 'novel alternating attention mechanism']]",[],[],"[['novel alternating attention mechanism', 'has', 'result']]",natural_language_inference,48,22
model,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,"[('fed back into', (7, 10)), ('to seed', (14, 16))]","[('result', (1, 2)), ('iterative inference process', (11, 14)), ('next search step', (17, 20))]","[['result', 'fed back into', 'iterative inference process'], ['iterative inference process', 'to seed', 'next search step']]",[],[],[],[],[],[],[],[],natural_language_inference,48,23
model,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .","[('After', (0, 1)), ('uses', (9, 10)), ('of', (12, 13)), ('to predict', (16, 18))]","[('fixed number of iterations', (2, 6)), ('model', (8, 9)), ('summary', (11, 12)), ('inference process', (14, 16)), ('answer', (19, 20))]","[['model', 'uses', 'summary'], ['summary', 'of', 'inference process'], ['summary', 'to predict', 'answer']]","[['fixed number of iterations', 'has', 'model']]",[],[],[],"[['iterative inference process', 'After', 'fixed number of iterations']]",[],[],[],natural_language_inference,48,25
experimental-setup,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","[('To train', (0, 2)), ('used', (6, 7)), ('with', (10, 11)), ('of', (27, 28))]","[('our model', (2, 4)), ('stochastic gradient descent', (7, 10)), ('ADAM optimizer', (12, 14)), ('initial learning rate', (24, 27)), ('0.001', (28, 29))]","[['our model', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'ADAM optimizer'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]",[],"[['Experimental setup', 'To train', 'our model']]",[],[],[],[],[],[],natural_language_inference,48,118
experimental-setup,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",[],[],"[['batch size', 'to', '32'], ['learning rate', 'by', '0.8'], ['learning rate', 'if', 'accuracy'], ['accuracy', 'on', 'validation set'], ['not increase', 'after', 'half - epoch'], ['half - epoch', 'i.e.', '2000 batches'], ['2000 batches', 'for', 'CBT'], ['half - epoch', 'i.e.', '5000 batches'], ['5000 batches', 'for', 'CNN']]","[['validation set', 'has', 'not increase']]","[['Experimental setup', 'set', 'batch size'], ['Experimental setup', 'decay', 'learning rate']]",[],[],[],[],[],[],natural_language_inference,48,119
experimental-setup,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .","[('initialize', (1, 2)), ('of', (4, 5)), ('by', (7, 8)), ('from', (9, 10))]","[('all weights', (2, 4)), ('our model', (5, 7)), ('sampling', (8, 9)), ('normal distribution N ( 0 , 0.05 )', (11, 19))]","[['all weights', 'of', 'our model'], ['all weights', 'by', 'sampling'], ['sampling', 'from', 'normal distribution N ( 0 , 0.05 )']]",[],"[['Experimental setup', 'initialize', 'all weights']]",[],[],[],[],[],[],natural_language_inference,48,120
experimental-setup,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",[],[],"[['biases', 'are', 'initialized'], ['initialized', 'to', 'zero'], ['GRU recurrent weights', 'are', 'initialized'], ['initialized', 'to be', 'orthogonal']]",[],[],"[['Experimental setup', 'has', 'biases'], ['Experimental setup', 'has', 'GRU recurrent weights']]",[],[],[],[],[],natural_language_inference,48,121
experimental-setup,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .","[('to stabilize', (2, 4)), ('clip', (8, 9)), ('if', (11, 12)), ('greater than', (15, 17))]","[('learning', (5, 6)), ('gradients', (10, 11)), ('norm', (13, 14)), ('5', (17, 18))]","[['learning', 'clip', 'gradients'], ['gradients', 'if', 'norm'], ['norm', 'greater than', '5']]",[],"[['Experimental setup', 'to stabilize', 'learning']]",[],[],[],[],[],[],natural_language_inference,48,122
experimental-setup,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .","[('setting', (3, 4)), ('to', (6, 7)), ('worked', (24, 25)), ('across', (26, 27))]","[('embedding regularization', (4, 6)), ('0.0001', (7, 8)), ('robustly', (25, 26)), ('datasets', (28, 29))]","[['embedding regularization', 'worked', 'robustly'], ['robustly', 'across', 'datasets'], ['embedding regularization', 'to', '0.0001']]",[],"[['Experimental setup', 'setting', 'embedding regularization']]",[],[],[],[],[],[],natural_language_inference,48,124
experimental-setup,"Our model is implemented in Theano , using the Keras library .","[('implemented in', (3, 5)), ('using', (7, 8))]","[('Our model', (0, 2)), ('Theano', (5, 6)), ('Keras library', (9, 11))]","[['Our model', 'implemented in', 'Theano'], ['Theano', 'using', 'Keras library']]",[],[],"[['Experimental setup', 'has', 'Our model']]",[],[],[],[],[],natural_language_inference,48,125
research-problem,Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE,[],"[('NATURAL LANGUAGE INFERENCE', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'NATURAL LANGUAGE INFERENCE']]",[],[],[],[],natural_language_inference,49,2
research-problem,"Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) .",[],"[('NLI', (4, 5)), ('recognizing textual entiailment', (8, 11)), ('RTE', (13, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI'], ['Contribution', 'has research problem', 'recognizing textual entiailment'], ['Contribution', 'has research problem', 'RTE']]",[],[],[],[],natural_language_inference,49,10
model,"In this work , we push the multi-head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor .","[('push', (5, 6)), ('to', (9, 10)), ('by building', (12, 14)), ('call', (27, 28))]","[('multi-head attention', (7, 9)), ('extreme', (11, 12)), ('word - by - word dimension - wise alignment tensor', (15, 25)), ('interaction tensor', (28, 30))]","[['multi-head attention', 'by building', 'word - by - word dimension - wise alignment tensor'], ['word - by - word dimension - wise alignment tensor', 'call', 'interaction tensor'], ['multi-head attention', 'to', 'extreme']]",[],"[['Model', 'push', 'multi-head attention']]",[],[],[],[],[],[],natural_language_inference,49,23
model,The interaction tensor encodes the high - order alignment relationship between sentences pair .,"[('encodes', (3, 4)), ('between', (10, 11))]","[('interaction tensor', (1, 3)), ('high - order alignment relationship', (5, 10)), ('sentences pair', (11, 13))]","[['interaction tensor', 'encodes', 'high - order alignment relationship'], ['high - order alignment relationship', 'between', 'sentences pair']]",[],[],"[['Model', 'has', 'interaction tensor']]",[],[],[],[],[],natural_language_inference,49,24
model,We dub the general framework as Interactive Inference Network ( IIN ) .,"[('dub', (1, 2)), ('as', (5, 6))]","[('general framework', (3, 5)), ('Interactive Inference Network ( IIN )', (6, 12))]","[['general framework', 'as', 'Interactive Inference Network ( IIN )']]",[],"[['Model', 'dub', 'general framework']]",[],[],[],[],[],[],natural_language_inference,49,26
,We implement our algorithm with Tensorflow framework .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,137
,"An Adadelta optimizer ( Zeiler , 2012 ) with ? as 0.95 and as 1e ? 8 is used to optimize all the trainable weights .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,138
,The initial learning rate is set to 0.5 and batch size to 70 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,139
,"When the model does not improve best in domain performance for 30,000 steps , an SGD optimizer with learning rate of 3e ? 4 is used to help model to find a better local optimum .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,140
,Dropout layers are applied before all linear layers and after word - embedding layer .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,141
,"We use an exponential decayed keep rate during training , where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10,000 step .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,142
,We initialize our word embeddings with pre-trained 300D Glo Ve 840B vectors while the out - of - vocabulary word are randomly initialized with uniform distribution .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,143
,The character embeddings are randomly initialized with 100D .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,144
,We crop or pad each token to have 16 characters .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,145
,The 1D convolution kernel size for character embedding is 5 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,146
,"All weights are constraint by L2 regularization , and the L2 regularization at step t is calculated as follows :",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,147
,The first scale down ratio ?,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,152
,in feature extraction layer is set to 0.3 and transitional scale down ratio ?,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,153
,is set to 0.5 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,154
,"The sequence length is set as a hard cutoff on all experiments : 48 for MultiNLI , 32 for SNLI and 24 for Quora Question Pair Dataset .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,49,155
results,EXPERIMENT ON MULTINLI,[],"[('MULTINLI', (2, 3))]",[],[],[],"[['Results', 'has', 'MULTINLI']]",[],[],[],[],"[['MULTINLI', 'has', 'Our approach']]",natural_language_inference,49,159
results,"Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % .","[('without using', (3, 5)), ('achieves', (9, 10)), ('of', (20, 21)), ('exceeding', (24, 25)), ('by', (34, 35))]","[('Our approach', (0, 2)), ('recurrent structure', (6, 8)), ('new state - of - the - art performance', (11, 20)), ('80.0 %', (21, 23)), ('current state - of - the - art performance', (25, 34)), ('more than 5 %', (35, 39))]","[['Our approach', 'achieves', 'new state - of - the - art performance'], ['new state - of - the - art performance', 'of', '80.0 %'], ['80.0 %', 'exceeding', 'current state - of - the - art performance'], ['current state - of - the - art performance', 'by', 'more than 5 %'], ['Our approach', 'without using', 'recurrent structure']]",[],[],[],[],[],[],[],[],natural_language_inference,49,164
results,"Unlike the observation from , we find the out - of - domain test performance is consistently lower than in - domain test performance .","[('find', (6, 7)), ('is', (15, 16)), ('than', (18, 19))]","[('out - of - domain test performance', (8, 15)), ('consistently lower', (16, 18)), ('in - domain test performance', (19, 24))]","[['out - of - domain test performance', 'is', 'consistently lower'], ['consistently lower', 'than', 'in - domain test performance']]",[],[],[],[],"[['MULTINLI', 'find', 'out - of - domain test performance']]",[],[],[],natural_language_inference,49,165
results,EXPERIMENT ON SNLI,[],"[('SNLI', (2, 3))]",[],[],[],"[['Results', 'has', 'SNLI']]",[],[],[],[],[],natural_language_inference,49,167
results,"We show our model , DIIN , achieves state - of - the - art performance on the competitive leaderboard .","[('show', (1, 2)), ('achieves', (7, 8)), ('on', (16, 17))]","[('our model , DIIN', (2, 6)), ('state - of - the - art performance', (8, 16)), ('competitive leaderboard', (18, 20))]","[['our model , DIIN', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'competitive leaderboard']]",[],[],[],[],"[['SNLI', 'show', 'our model , DIIN']]",[],[],[],natural_language_inference,49,175
results,EXPERIMENT ON QUORA QUESTION PAIR DATASET,[],"[('QUORA QUESTION PAIR DATASET', (2, 6))]",[],[],[],"[['Results', 'has', 'QUORA QUESTION PAIR DATASET']]",[],[],[],[],"[['QUORA QUESTION PAIR DATASET', 'has', 'BIMPM']]",natural_language_inference,49,179
results,"BIMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM .","[('models', (1, 2)), ('between', (6, 7)), ('on', (9, 10)), ('aggregates', (14, 15)), ('with', (17, 18))]","[('BIMPM', (0, 1)), ('different perspective of matching', (2, 6)), ('sentence pair', (7, 9)), ('both direction', (10, 12)), ('matching vector', (15, 17)), ('LSTM', (18, 19))]","[['BIMPM', 'models', 'different perspective of matching'], ['different perspective of matching', 'between', 'sentence pair'], ['sentence pair', 'on', 'both direction'], ['BIMPM', 'aggregates', 'matching vector'], ['matching vector', 'with', 'LSTM']]",[],[],[],[],[],[],[],[],natural_language_inference,49,182
results,DECATT word and DECATT char uses automatically collected in - domain paraphrase data to noisy pretrain n-gram word embedding and ngram subword embedding correspondingly on decomposable attention model proposed by .,"[('uses', (5, 6)), ('to noisy pretrain', (13, 16)), ('on', (24, 25))]","[('DECATT word and DECATT char', (0, 5)), ('automatically collected in - domain paraphrase data', (6, 13)), ('n-gram word embedding and ngram subword embedding', (16, 23)), ('decomposable attention model', (25, 28))]","[['DECATT word and DECATT char', 'uses', 'automatically collected in - domain paraphrase data'], ['automatically collected in - domain paraphrase data', 'to noisy pretrain', 'n-gram word embedding and ngram subword embedding'], ['n-gram word embedding and ngram subword embedding', 'on', 'decomposable attention model']]",[],[],[],[],[],[],"[['QUORA QUESTION PAIR DATASET', 'has', 'DECATT word and DECATT char']]",[],natural_language_inference,49,183
ablation-analysis,"After removing the exact match binary feature , we find the performance degrade to 78.2 on matched score on development set and 78.0 on mismatched score .",[],[],"[['exact match binary feature', 'find', 'performance'], ['degrade', 'to', '78.0'], ['78.0', 'on', 'mismatched score'], ['degrade', 'to', '78.2'], ['78.2', 'on', 'matched score'], ['matched score', 'on', 'development set']]","[['performance', 'has', 'degrade'], ['performance', 'has', 'degrade']]","[['Ablation analysis', 'After removing', 'exact match binary feature']]",[],[],[],[],[],[],natural_language_inference,49,192
ablation-analysis,We obtain 73.2 for matched score and 73.6 on mismatched data .,"[('obtain', (1, 2)), ('for', (3, 4)), ('on', (8, 9))]","[('73.2', (2, 3)), ('matched score', (4, 6)), ('73.6', (7, 8)), ('mismatched data', (9, 11))]","[['73.2', 'for', 'matched score'], ['73.6', 'on', 'mismatched data']]",[],"[['Ablation analysis', 'obtain', '73.2'], ['Ablation analysis', 'obtain', '73.6']]",[],[],[],[],[],[],natural_language_inference,49,197
ablation-analysis,"If we remove encoding layer completely , then we 'll obtain a 73.5 for matched score and 73.2 for mismatched score .",[],[],"[['encoding layer completely', 'obtain', '73.2'], ['73.2', 'for', 'mismatched score'], ['encoding layer completely', 'obtain', '73.5'], ['73.5', 'for', 'matched score'], ['73.5', 'for', 'matched score']]",[],"[['Ablation analysis', 'remove', 'encoding layer completely']]",[],[],[],[],[],[],natural_language_inference,49,200
ablation-analysis,The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature .,"[('demonstrate', (2, 3)), ('have', (7, 8)), ('to capture', (10, 12))]","[('feature extraction layer', (4, 7)), ('powerful capability', (8, 10)), ('semantic feature', (13, 15))]","[['feature extraction layer', 'have', 'powerful capability'], ['powerful capability', 'to capture', 'semantic feature']]",[],[],[],[],"[['encoding layer completely', 'demonstrate', 'feature extraction layer']]",[],[],[],natural_language_inference,49,201
ablation-analysis,"In experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network .","[('retaining only', (15, 17))]","[('both self - attention and fuse gate', (6, 13)), ('highway network', (17, 19))]","[['both self - attention and fuse gate', 'retaining only', 'highway network']]",[],[],"[['Ablation analysis', 'remove', 'both self - attention and fuse gate']]",[],[],[],[],"[['both self - attention and fuse gate', 'has', 'result']]",natural_language_inference,49,202
ablation-analysis,The result improves to 77.7 and 77.3 respectively on matched and mismatched development set .,"[('to', (3, 4)), ('on', (8, 9))]","[('result', (1, 2)), ('improves', (2, 3)), ('77.7 and 77.3', (4, 7)), ('matched and mismatched development set', (9, 14))]","[['improves', 'to', '77.7 and 77.3'], ['77.7 and 77.3', 'on', 'matched and mismatched development set']]","[['result', 'has', 'improves']]",[],[],[],[],[],[],[],natural_language_inference,49,203
ablation-analysis,"However , in experiment 6 , when we only remove fuse gate , to our surprise , the performance degrade to 73.5 for matched score and 73.8 for mismatched .",[],[],"[['degrade', 'to', '73.5'], ['degrade', 'to', '73.8'], ['73.8', 'for', 'mismatched']]","[['fuse gate', 'has', 'performance']]",[],"[['Ablation analysis', 'remove', 'fuse gate']]",[],[],[],[],[],natural_language_inference,49,204
ablation-analysis,"On the other hand , if we use the addition of the representation after highway network and the representation after self - attention as skip connection as in experiment 7 , the performance increase to 77.3 and 76.3 .",[],[],"[['addition', 'as', 'skip connection'], ['performance increase', 'to', '77.3 and 76.3'], ['addition', 'of', 'representation'], ['representation', 'after', 'highway network'], ['highway network', 'and', 'representation'], ['representation', 'after', 'self - attention']]","[['skip connection', 'has', 'performance increase']]","[['Ablation analysis', 'use', 'addition']]",[],[],[],[],[],[],natural_language_inference,49,205
research-problem,A Compare - Aggregate Model with Latent Clustering for Answer Selection,[],"[('Answer Selection', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Answer Selection']]",[],[],[],[],natural_language_inference,5,2
research-problem,"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .",[],"[('sentence - level answer- selection', (11, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentence - level answer- selection']]",[],[],[],[],natural_language_inference,5,4
research-problem,Automatic question answering ( QA ) is a primary objective of artificial intelligence .,[],"[('Automatic question answering ( QA )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic question answering ( QA )']]",[],[],[],[],natural_language_inference,5,10
approach,"First , we explore the effect of additional information by adopting a pretrained language model ( LM ) to compute the vector representation of the input text .",[],[],"[['effect', 'of', 'additional information'], ['additional information', 'by adopting', 'pretrained language model ( LM )'], ['pretrained language model ( LM )', 'to compute', 'vector representation'], ['vector representation', 'of', 'input text']]",[],"[['Approach', 'explore', 'effect']]",[],[],[],[],[],[],natural_language_inference,5,21
approach,"Following this study , we select an ELMo language model for this study .","[('select', (5, 6))]","[('ELMo language model', (7, 10))]",[],[],"[['Approach', 'select', 'ELMo language model']]",[],[],[],[],[],[],natural_language_inference,5,23
approach,"We investigate the applicability of transfer learning ( TL ) using a large - scale corpus that is created for a relevant - sentence - selection task ( i.e. , question - answering NLI ( QNLI ) dataset ) .","[('investigate', (1, 2)), ('of', (4, 5)), ('using', (10, 11)), ('created for', (18, 20)), ('i.e.', (28, 29))]","[('applicability', (3, 4)), ('transfer learning ( TL )', (5, 10)), ('large - scale corpus', (12, 16)), ('relevant - sentence - selection task', (21, 27)), ('question - answering NLI ( QNLI ) dataset', (30, 38))]","[['applicability', 'of', 'transfer learning ( TL )'], ['transfer learning ( TL )', 'using', 'large - scale corpus'], ['large - scale corpus', 'created for', 'relevant - sentence - selection task'], ['relevant - sentence - selection task', 'i.e.', 'question - answering NLI ( QNLI ) dataset']]",[],"[['Approach', 'investigate', 'applicability']]",[],[],[],[],[],[],natural_language_inference,5,24
approach,"Second , we further enhance one of the baseline models , Comp - Clip ( refer to the discussion in 3.1 ) , for the target QA task by proposing a novel latent clustering ( LC ) method .","[('enhance', (4, 5)), ('for', (23, 24)), ('by proposing', (28, 30))]","[('one of the baseline models', (5, 10)), ('Comp - Clip', (11, 14)), ('target QA task', (25, 28)), ('novel latent clustering ( LC ) method', (31, 38))]","[['one of the baseline models', 'for', 'target QA task'], ['target QA task', 'by proposing', 'novel latent clustering ( LC ) method']]","[['one of the baseline models', 'name', 'Comp - Clip']]","[['Approach', 'enhance', 'one of the baseline models']]",[],[],[],[],[],[],natural_language_inference,5,25
approach,The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory .,"[('computes', (3, 4)), ('for', (7, 8)), ('by creating', (10, 12)), ('calculating', (17, 18)), ('between', (20, 21))]","[('LC method', (1, 3)), ('latent cluster information', (4, 7)), ('target samples', (8, 10)), ('latent memory space', (13, 16)), ('similarity', (19, 20)), ('sample', (22, 23)), ('memory', (25, 26))]","[['LC method', 'computes', 'latent cluster information'], ['latent cluster information', 'by creating', 'latent memory space'], ['latent cluster information', 'for', 'target samples'], ['latent cluster information', 'calculating', 'similarity'], ['similarity', 'between', 'sample'], ['similarity', 'between', 'memory']]",[],[],"[['Approach', 'has', 'LC method']]",[],[],[],[],[],natural_language_inference,5,26
approach,"By an endto - end learning process with the answer-selection task , the LC method assigns true - label question - answer pairs to similar clusters .","[('By', (0, 1)), ('assigns', (15, 16)), ('to', (23, 24))]","[('endto - end learning process', (2, 7)), ('true - label question - answer pairs', (16, 23)), ('similar clusters', (24, 26))]","[['true - label question - answer pairs', 'to', 'similar clusters'], ['similar clusters', 'By', 'endto - end learning process']]",[],[],[],[],"[['LC method', 'assigns', 'true - label question - answer pairs']]",[],[],[],natural_language_inference,5,27
approach,"Last , we explore the effect of different objective functions ( listwise and pointwise learning ) .",[],"[('different objective functions', (7, 10)), ('listwise and pointwise learning', (11, 15))]",[],"[['different objective functions', 'has', 'listwise and pointwise learning']]",[],[],[],[],[],"[['effect', 'of', 'different objective functions']]",[],natural_language_inference,5,29
hyperparameters,"To implement the Comp - Clip model , we apply a context projection weight matrix with 100 dimensions that are shared between the question part and the answer part ( eq. 1 ) .","[('implement', (1, 2)), ('apply', (9, 10)), ('with', (15, 16)), ('shared between', (20, 22))]","[('Comp - Clip model', (3, 7)), ('context projection weight matrix', (11, 15)), ('100 dimensions', (16, 18)), ('question part', (23, 25)), ('answer part', (27, 29))]","[['Comp - Clip model', 'apply', 'context projection weight matrix'], ['context projection weight matrix', 'with', '100 dimensions'], ['context projection weight matrix', 'shared between', 'question part'], ['context projection weight matrix', 'shared between', 'answer part']]",[],"[['Hyperparameters', 'implement', 'Comp - Clip model']]",[],[],[],[],[],[],natural_language_inference,5,112
hyperparameters,"In the aggregation part , we use 1 - D CNN with a total of 500 filters , which involves five types of filters K ? R {1,2,3,4,5}100 , 100 per type .","[('In', (0, 1)), ('use', (6, 7)), ('with', (11, 12))]","[('aggregation part', (2, 4)), ('1 - D CNN', (7, 11)), ('total of 500 filters', (13, 17))]","[['aggregation part', 'use', '1 - D CNN'], ['1 - D CNN', 'with', 'total of 500 filters']]",[],"[['Hyperparameters', 'In', 'aggregation part']]",[],[],[],[],[],[],natural_language_inference,5,113
hyperparameters,"We select k ( for the kmax - pool in equation 5 ) as 6 and 4 for the WikiQA and TREC - QA case , respectively .","[('select', (1, 2)), ('for', (4, 5)), ('as', (13, 14))]","[('k', (2, 3)), ('6 and 4', (14, 17)), ('WikiQA and TREC - QA case', (19, 25))]","[['k', 'as', '6 and 4'], ['6 and 4', 'for', 'WikiQA and TREC - QA case']]",[],"[['Hyperparameters', 'select', 'k']]",[],[],[],[],[],[],natural_language_inference,5,117
hyperparameters,"In both datasets , we apply 8 latent clusters .","[('apply', (5, 6))]","[('8 latent clusters', (6, 9))]",[],[],[],[],[],"[['WikiQA and TREC - QA case', 'apply', '8 latent clusters']]",[],[],[],natural_language_inference,5,118
hyperparameters,"The vocabulary size in the WiKiQA , TREC - QA and QNLI dataset are 30,104 , 56,908 and 154,442 , respectively .","[('in', (3, 4)), ('are', (13, 14))]","[('vocabulary size', (1, 3)), ('WiKiQA , TREC - QA and QNLI dataset', (5, 13)), ('30,104 , 56,908 and 154,442', (14, 19))]","[['vocabulary size', 'in', 'WiKiQA , TREC - QA and QNLI dataset'], ['WiKiQA , TREC - QA and QNLI dataset', 'are', '30,104 , 56,908 and 154,442']]",[],[],"[['Hyperparameters', 'has', 'vocabulary size']]",[],[],[],[],[],natural_language_inference,5,119
hyperparameters,"When applying the TL , the vocabulary size is set to 154,442 , and the dimension of the context projection weight matrix is set to 300 .",[],[],"[['vocabulary size', 'set to', '154,442'], ['dimension', 'of', 'context projection weight matrix'], ['context projection weight matrix', 'set to', '300']]","[['TL', 'has', 'vocabulary size'], ['TL', 'has', 'dimension']]","[['Hyperparameters', 'applying', 'TL']]",[],[],[],[],[],[],natural_language_inference,5,120
hyperparameters,"We use the Adam optimizer , including gradient clipping , by the norm at a threshold of 5 .","[('use', (1, 2)), ('including', (6, 7)), ('by', (10, 11)), ('at', (13, 14)), ('of', (16, 17))]","[('Adam optimizer', (3, 5)), ('gradient clipping', (7, 9)), ('norm', (12, 13)), ('threshold', (15, 16)), ('5', (17, 18))]","[['Adam optimizer', 'including', 'gradient clipping'], ['gradient clipping', 'by', 'norm'], ['norm', 'at', 'threshold'], ['threshold', 'of', '5']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],[],[],[],natural_language_inference,5,121
hyperparameters,"For the purpose of regularization , we applied a dropout with a ratio of 0.5 ..","[('For', (0, 1)), ('of', (3, 4)), ('applied', (7, 8)), ('with', (10, 11))]","[('regularization', (4, 5)), ('dropout', (9, 10)), ('ratio', (12, 13)), ('0.5', (14, 15))]","[['regularization', 'applied', 'dropout'], ['dropout', 'with', 'ratio'], ['ratio', 'of', '0.5']]",[],"[['Hyperparameters', 'For', 'regularization']]",[],[],[],[],[],[],natural_language_inference,5,122
results,"Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a better performance than the listwise learning approach .","[('For', (3, 4)), ('shows', (12, 13)), ('than', (16, 17))]","[('WikiQA dataset', (5, 7)), ('pointwise learning approach', (9, 12)), ('better performance', (14, 16)), ('listwise learning approach', (18, 21))]","[['pointwise learning approach', 'shows', 'better performance'], ['better performance', 'than', 'listwise learning approach']]","[['WikiQA dataset', 'has', 'pointwise learning approach']]","[['Results', 'For', 'WikiQA dataset']]",[],[],[],[],[],[],natural_language_inference,5,127
results,We combine LM with the base model ( Comp - Clip + LM ) and observe a significant improvement in performance in terms of MAP ( 0.714 to 0.746 absolute ) .,"[('combine', (1, 2)), ('with', (3, 4)), ('observe', (15, 16)), ('in', (19, 20)), ('in terms of', (21, 24))]","[('LM', (2, 3)), ('base model ( Comp - Clip + LM )', (5, 14)), ('significant improvement', (17, 19)), ('performance', (20, 21)), ('MAP ( 0.714 to 0.746 absolute )', (24, 31))]","[['LM', 'with', 'base model ( Comp - Clip + LM )'], ['LM', 'observe', 'significant improvement'], ['significant improvement', 'in', 'performance'], ['performance', 'in terms of', 'MAP ( 0.714 to 0.746 absolute )']]",[],[],[],[],"[['WikiQA dataset', 'combine', 'LM']]",[],[],[],natural_language_inference,5,128
results,"When we add the LC method ( Comp - Clip + LM + LC ) , the best previous results are surpassed in terms of MAP ( 0.718 to 0.764 absolute ) .","[('add', (2, 3)), ('are', (20, 21)), ('in terms of', (22, 25))]","[('LC method ( Comp - Clip + LM + LC )', (4, 15)), ('best previous results', (17, 20)), ('surpassed', (21, 22)), ('MAP ( 0.718 to 0.764 absolute )', (25, 32))]","[['best previous results', 'are', 'surpassed'], ['surpassed', 'in terms of', 'MAP ( 0.718 to 0.764 absolute )']]","[['LC method ( Comp - Clip + LM + LC )', 'has', 'best previous results']]",[],[],[],"[['WikiQA dataset', 'add', 'LC method ( Comp - Clip + LM + LC )']]",[],[],[],natural_language_inference,5,129
results,The pointwise learning approach also shows excellent performance with the TREC - QA dataset .,"[('shows', (5, 6)), ('with', (8, 9))]","[('pointwise learning approach', (1, 4)), ('excellent performance', (6, 8)), ('TREC - QA dataset', (10, 14))]","[['pointwise learning approach', 'shows', 'excellent performance']]","[['TREC - QA dataset', 'has', 'pointwise learning approach']]","[['Results', 'with', 'TREC - QA dataset']]",[],[],[],[],[],[],natural_language_inference,5,132
results,"As in the WikiQA case , we achieve additional performance gains in terms of the MAP as we apply LM , LC , and TL ( 0.850 , 0.868 and 0.875 , respectively ) .","[('achieve', (7, 8)), ('in terms of', (11, 14)), ('apply', (18, 19))]","[('additional performance gains', (8, 11)), ('MAP', (15, 16)), ('LM , LC , and TL', (19, 25)), ('0.850 , 0.868 and 0.875', (26, 31))]","[['additional performance gains', 'in terms of', 'MAP'], ['MAP', 'apply', 'LM , LC , and TL']]","[['LM , LC , and TL', 'has', '0.850 , 0.868 and 0.875']]",[],[],[],"[['TREC - QA dataset', 'achieve', 'additional performance gains']]",[],[],[],natural_language_inference,5,135
results,"In particular , our model outperforms the best previous result when we add LC method , ( Comp - Clip + LM + LC ) in terms of MAP ( 0.865 to 0.868 ) .","[('outperforms', (5, 6)), ('add', (12, 13)), ('in terms of', (25, 28)), ('to', (31, 32))]","[('our model', (3, 5)), ('best previous result', (7, 10)), ('LC method , ( Comp - Clip + LM + LC )', (13, 25)), ('MAP', (28, 29)), ('0.865', (30, 31)), ('0.868', (32, 33))]","[['our model', 'outperforms', 'best previous result'], ['best previous result', 'add', 'LC method , ( Comp - Clip + LM + LC )'], ['best previous result', 'in terms of', 'MAP'], ['0.865', 'to', '0.868']]","[['MAP', 'has', '0.865']]",[],[],[],[],[],"[['TREC - QA dataset', 'has', 'our model']]",[],natural_language_inference,5,136
research-problem,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,[],"[('Neural Question Answering', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Question Answering']]",[],[],[],[],natural_language_inference,50,2
model,"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .","[('propose', (5, 6)), ('for', (12, 13))]","[('extremely simple neural ranking model', (7, 12)), ('question answering', (13, 15))]","[['extremely simple neural ranking model', 'for', 'question answering']]",[],"[['Model', 'propose', 'extremely simple neural ranking model']]",[],[],[],[],[],[],natural_language_inference,50,24
model,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,"[('models', (3, 4)), ('between', (6, 7)), ('in', (9, 10)), ('instead of', (12, 14))]","[('neural ranking', (1, 3)), ('relationships', (5, 6)), ('QA pairs', (7, 9)), ('Hyperbolic space', (10, 12)), ('Euclidean space', (14, 16))]","[['neural ranking', 'models', 'relationships'], ['relationships', 'between', 'QA pairs'], ['QA pairs', 'in', 'Hyperbolic space'], ['Hyperbolic space', 'instead of', 'Euclidean space']]",[],[],"[['Model', 'has', 'neural ranking']]",[],[],[],[],[],natural_language_inference,50,25
model,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,[],[],"[['Hyperbolic space', 'is', 'embedding space'], ['embedding space', 'with', 'constant negative curvature'], ['constant negative curvature', 'in which', 'distance'], ['distance', 'is', 'increasing exponentially'], ['distance', 'towards', 'border']]",[],[],"[['Model', 'has', 'Hyperbolic space']]",[],[],[],[],[],natural_language_inference,50,26
baselines,YahooCQA,[],"[('YahooCQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'YahooCQA']]",[],[],[],[],"[['YahooCQA', 'has', 'key competitors']]",natural_language_inference,50,187
baselines,- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from Tay et al.,"[('are', (7, 8)), ('from', (21, 22))]","[('key competitors', (2, 4)), ('Neural Tensor LSTM ( NTN - LSTM )', (9, 17)), ('HD - LSTM', (18, 21)), ('Tay et al.', (22, 25))]","[['key competitors', 'are', 'Neural Tensor LSTM ( NTN - LSTM )'], ['key competitors', 'are', 'HD - LSTM'], ['HD - LSTM', 'from', 'Tay et al.']]",[],[],[],[],[],[],[],[],natural_language_inference,50,188
baselines,"along with their implementation of the Convolutional Neural Tensor Network , vanilla CNN model , and the Okapi BM - 25 benchmark .","[('of', (4, 5))]","[('implementation', (3, 4)), ('Convolutional Neural Tensor Network', (6, 10)), ('vanilla CNN model', (11, 14)), ('Okapi BM - 25 benchmark', (17, 22))]","[['implementation', 'of', 'Convolutional Neural Tensor Network'], ['implementation', 'of', 'vanilla CNN model'], ['implementation', 'of', 'Okapi BM - 25 benchmark']]",[],[],[],[],[],[],"[['YahooCQA', 'has', 'implementation']]",[],natural_language_inference,50,189
baselines,"Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA","[('of', (8, 9))]","[('WikiQA', (33, 34))]",[],[],[],"[['Baselines', 'has', 'WikiQA']]",[],[],[],[],"[['WikiQA', 'has', 'key competitors']]",natural_language_inference,50,190
baselines,"- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .","[('are', (7, 8)), ('from', (28, 29))]","[('key competitors', (2, 4)), ('Paragraph Vector ( PV )', (9, 14)), ('PV + Cnt models', (15, 19)), ('Le and Mikolv', (20, 23)), ('CNN + Cnt model', (24, 28)), ('Yu et al.', (29, 32)), ('LCLR ( Yih et al . )', (33, 40))]","[['key competitors', 'are', 'Paragraph Vector ( PV )'], ['key competitors', 'are', 'PV + Cnt models'], ['key competitors', 'are', 'CNN + Cnt model'], ['CNN + Cnt model', 'from', 'Yu et al.'], ['key competitors', 'are', 'LCLR ( Yih et al . )']]","[['PV + Cnt models', 'of', 'Le and Mikolv']]",[],[],[],[],[],[],[],natural_language_inference,50,191
baselines,"For the clean version of this dataset , we also compare with AP - CNN and QA - BiLSTM / CNN .","[('compare with', (10, 12))]","[('AP - CNN', (12, 15)), ('QA - BiLSTM / CNN', (16, 21))]",[],[],[],[],[],"[['WikiQA', 'compare with', 'AP - CNN'], ['WikiQA', 'compare with', 'QA - BiLSTM / CNN']]",[],[],[],natural_language_inference,50,194
hyperparameters,Hyper QA is implemented in Tensor - Flow .,"[('implemented in', (3, 5))]","[('Hyper QA', (0, 2)), ('Tensor - Flow', (5, 8))]","[['Hyper QA', 'implemented in', 'Tensor - Flow']]",[],[],"[['Hyperparameters', 'has', 'Hyper QA']]",[],[],[],[],[],natural_language_inference,50,210
hyperparameters,"We adopt the AdaGrad optimizer with initial learning rate tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .","[('adopt', (1, 2)), ('with', (5, 6)), ('tuned amongst', (9, 11))]","[('AdaGrad optimizer', (3, 5)), ('initial learning rate', (6, 9)), ('{ 0.2 , 0.1 , 0.05 , 0.01 }', (11, 20))]","[['AdaGrad optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'tuned amongst', '{ 0.2 , 0.1 , 0.05 , 0.01 }']]",[],"[['Hyperparameters', 'adopt', 'AdaGrad optimizer']]",[],[],[],[],[],[],natural_language_inference,50,211
hyperparameters,"The batch size is tuned amongst { 50 , 100 , 200 } .","[('is', (3, 4)), ('amongst', (5, 6))]","[('batch size', (1, 3)), ('tuned', (4, 5)), ('{ 50 , 100 , 200 }', (6, 13))]","[['batch size', 'is', 'tuned'], ['tuned', 'amongst', '{ 50 , 100 , 200 }']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,50,212
hyperparameters,Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped .,"[('are', (1, 2)), ('trained for', (2, 4)), ('each time', (12, 14)), ('on', (16, 17)), ('is', (20, 21))]","[('Models', (0, 1)), ('25 epochs', (4, 6)), ('model parameters', (8, 10)), ('saved', (11, 12)), ('performance', (15, 16)), ('validation set', (18, 20)), ('topped', (21, 22))]","[['Models', 'trained for', '25 epochs'], ['model parameters', 'are', 'saved'], ['saved', 'each time', 'performance'], ['performance', 'is', 'topped'], ['performance', 'on', 'validation set']]","[['Models', 'has', 'model parameters']]",[],"[['Hyperparameters', 'has', 'Models']]",[],[],[],[],[],natural_language_inference,50,213
hyperparameters,"The dimension of the projection layer is tuned amongst { 100 , 200 , 300 , 400 } .","[('of', (2, 3)), ('is', (6, 7)), ('amongst', (8, 9))]","[('dimension', (1, 2)), ('projection layer', (4, 6)), ('tuned', (7, 8)), ('{ 100 , 200 , 300 , 400 }', (9, 18))]","[['dimension', 'of', 'projection layer'], ['projection layer', 'is', 'tuned'], ['tuned', 'amongst', '{ 100 , 200 , 300 , 400 }']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],natural_language_inference,50,214
hyperparameters,"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.","[('is', (2, 3)), ('amongst', (4, 5))]","[('L2 regularization', (0, 2)), ('tuned', (3, 4))]","[['L2 regularization', 'is', 'tuned']]",[],[],"[['Hyperparameters', 'has', 'L2 regularization']]",[],[],"[['tuned', 'amongst', '{ 0.001 , 0.0001 , 0.00001 }']]",[],[],natural_language_inference,50,215
hyperparameters,The negative sampling rate is tuned from 2 to 8 .,"[('is', (4, 5)), ('from', (6, 7))]","[('negative sampling rate', (1, 4)), ('tuned', (5, 6)), ('2 to 8', (7, 10))]","[['negative sampling rate', 'is', 'tuned'], ['tuned', 'from', '2 to 8']]",[],[],"[['Hyperparameters', 'has', 'negative sampling rate']]",[],[],[],[],[],natural_language_inference,50,216
results,reports the experimental results on SemEvalCQA .,"[('on', (4, 5))]","[('SemEvalCQA', (5, 6))]",[],[],"[['Results', 'on', 'SemEvalCQA']]",[],[],[],[],[],"[['SemEvalCQA', 'has', 'proposed approach']]",natural_language_inference,50,227
results,Our proposed approach achieves highly competitive performance on this dataset .,"[('achieves', (3, 4))]","[('proposed approach', (1, 3)), ('highly competitive performance', (4, 7))]","[['proposed approach', 'achieves', 'highly competitive performance']]",[],[],[],[],[],[],[],[],natural_language_inference,50,228
results,"Specifically , we have obtained the best P@1 performance over all , outperforming the state - of - the - art AI - CNN model by 3 % in terms of P@1 .","[('obtained', (4, 5)), ('by', (25, 26)), ('in terms of', (28, 31))]","[('best P@1 performance over all', (6, 11)), ('outperforming', (12, 13)), ('state - of - the - art AI - CNN model', (14, 25)), ('3 %', (26, 28)), ('P@1', (31, 32))]","[['state - of - the - art AI - CNN model', 'by', '3 %'], ['3 %', 'in terms of', 'P@1']]","[['best P@1 performance over all', 'has', 'outperforming'], ['outperforming', 'has', 'state - of - the - art AI - CNN model']]",[],[],[],"[['SemEvalCQA', 'obtained', 'best P@1 performance over all']]",[],[],[],natural_language_inference,50,229
results,The performance of our model on MAP is marginally short from the best performing model .,"[('of', (2, 3)), ('on', (5, 6)), ('is', (7, 8)), ('from', (10, 11))]","[('performance', (1, 2)), ('our model', (3, 5)), ('MAP', (6, 7)), ('marginally short', (8, 10)), ('best performing model', (12, 15))]","[['performance', 'of', 'our model'], ['our model', 'is', 'marginally short'], ['marginally short', 'from', 'best performing model'], ['our model', 'on', 'MAP']]",[],[],[],[],[],[],"[['SemEvalCQA', 'has', 'performance']]",[],natural_language_inference,50,230
results,reports the results on TrecQA ( raw ) .,"[('on', (3, 4))]","[('TrecQA ( raw )', (4, 8))]",[],[],"[['Results', 'on', 'TrecQA ( raw )']]",[],[],[],[],[],"[['TrecQA ( raw )', 'has', 'Hyper QA']]",natural_language_inference,50,239
results,Hyper QA achieves very competitive performance on both MAP and MRR metrics .,"[('achieves', (2, 3))]","[('Hyper QA', (0, 2)), ('competitive performance', (4, 6)), ('MAP and MRR metrics', (8, 12))]","[['Hyper QA', 'achieves', 'competitive performance']]","[['competitive performance', 'on', 'MAP and MRR metrics']]",[],[],[],[],[],[],[],natural_language_inference,50,240
results,"Specifically , Hyper QA outperforms the basic CNN model of ( S&M ) by 2 % ? 3 % in terms of MAP / MRR .","[('by', (13, 14)), ('in terms of', (19, 22))]","[('outperforms', (4, 5)), ('basic CNN model of ( S&M )', (6, 13)), ('2 % ? 3 %', (14, 19)), ('MAP / MRR', (22, 25))]","[['basic CNN model of ( S&M )', 'by', '2 % ? 3 %'], ['basic CNN model of ( S&M )', 'in terms of', 'MAP / MRR']]","[['outperforms', 'has', 'basic CNN model of ( S&M )']]",[],[],[],[],[],"[['Hyper QA', 'has', 'outperforms'], ['Hyper QA', 'has', 'outperforms']]",[],natural_language_inference,50,241
results,"Similarly , reports the results on TrecQA ( clean ) .",[],"[('TrecQA ( clean )', (6, 10))]",[],[],[],"[['Results', 'on', 'TrecQA ( clean )']]",[],[],[],[],"[['TrecQA ( clean )', 'has', 'Hyper QA']]",natural_language_inference,50,247
results,"Similarly , Hyper QA also outperforms MP - CNN , AP - CNN and QA - CNN .",[],"[('Hyper QA', (2, 4)), ('outperforms', (5, 6)), ('MP - CNN', (6, 9)), ('AP - CNN', (10, 13)), ('QA - CNN', (14, 17))]",[],"[['outperforms', 'has', 'MP - CNN'], ['outperforms', 'has', 'AP - CNN'], ['outperforms', 'has', 'QA - CNN']]",[],[],[],[],[],[],[],natural_language_inference,50,248
research-problem,Neural Stored - program Memory,[],"[('Neural Stored - program Memory', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Stored - program Memory']]",[],[],[],[],natural_language_inference,51,2
research-problem,"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures .",[],"[('stored - program memory', (19, 23))]",[],[],[],[],"[['Contribution', 'has research problem', 'stored - program memory']]",[],[],[],[],natural_language_inference,51,6
model,Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory .,"[('step further towards', (6, 9)), ('by coupling', (10, 12)), ('with', (14, 15))]","[('UTM', (9, 10)), ('MANN', (13, 14)), ('external program memory', (16, 19))]","[['UTM', 'by coupling', 'MANN'], ['MANN', 'with', 'external program memory']]",[],"[['Model', 'step further towards', 'UTM']]",[],[],[],[],[],[],natural_language_inference,51,21
model,"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .","[('co-exists with', (3, 5)), ('in', (8, 9)), ('providing', (12, 13))]","[('program memory', (1, 3)), ('data memory', (6, 8)), ('MANN', (10, 11)), ('learning', (20, 21)), ('complicated tasks', (21, 23))]","[['program memory', 'co-exists with', 'data memory'], ['data memory', 'in', 'MANN'], ['data memory', 'in', 'learning']]","[['learning', 'has', 'complicated tasks']]",[],"[['Model', 'has', 'program memory']]",[],[],"[['learning', 'providing', 'flexibility'], ['learning', 'providing', 'reuseability'], ['learning', 'providing', 'modularity']]",[],[],natural_language_inference,51,22
model,"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .",[],[],"[['weights', 'of', ""MANN 's controller network""], ['weights', 'retrieved', 'quickly'], ['quickly', 'via', 'key - value attention mechanism'], ['key - value attention mechanism', 'across', 'timesteps'], ['weights', 'updated', 'slowly'], ['slowly', 'via', 'backpropagation']]",[],[],[],[],"[['program memory', 'stores', 'weights']]",[],[],[],natural_language_inference,51,23
model,"By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .",[],[],"[['Neural Stored - program Memory ( NSM )', 'learn to', 'switch'], ['programs / weights', 'in', 'controller network'], ['programs / weights', 'adapting to', 'different functionalities'], ['different functionalities', 'aligning with', 'different parts'], ['different parts', 'of', 'sequential task'], ['different functionalities', 'aligning with', 'different tasks'], ['different tasks', 'in', 'continual and few - shot learning']]","[['switch', 'has', 'programs / weights'], ['controller network', 'has', 'appropriately']]","[['Model', 'referred to', 'Neural Stored - program Memory ( NSM )']]",[],[],[],[],[],[],natural_language_inference,51,24
results,"Except for the Copy task , which is too simple , other tasks observe convergence speed improvement of NUTM over that of NTM , thereby validating the benefit of using two programs across timesteps even for the single task setting .","[('observe', (13, 14)), ('of', (17, 18)), ('over', (19, 20)), ('validating', (25, 26)), ('of using', (28, 30)), ('across', (32, 33)), ('even for', (34, 36))]","[('other tasks', (11, 13)), ('convergence speed improvement', (14, 17)), ('NUTM', (18, 19)), ('NTM', (22, 23)), ('benefit', (27, 28)), ('two programs', (30, 32)), ('timesteps', (33, 34)), ('single task setting', (37, 40))]","[['other tasks', 'observe', 'convergence speed improvement'], ['convergence speed improvement', 'validating', 'benefit'], ['benefit', 'of using', 'two programs'], ['two programs', 'across', 'timesteps'], ['two programs', 'even for', 'single task setting'], ['convergence speed improvement', 'of', 'NUTM'], ['NUTM', 'over', 'NTM']]",[],[],"[['Results', 'has', 'other tasks']]",[],[],[],[],[],natural_language_inference,51,148
results,NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences .,[],[],"[['NUTM', 'generalizes', 'better'], ['better', 'to', 'unseen sequences'], ['unseen sequences', 'that are', 'longer'], ['longer', 'than', 'training sequences'], ['NUTM', 'requires', 'fewer training samples'], ['fewer training samples', 'to', 'converge']]",[],[],"[['Results', 'has', 'NUTM']]",[],[],[],[],[],natural_language_inference,51,149
ablation-analysis,"We run the task with three additional baselines : NUTM using direct attention ( DA ) , NUTM using key - value without regularization ( KV ) , NUTM using fixed , uniform program distribution ( UP ) and a vanilla NTM with 2 memory heads ( h = 2 ) .",[],[],"[['NUTM', 'using', 'direct attention ( DA )'], ['NUTM', 'using', 'key - value without regularization ( KV )'], ['NUTM', 'using', 'fixed , uniform program distribution ( UP )'], ['vanilla NTM', 'with', '2 memory heads ( h = 2 )']]","[['three additional baselines', 'name', 'NUTM'], ['three additional baselines', 'name', 'NUTM'], ['three additional baselines', 'name', 'NUTM'], ['three additional baselines', 'name', 'vanilla NTM']]","[['Ablation analysis', 'with', 'three additional baselines']]",[],[],[],[],[],[],natural_language_inference,51,159
ablation-analysis,The results demonstrate that DA exhibits fast yet shallow convergence .,"[('demonstrate', (2, 3)), ('exhibits', (5, 6))]","[('DA', (4, 5)), ('fast yet shallow convergence', (6, 10))]","[['DA', 'exhibits', 'fast yet shallow convergence']]",[],"[['Ablation analysis', 'demonstrate', 'DA']]",[],[],[],[],[],"[['DA', 'has', 'fails']]",natural_language_inference,51,163
ablation-analysis,"It tends to fall into local minima , which finally fails to reach zero loss .","[('fall into', (3, 5)), ('to reach', (11, 13))]","[('local minima', (5, 7)), ('fails', (10, 11)), ('zero loss', (13, 15))]","[['fails', 'to reach', 'zero loss']]",[],[],[],[],"[['DA', 'fall into', 'local minima']]",[],[],[],natural_language_inference,51,164
ablation-analysis,Key- value attention helps NUTM converge completely with fewer iterations .,"[('helps', (3, 4)), ('with', (7, 8))]","[('Key- value attention', (0, 3)), ('NUTM converge', (4, 6)), ('fewer iterations', (8, 10))]","[['Key- value attention', 'helps', 'NUTM converge'], ['NUTM converge', 'with', 'fewer iterations']]",[],[],"[['Ablation analysis', 'has', 'Key- value attention']]",[],[],[],[],"[['Key- value attention', 'has', 'performance']]",natural_language_inference,51,165
ablation-analysis,The performance is further improved with the proposed regularization loss .,"[('is', (2, 3)), ('with', (5, 6))]","[('performance', (1, 2)), ('further improved', (3, 5)), ('proposed regularization loss', (7, 10))]","[['performance', 'is', 'further improved'], ['further improved', 'with', 'proposed regularization loss']]",[],[],[],[],[],[],[],[],natural_language_inference,51,166
ablation-analysis,UP underperforms NUTM as it lacks dynamic programs .,"[('as', (3, 4))]","[('UP', (0, 1)), ('underperforms', (1, 2)), ('NUTM', (2, 3)), ('lacks', (5, 6)), ('dynamic programs', (6, 8))]","[['underperforms', 'as', 'lacks']]","[['UP', 'has', 'underperforms'], ['lacks', 'has', 'dynamic programs'], ['underperforms', 'has', 'NUTM']]",[],"[['Ablation analysis', 'has', 'UP']]",[],[],[],[],[],natural_language_inference,51,167
ablation-analysis,"The NTM with 2 heads shows slightly better convergence compared to the NTM , yet obviously underperforms NUTM ( p = 2 ) with 1 head and fewer parameters .",[],[],"[['NTM', 'with', '2 heads'], ['NTM', 'shows', 'underperforms'], ['NUTM ( p = 2 )', 'with', '1 head and fewer parameters'], ['NTM', 'shows', 'slightly better convergence'], ['slightly better convergence', 'compared to', 'NTM']]","[['underperforms', 'has', 'NUTM ( p = 2 )']]",[],"[['Ablation analysis', 'has', 'NTM']]",[],[],[],[],[],natural_language_inference,51,168
research-problem,Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,[],"[('Answer Justification', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Answer Justification']]",[],[],[],[],natural_language_inference,52,2
research-problem,"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .",[],"[('Developing interpretable machine learning ( ML ) models', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Developing interpretable machine learning ( ML ) models']]",[],[],[],[],natural_language_inference,52,11
approach,"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .","[('that', (8, 9)), ('to', (10, 11)), ('when', (17, 18)), ('is', (22, 23)), ('not', (30, 31))]","[('learns', (9, 10)), ('select and explain answers', (12, 16)), ('only supervision available', (19, 22)), ('which answer is correct', (24, 28)), ('how to explain it', (31, 35))]","[['learns', 'to', 'select and explain answers'], ['learns', 'when', 'only supervision available'], ['only supervision available', 'is', 'which answer is correct'], ['which answer is correct', 'not', 'how to explain it']]",[],"[['Approach', 'that', 'learns']]",[],[],[],[],[],[],natural_language_inference,52,32
approach,"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .","[('chooses', (4, 5)), ('that provide', (7, 9)), ('towards', (12, 13)), ('than', (18, 19))]","[('justifications', (6, 7)), ('most help', (10, 12)), ('ranking', (13, 14)), ('correct answers', (15, 17)), ('higher', (17, 18)), ('incorrect ones', (19, 21))]","[['justifications', 'that provide', 'most help'], ['most help', 'towards', 'ranking'], ['higher', 'than', 'incorrect ones']]","[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","[['Approach', 'chooses', 'justifications']]",[],[],[],[],[],[],natural_language_inference,52,33
approach,"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .","[('formally', (1, 2)), ('using', (9, 10)), ('with', (13, 14)), ('to choose', (17, 19)), ('for', (23, 24)), ('optimizing', (28, 29)), ('given', (33, 34))]","[('our neural network approach', (3, 7)), ('alternates', (7, 8)), ('current model', (11, 13)), ('max - pooling', (14, 17)), ('highest scoring justifications', (20, 23)), ('correct answers', (24, 26)), ('answer ranking model', (30, 33)), ('justifications', (35, 36))]","[['alternates', 'using', 'current model'], ['current model', 'with', 'max - pooling'], ['current model', 'to choose', 'highest scoring justifications'], ['highest scoring justifications', 'for', 'correct answers'], ['alternates', 'optimizing', 'answer ranking model'], ['answer ranking model', 'given', 'justifications']]","[['our neural network approach', 'has', 'alternates']]","[['Approach', 'formally', 'our neural network approach']]",[],[],[],[],[],[],natural_language_inference,52,34
baselines,IR Baseline :,[],"[('IR Baseline', (0, 2))]",[],[],[],"[['Baselines', 'has', 'IR Baseline']]",[],[],[],[],[],natural_language_inference,52,148
baselines,"For this baseline , we rank answer candidates by the maximum tf .idf document retrieval score using an unboosted query of question and answer terms ( see Section 4.1 for retrieval details ) .","[('rank', (5, 6)), ('by', (8, 9)), ('using', (16, 17)), ('of', (20, 21))]","[('answer candidates', (6, 8)), ('maximum tf .idf document retrieval score', (10, 16)), ('unboosted query', (18, 20)), ('question and answer terms', (21, 25))]","[['answer candidates', 'using', 'unboosted query'], ['unboosted query', 'of', 'question and answer terms'], ['answer candidates', 'by', 'maximum tf .idf document retrieval score']]",[],[],[],[],"[['IR Baseline', 'rank', 'answer candidates']]",[],[],[],natural_language_inference,52,149
baselines,IR ++ :,[],"[('IR ++', (0, 2))]",[],[],[],"[['Baselines', 'has', 'IR ++']]",[],[],[],[],[],natural_language_inference,52,150
baselines,"This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .","[('uses', (2, 3)), ('as', (6, 7)), ('with', (18, 19))]","[('same architecture', (4, 6)), ('full model', (8, 10)), ('only the IR ++ feature group', (19, 25))]","[['same architecture', 'as', 'full model']]",[],[],[],[],"[['IR ++', 'with', 'only the IR ++ feature group'], ['IR ++', 'uses', 'same architecture']]",[],[],[],natural_language_inference,52,151
results,QA Performance,[],"[('QA Performance', (0, 2))]",[],[],[],"[['Results', 'has', 'QA Performance']]",[],[],[],[],"[['QA Performance', 'has', 'Our full model']]",natural_language_inference,52,171
results,"Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .",[],[],"[['Our full model', 'combines', 'IR ++ , lexical overlap , discourse , and embeddings - based features'], ['P@1', 'of', '53.3 %'], ['absolute gain', 'over', 'strong IR baseline'], ['absolute gain', 'of', '6.3 %']]","[['IR ++ , lexical overlap , discourse , and embeddings - based features', 'has', 'P@1'], ['53.3 %', 'has', 'absolute gain']]",[],[],[],[],[],[],[],natural_language_inference,52,177
results,also tackle the AI2 Kaggle question set with an approach that learns alignments between questions and structured and semistructured KB data .,"[('tackle', (1, 2))]","[('AI2 Kaggle question set', (3, 7))]",[],[],[],[],[],"[['QA Performance', 'tackle', 'AI2 Kaggle question set']]",[],[],[],natural_language_inference,52,183
results,"By way of a loose comparison ( since we are evaluating on different data partitions ) , our model has approximately 5 % higher performance despite our simpler set of features and unstructured KB .","[('By way of', (0, 3))]","[('loose comparison', (4, 6)), ('our model', (17, 19)), ('approximately 5 % higher performance', (20, 25))]",[],"[['loose comparison', 'has', 'our model'], ['our model', 'has', 'approximately 5 % higher performance']]",[],[],[],"[['AI2 Kaggle question set', 'By way of', 'loose comparison']]",[],[],[],natural_language_inference,52,185
results,"In comparison to other systems that competed in the Kaggle challenge , our system comes in in 7th place out of 170 competitors ( top 4 % ) .","[('In comparison to', (0, 3)), ('that', (5, 6)), ('in', (7, 8)), ('comes in', (14, 16)), ('out of', (19, 21))]","[('other systems', (3, 5)), ('competed', (6, 7)), ('Kaggle challenge', (9, 11)), ('our system', (12, 14)), ('7th place', (17, 19)), ('170 competitors', (21, 23))]","[['other systems', 'that', 'competed'], ['competed', 'in', 'Kaggle challenge'], ['our system', 'comes in', '7th place'], ['7th place', 'out of', '170 competitors']]","[['Kaggle challenge', 'has', 'our system']]",[],[],[],"[['QA Performance', 'In comparison to', 'other systems']]",[],[],[],natural_language_inference,52,192
results,Justification Performance,[],"[('Justification Performance', (0, 2))]",[],[],[],"[['Results', 'has', 'Justification Performance']]",[],[],[],[],"[['Justification Performance', 'has', '61 %']]",natural_language_inference,52,197
results,"Note that 61 % of the top - ranked justifications from our system were rated as Good as compared to 52 % from the IR baseline ( a gain of 9 % ) , despite the systems using identical corpora .",[],[],"[['61 %', 'of', 'top - ranked justifications'], ['top - ranked justifications', 'from', 'our system'], ['top - ranked justifications', 'rated as', 'Good'], ['Good', 'compared to', '52 %'], ['52 %', 'from', 'IR baseline']]",[],[],[],[],[],[],[],[],natural_language_inference,52,228
research-problem,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,[],"[('Supervised Learning of Universal Sentence Representations', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Supervised Learning of Universal Sentence Representations']]",[],[],[],[],natural_language_inference,53,2
approach,"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .","[('study', (5, 6)), ('of', (8, 9)), ('trained on', (23, 25)), ('subsequently transferred to', (29, 32))]","[('task', (7, 8)), ('learning', (9, 10)), ('universal representations of sentences', (10, 14)), ('sentence encoder model', (18, 21)), ('large corpus', (26, 28)), ('other tasks', (32, 34))]","[['task', 'of', 'learning'], ['sentence encoder model', 'subsequently transferred to', 'other tasks'], ['sentence encoder model', 'trained on', 'large corpus']]","[['learning', 'has', 'universal representations of sentences']]","[['Approach', 'study', 'task']]","[['Approach', 'has', 'sentence encoder model']]",[],[],[],[],[],natural_language_inference,53,15
approach,"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .","[('investigate', (3, 4)), ('can be', (7, 9))]","[('supervised learning', (5, 7)), ('leveraged', (9, 10))]","[['supervised learning', 'can be', 'leveraged']]",[],"[['Approach', 'investigate', 'supervised learning']]",[],[],[],[],[],[],natural_language_inference,53,18
approach,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .","[('of', (6, 7)), ('on', (11, 12)), ('compare', (16, 17))]","[('impact', (5, 6)), ('sentence encoding architecture', (8, 11)), ('representational transferability', (12, 14)), ('convolutional , recurrent and even simpler word composition schemes', (17, 26))]","[['impact', 'of', 'sentence encoding architecture'], ['sentence encoding architecture', 'compare', 'convolutional , recurrent and even simpler word composition schemes'], ['sentence encoding architecture', 'on', 'representational transferability']]",[],[],"[['Approach', 'investigate', 'impact']]",[],[],[],[],[],natural_language_inference,53,22
hyperparameters,"For all our models trained on SNLI , we use SGD with a learning rate of 0.1 and a weight decay of 0.99 .",[],[],"[['models', 'use', 'SGD'], ['SGD', 'with', 'learning rate'], ['learning rate', 'of', '0.1'], ['SGD', 'with', 'weight decay'], ['weight decay', 'of', '0.99'], ['models', 'trained on', 'SNLI']]",[],"[['Hyperparameters', 'For', 'models']]",[],[],[],[],[],[],natural_language_inference,53,88
hyperparameters,"At each epoch , we divide the learning rate by 5 if the dev accuracy decreases .","[('At', (0, 1)), ('divide', (5, 6)), ('by', (9, 10)), ('if', (11, 12))]","[('each epoch', (1, 3)), ('learning rate', (7, 9)), ('5', (10, 11)), ('dev accuracy', (13, 15)), ('decreases', (15, 16))]","[['each epoch', 'divide', 'learning rate'], ['learning rate', 'by', '5'], ['learning rate', 'if', 'dev accuracy']]","[['dev accuracy', 'has', 'decreases']]","[['Hyperparameters', 'At', 'each epoch']]",[],[],[],[],[],[],natural_language_inference,53,89
hyperparameters,We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,"[('use', (1, 2)), ('of size', (3, 5)), ('is', (8, 9)), ('when', (10, 11)), ('goes under', (14, 16))]","[('minibatches', (2, 3)), ('64', (5, 6)), ('training', (7, 8)), ('stopped', (9, 10)), ('learning rate', (12, 14)), ('threshold of 10 ?5', (17, 21))]","[['minibatches', 'of size', '64'], ['training', 'is', 'stopped'], ['stopped', 'when', 'learning rate'], ['learning rate', 'goes under', 'threshold of 10 ?5']]","[['minibatches', 'has', 'training']]","[['Hyperparameters', 'use', 'minibatches']]",[],[],[],[],[],[],natural_language_inference,53,90
hyperparameters,"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .","[('use', (5, 6)), ('with', (11, 12)), ('of', (16, 17))]","[('classifier', (2, 3)), ('multi - layer perceptron', (7, 11)), ('1 hidden - layer', (12, 16)), ('512 hidden units', (17, 20))]","[['classifier', 'use', 'multi - layer perceptron'], ['multi - layer perceptron', 'with', '1 hidden - layer'], ['1 hidden - layer', 'of', '512 hidden units']]",[],[],"[['Hyperparameters', 'For', 'classifier']]",[],[],[],[],[],natural_language_inference,53,91
hyperparameters,We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,"[('trained on', (5, 7)), ('with', (10, 11)), ('as', (13, 14))]","[('opensource GloVe vectors', (2, 5)), ('Common Crawl 840B', (7, 10)), ('300 dimensions', (11, 13)), ('fixed word embeddings', (14, 17))]","[['opensource GloVe vectors', 'trained on', 'Common Crawl 840B'], ['Common Crawl 840B', 'with', '300 dimensions'], ['Common Crawl 840B', 'as', 'fixed word embeddings']]",[],[],"[['Hyperparameters', 'use', 'opensource GloVe vectors']]",[],[],[],[],[],natural_language_inference,53,92
results,Architecture impact,[],"[('Architecture impact', (0, 2))]",[],[],[],"[['Results', 'has', 'Architecture impact']]",[],[],[],[],"[['Architecture impact', 'has', 'BiLSTM - 4096']]",natural_language_inference,53,141
results,The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .,"[('with', (4, 5)), ('performs', (10, 11)), ('on', (12, 13))]","[('BiLSTM - 4096', (1, 4)), ('max - pooling operation', (6, 10)), ('best', (11, 12)), ('SNLI and transfer tasks', (14, 18))]","[['BiLSTM - 4096', 'with', 'max - pooling operation'], ['BiLSTM - 4096', 'performs', 'best'], ['best', 'on', 'SNLI and transfer tasks']]",[],[],[],[],[],[],[],[],natural_language_inference,53,144
results,"Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .","[('Looking at', (0, 2)), ('performs', (12, 13)), ('than', (15, 16))]","[('micro and macro averages', (3, 7)), ('significantly better', (13, 15)), ('other models', (17, 19)), ('LSTM', (19, 20)), ('GRU', (21, 22)), ('BiGRU - last', (23, 26)), ('BiLSTM - Mean', (27, 30)), ('inner-attention', (31, 32))]","[['micro and macro averages', 'performs', 'significantly better'], ['significantly better', 'than', 'other models']]","[['other models', 'name', 'LSTM'], ['other models', 'name', 'GRU'], ['other models', 'name', 'BiGRU - last'], ['other models', 'name', 'BiLSTM - Mean'], ['other models', 'name', 'inner-attention']]",[],[],[],"[['BiLSTM - 4096', 'Looking at', 'micro and macro averages']]",[],[],"[['other models', 'name', 'hierarchical - ConvNet']]",natural_language_inference,53,145
results,"For a given model , the transfer quality is also sensitive to the optimization algorithm : when training with Adam instead of SGD , we observed that the BiLSTM - max converged faster on SNLI ( 5 epochs instead of 10 ) , but obtained worse results on the transfer tasks , most likely because of the model and classifier 's increased capability to over-specialize on the training task .",[],[],"[['transfer quality', 'sensitive to', 'optimization algorithm'], ['transfer quality', 'when training with', 'Adam'], ['Adam', 'instead of', 'SGD'], ['Adam', 'observed', 'BiLSTM - max'], ['BiLSTM - max', 'converged', 'faster'], ['faster', 'on', 'SNLI'], ['BiLSTM - max', 'obtained', 'worse results'], ['worse results', 'on', 'transfer tasks']]",[],[],[],[],[],[],"[['Architecture impact', 'has', 'transfer quality']]",[],natural_language_inference,53,150
results,Embedding size,[],"[('Embedding size', (0, 2))]",[],[],[],"[['Results', 'has', 'Embedding size']]",[],[],[],[],"[['Embedding size', 'has', 'increased embedding sizes']]",natural_language_inference,53,152
results,"Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .","[('lead to', (24, 26)), ('for', (28, 29))]","[('increased embedding sizes', (21, 24)), ('increased performance', (26, 28)), ('almost all models', (29, 32))]","[['increased embedding sizes', 'lead to', 'increased performance'], ['increased performance', 'for', 'almost all models']]",[],[],[],[],[],[],[],[],natural_language_inference,53,153
results,Comparison with SkipThought,[],"[('Comparison with SkipThought', (0, 3))]",[],[],[],"[['Results', 'has', 'Comparison with SkipThought']]",[],[],[],[],[],natural_language_inference,53,170
results,"With much less data ( 570 k compared to 64M sentences ) but with high - quality supervision from the SNLI dataset , we are able to consistently outperform the results obtained by SkipThought vectors .","[('With', (0, 1)), ('with', (13, 14)), ('from', (18, 19)), ('able to', (25, 27)), ('obtained by', (31, 33))]","[('much less data ( 570 k compared to 64M sentences )', (1, 12)), ('high - quality supervision', (14, 18)), ('SNLI dataset', (20, 22)), ('consistently outperform', (27, 29)), ('results', (30, 31)), ('SkipThought vectors', (33, 35))]","[['much less data ( 570 k compared to 64M sentences )', 'with', 'high - quality supervision'], ['high - quality supervision', 'from', 'SNLI dataset'], ['much less data ( 570 k compared to 64M sentences )', 'able to', 'consistently outperform'], ['results', 'obtained by', 'SkipThought vectors']]","[['consistently outperform', 'has', 'results']]",[],[],[],"[['Comparison with SkipThought', 'With', 'much less data ( 570 k compared to 64M sentences )']]",[],[],[],natural_language_inference,53,172
results,"Our BiLSTM - max trained on SNLI performs much better than released SkipThought vectors on MR , CR , MPQA , SST , MRPC - accuracy , SICK - R , SICK - E and STS14 ( see ) .","[('trained on', (4, 6)), ('performs', (7, 8)), ('than', (10, 11)), ('on', (14, 15))]","[('Our BiLSTM - max', (0, 4)), ('SNLI', (6, 7)), ('much better', (8, 10)), ('released SkipThought vectors', (11, 14)), ('MR', (15, 16)), ('CR', (17, 18)), ('MPQA', (19, 20)), ('SST', (21, 22)), ('MRPC - accuracy', (23, 26)), ('SICK - R', (27, 30)), ('SICK - E', (31, 34))]","[['Our BiLSTM - max', 'performs', 'much better'], ['much better', 'than', 'released SkipThought vectors'], ['released SkipThought vectors', 'on', 'MR'], ['released SkipThought vectors', 'on', 'CR'], ['released SkipThought vectors', 'on', 'MPQA'], ['released SkipThought vectors', 'on', 'SST'], ['released SkipThought vectors', 'on', 'MRPC - accuracy'], ['released SkipThought vectors', 'on', 'SICK - R'], ['released SkipThought vectors', 'on', 'SICK - E'], ['Our BiLSTM - max', 'trained on', 'SNLI']]",[],[],[],[],[],"[['released SkipThought vectors', 'on', 'STS']]","[['Comparison with SkipThought', 'has', 'Our BiLSTM - max']]",[],natural_language_inference,53,174
results,"Except for the SUBJ dataset , it also performs better than SkipThought - LN on MR , CR and MPQA .","[('Except for', (0, 2)), ('performs', (8, 9)), ('than', (10, 11)), ('on', (14, 15))]","[('SUBJ dataset', (3, 5)), ('better', (9, 10)), ('SkipThought', (11, 12)), ('MR', (15, 16)), ('CR', (17, 18)), ('MPQA', (19, 20))]","[['better', 'than', 'SkipThought'], ['SkipThought', 'Except for', 'SUBJ dataset'], ['SkipThought', 'on', 'MR'], ['SkipThought', 'on', 'CR'], ['SkipThought', 'on', 'MPQA']]",[],[],[],[],"[['Comparison with SkipThought', 'performs', 'better']]",[],[],[],natural_language_inference,53,175
results,We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space ( pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST - LN ) .,"[('observe', (2, 3)), ('looking at', (4, 6)), ('in', (13, 14)), ('is', (17, 18)), ('than', (22, 23))]","[('STS14 results', (7, 9)), ('cosine metrics', (11, 13)), ('our embedding space', (14, 17)), ('more semantically informative', (19, 22)), ('SkipThought embedding space', (24, 27))]","[['STS14 results', 'observe', 'cosine metrics'], ['cosine metrics', 'in', 'our embedding space'], ['cosine metrics', 'is', 'more semantically informative'], ['more semantically informative', 'than', 'SkipThought embedding space']]",[],[],[],[],"[['Comparison with SkipThought', 'looking at', 'STS14 results']]",[],[],[],natural_language_inference,53,176
results,NLI as a supervised training set,[],"[('NLI as a supervised training set', (0, 6))]",[],[],[],"[['Results', 'has', 'NLI as a supervised training set']]",[],[],[],[],[],natural_language_inference,53,178
results,"Our findings indicate that our model trained on SNLI obtains much better over all results than models trained on other supervised tasks such as COCO , dictionary definitions , NMT , PPDB and SST .",[],[],"[['our model', 'obtains', 'much better over all results'], ['much better over all results', 'than', 'models'], ['models', 'trained on', 'other supervised tasks'], ['other supervised tasks', 'such as', 'COCO'], ['other supervised tasks', 'such as', 'dictionary definitions'], ['other supervised tasks', 'such as', 'NMT'], ['other supervised tasks', 'such as', 'PPDB'], ['other supervised tasks', 'such as', 'SST'], ['our model', 'trained on', 'SNLI']]",[],[],[],[],"[['NLI as a supervised training set', 'indicate', 'our model']]",[],[],[],natural_language_inference,53,179
results,Domain adaptation on SICK tasks,[],"[('Domain adaptation on SICK tasks', (0, 5))]",[],[],[],"[['Results', 'has', 'Domain adaptation on SICK tasks']]",[],[],[],[],"[['Domain adaptation on SICK tasks', 'has', 'Our transfer learning approach']]",natural_language_inference,53,183
results,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,"[('obtains', (4, 5)), ('than', (7, 8))]","[('Our transfer learning approach', (0, 4)), ('better results', (5, 7)), ('previous state - of - the - art', (8, 16))]","[['Our transfer learning approach', 'obtains', 'better results'], ['better results', 'than', 'previous state - of - the - art']]",[],[],[],[],[],[],[],[],natural_language_inference,53,184
results,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .",[],[],"[['86.3 % test accuracy', 'while', 'previous best handengineered models'], ['previous best handengineered models', 'obtained', '84.5 %'], ['86.3 % test accuracy', 'on', 'SICK - E'], ['pearson score', 'of', '0.885'], ['0.885', 'on', 'SICK - R']]",[],[],[],[],"[['Domain adaptation on SICK tasks', 'obtain', '86.3 % test accuracy'], ['Domain adaptation on SICK tasks', 'obtain', 'pearson score']]",[],[],[],natural_language_inference,53,185
results,"We also significantly outperformed previous transfer learning approaches on SICK - E ( Bowman et al. , 2015 ) that used the parameters of an LSTM model trained on SNLI to fine - tune on SICK ( 80.8 % accuracy ) .","[('on', (8, 9)), ('that used', (19, 21)), ('of', (23, 24)), ('trained on', (27, 29)), ('to fine - tune on', (30, 35))]","[('significantly outperformed', (2, 4)), ('previous transfer learning approaches', (4, 8)), ('SICK - E', (9, 12)), ('parameters', (22, 23)), ('LSTM model', (25, 27)), ('SNLI', (29, 30)), ('SICK', (35, 36))]","[['previous transfer learning approaches', 'that used', 'parameters'], ['parameters', 'to fine - tune on', 'SICK'], ['parameters', 'of', 'LSTM model'], ['parameters', 'trained on', 'SNLI'], ['previous transfer learning approaches', 'on', 'SICK - E']]","[['significantly outperformed', 'has', 'previous transfer learning approaches']]",[],[],[],[],[],"[['Domain adaptation on SICK tasks', 'has', 'significantly outperformed']]",[],natural_language_inference,53,186
results,Image - caption retrieval results,[],"[('Image - caption retrieval results', (0, 5))]",[],[],[],"[['Results', 'has', 'Image - caption retrieval results']]",[],[],[],[],[],natural_language_inference,53,188
results,"When trained with ResNet features and 30 k more training data , the SkipThought vectors perform significantly better than the original setting , going from 33.8 to 37.9 for caption retrieval R@1 , and from 25.9 to 30.6 on image retrieval R@1 .","[('trained with', (1, 3)), ('perform', (15, 16)), ('than', (18, 19)), ('going from', (23, 25)), ('for', (28, 29)), ('from', (34, 35)), ('on', (38, 39))]","[('ResNet features and 30 k more training data', (3, 11)), ('SkipThought vectors', (13, 15)), ('significantly better', (16, 18)), ('original setting', (20, 22)), ('33.8 to 37.9', (25, 28)), ('caption retrieval R@1', (29, 32)), ('25.9 to 30.6', (35, 38)), ('image retrieval R@1', (39, 42))]","[['SkipThought vectors', 'going from', '33.8 to 37.9'], ['33.8 to 37.9', 'for', 'caption retrieval R@1'], ['SkipThought vectors', 'perform', 'significantly better'], ['significantly better', 'than', 'original setting'], ['SkipThought vectors', 'from', '25.9 to 30.6'], ['25.9 to 30.6', 'on', 'image retrieval R@1']]","[['ResNet features and 30 k more training data', 'has', 'SkipThought vectors']]",[],[],[],"[['Image - caption retrieval results', 'trained with', 'ResNet features and 30 k more training data']]",[],[],[],natural_language_inference,53,191
results,"Our approach pushes the results even further , from 37.9 to 42.4 on cap-tion retrieval , and 30.6 to 33.2 on image retrieval .",[],[],"[['Our approach', 'pushes', 'results'], ['results', 'from', '30.6'], ['30.6', 'to', '33.2'], ['33.2', 'on', 'image retrieval'], ['results', 'from', '37.9'], ['37.9', 'to', '42.4'], ['42.4', 'on', 'cap-tion retrieval']]","[['results', 'has', 'even further']]",[],[],[],[],[],"[['Image - caption retrieval results', 'has', 'Our approach']]",[],natural_language_inference,53,192
results,MultiGenre NLI,[],"[('MultiGenre NLI', (0, 2))]",[],[],[],"[['Results', 'has', 'MultiGenre NLI']]",[],[],[],[],[],natural_language_inference,53,195
results,We observe a significant boost in performance over all compared to the model trained only on SLNI .,"[('observe', (1, 2)), ('in', (5, 6)), ('compared to', (9, 11)), ('trained only on', (13, 16))]","[('significant boost', (3, 5)), ('performance over all', (6, 9)), ('model', (12, 13)), ('SLNI', (16, 17))]","[['significant boost', 'in', 'performance over all'], ['significant boost', 'compared to', 'model'], ['model', 'trained only on', 'SLNI']]",[],[],[],[],"[['MultiGenre NLI', 'observe', 'significant boost']]",[],[],[],natural_language_inference,53,199
results,"Our model even reaches AdaSent performance on CR , suggesting that having a larger coverage for the training task helps learn even better general representations .","[('reaches', (3, 4)), ('on', (6, 7))]","[('Our model', (0, 2)), ('AdaSent performance', (4, 6)), ('CR', (7, 8))]","[['Our model', 'reaches', 'AdaSent performance'], ['AdaSent performance', 'on', 'CR']]",[],[],[],[],[],[],"[['MultiGenre NLI', 'has', 'Our model']]",[],natural_language_inference,53,200
research-problem,Structural Embedding of Syntactic Trees for Machine Comprehension,[],"[('Machine Comprehension', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,54,2
research-problem,"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language .",[],"[('Reading comprehension', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading comprehension']]",[],[],[],[],natural_language_inference,54,9
model,"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .","[('propose', (5, 6)), ('encode', (15, 16)), ('structured by', (18, 20)), ('into', (25, 26)), ('for', (29, 30))]","[('Structural Embedding of Syntactic Trees ( SEST )', (6, 14)), ('syntactic information', (16, 18)), ('constituency tree and dependency tree', (20, 25)), ('neural attention models', (26, 29)), ('question answering task', (31, 34))]","[['Structural Embedding of Syntactic Trees ( SEST )', 'encode', 'syntactic information'], ['syntactic information', 'into', 'neural attention models'], ['neural attention models', 'for', 'question answering task'], ['syntactic information', 'structured by', 'constituency tree and dependency tree']]",[],"[['Model', 'propose', 'Structural Embedding of Syntactic Trees ( SEST )']]",[],[],[],[],[],[],natural_language_inference,54,26
experimental-setup,We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM .,"[('run', (1, 2)), ('on', (4, 5)), ('contains', (8, 9)), ('with', (14, 15))]","[('our experiments', (2, 4)), ('machine', (6, 7)), ('single GTX 1080 GPU', (10, 14)), ('8 GB VRAM', (15, 18))]","[['our experiments', 'on', 'machine'], ['machine', 'contains', 'single GTX 1080 GPU'], ['single GTX 1080 GPU', 'with', '8 GB VRAM']]",[],"[['Experimental setup', 'run', 'our experiments']]",[],[],[],[],[],[],natural_language_inference,54,124
experimental-setup,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model .","[('use', (7, 8)), ('with', (12, 13)), ('to serve', (18, 20)), ('of', (22, 23)), ('into', (25, 26))]","[('variable character embedding', (9, 12)), ('fixed pre-trained word embedding', (14, 18)), ('part', (21, 22)), ('input', (24, 25)), ('model', (27, 28))]","[['variable character embedding', 'with', 'fixed pre-trained word embedding'], ['variable character embedding', 'to serve', 'part'], ['part', 'of', 'input'], ['input', 'into', 'model']]",[],"[['Experimental setup', 'use', 'variable character embedding']]",[],[],[],[],[],[],natural_language_inference,54,126
experimental-setup,The character embedding is implemented using CNN with a one -dimensional layer consists of 100 units with a channel size of 5 .,[],[],"[['character embedding', 'with', 'one -dimensional layer'], ['one -dimensional layer', 'consists of', '100 units'], ['100 units', 'with', 'channel size'], ['channel size', 'of', '5'], ['character embedding', 'implemented using', 'CNN']]",[],[],"[['Experimental setup', 'has', 'character embedding']]",[],[],[],[],[],natural_language_inference,54,127
experimental-setup,It has an input depth of 8 .,"[('of', (5, 6))]","[('input depth', (3, 5)), ('8', (6, 7))]","[['input depth', 'of', '8']]",[],[],"[['Experimental setup', 'has', 'input depth']]",[],[],[],[],[],natural_language_inference,54,128
experimental-setup,The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence .,"[('of', (3, 4)), ('is', (5, 6))]","[('max length', (1, 3)), ('SQuAD', (4, 5)), ('16', (6, 7))]","[['max length', 'of', 'SQuAD'], ['SQuAD', 'is', '16']]",[],[],"[['Experimental setup', 'has', 'max length']]",[],[],[],[],[],natural_language_inference,54,129
experimental-setup,"The fixed word embedding has a dimension of 100 , which is provided by the GloVe data set .","[('of', (7, 8)), ('provided by', (12, 14))]","[('fixed word embedding', (1, 4)), ('dimension', (6, 7)), ('100', (8, 9)), ('GloVe data set', (15, 18))]","[['dimension', 'of', '100'], ['fixed word embedding', 'provided by', 'GloVe data set']]","[['fixed word embedding', 'has', 'dimension']]",[],"[['Experimental setup', 'has', 'fixed word embedding']]",[],[],[],[],[],natural_language_inference,54,130
experimental-setup,The POS model contains syntactic information with 39 different POS tags that serve as both input and output .,"[('contains', (3, 4)), ('with', (6, 7)), ('serve', (12, 13))]","[('POS model', (1, 3)), ('syntactic information', (4, 6)), ('39 different POS tags', (7, 11)), ('both input and output', (14, 18))]","[['POS model', 'contains', 'syntactic information'], ['syntactic information', 'with', '39 different POS tags'], ['syntactic information', 'serve', 'both input and output']]",[],[],"[['Experimental setup', 'has', 'POS model']]",[],[],[],[],[],natural_language_inference,54,133
experimental-setup,For SECT and SEDT the input of the model has a size of 8 with 30 units to be output .,[],[],"[['input', 'with', '30 units'], ['30 units', 'to be', 'output'], ['input', 'of', 'model'], ['size', 'of', '8']]","[['SECT and SEDT', 'has', 'input'], ['input', 'has', 'size']]","[['Experimental setup', 'For', 'SECT and SEDT']]",[],[],[],[],[],[],natural_language_inference,54,134
experimental-setup,"Both of them has a maximum length size that is set to be 10 and 20 respectively , which values will be further discussed in Section 4.5 .","[('set to be', (10, 13))]","[('maximum length size', (5, 8)), ('10 and 20', (13, 16))]","[['maximum length size', 'set to be', '10 and 20']]",[],[],[],[],[],[],"[['SECT and SEDT', 'has', 'maximum length size']]",[],natural_language_inference,54,135
experiments,Predictive Performance,[],"[('Predictive Performance', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Predictive Performance']]","[['Predictive Performance', 'has', 'Baselines']]",natural_language_inference,54,139
experiments,"We first compared the performance of single models between the baseline approach BiDAF and the proposed SEST approaches , including SE - POS , SECT - LSTM , SECT - CNN , SEDT - LSTM , and SEDT - CNN , on the development dataset of SQuAD .","[('compared', (2, 3)), ('of', (5, 6)), ('including', (19, 20)), ('on', (41, 42))]","[('performance', (4, 5)), ('baseline approach BiDAF', (10, 13)), ('proposed SEST approaches', (15, 18)), ('SE - POS', (20, 23)), ('SECT - LSTM', (24, 27)), ('SECT - CNN', (28, 31)), ('SEDT - LSTM', (32, 35)), ('SEDT - CNN', (37, 40)), ('development dataset', (43, 45)), ('SQuAD', (46, 47))]","[['proposed SEST approaches', 'including', 'SE - POS'], ['proposed SEST approaches', 'including', 'SECT - LSTM'], ['proposed SEST approaches', 'including', 'SECT - CNN'], ['proposed SEST approaches', 'including', 'SEDT - LSTM'], ['proposed SEST approaches', 'including', 'SEDT - CNN'], ['performance', 'on', 'development dataset'], ['development dataset', 'of', 'SQuAD']]","[['performance', 'has', 'baseline approach BiDAF'], ['performance', 'has', 'proposed SEST approaches']]",[],[],[],"[['Baselines', 'compared', 'performance']]",[],[],[],natural_language_inference,54,140
experiments,"Another observation is that our propose models achieve higher relative improvements in EM scores than F 1 scores over the baseline methods , providing the evidence that syntactic information can accurately locate the boundaries of the answer .","[('achieve', (7, 8)), ('in', (11, 12)), ('than', (14, 15)), ('over', (18, 19))]","[('our propose models', (4, 7)), ('higher relative improvements', (8, 11)), ('EM scores', (12, 14)), ('F 1 scores', (15, 18)), ('baseline methods', (20, 22))]","[['our propose models', 'achieve', 'higher relative improvements'], ['higher relative improvements', 'in', 'EM scores'], ['EM scores', 'over', 'baseline methods'], ['EM scores', 'than', 'F 1 scores']]",[],[],[],[],[],[],"[['Results', 'has', 'our propose models']]",[],natural_language_inference,54,145
experiments,"Moreover , we found that both SECT - LSTM and SEDT - LSTM have better performance than their CNN counterparts , which suggests that LSTM can more effectively preserve the syntactic information .","[('found that', (3, 5)), ('have', (13, 14)), ('than', (16, 17))]","[('SECT - LSTM and SEDT - LSTM', (6, 13)), ('better performance', (14, 16)), ('CNN counterparts', (18, 20))]","[['SECT - LSTM and SEDT - LSTM', 'have', 'better performance'], ['better performance', 'than', 'CNN counterparts']]",[],[],[],[],"[['Results', 'found that', 'SECT - LSTM and SEDT - LSTM']]",[],[],[],natural_language_inference,54,146
experiments,Contribution of Syntactic Sequence,[],"[('Contribution of Syntactic Sequence', (0, 4))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Contribution of Syntactic Sequence']]","[['Contribution of Syntactic Sequence', 'has', 'Results']]",natural_language_inference,54,152
experiments,From the table we see that both the ordering and the contents of the syntactic tree are important for the models to work properly : constituency and dependency trees achieved over 20 % boost on performance compared to the randomly generated ones and our proposed ordering also out - performed the random ordering .,"[('both', (6, 7)), ('of', (12, 13)), ('are', (16, 17)), ('for', (18, 19)), ('to work', (21, 23)), ('achieved', (29, 30)), ('on', (34, 35)), ('compared to', (36, 38)), ('out - performed', (47, 50))]","[('ordering', (8, 9)), ('contents', (11, 12)), ('syntactic tree', (14, 16)), ('important', (17, 18)), ('models', (20, 21)), ('properly', (23, 24)), ('constituency and dependency trees', (25, 29)), ('over 20 % boost', (30, 34)), ('performance', (35, 36)), ('randomly generated ones', (39, 42)), ('our proposed ordering', (43, 46)), ('random ordering', (51, 53))]","[['important', 'for', 'models'], ['models', 'to work', 'properly'], ['important', 'both', 'ordering'], ['important', 'both', 'contents'], ['contents', 'of', 'syntactic tree'], ['constituency and dependency trees', 'achieved', 'over 20 % boost'], ['our proposed ordering', 'out - performed', 'random ordering'], ['over 20 % boost', 'compared to', 'randomly generated ones'], ['over 20 % boost', 'on', 'performance']]","[['over 20 % boost', 'has', 'our proposed ordering']]",[],[],[],"[['Results', 'are', 'important']]",[],"[['Results', 'has', 'constituency and dependency trees']]",[],natural_language_inference,54,160
experiments,It also worth mentioning that the ordering of dependency trees seems to have less impact on the performance compared to that of the constituency trees .,"[('of', (7, 8)), ('seems to have', (10, 13)), ('on', (15, 16)), ('compared to', (18, 20))]","[('ordering', (6, 7)), ('dependency trees', (8, 10)), ('less impact', (13, 15)), ('performance', (17, 18)), ('constituency trees', (23, 25))]","[['ordering', 'of', 'dependency trees'], ['dependency trees', 'seems to have', 'less impact'], ['less impact', 'compared to', 'constituency trees'], ['less impact', 'on', 'performance']]",[],[],[],[],[],[],"[['Results', 'has', 'ordering']]",[],natural_language_inference,54,161
experiments,Window Size Analysis,[],"[('Window Size Analysis', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Window Size Analysis']]","[['Window Size Analysis', 'has', 'Results']]",natural_language_inference,54,164
experiments,"In practice , we found that limiting the window size also benefits the performance of our models .","[('limiting', (6, 7)), ('of', (14, 15))]","[('window size', (8, 10)), ('benefits', (11, 12)), ('performance', (13, 14)), ('our models', (15, 17))]","[['performance', 'of', 'our models']]","[['window size', 'has', 'benefits'], ['benefits', 'has', 'performance']]",[],[],[],"[['Results', 'limiting', 'window size']]",[],[],[],natural_language_inference,54,166
experiments,In general the results illustrate that performances of the models increase with the length of the window .,[],[],"[['performances', 'of', 'models'], ['increase', 'with', 'length'], ['length', 'of', 'window']]","[['performances', 'has', 'increase']]",[],[],[],"[['Results', 'illustrate', 'performances']]",[],[],[],natural_language_inference,54,169
experiments,We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10 .,"[('observed', (2, 3)), ('does not generate', (7, 10)), ('as good as', (14, 17)), ('set to', (22, 24))]","[('larger window size', (4, 7)), ('predictive results', (10, 12)), ('one with window size', (18, 22)), ('10', (24, 25))]","[['larger window size', 'does not generate', 'predictive results'], ['predictive results', 'as good as', 'one with window size'], ['one with window size', 'set to', '10']]",[],[],[],[],"[['Results', 'observed', 'larger window size']]",[],[],[],natural_language_inference,54,171
research-problem,Question Answering with Subgraph Embeddings,[],"[('Question Answering', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering']]",[],[],[],[],natural_language_inference,55,2
research-problem,This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,[],"[('answer questions on a broad range of topics', (8, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'answer questions on a broad range of topics']]",[],[],[],[],natural_language_inference,55,4
research-problem,Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .,[],"[('automatically answer questions asked in natural language on any topic or in any domain', (4, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'automatically answer questions asked in natural language on any topic or in any domain']]",[],[],[],[],natural_language_inference,55,8
research-problem,"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language .",[],"[('open - domain question answering ( or open QA )', (18, 28))]",[],[],[],[],"[['Contribution', 'has research problem', 'open - domain question answering ( or open QA )']]",[],[],[],[],natural_language_inference,55,9
research-problem,"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format .",[],"[('open QA', (15, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'open QA']]",[],[],[],[],natural_language_inference,55,10
model,"In this paper , we improve the model of by providing the ability to answer more complicated questions .","[('providing', (10, 11)), ('to answer', (13, 15))]","[('ability', (12, 13)), ('more complicated questions', (15, 18))]","[['ability', 'to answer', 'more complicated questions']]",[],"[['Model', 'providing', 'ability']]",[],[],[],[],[],[],natural_language_inference,55,23
model,s The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB .,[],[],"[['richer representation', 'of', 'answers'], ['richer representation', 'encodes', 'question - answer path'], ['richer representation', 'encodes', 'surrounding subgraph'], ['surrounding subgraph', 'of', 'KB'], ['more sophisticated inference procedure', 'is', 'efficient'], ['more sophisticated inference procedure', 'consider', 'longer paths']]",[],[],"[['Model', 'has', 'richer representation'], ['Model', 'has', 'more sophisticated inference procedure']]",[],[],[],[],[],natural_language_inference,55,24
results,Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers that are directly connected to their inluded entity ( not in C 1 ) .,"[('Replacing', (0, 1)), ('by', (3, 4)), ('induces', (6, 7)), ('in', (10, 11))]","[('C 2', (1, 3)), ('C 1', (4, 6)), ('large drop', (8, 10)), ('performance', (11, 12))]","[['C 2', 'induces', 'large drop'], ['large drop', 'in', 'performance'], ['C 2', 'by', 'C 1']]",[],"[['Results', 'Replacing', 'C 2']]",[],[],[],[],[],[],natural_language_inference,55,136
results,"However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference .","[('using', (2, 3)), ('as', (8, 9)), ('is', (12, 13))]","[('all 2 - hops connections', (3, 8)), ('candidate set', (10, 12)), ('detrimental', (14, 15))]","[['all 2 - hops connections', 'as', 'candidate set'], ['all 2 - hops connections', 'is', 'detrimental']]",[],"[['Results', 'using', 'all 2 - hops connections']]",[],[],[],[],[],[],natural_language_inference,55,137
results,"Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information .","[('verify', (3, 4)), ('that', (10, 11)), ('for', (14, 15)), ('store', (23, 24))]","[('hypothesis', (5, 6)), ('richer representation', (12, 14)), ('answers', (15, 16)), ('more pertinent information', (24, 27))]","[['hypothesis', 'that', 'richer representation'], ['richer representation', 'for', 'answers'], ['richer representation', 'store', 'more pertinent information']]",[],"[['Results', 'verify', 'hypothesis']]",[],[],[],[],[],[],natural_language_inference,55,138
results,"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .","[('demonstrate', (3, 4)), ('upon', (8, 9)), ('corresponds to', (15, 17)), ('with', (19, 20)), ('as', (26, 27))]","[('greatly improve', (6, 8)), ('model', (10, 11)), ('setting', (18, 19)), ('Path representation', (21, 23)), ('C 1', (24, 26)), ('candidate set', (27, 29))]","[['greatly improve', 'corresponds to', 'setting'], ['setting', 'with', 'Path representation'], ['setting', 'with', 'C 1'], ['C 1', 'as', 'candidate set'], ['greatly improve', 'upon', 'model']]",[],"[['Results', 'demonstrate', 'greatly improve']]",[],[],[],[],[],[],natural_language_inference,55,139
results,"The ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design .",[],"[('ensemble', (1, 2)), ('improves', (2, 3)), ('state - of - the - art', (4, 11))]",[],"[['ensemble', 'has', 'improves'], ['improves', 'has', 'state - of - the - art']]",[],"[['Results', 'has', 'ensemble']]",[],[],[],[],[],natural_language_inference,55,145
research-problem,Recurrent Relational Networks,[],"[('Recurrent Relational Networks', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Recurrent Relational Networks']]",[],[],[],[],natural_language_inference,56,2
research-problem,"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects .",[],"[('recurrent relational network', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'recurrent relational network']]",[],[],[],[],natural_language_inference,56,5
model,"Toward generally realizing the ability to methodically reason about objects and their interactions over many steps , this paper introduces a composite function , the recurrent relational network .","[('Toward generally realizing', (0, 3)), ('to methodically reason about', (5, 9)), ('over', (13, 14)), ('introduces', (19, 20))]","[('ability', (4, 5)), ('objects', (9, 10)), ('interactions', (12, 13)), ('many steps', (14, 16)), ('composite function', (21, 23)), ('recurrent relational network', (25, 28))]","[['composite function', 'Toward generally realizing', 'ability'], ['ability', 'to methodically reason about', 'objects'], ['ability', 'to methodically reason about', 'interactions'], ['interactions', 'over', 'many steps']]","[['composite function', 'name', 'recurrent relational network']]","[['Model', 'introduces', 'composite function']]",[],[],[],[],[],[],natural_language_inference,56,25
model,It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,"[('serves', (1, 2)), ('for', (6, 7)), ('in', (12, 13))]","[('modular component', (4, 6)), ('many - step relational reasoning', (7, 12)), ('end - to - end differentiable learning systems', (13, 21))]","[['modular component', 'for', 'many - step relational reasoning'], ['many - step relational reasoning', 'in', 'end - to - end differentiable learning systems']]",[],[],[],[],"[['composite function', 'serves', 'modular component']]",[],[],[],natural_language_inference,56,26
model,"It encodes the inductive biases that 1 ) objects exists in the world 2 ) they can be sufficiently described by properties 3 ) properties can changeover time 4 ) objects can affect each other and 5 ) given the properties , the effects object have on each other is invariant to time .",[],[],"[['inductive biases', 'that', 'objects'], ['objects', 'exists in', 'world'], ['objects', 'affect', 'each other'], ['each other', 'invariant to', 'time'], ['objects', 'described by', 'properties'], ['properties', 'changeover', 'time']]",[],[],[],[],"[['composite function', 'encodes', 'inductive biases']]",[],[],[],natural_language_inference,56,27
model,"An important insight from the work of is to decompose a function for relational reasoning into two components or "" modules "" :","[('decompose', (9, 10)), ('for', (12, 13)), ('into', (15, 16))]","[('function', (11, 12)), ('relational reasoning', (13, 15)), ('two components', (16, 18))]","[['function', 'into', 'two components'], ['function', 'for', 'relational reasoning']]",[],"[['Model', 'decompose', 'function']]",[],[],[],[],[],"[['two components', 'name', 'perceptual front - end'], ['two components', 'name', 'relational reasoning module']]",natural_language_inference,56,28
model,"a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .","[('to recognize', (9, 11)), ('in', (12, 13)), ('represent them as', (17, 20)), ('uses', (29, 30))]","[('perceptual front - end', (1, 5)), ('objects', (11, 12)), ('raw input', (14, 16)), ('vectors', (20, 21)), ('relational reasoning module', (24, 27)), ('representation', (31, 32))]","[['perceptual front - end', 'to recognize', 'objects'], ['objects', 'in', 'raw input'], ['perceptual front - end', 'represent them as', 'vectors'], ['relational reasoning module', 'uses', 'representation']]",[],[],[],[],[],[],[],"[['representation', 'to reason about', 'objects and their interactions']]",natural_language_inference,56,29
model,Both modules are trained jointly end - to - end .,"[('trained', (3, 4))]","[('jointly end - to - end', (4, 10))]",[],[],[],[],[],"[['two components', 'trained', 'jointly end - to - end']]",[],[],[],natural_language_inference,56,30
model,"In computer science parlance , the relational reasoning module implements an interface : it operates on a graph of nodes and directed edges , where the nodes are represented by real valued vectors , and is differentiable .","[('implements', (9, 10)), ('operates on', (14, 16)), ('of', (18, 19)), ('where', (24, 25)), ('represented by', (28, 30)), ('is', (35, 36))]","[('relational reasoning module', (6, 9)), ('interface', (11, 12)), ('graph', (17, 18)), ('nodes and directed edges', (19, 23)), ('nodes', (26, 27)), ('real valued vectors', (30, 33)), ('differentiable', (36, 37))]","[['relational reasoning module', 'implements', 'interface'], ['interface', 'operates on', 'graph'], ['graph', 'of', 'nodes and directed edges'], ['graph', 'where', 'nodes'], ['nodes', 'represented by', 'real valued vectors'], ['nodes', 'is', 'differentiable']]",[],[],"[['Model', 'has', 'relational reasoning module']]",[],[],[],[],[],natural_language_inference,56,31
research-problem,Recurrent Relational Networks,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,56,42
code,Code to reproduce all experiments can be found at github.com/rasmusbergpalm/recurrent-relationalnetworks. designed as a set of prerequisite tasks for reasoning .,[],"[('github.com/rasmusbergpalm/recurrent-relationalnetworks.', (9, 10))]",[],[],[],[],"[['Contribution', 'Code', 'github.com/rasmusbergpalm/recurrent-relationalnetworks.']]",[],[],[],[],natural_language_inference,56,81
experiments,bAbI question - answering tasks,[],"[('bAbI question - answering tasks', (0, 5))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'bAbI question - answering tasks']]","[['bAbI question - answering tasks', 'has', 'Results']]",natural_language_inference,56,90
experiments,"Surprisingly , we find that we only need a single step of relational reasoning to solve all the bAbI tasks .","[('need', (7, 8)), ('to solve', (14, 16))]","[('single step of relational reasoning', (9, 14)), ('all the bAbI tasks', (16, 20))]","[['single step of relational reasoning', 'to solve', 'all the bAbI tasks']]",[],[],[],[],"[['Results', 'need', 'single step of relational reasoning']]",[],[],[],natural_language_inference,56,104
experiments,"Regardless , it appears multiple steps of relational reasoning are not important for the bAbI dataset .","[('appears', (3, 4)), ('not', (10, 11)), ('for', (12, 13))]","[('multiple steps of relational reasoning', (4, 9)), ('important', (11, 12)), ('bAbI dataset', (14, 16))]","[['multiple steps of relational reasoning', 'not', 'important'], ['important', 'for', 'bAbI dataset']]",[],[],[],[],"[['Results', 'appears', 'multiple steps of relational reasoning']]",[],[],[],natural_language_inference,56,108
experiments,Pretty - CLEVR,[],"[('Pretty - CLEVR', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Pretty - CLEVR']]","[['Pretty - CLEVR', 'has', 'Results']]",natural_language_inference,56,109
experiments,"Mirroring the results from the "" Sort - of - CLEVR "" dataset the MLP perfectly solves the non-relational questions , but struggle with even single jump questions and seem to lower bound the performance of the relational networks .","[('with', (23, 24))]","[('MLP', (14, 15)), ('perfectly solves', (15, 17)), ('non-relational questions', (18, 20)), ('struggle', (22, 23)), ('single jump questions', (25, 28))]","[['struggle', 'with', 'single jump questions']]","[['MLP', 'has', 'perfectly solves'], ['perfectly solves', 'has', 'non-relational questions'], ['MLP', 'has', 'struggle']]",[],[],[],[],[],"[['Results', 'has', 'MLP']]",[],natural_language_inference,56,140
experiments,"The relational network solves the non-relational questions as well as the ones requiring a single jump , but the accuracy sharply drops off with more jumps .","[('solves', (3, 4)), ('as well as', (7, 10)), ('requiring', (12, 13)), ('with', (23, 24))]","[('relational network', (1, 3)), ('non-relational questions', (5, 7)), ('ones', (11, 12)), ('single jump', (14, 16)), ('accuracy', (19, 20)), ('sharply drops off', (20, 23)), ('more jumps', (24, 26))]","[['relational network', 'solves', 'non-relational questions'], ['sharply drops off', 'with', 'more jumps'], ['relational network', 'as well as', 'ones'], ['ones', 'requiring', 'single jump']]","[['relational network', 'has', 'accuracy'], ['accuracy', 'has', 'sharply drops off']]",[],[],[],[],[],"[['Results', 'has', 'relational network']]",[],natural_language_inference,56,141
experiments,Sudoku,[],"[('Sudoku', (0, 1))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Sudoku']]","[['Sudoku', 'has', 'Results']]",natural_language_inference,56,145
experiments,Our network learns to solve 94.1 % of even the hardest 17 - givens Sudokus after 32 steps .,"[('learns to', (2, 4)), ('of', (7, 8)), ('after', (15, 16))]","[('Our network', (0, 2)), ('solve', (4, 5)), ('94.1 %', (5, 7)), ('even the hardest 17 - givens Sudokus', (8, 15)), ('32 steps', (16, 18))]","[['Our network', 'learns to', 'solve'], ['94.1 %', 'of', 'even the hardest 17 - givens Sudokus'], ['94.1 %', 'after', '32 steps']]","[['solve', 'has', '94.1 %']]",[],[],[],[],[],"[['Results', 'has', 'Our network']]",[],natural_language_inference,56,153
experiments,"See figure 4 . We can see that even simple Sudokus with 33 givens require upwards of 10 steps of relational reasoning , whereas the harder 17 givens continue to improve even after 32 steps .","[('see that', (6, 8)), ('with', (11, 12)), ('require', (14, 15)), ('of', (19, 20)), ('continue to', (28, 30)), ('after', (32, 33))]","[('even simple Sudokus', (8, 11)), ('33 givens', (12, 14)), ('upwards of 10 steps', (15, 19)), ('relational reasoning', (20, 22)), ('harder', (25, 26)), ('17 givens', (26, 28)), ('improve', (30, 31)), ('32 steps', (33, 35))]","[['harder', 'continue to', 'improve'], ['improve', 'after', '32 steps'], ['even simple Sudokus', 'with', '33 givens'], ['even simple Sudokus', 'require', 'upwards of 10 steps'], ['upwards of 10 steps', 'of', 'relational reasoning']]","[['harder', 'has', '17 givens']]",[],[],[],"[['Results', 'see that', 'harder'], ['Results', 'see that', 'even simple Sudokus']]",[],[],[],natural_language_inference,56,159
experiments,At 64 steps the accuracy for the 17 givens puzzles increases to 96.6 % .,"[('At', (0, 1)), ('for', (5, 6)), ('to', (11, 12))]","[('64 steps', (1, 3)), ('accuracy', (4, 5)), ('17 givens puzzles', (7, 10)), ('increases', (10, 11)), ('96.6 %', (12, 14))]","[['accuracy', 'for', '17 givens puzzles'], ['increases', 'to', '96.6 %']]","[['64 steps', 'has', 'accuracy'], ['accuracy', 'has', 'increases']]",[],[],[],"[['Results', 'At', '64 steps']]",[],[],[],natural_language_inference,56,162
experiments,"Our network outperforms loopy belief propagation , with parallel and random messages passing updates .","[('with', (7, 8)), ('passing', (12, 13))]","[('outperforms', (2, 3)), ('loopy belief propagation', (3, 6)), ('parallel and random messages', (8, 12)), ('updates', (13, 14))]","[['loopy belief propagation', 'with', 'parallel and random messages'], ['parallel and random messages', 'passing', 'updates']]","[['outperforms', 'has', 'loopy belief propagation']]",[],[],[],[],[],"[['Our network', 'has', 'outperforms']]",[],natural_language_inference,56,172
experiments,"It also outperforms a version of loopy belief propagation modified specifically for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .","[('of', (5, 6)), ('for solving', (11, 13)), ('that uses', (14, 16))]","[('version', (4, 5)), ('loopy belief propagation', (6, 9)), ('modified specifically', (9, 11)), ('Sudokus', (13, 14)), ('250 steps', (16, 18))]","[['version', 'of', 'loopy belief propagation'], ['modified specifically', 'for solving', 'Sudokus'], ['Sudokus', 'that uses', '250 steps']]","[['loopy belief propagation', 'has', 'modified specifically']]",[],[],[],[],[],"[['outperforms', 'has', 'version']]",[],natural_language_inference,56,173
experiments,"Finally we outperform Park which treats the Sudoku as a 9x9 image , uses 10 convolutional layers , iteratively picks the most probable digit , and evaluate on easier Sudokus with 24 - 36 givens .","[('Park', (3, 4)), ('treats', (5, 6)), ('as', (8, 9))]","[('outperform', (2, 3)), ('Sudoku', (7, 8)), ('9x9 image', (10, 12)), ('10 convolutional layers', (14, 17))]","[['Sudoku', 'as', '9x9 image'], ['outperform', 'Park', '10 convolutional layers']]",[],[],[],[],"[['has', 'treats', 'Sudoku']]","[['outperform', 'Park', 'has']]","[['Results', 'has', 'outperform']]",[],natural_language_inference,56,176
research-problem,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,[],"[('ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE']]",[],[],[],[],natural_language_inference,57,2
research-problem,"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .",[],"[('single visual - semantic hierarchy over words , sentences , and images', (16, 28))]",[],[],[],[],"[['Contribution', 'has research problem', 'single visual - semantic hierarchy over words , sentences , and images']]",[],[],[],[],natural_language_inference,57,4
research-problem,"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .",[],"[('visualsemantic hierarchy', (30, 32))]",[],[],[],[],"[['Contribution', 'has research problem', 'visualsemantic hierarchy']]",[],[],[],[],natural_language_inference,57,15
model,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,57,26
model,We call embeddings learned in this way order- embeddings .,"[('call', (1, 2)), ('learned', (3, 4))]","[('embeddings', (2, 3)), ('order- embeddings', (7, 9))]","[['embeddings', 'learned', 'order- embeddings']]",[],[],[],[],"[['mapping', 'call', 'embeddings']]",[],[],[],natural_language_inference,57,27
experiments,HYPERNYM PREDICTION,[],"[('HYPERNYM PREDICTION', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'HYPERNYM PREDICTION']]","[['HYPERNYM PREDICTION', 'has', 'Hyperparameters']]",natural_language_inference,57,77
experiments,"To train the model , we use the standard pairwise ranking objective from Eq. ( 5 ) .","[('use', (6, 7))]","[('standard pairwise ranking objective', (8, 12))]",[],[],[],[],[],"[['Hyperparameters', 'use', 'standard pairwise ranking objective']]",[],[],[],natural_language_inference,57,96
experiments,"We sample minibatches of 128 random image - caption pairs , and draw all contrastive terms from the minibatch , giving us 127 contrastive images for each caption and captions for each image .",[],[],"[['contrastive terms', 'from', 'minibatch'], ['contrastive terms', 'giving', '127 contrastive images'], ['127 contrastive images', 'for', 'each caption'], ['contrastive terms', 'giving', 'captions'], ['captions', 'for', 'each image'], ['minibatches', 'of', '128 random image - caption pairs']]",[],[],[],[],"[['Hyperparameters', 'draw', 'contrastive terms'], ['Hyperparameters', 'sample', 'minibatches']]",[],[],[],natural_language_inference,57,97
experiments,"We train for 15 - 30 epochs using the Adam optimizer with learning rate 0.001 , and early stopping on the validation set .","[('train for', (1, 3)), ('using', (7, 8)), ('with', (11, 12)), ('on', (19, 20))]","[('15 - 30 epochs', (3, 7)), ('Adam optimizer', (9, 11)), ('learning rate 0.001', (12, 15)), ('early stopping', (17, 19)), ('validation set', (21, 23))]","[['15 - 30 epochs', 'with', 'learning rate 0.001'], ['15 - 30 epochs', 'with', 'early stopping'], ['early stopping', 'on', 'validation set'], ['15 - 30 epochs', 'using', 'Adam optimizer'], ['early stopping', 'on', 'validation set']]",[],[],[],[],"[['Hyperparameters', 'train for', '15 - 30 epochs']]",[],[],[],natural_language_inference,57,98
experiments,"We set the dimension of the embedding space and the GRU hidden state N to 1024 , the dimension of the learned word embeddings to 300 , and the margin ? to 0.05 .",[],[],"[['dimension', 'of', 'margin'], ['margin', 'to', '0.05'], ['dimension', 'of', 'embedding space and the GRU hidden state N'], ['embedding space and the GRU hidden state N', 'to', '1024'], ['dimension', 'of', 'learned word embeddings'], ['learned word embeddings', 'to', '300']]",[],[],[],[],"[['Hyperparameters', 'set', 'dimension'], ['Hyperparameters', 'set', 'dimension']]",[],[],[],natural_language_inference,57,99
experiments,"For consistency with and to mitigate overfitting , we constrain the caption and image embeddings to have unit L2 norm .","[('constrain', (9, 10)), ('to have', (15, 17))]","[('caption and image embeddings', (11, 15)), ('unit L2 norm', (17, 20))]","[['caption and image embeddings', 'to have', 'unit L2 norm']]",[],[],[],[],"[['Hyperparameters', 'constrain', 'caption and image embeddings']]",[],[],[],natural_language_inference,57,101
experiments,We see that order- embeddings outperform the skipthought baseline despite not using external text corpora .,"[('see that', (1, 3)), ('not using', (10, 12))]","[('order- embeddings', (3, 5)), ('outperform', (5, 6)), ('skipthought baseline', (7, 9)), ('external text corpora', (12, 15))]","[['outperform', 'not using', 'external text corpora']]","[['order- embeddings', 'has', 'outperform'], ['outperform', 'has', 'skipthought baseline']]",[],[],[],"[['Results', 'see that', 'order- embeddings']]",[],[],[],natural_language_inference,57,115
experiments,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,[],"[('TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE', (0, 6))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE']]","[['TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE', 'has', 'Hyperparameters']]",natural_language_inference,57,144
experiments,"Just as for caption - image ranking , we set the dimensions of the embedding space and GRU hidden state to be 1024 , the dimension of the word embeddings to be 300 , and constrain the embeddings to have unit L2 norm .",[],[],"[['dimension', 'of', 'word embeddings'], ['word embeddings', 'to be', '300'], ['dimensions', 'of', 'embedding space and GRU hidden state'], ['embedding space and GRU hidden state', 'to be', '1024'], ['embeddings', 'to have', 'unit L2 norm']]",[],[],[],[],"[['Hyperparameters', 'set', 'dimensions'], ['Hyperparameters', 'constrain', 'embeddings']]",[],[],[],natural_language_inference,57,149
experiments,We train for 10 epochs with batches of 128 sentence pairs .,"[('train for', (1, 3)), ('with', (5, 6)), ('of', (7, 8))]","[('10 epochs', (3, 5)), ('batches', (6, 7)), ('128 sentence pairs', (8, 11))]","[['10 epochs', 'with', 'batches'], ['batches', 'of', '128 sentence pairs']]",[],[],[],[],"[['Hyperparameters', 'train for', '10 epochs']]",[],[],[],natural_language_inference,57,150
experiments,We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set .,[],[],"[['Adam optimizer', 'with', 'learning rate 0.001'], ['Adam optimizer', 'with', 'early stopping']]",[],[],[],[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],natural_language_inference,57,151
research-problem,ReasoNet : Learning to Stop Reading in Machine Comprehension,[],"[('Machine Comprehension', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,58,2
model,"With this motivation , we propose a novel neural network architecture called Reasoning Network ( ReasoNet ) .","[('propose', (5, 6)), ('called', (11, 12))]","[('novel neural network architecture', (7, 11)), ('Reasoning Network ( ReasoNet )', (12, 17))]","[['novel neural network architecture', 'called', 'Reasoning Network ( ReasoNet )']]",[],"[['Model', 'propose', 'novel neural network architecture']]",[],[],[],[],[],[],natural_language_inference,58,33
model,"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .",[],[],"[['question', 'in', 'mind'], ['ReasoNets', 'read', 'document'], ['document', 'focusing on', 'di erent parts'], ['di erent parts', 'of', 'document'], ['di erent parts', 'until', 'satisfying answer'], ['satisfying answer', 'is', 'found or formed']]","[['mind', 'has', 'ReasoNets'], ['document', 'has', 'repeatedly']]","[['Model', 'With', 'question']]",[],[],[],[],[],[],natural_language_inference,58,35
model,"Moreover , unlike previous approaches using xed number of hops or iterations , ReasoNets introduce a termination state in the inference .","[('introduce', (14, 15)), ('in', (18, 19))]","[('termination state', (16, 18)), ('inference', (20, 21))]","[['termination state', 'in', 'inference']]",[],[],[],[],"[['ReasoNets', 'introduce', 'termination state']]",[],[],[],natural_language_inference,58,37
model,"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .","[('proposing', (8, 9)), ('utilizes', (15, 16)), ('to successfully train', (23, 26))]","[('reinforcement learning approach', (10, 13)), ('instance - dependent reward baseline', (17, 22)), ('ReasoNets', (26, 27))]","[['ReasoNets', 'proposing', 'reinforcement learning approach'], ['reinforcement learning approach', 'utilizes', 'instance - dependent reward baseline'], ['instance - dependent reward baseline', 'to successfully train', 'ReasoNets']]",[],[],[],[],[],[],[],[],natural_language_inference,58,42
experiments,CNN and Daily Mail Datasets,[],"[('CNN and Daily Mail Datasets', (0, 5))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'CNN and Daily Mail Datasets']]","[['CNN and Daily Mail Datasets', 'has', 'Experimental setup']]",natural_language_inference,58,146
experiments,"Vocab Size : For training our ReasoNet , we keep the most frequent | V | = 101 k words ( not including 584 entities and 1 placeholder marker ) in the CNN dataset , and | V | = 151 k words ( not including 530 entities and 1 placeholder marker ) in the Daily Mail dataset .",[],[],"[['Vocab Size', 'keep', 'most frequent | V | = 101 k words'], ['most frequent | V | = 101 k words', 'in', 'CNN dataset'], ['Vocab Size', 'keep', '| V | = 151 k words'], ['| V | = 151 k words', 'in', 'Daily Mail dataset']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'Vocab Size'], ['Experimental setup', 'has', 'Vocab Size']]",[],natural_language_inference,58,149
experiments,Embedding Layer :,[],"[('Embedding Layer', (0, 2))]",[],[],[],[],[],[],[],"[['Experimental setup', 'has', 'Embedding Layer'], ['Experimental setup', 'has', 'Embedding Layer'], ['Experimental setup', 'has', 'Embedding Layer']]",[],natural_language_inference,58,150
experiments,"We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .","[('choose', (1, 2)), ('use', (9, 10)), ('for', (18, 19))]","[('300 - dimensional word embeddings', (2, 7)), ('300 - dimensional pretrained Glove word embeddings', (11, 18)), ('initialization', (19, 20))]","[['300 - dimensional pretrained Glove word embeddings', 'for', 'initialization']]",[],[],[],[],"[['Embedding Layer', 'use', '300 - dimensional pretrained Glove word embeddings'], ['Embedding Layer', 'choose', '300 - dimensional word embeddings']]",[],[],[],natural_language_inference,58,151
experiments,We also apply dropout with probability 0.2 to the embedding layer .,"[('apply', (2, 3)), ('with', (4, 5)), ('to', (7, 8))]","[('dropout', (3, 4)), ('probability 0.2', (5, 7)), ('embedding layer', (9, 11))]","[['dropout', 'with', 'probability 0.2'], ['probability 0.2', 'to', 'embedding layer']]",[],[],[],[],"[['Embedding Layer', 'apply', 'dropout']]",[],[],[],natural_language_inference,58,152
experiments,"We use ADAM optimizer for parameter optimization with an initial learning rate of 0.0005 , ? 1 = 0.9 and ? 2 = 0.999 ;","[('use', (1, 2)), ('for', (4, 5)), ('with', (7, 8)), ('of', (12, 13))]","[('ADAM optimizer', (2, 4)), ('parameter optimization', (5, 7)), ('initial learning rate', (9, 12)), ('0.0005 , ? 1 = 0.9 and ? 2 = 0.999', (13, 24))]","[['ADAM optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0005 , ? 1 = 0.9 and ? 2 = 0.999'], ['ADAM optimizer', 'for', 'parameter optimization']]",[],[],[],[],"[['Experimental setup', 'use', 'ADAM optimizer']]",[],[],[],natural_language_inference,58,173
experiments,The absolute value of gradient on each parameter is clipped within 0.001 .,"[('of', (3, 4)), ('on', (5, 6)), ('is', (8, 9)), ('within', (10, 11))]","[('absolute value', (1, 3)), ('gradient', (4, 5)), ('each parameter', (6, 8)), ('clipped', (9, 10)), ('0.001', (11, 12))]","[['absolute value', 'of', 'gradient'], ['gradient', 'is', 'clipped'], ['clipped', 'within', '0.001'], ['gradient', 'on', 'each parameter']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'absolute value']]",[],natural_language_inference,58,174
experiments,The batch size is 64 for both CNN and Daily Mail datasets .,"[('is', (3, 4)), ('for', (5, 6))]","[('batch size', (1, 3)), ('64', (4, 5)), ('both CNN and Daily Mail datasets', (6, 12))]","[['batch size', 'is', '64'], ['64', 'for', 'both CNN and Daily Mail datasets']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'batch size']]",[],natural_language_inference,58,175
experiments,Models are trained on GTX TitanX 12 GB .,"[('trained on', (2, 4))]","[('GTX TitanX 12 GB', (4, 8))]",[],[],[],[],[],"[['Experimental setup', 'trained on', 'GTX TitanX 12 GB']]",[],[],[],natural_language_inference,58,179
experiments,"Comparing with the AS Reader , ReasoNet shows the signi cant improvement by capturing multi-turn reasoning in the paragraph .","[('Comparing with', (0, 2)), ('shows', (7, 8)), ('by capturing', (12, 14)), ('in', (16, 17))]","[('AS Reader', (3, 5)), ('ReasoNet', (6, 7)), ('signi cant improvement', (9, 12)), ('reasoning', (15, 16)), ('paragraph', (18, 19))]","[['ReasoNet', 'shows', 'signi cant improvement'], ['signi cant improvement', 'by capturing', 'reasoning'], ['reasoning', 'in', 'paragraph']]","[['AS Reader', 'has', 'ReasoNet']]",[],[],[],"[['Results', 'Comparing with', 'AS Reader']]",[],[],[],natural_language_inference,58,197
experiments,"Iterative Attention Reader , EpiReader and GA Reader are the three multi-turn reasoning models with xed reasoning steps .","[('are', (8, 9)), ('with', (14, 15))]","[('Iterative Attention Reader , EpiReader and GA Reader', (0, 8)), ('three multi-turn reasoning models', (10, 14)), ('xed reasoning steps', (15, 18))]","[['Iterative Attention Reader , EpiReader and GA Reader', 'are', 'three multi-turn reasoning models'], ['three multi-turn reasoning models', 'with', 'xed reasoning steps']]",[],[],[],[],[],[],"[['Results', 'has', 'Iterative Attention Reader , EpiReader and GA Reader']]","[['Iterative Attention Reader , EpiReader and GA Reader', 'has', 'outperforms']]",natural_language_inference,58,198
experiments,ReasoNet also outperforms all of them by integrating termination gate in the model which allows di erent reasoning steps for di erent test cases .,"[('by integrating', (6, 8)), ('in', (10, 11)), ('allows', (14, 15)), ('for', (19, 20))]","[('ReasoNet', (0, 1)), ('outperforms', (2, 3)), ('termination gate', (8, 10)), ('model', (12, 13)), ('di erent reasoning steps', (15, 19)), ('di erent test cases', (20, 24))]","[['outperforms', 'by integrating', 'termination gate'], ['termination gate', 'allows', 'di erent reasoning steps'], ['di erent reasoning steps', 'for', 'di erent test cases'], ['termination gate', 'in', 'model']]","[['outperforms', 'has', 'ReasoNet'], ['ReasoNet', 'has', 'outperforms']]",[],[],[],[],[],[],[],natural_language_inference,58,199
experiments,ReasoNet obtains comparable results with AoA Reader on CNN test set .,"[('obtains', (1, 2)), ('with', (4, 5)), ('on', (7, 8))]","[('ReasoNet', (0, 1)), ('comparable results', (2, 4)), ('AoA Reader', (5, 7)), ('CNN test set', (8, 11))]","[['ReasoNet', 'obtains', 'comparable results'], ['comparable results', 'with', 'AoA Reader'], ['comparable results', 'on', 'CNN test set']]",[],[],[],[],[],[],"[['Results', 'has', 'ReasoNet'], ['Results', 'has', 'ReasoNet']]",[],natural_language_inference,58,201
experiments,SQuAD Dataset,[],"[('SQuAD Dataset', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'SQuAD Dataset']]","[['SQuAD Dataset', 'has', 'Experimental setup']]",natural_language_inference,58,210
experiments,Vocab Size :,[],"[('Vocab Size', (0, 2))]",[],[],[],[],[],[],[],[],[],natural_language_inference,58,217
experiments,"We use the python NLTK tokenizer 6 to preprocess passages and questions , and obtain about 100K words in the vocabulary .","[('use', (1, 2)), ('to preprocess', (7, 9)), ('obtain about', (14, 16)), ('in', (18, 19))]","[('python NLTK tokenizer', (3, 6)), ('passages and questions', (9, 12)), ('100K words', (16, 18)), ('vocabulary', (20, 21))]","[['python NLTK tokenizer', 'to preprocess', 'passages and questions'], ['passages and questions', 'obtain about', '100K words'], ['100K words', 'in', 'vocabulary']]",[],[],[],[],"[['Vocab Size', 'use', 'python NLTK tokenizer']]",[],[],[],natural_language_inference,58,218
experiments,Embedding Layer : We use the 100 - dimensional pretrained Glove vectors as word embeddings .,"[('use', (4, 5)), ('as', (12, 13))]","[('Embedding Layer', (0, 2)), ('100 - dimensional pretrained Glove vectors', (6, 12)), ('word embeddings', (13, 15))]","[['Embedding Layer', 'use', '100 - dimensional pretrained Glove vectors'], ['100 - dimensional pretrained Glove vectors', 'as', 'word embeddings']]",[],[],[],[],[],[],[],[],natural_language_inference,58,219
experiments,The maximum reasoning step T max is set to 10 in SQuAD experiments .,"[('set to', (7, 9))]","[('maximum reasoning step T max', (1, 6)), ('10', (9, 10))]","[['maximum reasoning step T max', 'set to', '10']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'maximum reasoning step T max'], ['Experimental setup', 'has', 'maximum reasoning step T max']]",[],natural_language_inference,58,247
experiments,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size Results :,"[('use', (1, 2)), ('for', (4, 5)), ('with', (7, 8)), ('of', (12, 13))]","[('AdaDelta optimizer', (2, 4)), ('parameter optimization', (5, 7)), ('initial learning rate', (9, 12)), ('0.5', (13, 14))]","[['initial learning rate', 'of', '0.5'], ['AdaDelta optimizer', 'for', 'parameter optimization'], ['AdaDelta optimizer', 'for', 'parameter optimization'], ['parameter optimization', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.5']]",[],[],[],[],"[['Experimental setup', 'use', 'AdaDelta optimizer'], ['Experimental setup', 'use', 'AdaDelta optimizer']]",[],[],[],natural_language_inference,58,248
experiments,"In , we demonstrate that ReasoNet outperforms all existing published approaches .","[('demonstrate', (3, 4))]","[('ReasoNet', (5, 6)), ('outperforms', (6, 7)), ('all existing published approaches', (7, 11))]",[],"[['outperforms', 'has', 'all existing published approaches']]",[],[],[],"[['Results', 'demonstrate', 'ReasoNet']]",[],[],[],natural_language_inference,58,254
experiments,"While we compare ReasoNet with BiDAF , ReasoNet exceeds BiDAF both in single model and ensemble model cases .","[('compare', (2, 3)), ('exceeds', (8, 9)), ('both in', (10, 12))]","[('ReasoNet', (3, 4)), ('BiDAF', (5, 6)), ('single model and ensemble model cases', (12, 18))]","[['ReasoNet', 'exceeds', 'BiDAF'], ['ReasoNet', 'both in', 'single model and ensemble model cases']]",[],[],[],[],"[['Results', 'compare', 'ReasoNet']]",[],[],[],natural_language_inference,58,255
experiments,"In the bottom part of , we compare ReasoNet with all unpublished methods at the time of this submission , ReasoNet holds the second position in all the competing approaches in the SQuAD leaderboard .",[],[],"[['ReasoNet', 'holds', 'second position'], ['second position', 'in', 'all the competing approaches'], ['all the competing approaches', 'in', 'SQuAD leaderboard']]",[],[],[],[],[],[],[],[],natural_language_inference,58,257
experiments,Graph Reachability,[],"[('Graph Reachability', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Graph Reachability']]","[['Graph Reachability', 'has', 'Experimental setup']]",natural_language_inference,58,258
experiments,Embedding Layer,[],"[('Embedding Layer', (0, 2))]",[],[],[],[],[],[],[],[],[],natural_language_inference,58,283
experiments,We use a 100 - dimensional embedding vector for each symbol in the query and graph description .,"[('use', (1, 2)), ('for', (8, 9)), ('in', (11, 12))]","[('100 - dimensional embedding vector', (3, 8)), ('each symbol', (9, 11)), ('query and graph description', (13, 17))]","[['100 - dimensional embedding vector', 'for', 'each symbol'], ['each symbol', 'in', 'query and graph description']]",[],[],[],[],"[['Embedding Layer', 'use', '100 - dimensional embedding vector']]",[],[],[],natural_language_inference,58,284
experiments,"The maximum reasoning step T max is set to 15 and 25 for the small graph and large graph dataset , respectively .","[('set to', (7, 9)), ('for', (12, 13))]","[('maximum reasoning step T max', (1, 6)), ('15 and 25', (9, 12)), ('small graph and large graph dataset', (14, 20))]","[['maximum reasoning step T max', 'set to', '15 and 25'], ['15 and 25', 'for', 'small graph and large graph dataset']]",[],[],[],[],[],[],[],[],natural_language_inference,58,300
experiments,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size of 32 .,[],[],"[['AdaDelta optimizer', 'with', 'initial learning rate'], ['AdaDelta optimizer', 'with', 'batch size'], ['batch size', 'of', '32']]",[],[],[],[],[],[],[],[],natural_language_inference,58,301
experiments,"Deep LSTM Reader achieves 90.92 % and 71.55 % accuracy in the small and large graph dataset , respectively , which indicates the graph reachibility task is not trivial .","[('achieves', (3, 4)), ('in', (10, 11))]","[('Deep LSTM Reader', (0, 3)), ('90.92 % and 71.55 % accuracy', (4, 10)), ('small and large graph dataset', (12, 17))]","[['Deep LSTM Reader', 'achieves', '90.92 % and 71.55 % accuracy'], ['90.92 % and 71.55 % accuracy', 'in', 'small and large graph dataset']]",[],[],[],[],[],[],"[['Results', 'has', 'Deep LSTM Reader']]",[],natural_language_inference,58,307
research-problem,Neural Paraphrase Identification of Questions with Noisy Pretraining,[],"[('Neural Paraphrase Identification of Questions', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Paraphrase Identification of Questions']]",[],[],[],[],natural_language_inference,59,2
research-problem,We present a solution to the problem of paraphrase identification of questions .,[],"[('paraphrase identification of questions', (8, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'paraphrase identification of questions']]",[],[],[],[],natural_language_inference,59,4
research-problem,Question paraphrase identification is a widely useful NLP application .,[],"[('Question paraphrase identification', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question paraphrase identification']]",[],[],[],[],natural_language_inference,59,8
approach,"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .","[('examine', (1, 2)), ('shown promise in modeling', (15, 19))]","[('simple model family', (3, 6)), ('decomposable attention model', (8, 11)), ('natural language inference', (19, 22))]","[['decomposable attention model', 'shown promise in modeling', 'natural language inference']]","[['simple model family', 'has', 'decomposable attention model']]","[['Approach', 'examine', 'simple model family']]",[],[],[],[],[],[],natural_language_inference,59,14
approach,"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .",[],[],"[['data sparsity', 'modify', 'input representation'], ['input representation', 'of', 'decomposable attention model'], ['input representation', 'to use', 'sums'], ['sums', 'of', 'character n-gram embeddings'], ['character n-gram embeddings', 'instead of', 'word embeddings']]",[],"[['Approach', 'to mitigate', 'data sparsity']]",[],[],[],[],[],[],natural_language_inference,59,16
approach,"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .",[],[],"[['all our model parameters', 'on', 'noisy , automatically collected question - paraphrase corpus'], ['noisy , automatically collected question - paraphrase corpus', 'followed by', 'fine - tuning'], ['parameters', 'on', 'Quora dataset']]","[['fine - tuning', 'has', 'parameters'], ['noisy , automatically collected question - paraphrase corpus', 'name', 'Paralex']]","[['Approach', 'pretrain', 'all our model parameters']]",[],[],[],[],[],[],natural_language_inference,59,18
hyperparameters,"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .",[],[],"[['two layers', 'with', '400 and 200 width'], ['positive paraphrase', 'for', 'score ? 0.3'], ['0.1', 'for', 'pretraining and tuning'], ['0.1', 'for', 'tuning'], ['256', 'for', 'pretraining'], ['64', 'for', 'tuning'], ['grid search', 'on', 'development set']]","[['grid search', 'has', 'shape of all feedforward networks'], ['shape of all feedforward networks', 'has', 'two layers'], ['grid search', 'has', 'prediction threshold'], ['prediction threshold', 'has', 'positive paraphrase'], ['grid search', 'has', 'character n -gram sizes'], ['character n -gram sizes', 'has', '5'], ['grid search', 'has', 'learning rate'], ['learning rate', 'has', '0.1'], ['grid search', 'has', 'dropout ratio'], ['dropout ratio', 'has', '0.1'], ['grid search', 'has', 'context size'], ['context size', 'has', '1'], ['grid search', 'has', 'batch size'], ['batch size', 'has', '256'], ['batch size', 'has', '64'], ['grid search', 'has', 'embedding dimension'], ['embedding dimension', 'has', '300']]","[['Hyperparameters', 'by', 'grid search']]",[],[],[],[],[],[],natural_language_inference,59,99
results,"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .","[('observe', (1, 2)), ('than', (9, 10))]","[('simple FFNN baselines', (4, 7)), ('work better', (7, 9)), ('more complex Siamese and Multi - Perspective CNN or LSTM models', (10, 21))]","[['work better', 'than', 'more complex Siamese and Multi - Perspective CNN or LSTM models']]","[['simple FFNN baselines', 'has', 'work better']]","[['Results', 'observe', 'simple FFNN baselines']]",[],[],[],[],[],[],natural_language_inference,59,115
results,"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .","[('without', (7, 8)), ('is', (10, 11)), ('than', (12, 13)), ('used', (21, 22))]","[('Our basic decomposable attention model DECATT word', (0, 7)), ('pre-trained embeddings', (8, 10)), ('better', (11, 12)), ('most of the models', (13, 17)), ('GloVe embeddings', (22, 24))]","[['Our basic decomposable attention model DECATT word', 'is', 'better'], ['better', 'than', 'most of the models'], ['most of the models', 'used', 'GloVe embeddings'], ['Our basic decomposable attention model DECATT word', 'without', 'pre-trained embeddings']]",[],[],"[['Results', 'has', 'Our basic decomposable attention model DECATT word']]",[],[],[],[],[],natural_language_inference,59,116
results,An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,"[('without', (8, 9)), ('outperforms', (12, 13)), ('that uses', (17, 19))]","[('DECATT char model', (5, 8)), ('any pretrained embeddings', (9, 12)), ('DE - CATT glove', (13, 17)), ('task - agnostic GloVe embeddings', (19, 24))]","[['DECATT char model', 'without', 'any pretrained embeddings'], ['DECATT char model', 'outperforms', 'DE - CATT glove'], ['DE - CATT glove', 'that uses', 'task - agnostic GloVe embeddings']]",[],[],"[['Results', 'has', 'DECATT char model']]",[],[],[],[],[],natural_language_inference,59,117
results,"Furthermore , when character n-gram embeddings are pre-trained in a task - specific manner in DECATT paralex ? char model , we observe a significant boost in performance .",[],[],"[['character n-gram embeddings', 'are', 'pre-trained'], ['pre-trained', 'in', 'task - specific manner'], ['task - specific manner', 'in', 'DECATT paralex ? char model'], ['task - specific manner', 'observe', 'significant boost'], ['significant boost', 'in', 'performance']]",[],"[['Results', 'when', 'character n-gram embeddings']]",[],[],[],[],[],[],natural_language_inference,59,118
results,"Finally , we note that our best performing model is pt - DECATT char , which leverages the full power of character embeddings and pretraining the model on Paralex .","[('note', (3, 4)), ('is', (9, 10)), ('leverages', (16, 17)), ('of', (20, 21))]","[('our best performing model', (5, 9)), ('pt - DECATT char', (10, 14)), ('full power', (18, 20)), ('character embeddings', (21, 23))]","[['our best performing model', 'is', 'pt - DECATT char'], ['pt - DECATT char', 'leverages', 'full power'], ['full power', 'of', 'character embeddings']]",[],"[['Results', 'note', 'our best performing model']]",[],[],[],[],[],[],natural_language_inference,59,122
research-problem,Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond,[],"[('Massively Multilingual Sentence Embeddings', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Massively Multilingual Sentence Embeddings']]",[],[],[],[],natural_language_inference,6,2
research-problem,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .",[],"[('joint multilingual sentence representations', (6, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'joint multilingual sentence representations']]",[],[],[],[],natural_language_inference,6,4
code,"Our implementation , the pretrained encoder and the multilingual test set are available at https://github.com / facebookresearch/LASER . . 2018 .",[],"[('https://github.com / facebookresearch/LASER', (14, 17))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com / facebookresearch/LASER']]",[],[],[],[],natural_language_inference,6,9
model,"In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task .","[('interested in', (6, 8)), ('that is', (14, 16)), ('of', (19, 20)), ('that are', (21, 23)), ('with respect to', (24, 27))]","[('universal language agnostic sentence embeddings', (8, 13)), ('vector representations', (17, 19)), ('sentences', (20, 21)), ('general', (23, 24)), ('two dimensions', (27, 29)), ('input language', (31, 33)), ('NLP task', (35, 37))]","[['universal language agnostic sentence embeddings', 'that is', 'vector representations'], ['vector representations', 'of', 'sentences'], ['sentences', 'that are', 'general'], ['general', 'with respect to', 'two dimensions']]","[['two dimensions', 'name', 'input language'], ['two dimensions', 'name', 'NLP task']]","[['Model', 'interested in', 'universal language agnostic sentence embeddings']]",[],[],[],[],[],[],natural_language_inference,6,21
model,"To that end , we train a single encoder to handle multiple languages , so that semantically similar sentences in different languages are close in the embedding space .","[('train', (5, 6)), ('to handle', (9, 11))]","[('single encoder', (7, 9)), ('multiple languages', (11, 13))]","[['single encoder', 'to handle', 'multiple languages']]",[],"[['Model', 'train', 'single encoder']]",[],[],[],[],[],[],natural_language_inference,6,23
experiments,XNLI : cross - lingual NLI,[],"[('XNLI : cross - lingual NLI', (0, 6))]",[],[],[],"[['Experiments', 'has', 'XNLI : cross - lingual NLI']]",[],[],[],[],"[['XNLI : cross - lingual NLI', 'has', 'Our proposed method']]",natural_language_inference,6,95
experiments,9 Our proposed method obtains the best results in zero - shot cross - lingual transfer for all languages but Spanish .,"[('obtains', (4, 5)), ('in', (8, 9)), ('for', (16, 17))]","[('Our proposed method', (1, 4)), ('best results', (6, 8)), ('zero - shot cross - lingual transfer', (9, 16)), ('all languages but Spanish', (17, 21))]","[['Our proposed method', 'obtains', 'best results'], ['best results', 'in', 'zero - shot cross - lingual transfer'], ['zero - shot cross - lingual transfer', 'for', 'all languages but Spanish']]",[],[],[],[],[],[],[],[],natural_language_inference,6,106
experiments,"Moreover , our transfer results are strong and homogeneous across all languages :","[('are', (5, 6)), ('across', (9, 10))]","[('transfer results', (3, 5)), ('strong and homogeneous', (6, 9)), ('all languages', (10, 12))]","[['transfer results', 'are', 'strong and homogeneous'], ['strong and homogeneous', 'across', 'all languages']]",[],[],[],[],[],[],"[['XNLI : cross - lingual NLI', 'has', 'transfer results']]",[],natural_language_inference,6,107
experiments,"for 11 of them , the zero - short performance is ( at most ) 5 % lower than the one on English , including distant languages like Arabic , Chinese and Vietnamese , and we also achieve remarkable good results on low - resource languages like Swahili .",[],[],"[['remarkable good results', 'on', 'low - resource languages'], ['low - resource languages', 'like', 'Swahili'], ['zero - short performance', 'is', '( at most ) 5 % lower'], ['( at most ) 5 % lower', 'than', 'English'], ['English', 'including', 'distant languages'], ['distant languages', 'like', 'Arabic , Chinese and Vietnamese']]",[],[],[],[],"[['XNLI : cross - lingual NLI', 'achieve', 'remarkable good results']]",[],"[['XNLI : cross - lingual NLI', 'has', 'zero - short performance']]",[],natural_language_inference,6,110
experiments,"10 Finally , we also outperform all baselines of Conneau et al. by a substantial margin , with the additional advantage that we use a single pre-trained encoder , whereas X - BiLSTM learns a separate encoder for each language .","[('outperform', (5, 6)), ('by', (12, 13))]","[('all baselines', (6, 8)), ('substantial margin', (14, 16))]","[['all baselines', 'by', 'substantial margin']]",[],[],[],[],"[['XNLI : cross - lingual NLI', 'outperform', 'all baselines']]",[],[],[],natural_language_inference,6,113
experiments,MLDoc : cross - lingual classification,[],"[('MLDoc : cross - lingual classification', (0, 6))]",[],[],[],"[['Experiments', 'has', 'MLDoc : cross - lingual classification']]",[],[],[],[],"[['MLDoc : cross - lingual classification', 'has', 'our system']]",natural_language_inference,6,125
experiments,"As shown in , our system obtains the best published results for 5 of the 7 transfer languages .","[('obtains', (6, 7)), ('for', (11, 12))]","[('our system', (4, 6)), ('best published results', (8, 11)), ('5 of the 7 transfer languages', (12, 18))]","[['our system', 'obtains', 'best published results'], ['best published results', 'for', '5 of the 7 transfer languages']]",[],[],[],[],[],[],[],[],natural_language_inference,6,131
experiments,BUCC : bitext mining,[],"[('BUCC : bitext mining', (0, 4))]",[],[],[],"[['Experiments', 'has', 'BUCC : bitext mining']]",[],[],[],[],"[['BUCC : bitext mining', 'has', 'our system']]",natural_language_inference,6,133
experiments,"As shown in , our system establishes a new state - of - the - art for all language pairs with the exception of English - Chinese test .","[('establishes', (6, 7)), ('for', (16, 17)), ('with the exception of', (20, 24))]","[('our system', (4, 6)), ('new state - of - the - art', (8, 16)), ('all language pairs', (17, 20)), ('English - Chinese test', (24, 28))]","[['our system', 'establishes', 'new state - of - the - art'], ['new state - of - the - art', 'for', 'all language pairs'], ['new state - of - the - art', 'with the exception of', 'English - Chinese test']]",[],[],[],[],[],[],[],[],natural_language_inference,6,145
experiments,"We also outperform Artetxe and Schwenk ( 2018 ) themselves , who use two separate models covering 4 languages each .","[('outperform', (2, 3))]","[('Artetxe and Schwenk ( 2018 )', (3, 9))]",[],[],[],[],[],"[['BUCC : bitext mining', 'outperform', 'Artetxe and Schwenk ( 2018 )']]",[],[],[],natural_language_inference,6,146
experiments,Tatoeba : similarity search,[],"[('Tatoeba : similarity search', (0, 4))]",[],[],[],"[['Experiments', 'has', 'Tatoeba : similarity search']]",[],[],[],[],"[['Tatoeba : similarity search', 'are', '55'], ['Tatoeba : similarity search', 'are', '48 languages'], ['Tatoeba : similarity search', 'are', '15 languages'], ['Tatoeba : similarity search', 'has', 'similarity error rates']]",natural_language_inference,6,148
experiments,"Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .",[],[],[],"[['similarity error rates', 'has', 'below 5 %']]",[],[],[],[],[],[],[],natural_language_inference,6,156
experiments,"11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .","[('for', (5, 6)), ('with', (14, 15))]","[('37 languages', (6, 8)), ('48 languages', (12, 14)), ('error rate', (16, 18)), ('below 10 %', (18, 21)), ('55', (22, 23)), ('less than 20 %', (24, 28))]","[['55', 'with', 'less than 20 %'], ['48 languages', 'with', 'error rate']]","[['error rate', 'has', 'below 10 %']]",[],[],[],"[['below 5 %', 'for', '37 languages']]",[],[],[],natural_language_inference,6,157
experiments,There are only 15 languages with error rates above 50 % .,"[('with error rates', (5, 8))]","[('15 languages', (3, 5)), ('above 50 %', (8, 11))]","[['15 languages', 'with error rates', 'above 50 %']]",[],[],[],[],[],[],[],[],natural_language_inference,6,158
ablation-analysis,We were notable to achieve good convergence with deeper models .,"[('achieve', (4, 5)), ('with', (7, 8))]","[('good convergence', (5, 7)), ('deeper models', (8, 10))]","[['good convergence', 'with', 'deeper models']]",[],"[['Ablation analysis', 'achieve', 'good convergence']]",[],[],[],[],[],[],natural_language_inference,6,167
ablation-analysis,"It can be seen that all tasks benefit from deeper models , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages .","[('seen that', (3, 5)), ('benefit from', (7, 9))]","[('all tasks', (5, 7)), ('deeper models', (9, 11))]","[['all tasks', 'benefit from', 'deeper models']]",[],"[['Ablation analysis', 'seen that', 'all tasks']]",[],[],[],[],[],[],natural_language_inference,6,168
ablation-analysis,Multitask learning has been shown to be helpful to learn English sentence embeddings .,[],"[('Multitask learning', (0, 2))]",[],[],[],"[['Ablation analysis', 'has', 'Multitask learning']]",[],[],[],[],"[['Multitask learning', 'has', 'NLI objective']]",natural_language_inference,6,171
ablation-analysis,"As shown in , the NLI objective leads to a better performance on the English NLI test set , but this comes at the cost of a worse cross - lingual transfer performance in XNLI and Tatoeba .","[('in', (2, 3)), ('leads to', (7, 9)), ('on', (12, 13)), ('comes at', (21, 23)), ('of', (25, 26))]","[('NLI objective', (5, 7)), ('better performance', (10, 12)), ('English NLI test set', (14, 18)), ('cost', (24, 25)), ('worse cross - lingual transfer performance', (27, 33)), ('XNLI and Tatoeba', (34, 37))]","[['NLI objective', 'leads to', 'better performance'], ['better performance', 'comes at', 'cost'], ['cost', 'of', 'worse cross - lingual transfer performance'], ['worse cross - lingual transfer performance', 'in', 'XNLI and Tatoeba'], ['better performance', 'on', 'English NLI test set']]",[],[],[],[],[],[],[],[],natural_language_inference,6,173
research-problem,Dynamic Meta - Embeddings for Improved Sentence Representations,[],"[('Improved Sentence Representations', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Improved Sentence Representations']]",[],[],[],[],natural_language_inference,60,2
research-problem,"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .","[('explore', (5, 6)), ('of', (9, 10)), ('apply', (18, 19))]","[('supervised learning', (7, 9)), ('task - specific , dynamic meta-embeddings', (10, 16)), ('sentence representations', (22, 24))]","[['supervised learning', 'of', 'task - specific , dynamic meta-embeddings'], ['task - specific , dynamic meta-embeddings', 'apply', 'sentence representations']]",[],"[['Approach', 'explore', 'supervised learning']]",[],"[['Contribution', 'has research problem', 'sentence representations']]",[],[],[],[],natural_language_inference,60,18
approach,"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .","[('is', (3, 4))]","[('embedding - agnostic', (4, 7))]",[],[],"[['Approach', 'is', 'embedding - agnostic']]",[],[],[],[],[],[],natural_language_inference,60,22
research-problem,"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .",[],"[('natural language inference', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference']]",[],[],[],[],natural_language_inference,61,5
research-problem,Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .,[],"[('Natural language inference ( NLI )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,61,9
research-problem,"In the literature , the task of NLI is usually viewed as a relation classi cation .",[],"[('NLI', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,61,12
model,"erefore , in this study , using ESIM model as the baseline , we add an a ention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .","[('using', (6, 7)), ('as', (9, 10)), ('add', (14, 15)), ('behind', (19, 20)), ('use', (27, 28)), ('to jointly represent', (33, 36))]","[('ESIM model', (7, 9)), ('baseline', (11, 12)), ('a ention layer', (16, 19)), ('each Bi - LSTM layer', (20, 25)), ('adaptive orientation embedding layer', (29, 33)), ('forward and backward vectors', (37, 41))]","[['ESIM model', 'add', 'a ention layer'], ['a ention layer', 'behind', 'each Bi - LSTM layer'], ['ESIM model', 'as', 'baseline'], ['ESIM model', 'use', 'adaptive orientation embedding layer'], ['adaptive orientation embedding layer', 'to jointly represent', 'forward and backward vectors']]",[],"[['Model', 'using', 'ESIM model']]",[],[],[],[],[],[],natural_language_inference,61,51
model,"We name this a ention boosted Bi - LSTM as Bi - a LSTM , and denote the modi ed ESIM as aESIM .",[],[],"[['modi ed ESIM', 'as', 'aESIM'], ['a ention boosted Bi - LSTM', 'as', 'Bi - a LSTM']]",[],"[['Model', 'denote', 'modi ed ESIM']]","[['Model', 'name', 'a ention boosted Bi - LSTM']]",[],[],[],[],[],natural_language_inference,61,52
hyperparameters,We use the Adam method for optimization .,"[('use', (1, 2)), ('for', (5, 6))]","[('Adam method', (3, 5)), ('optimization', (6, 7))]","[['Adam method', 'for', 'optimization']]",[],"[['Hyperparameters', 'use', 'Adam method']]",[],[],[],[],[],[],natural_language_inference,61,121
hyperparameters,"e initial learning rate is set to 0.0005 , and the batch size is 128 .","[('is', (4, 5)), ('set to', (5, 7))]","[('initial learning rate', (1, 4)), ('0.0005', (7, 8)), ('batch size', (11, 13)), ('128', (14, 15))]","[['initial learning rate', 'set to', '0.0005'], ['batch size', 'is', '128']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate'], ['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,61,123
hyperparameters,e dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .,[],[],"[['dimensions', 'of', 'hidden states'], ['hidden states', 'of', 'Bi - aLSTM and word embedding'], ['Bi - aLSTM and word embedding', 'are', '300']]",[],[],"[['Hyperparameters', 'has', 'dimensions']]",[],[],[],[],[],natural_language_inference,61,124
hyperparameters,We employ non-linearity function f = selu replacing recti ed linear unit ReLU on account of its faster convergence rate .,"[('employ', (1, 2)), ('replacing', (7, 8)), ('on account of', (13, 16))]","[('non-linearity function f = selu', (2, 7)), ('linear unit ReLU', (10, 13)), ('faster convergence rate', (17, 20))]","[['non-linearity function f = selu', 'on account of', 'faster convergence rate'], ['non-linearity function f = selu', 'replacing', 'linear unit ReLU']]",[],"[['Hyperparameters', 'employ', 'non-linearity function f = selu']]",[],[],[],[],[],[],natural_language_inference,61,125
hyperparameters,Dropout rate is set to 0.2 during training .,"[('set to', (3, 5)), ('during', (6, 7))]","[('Dropout rate', (0, 2)), ('0.2', (5, 6)), ('training', (7, 8))]","[['Dropout rate', 'set to', '0.2'], ['0.2', 'during', 'training']]",[],[],"[['Hyperparameters', 'has', 'Dropout rate']]",[],[],[],[],[],natural_language_inference,61,126
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize word embeddings .,"[('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('word embeddings', (11, 13))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'word embeddings']]",[],[],"[['Hyperparameters', 'use', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],[],[],natural_language_inference,61,127
hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('are', (9, 10)), ('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('initialized randomly', (10, 12)), ('Gaussian samples', (13, 15))]","[['Out - of - vocabulary ( OOV ) words', 'are', 'initialized randomly'], ['initialized randomly', 'with', 'Gaussian samples']]",[],[],"[['Hyperparameters', 'has', 'Out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],natural_language_inference,61,128
results,"According to the results in , a ESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .",[],[],"[['ESIM model', 'achieved', '88.1 %'], ['elevating 0.8 percent', 'higher than', 'ESIM model'], ['88.1 %', 'on', 'SNLI corpus']]","[['88.1 %', 'has', 'elevating 0.8 percent']]",[],"[['Results', 'has', 'ESIM model']]",[],[],[],[],[],natural_language_inference,61,138
results,It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .,"[('promoted', (1, 2)), ('on', (10, 11))]","[('almost 0.5 percent accuracy', (2, 6)), ('outperformed', (7, 8)), ('baselines', (9, 10)), ('MultiNLI', (11, 12))]","[['baselines', 'on', 'MultiNLI']]","[['outperformed', 'has', 'baselines']]",[],[],[],"[['ESIM model', 'promoted', 'almost 0.5 percent accuracy']]",[],"[['ESIM model', 'has', 'outperformed']]",[],natural_language_inference,61,139
research-problem,Explicit Utilization of General Knowledge in Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,62,2
research-problem,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .",[],"[('Machine Reading Comprehension ( MRC )', (5, 11)), ('MRC', (44, 45))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension ( MRC )'], ['Contribution', 'has research problem', 'MRC']]",[],[],[],[],natural_language_inference,62,4
approach,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .","[('propose', (6, 7)), ('which uses', (12, 14)), ('to extract', (15, 17)), ('as', (20, 21)), ('from', (23, 24))]","[('data enrichment method', (8, 11)), ('WordNet', (14, 15)), ('inter-word semantic connections', (17, 20)), ('general knowledge', (21, 23)), ('each given passage - question pair', (24, 30))]","[['data enrichment method', 'to extract', 'inter-word semantic connections'], ['inter-word semantic connections', 'as', 'general knowledge'], ['general knowledge', 'from', 'each given passage - question pair'], ['data enrichment method', 'which uses', 'WordNet']]",[],"[['Approach', 'propose', 'data enrichment method']]",[],[],[],[],[],[],natural_language_inference,62,41
approach,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .","[('named', (15, 16)), ('explicitly uses', (25, 27)), ('to assist', (32, 34))]","[('end - to - end MRC model', (8, 15)), ('Knowledge Aided Reader ( KAR )', (17, 23)), ('extracted general knowledge', (29, 32)), ('attention mechanisms', (35, 37))]","[['end - to - end MRC model', 'named', 'Knowledge Aided Reader ( KAR )'], ['Knowledge Aided Reader ( KAR )', 'explicitly uses', 'extracted general knowledge'], ['extracted general knowledge', 'to assist', 'attention mechanisms']]",[],[],"[['Approach', 'propose', 'end - to - end MRC model']]",[],[],[],[],[],natural_language_inference,62,42
experimental-setup,"We tokenize the MRC dataset with spa Cy 2.0.13 , manipulate WordNet 3.0 with NLTK 3.3 , and implement KAR with TensorFlow 1.11.0 .",[],[],"[['KAR', 'with', 'TensorFlow 1.11.0'], ['MRC dataset', 'with', 'spa Cy 2.0.13'], ['WordNet 3.0', 'with', 'NLTK 3.3']]",[],"[['Experimental setup', 'implement', 'KAR'], ['Experimental setup', 'tokenize', 'MRC dataset'], ['Experimental setup', 'manipulate', 'WordNet 3.0']]",[],[],[],[],[],[],natural_language_inference,62,181
experimental-setup,"For the dense layers and the BiLSTMs , we set the dimensionality unit d to 600 .","[('For', (0, 1)), ('set', (9, 10)), ('to', (14, 15))]","[('dense layers and the BiLSTMs', (2, 7)), ('dimensionality unit d', (11, 14)), ('600', (15, 16))]","[['dense layers and the BiLSTMs', 'set', 'dimensionality unit d'], ['dimensionality unit d', 'to', '600']]",[],"[['Experimental setup', 'For', 'dense layers and the BiLSTMs']]",[],[],[],[],[],[],natural_language_inference,62,183
experimental-setup,"For model optimization , we apply the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.0005 and a minibatch size of 32 .",[],[],"[['model optimization', 'apply', 'Adam ( Kingma and Ba , 2014 ) optimizer'], ['Adam ( Kingma and Ba , 2014 ) optimizer', 'with', 'minibatch size'], ['minibatch size', 'of', '32'], ['Adam ( Kingma and Ba , 2014 ) optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.0005']]",[],[],"[['Experimental setup', 'For', 'model optimization']]",[],[],[],[],[],natural_language_inference,62,184
experimental-setup,"To avoid overfitting , we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0.3 .","[('To avoid', (0, 2)), ('apply', (5, 6)), ('to', (7, 8)), ('with', (14, 15)), ('of', (18, 19))]","[('overfitting', (2, 3)), ('dropout', (6, 7)), ('dense layers', (9, 11)), ('BiLSTMs', (13, 14)), ('dropout rate', (16, 18)), ('0.3', (19, 20))]","[['overfitting', 'apply', 'dropout'], ['dropout', 'with', 'dropout rate'], ['dropout rate', 'of', '0.3'], ['dropout', 'to', 'dense layers'], ['dropout', 'to', 'BiLSTMs']]",[],"[['Experimental setup', 'To avoid', 'overfitting']]",[],[],[],[],[],[],natural_language_inference,62,186
experimental-setup,"To boost the performance , we apply exponential moving average with a decay rate of 0.999 .","[('To boost', (0, 2)), ('apply', (6, 7)), ('with', (10, 11)), ('of', (14, 15))]","[('performance', (3, 4)), ('exponential moving average', (7, 10)), ('decay rate', (12, 14)), ('0.999', (15, 16))]","[['performance', 'apply', 'exponential moving average'], ['exponential moving average', 'with', 'decay rate'], ['decay rate', 'of', '0.999']]",[],"[['Experimental setup', 'To boost', 'performance']]",[],[],[],[],[],[],natural_language_inference,62,187
ablation-analysis,"Then we conduct an ablation study by replacing the knowledge aided attention mechanisms with the mutual attention proposed by and the self attention proposed by separately , and find that the F 1 score of KAR drops by 4.2 on the development set , 7.8 on AddSent , and 9.1 on AddOneSent .",[],[],"[['knowledge aided attention mechanisms', 'with', 'mutual attention'], ['knowledge aided attention mechanisms', 'with', 'self attention'], ['knowledge aided attention mechanisms', 'find that', 'F 1 score'], ['F 1 score', 'of', 'KAR'], ['F 1 score', 'drops by', '7.8'], ['7.8', 'on', 'AddSent'], ['F 1 score', 'drops by', '4.2'], ['4.2', 'on', 'development set'], ['F 1 score', 'drops by', '9.1'], ['9.1', 'on', 'AddOneSent']]",[],"[['Ablation analysis', 'replacing', 'knowledge aided attention mechanisms']]",[],[],[],[],[],[],natural_language_inference,62,198
results,"Finally we find that after only one epoch of training , KAR already achieves an EM of 71.9 and an F 1 score of 80.8 on the development set , which is even better than the final performance of several strong baselines , such as DCN ( EM / F1 : 65.4 / 75.6 ) and BiDAF ( EM / F1 : 67.7 / 77.3 ) .",[],[],"[['KAR', 'on', 'development set'], ['development set', 'achieves', 'F 1 score'], ['F 1 score', 'of', '80.8'], ['development set', 'achieves', 'EM'], ['EM', 'of', '71.9'], ['development set', 'achieves', 'even better'], ['even better', 'than', 'final performance'], ['final performance', 'of', 'several strong baselines'], ['several strong baselines', 'such as', 'DCN ( EM / F1 : 65.4 / 75.6 )'], ['several strong baselines', 'such as', 'BiDAF ( EM / F1 : 67.7 / 77.3 )']]",[],[],"[['Results', 'has', 'KAR']]",[],[],[],[],[],natural_language_inference,62,199
research-problem,A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,63,2
research-problem,Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .,[],"[('Reading Comprehension ( RC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension ( RC )']]",[],[],[],[],natural_language_inference,63,4
research-problem,"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .",[],"[('RC', (18, 19))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],natural_language_inference,63,5
research-problem,"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) .",[],"[('answer span prediction style Question Answering ( QA )', (11, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'answer span prediction style Question Answering ( QA )']]",[],[],[],[],natural_language_inference,63,12
research-problem,"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation .",[],"[('QA', (12, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,63,13
model,"In this work , we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks ( MANNs ) .","[('build', (5, 6)), ('understand', (11, 12)), ('by utilizing', (14, 16))]","[('QA model', (7, 9)), ('long documents', (12, 14)), ('Memory Augmented Neural Networks ( MANNs )', (16, 23))]","[['QA model', 'understand', 'long documents'], ['long documents', 'by utilizing', 'Memory Augmented Neural Networks ( MANNs )']]",[],"[['Model', 'build', 'QA model']]",[],[],[],[],[],[],natural_language_inference,63,24
model,This type of neural networks decouples the memory capacity from the number of model parameters .,"[('decouples', (5, 6)), ('from', (9, 10))]","[('memory capacity', (7, 9)), ('number of model parameters', (11, 15))]","[['memory capacity', 'from', 'number of model parameters']]",[],"[['Model', 'decouples', 'memory capacity']]",[],[],[],[],[],[],natural_language_inference,63,25
experimental-setup,We develop MAMCN using Tensorflow 1 deep learning framework and Sonnet 2 library .,"[('develop', (1, 2)), ('using', (3, 4))]","[('MAMCN', (2, 3)), ('Tensorflow 1 deep learning framework', (4, 9)), ('Sonnet 2 library', (10, 13))]","[['MAMCN', 'using', 'Tensorflow 1 deep learning framework'], ['MAMCN', 'using', 'Sonnet 2 library']]",[],"[['Experimental setup', 'develop', 'MAMCN']]",[],[],[],[],[],[],natural_language_inference,63,137
experimental-setup,"For the word - level embedding , we tokenize the documents using NLTK toolkit and substitute words with GloVe 6B 43.16 46.90 49.28 55.83 BiDAF 40.32 45.91 44.86 50.71 hidden size is set to 200 for QUASAR - T and Triv - iaQA , and 100 for SQuAD .","[('For', (0, 1)), ('tokenize', (8, 9)), ('using', (11, 12)), ('substitute', (15, 16)), ('with', (17, 18))]","[('word - level embedding', (2, 6)), ('documents', (10, 11)), ('NLTK toolkit', (12, 14)), ('words', (16, 17)), ('GloVe 6B', (18, 20))]","[['word - level embedding', 'using', 'NLTK toolkit'], ['word - level embedding', 'tokenize', 'documents'], ['word - level embedding', 'substitute', 'words'], ['words', 'with', 'GloVe 6B']]",[],"[['Experimental setup', 'For', 'word - level embedding']]",[],[],[],[],[],[],natural_language_inference,63,138
experimental-setup,"In the memory controller , we use 100 x 36 size memory initialized with zeros , 4 read heads and 1 write head .","[('In', (0, 1)), ('use', (6, 7)), ('initialized with', (12, 14))]","[('memory controller', (2, 4)), ('100 x 36 size memory', (7, 12)), ('zeros', (14, 15)), ('4', (16, 17)), ('read heads', (17, 19)), ('1', (20, 21)), ('write head', (21, 23))]","[['memory controller', 'use', '1'], ['memory controller', 'use', '4'], ['memory controller', 'use', '100 x 36 size memory'], ['100 x 36 size memory', 'initialized with', 'zeros']]","[['1', 'has', 'write head'], ['4', 'has', 'read heads']]","[['Experimental setup', 'In', 'memory controller']]",[],[],[],[],[],[],natural_language_inference,63,139
experimental-setup,"The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .","[('is', (2, 3)), ('with', (9, 10)), ('of', (14, 15))]","[('optimizer', (1, 2)), ('AdaDelta ( Zeiler , 2012 )', (3, 9)), ('initial learning rate', (11, 14)), ('0.5', (15, 16))]","[['optimizer', 'is', 'AdaDelta ( Zeiler , 2012 )'], ['AdaDelta ( Zeiler , 2012 )', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.5']]",[],[],"[['Experimental setup', 'has', 'optimizer']]",[],[],[],[],[],natural_language_inference,63,140
experimental-setup,"We train our model for 12 epochs , and batch size is set to 30 .","[('train', (1, 2)), ('for', (4, 5)), ('set to', (12, 14))]","[('model', (3, 4)), ('12 epochs', (5, 7)), ('batch size', (9, 11)), ('30', (14, 15))]","[['batch size', 'set to', '30'], ['model', 'for', '12 epochs']]",[],"[['Experimental setup', 'train', 'model']]","[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,63,141
experimental-setup,"During the training , we keep the exponential moving average of weights with 0.001 decay and use these averages at test time .","[('During', (0, 1)), ('keep', (5, 6)), ('of', (10, 11)), ('with', (12, 13)), ('use', (16, 17)), ('at', (19, 20))]","[('training', (2, 3)), ('exponential moving average', (7, 10)), ('weights', (11, 12)), ('0.001 decay', (13, 15)), ('averages', (18, 19)), ('test time', (20, 22))]","[['training', 'keep', 'exponential moving average'], ['exponential moving average', 'with', '0.001 decay'], ['exponential moving average', 'use', 'averages'], ['averages', 'at', 'test time'], ['exponential moving average', 'of', 'weights']]",[],"[['Experimental setup', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,63,142
results,QUASAR - T:,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,63,147
results,"As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .","[('gives', (23, 24))]","[('our proposed memory controller', (19, 23)), ('more performance improvement', (24, 27))]","[['our proposed memory controller', 'gives', 'more performance improvement']]",[],[],[],[],[],[],"[['QUASAR - T', 'has', 'our proposed memory controller']]",[],natural_language_inference,63,150
results,We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .,[],[],"[['68.13 EM and 70.32 F1', 'for', 'short documents'], ['63.44 and 65.19', 'for', 'long documents']]",[],[],[],[],"[['more performance improvement', 'achieve', '68.13 EM and 70.32 F1'], ['more performance improvement', 'achieve', '63.44 and 65.19']]",[],[],[],natural_language_inference,63,151
results,TriviaQA : We compare proposed model with all the previously suggested approaches as shown in .,[],"[('TriviaQA', (0, 1))]",[],[],[],"[['Results', 'has', 'TriviaQA']]",[],[],[],[],"[['TriviaQA', 'has', 'Our model']]",natural_language_inference,63,152
results,Our model achieves the state of the art performance over the existing approaches as shown in 77.58 84.16 O - QANet 76.24 84.60 O O SAN 76.83 84.40 O O Fusion Net 75.97 83.90 O O RaSoR + TR 75.79 83.26 O - Conducter- net 74.41 82.74 O O Reinforced Mnemonic Reader 73.20 81.80 O O BiDAF + Self-attention 72.14 81.05 - O MEMEN 70.98 80.36 O - MAMCN 70.99 79.94 -r- net 71.30 79.70 - O Document Reader 70.73 79.35 O - FastQAExt 70 .,"[('achieves', (2, 3))]","[('Our model', (0, 2)), ('state of the art performance', (4, 9))]","[['Our model', 'achieves', 'state of the art performance']]",[],[],[],[],[],[],[],[],natural_language_inference,63,156
ablation-analysis,"First , we add ELMo which is the weighted sum of hidden layers of language model with regularization as an additional feature to our word embeddings .",[],[],[],[],"[['Ablation analysis', 'add', 'ELMo']]",[],[],[],[],[],[],natural_language_inference,63,161
ablation-analysis,This helped our model ( MAMCN + ELMo ) to improve F1 to 85.13 and EM to 77.44 and is the best among the models only with the additional feature augmentation .,"[('helped', (1, 2)), ('to improve', (9, 11)), ('to', (12, 13)), ('among', (22, 23)), ('only with', (25, 27))]","[('our model ( MAMCN + ELMo )', (2, 9)), ('F1', (11, 12)), ('85.13', (13, 14)), ('EM', (15, 16)), ('77.44', (17, 18)), ('best', (21, 22)), ('models', (24, 25)), ('additional feature augmentation', (28, 31))]","[['our model ( MAMCN + ELMo )', 'to improve', 'EM'], ['EM', 'to', '77.44'], ['our model ( MAMCN + ELMo )', 'to improve', 'F1'], ['F1', 'to', '85.13'], ['best', 'among', 'models'], ['models', 'only with', 'additional feature augmentation']]","[['our model ( MAMCN + ELMo )', 'is', 'best']]",[],[],[],"[['ELMo', 'helped', 'our model ( MAMCN + ELMo )']]",[],[],[],natural_language_inference,63,162
ablation-analysis,We replace all the BiGRU units with this embedding block except the controller layer in our model ( MAMCN + ELMo + DC ) .,"[('replace', (1, 2)), ('with', (6, 7)), ('except', (10, 11)), ('in', (14, 15))]","[('BiGRU units', (4, 6)), ('embedding block', (8, 10)), ('controller layer', (12, 14)), ('our model ( MAMCN + ELMo + DC )', (15, 24))]","[['BiGRU units', 'with', 'embedding block'], ['embedding block', 'in', 'our model ( MAMCN + ELMo + DC )'], ['embedding block', 'except', 'controller layer']]",[],"[['Ablation analysis', 'replace', 'BiGRU units']]",[],[],[],[],[],[],natural_language_inference,63,168
ablation-analysis,"We achieve the state of the art performance , 86.73 F1 and 79.69 EM , with the help of this em-bedding block .","[('achieve', (1, 2))]","[('state of the art performance', (3, 8)), ('86.73 F1 and 79.69 EM', (9, 14))]",[],"[['state of the art performance', 'has', '86.73 F1 and 79.69 EM']]",[],[],[],"[['our model ( MAMCN + ELMo + DC )', 'achieve', 'state of the art performance']]",[],[],[],natural_language_inference,63,169
research-problem,TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,[],"[('Answer Sentence Selection', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Answer Sentence Selection']]",[],[],[],[],natural_language_inference,64,2
research-problem,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .",[],"[('Question Answering', (21, 23))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering']]",[],[],[],[],natural_language_inference,64,7
research-problem,"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :",[],"[('Question Answering ( QA )', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question Answering ( QA )']]",[],[],[],[],natural_language_inference,64,17
research-problem,"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .",[],"[('answer sentence selection ( AS2 )', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'answer sentence selection ( AS2 )']]",[],[],[],[],natural_language_inference,64,18
research-problem,"Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .",[],"[('AS2', (11, 12)), ('QA', (35, 36))]",[],[],[],[],"[['Contribution', 'has research problem', 'AS2'], ['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,64,19
model,"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .",[],[],"[['Transformer - based models', 'provide', 'effective solutions'], ['effective solutions', 'to tackle', 'instability'], ['instability', 'of', 'finetuning step'], ['effective solutions', 'to tackle', 'data scarceness problem'], ['data scarceness problem', 'for', 'AS2'], ['Transformer - based models', 'for', 'AS2']]",[],"[['Model', 'study', 'Transformer - based models']]",[],[],[],[],[],[],natural_language_inference,64,31
model,"We improve stability of Transformer models by adding an intermediate fine - tuning step , which aims at specializing them to the target task ( AS2 ) , i.e. , this step transfers a pretrained language model to a model for the target task .","[('of', (3, 4)), ('by adding', (6, 8)), ('aims at', (16, 18)), ('to', (20, 21))]","[('improve', (1, 2)), ('stability', (2, 3)), ('Transformer models', (4, 6)), ('intermediate fine - tuning step', (9, 14)), ('specializing', (18, 19)), ('target task ( AS2 )', (22, 27))]","[['stability', 'by adding', 'intermediate fine - tuning step'], ['intermediate fine - tuning step', 'aims at', 'specializing'], ['specializing', 'to', 'target task ( AS2 )'], ['stability', 'of', 'Transformer models']]","[['improve', 'has', 'stability']]",[],"[['Model', 'has', 'improve']]",[],[],[],[],[],natural_language_inference,64,33
model,"We show that the transferred model can be effectively adapted to the target domain with a subsequent finetuning step , even when using target data of small size .","[('show', (1, 2)), ('can be', (6, 8)), ('to', (10, 11)), ('with', (14, 15)), ('when using', (21, 23)), ('of', (25, 26))]","[('transferred model', (4, 6)), ('effectively adapted', (8, 10)), ('target domain', (12, 14)), ('subsequent finetuning step', (16, 19)), ('target data', (23, 25)), ('small size', (26, 28))]","[['transferred model', 'can be', 'effectively adapted'], ['effectively adapted', 'with', 'subsequent finetuning step'], ['subsequent finetuning step', 'when using', 'target data'], ['target data', 'of', 'small size'], ['effectively adapted', 'to', 'target domain']]",[],"[['Model', 'show', 'transferred model']]",[],[],[],[],[],[],natural_language_inference,64,34
dataset,"We built ASNQ , a dataset for AS2 , by transforming the recently released Natural Questions ( NQ ) corpus ) from MR to AS2 task .","[('built', (1, 2)), ('for', (6, 7)), ('by transforming', (9, 11)), ('from', (21, 22)), ('to', (23, 24))]","[('ASNQ', (2, 3)), ('AS2', (7, 8)), ('recently released Natural Questions ( NQ ) corpus', (12, 20)), ('MR', (22, 23)), ('AS2 task', (24, 26))]","[['ASNQ', 'by transforming', 'recently released Natural Questions ( NQ ) corpus'], ['recently released Natural Questions ( NQ ) corpus', 'from', 'MR'], ['MR', 'to', 'AS2 task'], ['ASNQ', 'for', 'AS2']]",[],"[['Dataset', 'built', 'ASNQ']]",[],[],[],[],[],[],natural_language_inference,64,36
hyperparameters,We adopt Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset and a learning rate of 1e - 6 for the adapt step on the target dataset .,[],[],"[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '2e - 5'], ['2e - 5', 'for', 'transfer step'], ['transfer step', 'on', 'ASNQ dataset'], ['learning rate', 'of', '1e - 6'], ['1e - 6', 'for', 'adapt step'], ['adapt step', 'on', 'target dataset']]",[],"[['Hyperparameters', 'adopt', 'Adam optimizer']]",[],[],[],[],[],[],natural_language_inference,64,151
hyperparameters,We apply early stopping on the dev. set of the target corpus for both steps based on the highest MAP score .,"[('apply', (1, 2)), ('on', (4, 5)), ('of', (8, 9))]","[('early stopping', (2, 4)), ('dev. set', (6, 8)), ('target corpus', (10, 12))]","[['early stopping', 'on', 'dev. set'], ['dev. set', 'of', 'target corpus']]",[],"[['Hyperparameters', 'apply', 'early stopping']]",[],[],[],[],[],[],natural_language_inference,64,152
hyperparameters,"We set the max number of epochs equal to 3 and 9 for adapt and transfer steps , respectively .","[('set', (1, 2)), ('equal to', (7, 9)), ('for', (12, 13))]","[('max number of epochs', (3, 7)), ('3 and 9', (9, 12)), ('adapt and transfer steps', (13, 17))]","[['max number of epochs', 'equal to', '3 and 9'], ['3 and 9', 'for', 'adapt and transfer steps']]",[],"[['Hyperparameters', 'set', 'max number of epochs']]",[],[],[],[],[],[],natural_language_inference,64,153
hyperparameters,We set the maximum sequence length for BERT / RoBERTa to 128 tokens .,"[('for', (6, 7)), ('to', (10, 11))]","[('maximum sequence length', (3, 6)), ('BERT / RoBERTa', (7, 10)), ('128 tokens', (11, 13))]","[['maximum sequence length', 'for', 'BERT / RoBERTa'], ['BERT / RoBERTa', 'to', '128 tokens']]",[],[],"[['Hyperparameters', 'set', 'maximum sequence length']]",[],[],[],[],[],natural_language_inference,64,154
results,"TANDA provides a large improvement over the state of the art , which has been regularly contributed to by hundreds of researchers .","[('provides', (1, 2)), ('over', (5, 6))]","[('TANDA', (0, 1)), ('large improvement', (3, 5)), ('state of the art', (7, 11))]","[['TANDA', 'provides', 'large improvement'], ['large improvement', 'over', 'state of the art']]",[],[],"[['Results', 'has', 'TANDA']]",[],[],[],[],[],natural_language_inference,64,166
results,RoBERTa- Large TANDA using ASNQ ?,"[('using', (3, 4))]","[('RoBERTa- Large TANDA', (0, 3)), ('ASNQ', (4, 5))]","[['RoBERTa- Large TANDA', 'using', 'ASNQ']]",[],[],"[['Results', 'has', 'RoBERTa- Large TANDA']]",[],[],[],[],[],natural_language_inference,64,167
results,"Wiki QA establish an impressive new state of the art for AS2 on WikiQA of 0.920 and 0.933 in MAP and MRR , respectively .","[('establish', (2, 3)), ('for', (10, 11)), ('on', (12, 13)), ('of', (14, 15)), ('in', (18, 19))]","[('impressive new state of the art', (4, 10)), ('AS2', (11, 12)), ('WikiQA', (13, 14)), ('0.920 and 0.933', (15, 18)), ('MAP and MRR', (19, 22))]","[['impressive new state of the art', 'of', '0.920 and 0.933'], ['0.920 and 0.933', 'in', 'MAP and MRR'], ['impressive new state of the art', 'for', 'AS2'], ['AS2', 'on', 'WikiQA']]",[],[],[],[],"[['RoBERTa- Large TANDA', 'establish', 'impressive new state of the art']]",[],[],[],natural_language_inference,64,168
results,RoBERTa - Large TANDA with ASNQ ?,"[('with', (4, 5))]","[('RoBERTa - Large TANDA', (0, 4)), ('ASNQ', (5, 6))]","[['RoBERTa - Large TANDA', 'with', 'ASNQ']]",[],[],"[['Results', 'has', 'RoBERTa - Large TANDA']]",[],[],[],[],[],natural_language_inference,64,174
results,"TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .",[],[],"[['impressive performance', 'of', '0.943'], ['0.943', 'in', 'MAP'], ['impressive performance', 'of', '0.974'], ['0.974', 'in', 'MRR']]","[['impressive performance', 'has', 'outperforming']]",[],[],[],"[['RoBERTa - Large TANDA', 'establishes', 'impressive performance']]",[],[],[],natural_language_inference,64,175
results,"TANDA improves all the models : BERT - Base , RoBERTa- Base , BERT - Large and RoBERTa - Large , outperforming the previous state of the art with all of them .","[('improves', (1, 2))]","[('all the models', (2, 5)), ('BERT - Base', (6, 9)), ('RoBERTa- Base', (10, 12)), ('BERT - Large', (13, 16)), ('RoBERTa - Large', (17, 20)), ('outperforming', (21, 22)), ('previous state of the art', (23, 28))]",[],"[['all the models', 'name', 'BERT - Base'], ['all the models', 'name', 'RoBERTa- Base'], ['all the models', 'name', 'BERT - Large'], ['all the models', 'name', 'RoBERTa - Large'], ['all the models', 'has', 'outperforming'], ['outperforming', 'has', 'previous state of the art'], ['outperforming', 'has', 'previous state of the art']]",[],[],[],"[['TANDA', 'improves', 'all the models']]",[],[],[],natural_language_inference,64,177
research-problem,"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .",[],"[('Neural networks ( NN ) with attention mechanisms', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural networks ( NN ) with attention mechanisms']]",[],[],[],[],natural_language_inference,65,10
research-problem,"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .",[],"[('attention mechanisms', (15, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'attention mechanisms']]",[],[],[],[],natural_language_inference,65,11
model,"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .",[],[],"[['two - way attention mechanism', 'that', 'significantly improves'], [""discriminative models ' performance"", 'by enabling', 'joint learning'], ['joint learning', 'of', 'representations'], ['representations', 'of', 'inputs'], ['inputs', 'as well as', 'their similarity measurement'], [""discriminative models ' performance"", 'on', 'pair - wise ranking or classification']]","[['Attentive Pooling ( AP )', 'has', 'two - way attention mechanism'], ['significantly improves', 'has', ""discriminative models ' performance""]]","[['Model', 'propose', 'Attentive Pooling ( AP )']]",[],[],[],[],[],[],natural_language_inference,65,15
model,"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .","[('enables', (3, 4)), ('to be aware of', (7, 11)), ('in a way that', (16, 20)), ('from', (21, 22)), ('directly influence', (27, 29)), ('of', (31, 32))]","[('AP', (2, 3)), ('pooling layer', (5, 7)), ('current input pair', (12, 15)), ('information', (20, 21)), ('two input items', (23, 26)), ('computation', (30, 31)), (""each other 's representations"", (32, 36))]","[['AP', 'enables', 'pooling layer'], ['pooling layer', 'to be aware of', 'current input pair'], ['current input pair', 'in a way that', 'information'], ['information', 'from', 'two input items'], ['information', 'directly influence', 'computation'], ['computation', 'of', ""each other 's representations""]]",[],[],"[['Model', 'has', 'AP']]",[],[],[],[],[],natural_language_inference,65,16
model,"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .",[],[],"[['similarity measure', 'over', 'projected segments ( e.g. trigrams )'], ['projected segments ( e.g. trigrams )', 'using', 'similarity scores'], ['similarity scores', 'to compute', 'attention vectors'], ['attention vectors', 'in', 'both directions'], ['similarity scores', 'between', 'segments'], ['projected segments ( e.g. trigrams )', 'of', 'two items'], ['two items', 'in', 'input pair']]","[['learning', 'has', 'similarity measure']]",[],[],[],"[['AP', 'consists of', 'learning']]",[],[],[],natural_language_inference,65,17
model,"Next , the attention vectors are used to perform pooling .","[('to perform', (7, 9))]","[('attention vectors', (3, 5)), ('pooling', (9, 10))]","[['attention vectors', 'to perform', 'pooling']]",[],[],"[['Model', 'has', 'attention vectors']]",[],[],[],[],[],natural_language_inference,65,18
experimental-setup,"We use a context window of size 3 for Insurance QA , while we set this parameter to 4 for TREC - QA and Wiki QA .",[],[],"[['context window', 'of', 'size'], ['3', 'for', 'Insurance QA'], ['4', 'for', 'TREC - QA'], ['4', 'for', 'Wiki QA']]","[['size', 'has', '3'], ['size', 'has', '4']]","[['Experimental setup', 'use', 'context window']]",[],[],[],[],[],[],natural_language_inference,65,171
experimental-setup,"Using the selected hyperparameters , the best results are normally achieved using between 15 and 25 training epochs .","[('achieved using', (10, 12))]","[('best results', (6, 8)), ('between 15 and 25 training epochs', (12, 18))]","[['best results', 'achieved using', 'between 15 and 25 training epochs']]",[],[],"[['Experimental setup', 'has', 'best results']]",[],[],[],[],[],natural_language_inference,65,172
experimental-setup,"For AP - CNN , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that decreases the learning rate ?","[('For', (0, 1)), ('that', (20, 21))]","[('AP - CNN', (1, 4)), ('AP - biLSTM', (5, 8)), ('QA - LSTM', (9, 12)), ('learning rate schedule', (17, 20)), ('decreases', (21, 22)), ('learning rate', (23, 25))]","[['learning rate schedule', 'that', 'decreases'], ['learning rate', 'For', 'AP - CNN'], ['learning rate', 'For', 'AP - biLSTM'], ['learning rate', 'For', 'QA - LSTM']]","[['decreases', 'has', 'learning rate']]",[],"[['Experimental setup', 'use', 'learning rate schedule']]",[],[],[],[],[],natural_language_inference,65,173
experimental-setup,"In our experiments , the four NN architectures QA - CNN , AP - CNN , QA - biLSTM and AP - biLSTM are implemented using Theano .","[('implemented using', (24, 26))]","[('four NN architectures', (5, 8)), ('QA - CNN', (8, 11)), ('AP - CNN', (12, 15)), ('QA - biLSTM', (16, 19)), ('AP - biLSTM', (20, 23)), ('Theano', (26, 27))]","[['four NN architectures', 'implemented using', 'Theano']]","[['four NN architectures', 'name', 'QA - CNN'], ['four NN architectures', 'name', 'AP - CNN'], ['four NN architectures', 'name', 'QA - biLSTM'], ['four NN architectures', 'name', 'AP - biLSTM']]",[],"[['Experimental setup', 'has', 'four NN architectures']]",[],[],[],[],[],natural_language_inference,65,178
results,"In , we present the experimental results of the four NNs for the Insurance QA dataset .","[('for', (11, 12))]","[('Insurance QA dataset', (13, 16))]",[],[],"[['Results', 'for', 'Insurance QA dataset']]",[],[],[],[],[],[],natural_language_inference,65,181
results,"On the bottom part of this table , we can see that AP - CNN outperforms QA - CNN by a large margin in both test sets , as well as in the dev set .","[('see that', (10, 12)), ('by', (19, 20)), ('in', (23, 24))]","[('AP - CNN', (12, 15)), ('outperforms', (15, 16)), ('QA - CNN', (16, 19)), ('large margin', (21, 23)), ('both test sets', (24, 27)), ('dev set', (33, 35))]","[['outperforms', 'by', 'large margin'], ['large margin', 'in', 'both test sets'], ['large margin', 'in', 'dev set']]","[['AP - CNN', 'has', 'outperforms'], ['outperforms', 'has', 'QA - CNN'], ['AP - CNN', 'has', 'outperforms'], ['outperforms', 'has', 'QA - CNN'], ['AP - CNN', 'has', 'outperforms'], ['outperforms', 'has', 'QA - CNN']]",[],[],[],"[['Insurance QA dataset', 'see that', 'AP - CNN']]",[],[],[],natural_language_inference,65,183
results,AP - CNN and AP - biLSTM have similar performance .,"[('have', (7, 8))]","[('AP - CNN and AP - biLSTM', (0, 7)), ('similar performance', (8, 10))]","[['AP - CNN and AP - biLSTM', 'have', 'similar performance']]",[],[],[],[],[],[],"[['Insurance QA dataset', 'has', 'AP - CNN and AP - biLSTM']]",[],natural_language_inference,65,185
results,Both AP - CNN and AP - biLSTM outperform the state - of - the - art systems .,[],"[('outperform', (8, 9)), ('state - of - the - art systems', (10, 18))]",[],"[['outperform', 'has', 'state - of - the - art systems']]",[],[],[],[],[],"[['AP - CNN and AP - biLSTM', 'has', 'outperform']]",[],natural_language_inference,65,189
results,"In , we present the experimental results of the four NNs for the TREC - QA dataset .",[],"[('TREC - QA dataset', (13, 17))]",[],[],[],"[['Results', 'for', 'TREC - QA dataset']]",[],[],[],[],"[['TREC - QA dataset', 'has', 'AP - CNN']]",natural_language_inference,65,201
results,We use the official trec eval that AP - CNN outperforms QA - CNN by a large margin in both metrics .,[],[],"[['QA - CNN', 'by', 'large margin'], ['large margin', 'in', 'both metrics']]",[],[],[],[],[],[],[],[],natural_language_inference,65,203
results,"AP - biLSTM outperforms the QA - biLSTM , but its performance is not as good as the of AP - CNN .","[('as', (16, 17))]","[('AP - biLSTM', (0, 3)), ('outperforms', (3, 4)), ('QA - biLSTM', (5, 8)), ('performance', (11, 12)), ('not as good', (13, 16)), ('AP - CNN', (19, 22))]","[['not as good', 'as', 'AP - CNN']]","[['AP - biLSTM', 'has', 'outperforms'], ['outperforms', 'has', 'QA - biLSTM'], ['QA - biLSTM', 'has', 'performance'], ['performance', 'has', 'not as good'], ['AP - biLSTM', 'has', 'outperforms'], ['outperforms', 'has', 'QA - biLSTM']]",[],[],[],[],[],"[['TREC - QA dataset', 'has', 'AP - biLSTM']]",[],natural_language_inference,65,204
results,"AP - CNN outperforms the state - of - the - art systems in both metrics , MAP and MRR .","[('in', (13, 14))]","[('state - of - the - art systems', (5, 13)), ('both metrics', (14, 16)), ('MAP and MRR', (17, 20))]","[['state - of - the - art systems', 'in', 'both metrics']]","[['both metrics', 'name', 'MAP and MRR']]",[],[],[],[],[],"[['outperforms', 'has', 'state - of - the - art systems']]",[],natural_language_inference,65,210
results,shows the experimental results of the four NNs for the WikiQA dataset .,[],"[('WikiQA dataset', (10, 12))]",[],[],[],"[['Results', 'for', 'WikiQA dataset']]",[],[],[],[],"[['WikiQA dataset', 'has', 'AP - CNN'], ['WikiQA dataset', 'has', 'AP - biLSTM']]",natural_language_inference,65,211
results,"Like in the other two datasets , AP - CNN outperforms QA - CNN , and AP - biLSTM outperforms the QA - biLSTM .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,65,212
results,The difference of performance between AP - CNN and QA - CNN is smaller than the one for the Insurance QA dataset .,"[('between', (4, 5)), ('is', (12, 13)), ('than', (14, 15))]","[('difference of performance', (1, 4)), ('AP - CNN and QA - CNN', (5, 12)), ('smaller', (13, 14)), ('Insurance QA dataset', (19, 22))]","[['difference of performance', 'between', 'AP - CNN and QA - CNN'], ['AP - CNN and QA - CNN', 'is', 'smaller'], ['smaller', 'than', 'Insurance QA dataset']]",[],[],[],[],[],[],"[['WikiQA dataset', 'has', 'difference of performance']]",[],natural_language_inference,65,213
research-problem,Learning Natural Language Inference with LSTM,[],"[('Natural Language Inference', (1, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,66,2
research-problem,Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .,[],"[('Natural language inference ( NLI )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,66,4
research-problem,"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .",[],"[('NLI', (18, 19))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,66,6
model,"In this paper , we propose a new LSTM - based architecture for learning natural language inference .","[('propose', (5, 6)), ('for learning', (12, 14))]","[('new LSTM - based architecture', (7, 12)), ('natural language inference', (14, 17))]","[['new LSTM - based architecture', 'for learning', 'natural language inference']]",[],"[['Model', 'propose', 'new LSTM - based architecture']]",[],[],[],[],[],[],natural_language_inference,66,32
model,"Instead , we use an LSTM to perform word - by - word matching of the hypothesis with the premise .","[('use', (3, 4)), ('to perform', (6, 8)), ('of', (14, 15)), ('with', (17, 18))]","[('LSTM', (5, 6)), ('word - by - word matching', (8, 14)), ('hypothesis', (16, 17)), ('premise', (19, 20))]","[['LSTM', 'to perform', 'word - by - word matching'], ['word - by - word matching', 'of', 'hypothesis'], ['hypothesis', 'with', 'premise']]",[],"[['Model', 'use', 'LSTM']]",[],[],[],[],[],[],natural_language_inference,66,34
model,"Our LSTM sequentially processes the hypothesis , and at each position , it tries to match the current word in the hypothesis with an attention - weighted representation of the premise .",[],[],"[['Our LSTM', 'at each position', 'tries to match'], ['current word', 'with', 'attention - weighted representation'], ['attention - weighted representation', 'of', 'premise'], ['current word', 'in', 'hypothesis'], ['Our LSTM', 'sequentially processes', 'hypothesis']]","[['tries to match', 'has', 'current word']]",[],"[['Model', 'has', 'Our LSTM']]",[],[],[],[],[],natural_language_inference,66,35
model,"We refer to this architecture a match - LSTM , or m LSTM for short .","[('refer to', (1, 3)), ('for', (13, 14))]","[('match - LSTM', (6, 9)), ('m LSTM', (11, 13)), ('short', (14, 15))]","[['m LSTM', 'for', 'short']]",[],"[['Model', 'refer to', 'match - LSTM'], ['Model', 'refer to', 'm LSTM']]",[],[],[],[],[],[],natural_language_inference,66,37
code,1 https://github.com/shuohangwang/,[],"[('https://github.com/shuohangwang/', (1, 2))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/shuohangwang/']]",[],[],[],[],natural_language_inference,66,42
hyperparameters,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?","[('use', (1, 2)), ('with', (12, 13))]","[('Adam method', (3, 5)), ('hyperparameters', (13, 14))]",[],[],"[['Hyperparameters', 'use', 'Adam method']]",[],[],"[['optimization', 'with', 'hyperparameters']]",[],[],"[['hyperparameters', 'set to', '0.999']]",natural_language_inference,66,142
hyperparameters,1 set to 0.9 and ?,"[('set to', (1, 3))]","[('0.9', (3, 4))]",[],[],[],[],[],"[['hyperparameters', 'set to', '0.9']]",[],[],[],natural_language_inference,66,143
hyperparameters,2 set to 0.999 for optimization .,"[('for', (4, 5))]","[('0.999', (3, 4)), ('optimization', (5, 6))]",[],[],[],[],[],"[['Adam method', 'for', 'optimization']]",[],[],[],natural_language_inference,66,144
hyperparameters,The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration .,"[('set to', (5, 7)), ('of', (13, 14)), ('for', (15, 16))]","[('initial learning rate', (1, 4)), ('0.001', (8, 9)), ('decay ratio', (11, 13)), ('0.95', (14, 15)), ('each iteration', (16, 18))]","[['initial learning rate', 'set to', '0.001'], ['decay ratio', 'of', '0.95'], ['0.95', 'for', 'each iteration']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate'], ['Hyperparameters', 'has', 'decay ratio']]",[],[],[],[],[],natural_language_inference,66,145
hyperparameters,The batch size is set to be 30 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('30', (7, 8))]","[['batch size', 'set to', '30']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,66,146
hyperparameters,We experiment with d = 150 and d = 300 where d is the dimension of all the hidden states .,"[('experiment with', (1, 3))]","[('d = 150 and d = 300', (3, 10))]",[],[],"[['Hyperparameters', 'experiment with', 'd = 150 and d = 300']]",[],[],[],[],[],[],natural_language_inference,66,147
results,"We have the following observations : ( 1 ) First of all , we can see that when we set d to 300 , our model achieves an accuracy of 86.1 % on the test data , which to the best of our knowledge is the highest on and |?| M is the number of parameters excluding the word embeddings .","[('of', (10, 11)), ('see that', (15, 17)), ('to', (21, 22)), ('achieves', (26, 27)), ('on', (32, 33))]","[('set d', (19, 21)), ('300', (22, 23)), ('our model', (24, 26)), ('accuracy', (28, 29)), ('86.1 %', (30, 32)), ('test data', (34, 36))]","[['set d', 'to', '300'], ['our model', 'achieves', 'accuracy'], ['accuracy', 'of', '86.1 %'], ['86.1 %', 'on', 'test data']]","[['300', 'has', 'our model']]","[['Results', 'see that', 'set d']]",[],[],[],[],[],[],natural_language_inference,66,163
results,"( 2 ) If we compare our m LSTM model with our implementation of the word - by - word attention model by under the same setting with d = 150 , we can see that our performance on the test data ( 85.7 % ) is higher than that of their model ( 82.6 % ) .",[],[],"[['our m LSTM model', 'with', 'word - by - word attention model'], ['our m LSTM model', 'see that', 'our performance'], ['our performance', 'on', 'test data ( 85.7 % )'], ['test data ( 85.7 % )', 'is', 'higher'], ['higher', 'than', 'their model ( 82.6 % )'], ['our m LSTM model', 'under', 'same setting'], ['same setting', 'with', 'd'], ['d', '=', '150']]",[],"[['Results', 'compare', 'our m LSTM model']]",[],[],[],[],[],[],natural_language_inference,66,182
results,"( 3 ) The performance of mLSTM with bi - LSTM sentence modeling compared with the model with standard LSTM sentence modeling when d is set to 150 shows that using bi - LSTM to process the original sentences helps ( 86.0 % vs. 85.7 % on the test data ) , but the difference is small and the complexity of bi - LSTM is much higher than LSTM .",[],[],"[['performance', 'of', 'mLSTM'], ['mLSTM', 'with', 'bi - LSTM sentence modeling'], ['mLSTM', 'compared with', 'model'], ['model', 'with', 'standard LSTM sentence modeling'], ['model', 'shows that', 'bi - LSTM'], ['86.0 % vs. 85.7 %', 'on', 'test data'], ['bi - LSTM', 'to process', 'original sentences'], ['model', 'when', 'd'], ['d', 'set to', '150']]","[['bi - LSTM', 'has', 'helps'], ['helps', 'has', '86.0 % vs. 85.7 %']]",[],"[['Results', 'has', 'performance']]",[],[],[],[],[],natural_language_inference,66,184
results,"( 4 ) Interestingly , when we experimented with the m LSTM model using the pre-trained word embeddings instead of LSTMgenerated hidden states as initial representations of the premise and the hypothesis , we were able to achieve an accuracy of 85.3 % on the test data , which is still better than previously reported state of the art .",[],[],"[['m LSTM model', 'using', 'pre-trained word embeddings'], ['pre-trained word embeddings', 'as', 'initial representations'], ['initial representations', 'of', 'premise and the hypothesis'], ['initial representations', 'able to achieve', 'accuracy'], ['accuracy', 'of', '85.3 %'], ['85.3 %', 'is', 'better'], ['better', 'than', 'previously reported state of the art'], ['85.3 %', 'on', 'test data'], ['pre-trained word embeddings', 'instead of', 'LSTMgenerated hidden states']]",[],"[['Results', 'experimented with', 'm LSTM model']]",[],[],[],[],[],[],natural_language_inference,66,186
research-problem,End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension,[],"[('Reading Comprehension', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],natural_language_inference,67,2
research-problem,"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .",[],"[('neural reading comprehension ( RC )', (14, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural reading comprehension ( RC )']]",[],[],[],[],natural_language_inference,67,4
research-problem,Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .,[],"[('Reading comprehension - based question answering ( RCQA )', (0, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading comprehension - based question answering ( RCQA )']]",[],[],[],[],natural_language_inference,67,9
research-problem,"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .",[],"[('RCQA', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'RCQA']]",[],[],[],[],natural_language_inference,67,12
model,"Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .","[('called', (4, 5))]","[('dynamic chunk reader ( DCR )', (5, 11))]",[],[],"[['Model', 'called', 'dynamic chunk reader ( DCR )']]",[],[],[],[],[],[],natural_language_inference,67,25
model,"First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .","[('uses', (4, 5)), ('to learn', (7, 9)), ('for', (11, 12)), ('instead of', (16, 18))]","[('deep networks', (5, 7)), ('better representations', (9, 11)), ('candidate answer chunks', (12, 15)), ('fixed feature representations', (19, 22))]","[['deep networks', 'to learn', 'better representations'], ['better representations', 'for', 'candidate answer chunks'], ['better representations', 'instead of', 'fixed feature representations']]",[],"[['Model', 'uses', 'deep networks']]",[],[],[],[],[],[],natural_language_inference,67,26
model,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .","[('represents', (3, 4)), ('as', (6, 7)), ('instead of', (17, 19)), ('to make', (24, 26)), ('aware of', (28, 30)), ('among', (33, 34))]","[('answer candidates', (4, 6)), ('chunks', (7, 8)), ('word - level representations', (19, 23)), ('model', (27, 28)), ('subtle differences', (31, 33)), ('candidates', (34, 35))]","[['answer candidates', 'as', 'chunks'], ['chunks', 'to make', 'model'], ['model', 'aware of', 'subtle differences'], ['subtle differences', 'among', 'candidates'], ['chunks', 'instead of', 'word - level representations']]",[],"[['Model', 'represents', 'answer candidates']]",[],[],[],[],[],[],natural_language_inference,67,27
experimental-setup,We pre-processed the SQuAD dataset using Stanford CoreNLP tool 5 with its default setting to tokenize the text and obtain the POS and NE annotations .,"[('pre-processed', (1, 2)), ('using', (5, 6)), ('with', (10, 11)), ('to tokenize', (14, 16)), ('obtain', (19, 20))]","[('SQuAD dataset', (3, 5)), ('Stanford CoreNLP tool', (6, 9)), ('default setting', (12, 14)), ('text', (17, 18)), ('POS and NE annotations', (21, 25))]","[['SQuAD dataset', 'using', 'Stanford CoreNLP tool'], ['Stanford CoreNLP tool', 'with', 'default setting'], ['Stanford CoreNLP tool', 'to tokenize', 'text'], ['text', 'obtain', 'POS and NE annotations']]",[],"[['Experimental setup', 'pre-processed', 'SQuAD dataset']]",[],[],[],[],[],[],natural_language_inference,67,136
experimental-setup,"To train our model , we used stochastic gradient descent with the ADAM optimizer , with an initial learning rate of 0.001 .","[('used', (6, 7)), ('with', (10, 11)), ('of', (20, 21))]","[('stochastic gradient descent', (7, 10)), ('ADAM optimizer', (12, 14)), ('initial learning rate', (17, 20)), ('0.001', (21, 22))]","[['stochastic gradient descent', 'with', 'ADAM optimizer'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]",[],"[['Experimental setup', 'used', 'stochastic gradient descent']]",[],[],[],[],[],[],natural_language_inference,67,137
experimental-setup,"All GRU weights were initialized from a uniform distribution between ( - 0.01 , 0.01 ) .","[('initialized from', (4, 6)), ('between', (9, 10))]","[('All GRU weights', (0, 3)), ('uniform distribution', (7, 9)), ('( - 0.01 , 0.01 )', (10, 16))]","[['All GRU weights', 'initialized from', 'uniform distribution'], ['uniform distribution', 'between', '( - 0.01 , 0.01 )']]",[],[],"[['Experimental setup', 'has', 'All GRU weights']]",[],[],[],[],[],natural_language_inference,67,138
experimental-setup,"The hidden state size , d , was set to 300 for all GRUs .","[('set to', (8, 10)), ('for', (11, 12))]","[('hidden state size', (1, 4)), ('d', (5, 6)), ('300', (10, 11)), ('all GRUs', (12, 14))]","[['d', 'set to', '300'], ['300', 'for', 'all GRUs']]","[['hidden state size', 'has', 'd']]",[],"[['Experimental setup', 'has', 'hidden state size']]",[],[],[],[],[],natural_language_inference,67,139
experimental-setup,"We also applied dropout of rate 0.2 to the embedding layer of input bi - GRU encoder , and gradient clipping when the norm of gradients exceeded 10 .",[],[],"[['gradient clipping', 'when', 'norm'], ['norm', 'exceeded', '10'], ['norm', 'of', 'gradients'], ['dropout', 'of rate', '0.2'], ['0.2', 'to', 'embedding layer'], ['embedding layer', 'of', 'input bi - GRU encoder']]",[],[],"[['Experimental setup', 'applied', 'gradient clipping'], ['Experimental setup', 'applied', 'dropout']]",[],[],[],[],[],natural_language_inference,67,142
experimental-setup,We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .,"[('trained in', (1, 3)), ('is', (10, 11)), ('applied', (14, 15)), ('to', (18, 19)), ('in', (24, 25))]","[('mini-batch style', (3, 5)), ('mini - batch size', (6, 10)), ('180', (11, 12)), ('zero - padding', (15, 18)), ('passage and question inputs', (20, 24)), ('each batch', (25, 27))]","[['zero - padding', 'to', 'passage and question inputs'], ['passage and question inputs', 'in', 'each batch'], ['mini - batch size', 'is', '180']]","[['mini-batch style', 'has', 'mini - batch size']]","[['Experimental setup', 'applied', 'zero - padding'], ['Experimental setup', 'trained in', 'mini-batch style']]",[],[],[],[],[],[],natural_language_inference,67,143
experimental-setup,"We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .","[('set', (2, 3)), ('to be', (7, 9)), ('after', (17, 18)), ('in', (23, 24))]","[('maximum passage length', (4, 7)), ('300 tokens', (9, 11)), ('pruned', (13, 14)), ('all the tokens', (14, 17)), ('300 - th token', (19, 23)), ('training set', (25, 27))]","[['all the tokens', 'after', '300 - th token'], ['300 - th token', 'in', 'training set'], ['maximum passage length', 'to be', '300 tokens']]","[['maximum passage length', 'has', 'pruned'], ['pruned', 'has', 'all the tokens']]","[['Experimental setup', 'set', 'maximum passage length']]",[],[],[],[],[],[],natural_language_inference,67,144
experimental-setup,"We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .","[('trained', (1, 2)), ('for', (4, 5))]","[('model', (3, 4)), ('at most 30 epochs', (5, 9))]","[['model', 'for', 'at most 30 epochs']]",[],"[['Experimental setup', 'trained', 'model']]",[],[],[],[],[],[],natural_language_inference,67,147
experimental-setup,"For the feature ranking - based system , we used jforest ranker ( Ganjis affar , Caruana , and Lopes 2011 ) with Lambda MART - Regression Tree algorithm and the ranking metric was NDCG @ 10 .","[('For', (0, 1)), ('used', (9, 10)), ('with', (22, 23))]","[('feature ranking - based system', (2, 7)), ('jforest ranker', (10, 12)), ('Lambda MART - Regression Tree algorithm', (23, 29))]","[['feature ranking - based system', 'used', 'jforest ranker'], ['jforest ranker', 'with', 'Lambda MART - Regression Tree algorithm']]",[],"[['Experimental setup', 'For', 'feature ranking - based system']]",[],[],[],[],[],[],natural_language_inference,67,148
results,Results shows our main results on the SQuAD dataset .,"[('on', (5, 6))]","[('SQuAD dataset', (7, 9))]",[],[],"[['Results', 'on', 'SQuAD dataset']]",[],[],[],[],[],[],natural_language_inference,67,150
results,"Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .",[],[],"[['our exact match ( EM ) and F1', 'on', 'development set'], ['F1', 'on', 'test set']]","[['better', 'has', 'our exact match ( EM ) and F1'], ['comparable', 'has', 'F1']]",[],[],[],"[['SQuAD dataset', 'are', 'better'], ['SQuAD dataset', 'are', 'comparable']]",[],[],[],natural_language_inference,67,151
results,"As the first row of shows , our baseline system improves 10 % ( EM ) over , row 1 ) , the feature - based ranking system .","[('over', (16, 17))]","[('our baseline system', (7, 10)), ('improves', (10, 11)), ('10 % ( EM )', (11, 16)), ('feature - based ranking system', (23, 28))]","[['10 % ( EM )', 'over', 'feature - based ranking system']]","[['our baseline system', 'has', 'improves'], ['improves', 'has', '10 % ( EM )']]",[],"[['Results', 'has', 'our baseline system']]",[],[],[],[],[],natural_language_inference,67,154
results,"However when compared to our DCR model , row 2 ) , the baseline ( row 1 ) is more than 12 % ( EM ) behind even though it is based on the state - of - the - art model for cloze - style RC tasks .","[('compared to', (2, 4)), ('is', (18, 19))]","[('our DCR model', (4, 7)), ('baseline', (13, 14)), ('more than 12 % ( EM )', (19, 26)), ('behind', (26, 27))]","[['baseline', 'is', 'more than 12 % ( EM )']]","[['our DCR model', 'has', 'baseline'], ['more than 12 % ( EM )', 'has', 'behind']]","[['Results', 'compared to', 'our DCR model']]",[],[],[],[],[],[],natural_language_inference,67,155
ablation-analysis,"First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .","[('replacing', (2, 3)), ('with', (10, 11)), ('by about', (19, 21))]","[('word - by - word attention', (4, 10)), ('Attentive Reader', (11, 13)), ('style attention', (13, 15)), ('decreases', (15, 16)), ('EM score', (17, 19)), ('4.5 %', (21, 23))]","[['word - by - word attention', 'with', 'Attentive Reader'], ['decreases', 'by about', '4.5 %']]","[['Attentive Reader', 'has', 'style attention'], ['style attention', 'has', 'decreases'], ['decreases', 'has', 'EM score']]","[['Ablation analysis', 'replacing', 'word - by - word attention']]",[],[],[],[],[],[],natural_language_inference,67,159
ablation-analysis,The result shows that POS feature ( 1 ) and question - word feature ( 3 ) are the two most important features .,"[('shows', (2, 3)), ('are', (17, 18))]","[('POS feature ( 1 ) and question - word feature ( 3 )', (4, 17)), ('two most important features', (19, 23))]","[['POS feature ( 1 ) and question - word feature ( 3 )', 'are', 'two most important features']]",[],"[['Ablation analysis', 'shows', 'POS feature ( 1 ) and question - word feature ( 3 )']]",[],[],[],[],[],[],natural_language_inference,67,161
ablation-analysis,"Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .",[],[],"[['DCR model', 'with', 'proposed POS - trie constraints'], ['DCR model', 'yields', 'score'], ['score', 'similar to', 'DCR model'], ['DCR model', 'with', 'all possible n-gram chunks']]",[],"[['Ablation analysis', 'combining', 'DCR model']]",[],[],[],[],[],[],natural_language_inference,67,162
research-problem,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,[],"[('MACHINE COMPREHENSION', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'MACHINE COMPREHENSION']]",[],[],[],[],natural_language_inference,68,2
research-problem,Machine comprehension of text is an important problem in natural language processing .,[],"[('Machine comprehension of text', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine comprehension of text']]",[],[],[],[],natural_language_inference,68,4
model,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .","[('adopt', (20, 21))]","[('match - LSTM model', (22, 26))]",[],[],"[['Model', 'adopt', 'match - LSTM model']]",[],[],[],[],[],[],natural_language_inference,68,39
model,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .","[('enables', (16, 17)), ('of', (19, 20)), ('from', (21, 22))]","[('Pointer Net ( Ptr - Net ) model', (4, 12)), ('predictions', (18, 19)), ('tokens', (20, 21)), ('input sequence only', (23, 26))]","[['Pointer Net ( Ptr - Net ) model', 'enables', 'predictions'], ['predictions', 'of', 'tokens'], ['tokens', 'from', 'input sequence only']]",[],[],"[['Model', 'adopt', 'Pointer Net ( Ptr - Net ) model']]",[],[],[],[],[],natural_language_inference,68,40
model,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,"[('propose', (1, 2)), ('to apply', (4, 6)), ('for', (11, 12))]","[('two ways', (2, 4)), ('Ptr - Net model', (7, 11)), ('our task', (12, 14)), ('sequence model', (16, 18)), ('boundary model', (20, 22))]","[['two ways', 'to apply', 'Ptr - Net model'], ['Ptr - Net model', 'for', 'our task']]","[['two ways', 'name', 'sequence model'], ['two ways', 'name', 'boundary model']]","[['Model', 'propose', 'two ways']]",[],[],[],[],[],[],natural_language_inference,68,41
model,We also further extend the boundary model with a search mechanism .,"[('extend', (3, 4)), ('with', (7, 8))]","[('boundary model', (5, 7)), ('search mechanism', (9, 11))]","[['boundary model', 'with', 'search mechanism']]",[],"[['Model', 'extend', 'boundary model']]",[],[],[],[],[],[],natural_language_inference,68,42
experimental-setup,"We first tokenize all the passages , questions and answers .","[('tokenize', (2, 3))]","[('all the passages , questions and answers', (3, 10))]",[],[],"[['Experimental setup', 'tokenize', 'all the passages , questions and answers']]",[],[],[],[],[],[],natural_language_inference,68,174
experimental-setup,We use word embeddings from GloVe to initialize the model .,"[('use', (1, 2)), ('from', (4, 5)), ('to initialize', (6, 8))]","[('word embeddings', (2, 4)), ('GloVe', (5, 6)), ('model', (9, 10))]","[['word embeddings', 'to initialize', 'model'], ['word embeddings', 'from', 'GloVe']]",[],"[['Experimental setup', 'use', 'word embeddings']]",[],[],[],[],[],[],natural_language_inference,68,176
experimental-setup,Words not found in Glo Ve are initialized as zero vectors .,"[('not found in', (1, 4)), ('initialized as', (7, 9))]","[('Words', (0, 1)), ('Glo Ve', (4, 6)), ('zero vectors', (9, 11))]","[['Words', 'not found in', 'Glo Ve'], ['Words', 'initialized as', 'zero vectors']]",[],[],"[['Experimental setup', 'has', 'Words']]",[],[],[],[],[],natural_language_inference,68,177
experimental-setup,The dimensionality l of the hidden layers is set to be 150 or 300 .,"[('of', (3, 4)), ('set to be', (8, 11))]","[('dimensionality l', (1, 3)), ('hidden layers', (5, 7)), ('150 or 300', (11, 14))]","[['dimensionality l', 'set to be', '150 or 300'], ['dimensionality l', 'of', 'hidden layers']]",[],[],"[['Experimental setup', 'has', 'dimensionality l']]",[],[],[],[],[],natural_language_inference,68,179
experimental-setup,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,"[('with', (3, 4)), ('to optimize', (15, 17))]","[('ADAMAX', (2, 3)), ('coefficients', (5, 6)), ('? 1 = 0.9 and ? 2 = 0.999', (6, 15)), ('model', (18, 19))]","[['ADAMAX', 'with', 'coefficients'], ['coefficients', 'to optimize', 'model']]","[['coefficients', 'has', '? 1 = 0.9 and ? 2 = 0.999']]",[],"[['Experimental setup', 'use', 'ADAMAX']]",[],[],[],[],[],natural_language_inference,68,180
results,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",[],[],"[['our boundary model', 'achieving', 'exact match score'], ['exact match score', 'of', '61.1 %'], ['our boundary model', 'achieving', 'F1 score'], ['F1 score', 'of', '71.2 %']]","[['our boundary model', 'has', 'outperformed'], ['outperformed', 'has', 'sequence model']]",[],"[['Results', 'has', 'our boundary model']]",[],[],[],[],[],natural_language_inference,68,192
results,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .","[('in terms of', (3, 6)), ('over', (18, 19))]","[('exact match score', (7, 10)), ('boundary model', (12, 14)), ('clear advantage', (16, 18)), ('sequence model', (20, 22))]","[['clear advantage', 'over', 'sequence model']]","[['exact match score', 'has', 'boundary model'], ['boundary model', 'has', 'clear advantage']]","[['Results', 'in terms of', 'exact match score']]",[],[],[],[],[],[],natural_language_inference,68,193
results,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .","[('adding', (2, 3)), ('with', (8, 9)), ('get', (15, 16)), ('in', (19, 20))]","[('Bi - Ans - Ptr', (3, 8)), ('bi-directional pre-processing LSTM', (9, 12)), ('1.2 % improvement', (16, 19)), ('F1', (20, 21))]","[['Bi - Ans - Ptr', 'with', 'bi-directional pre-processing LSTM'], ['Bi - Ans - Ptr', 'get', '1.2 % improvement'], ['1.2 % improvement', 'in', 'F1']]",[],"[['Results', 'adding', 'Bi - Ans - Ptr']]",[],[],[],[],[],[],natural_language_inference,68,200
research-problem,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,[],"[('Multi-hop Reading Comprehension', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi-hop Reading Comprehension']]",[],[],[],[],natural_language_inference,69,2
research-problem,"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document .",[],"[('Reading Comprehension', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],natural_language_inference,69,4
research-problem,Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .,[],"[('Reading Comprehension ( RC )', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension ( RC )']]",[],[],[],[],natural_language_inference,69,17
dataset,"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .",[],[],"[['WIKIHOP', 'uses', 'sets'], ['sets', 'of', 'WIKIPEDIA articles'], ['WIKIPEDIA articles', 'where', 'answers'], ['answers', 'to', 'queries'], ['queries', 'about', 'specific properties'], ['specific properties', 'of', 'entity'], ['specific properties', 'can not be located in', ""entity 's article""]]",[],[],"[['Dataset', 'has', 'WIKIHOP']]",[],[],[],[],[],natural_language_inference,69,29
dataset,"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .","[('to establish', (10, 12)), ('based on', (16, 18)), ('about', (20, 21)), ('found across', (28, 30))]","[('MEDHOP', (5, 6)), ('goal', (8, 9)), ('drug - drug interactions', (12, 16)), ('scientific findings', (18, 20)), ('drugs and proteins and their interactions', (21, 27)), ('multiple MEDLINE abstracts', (30, 33))]","[['goal', 'to establish', 'drug - drug interactions'], ['drug - drug interactions', 'based on', 'scientific findings'], ['scientific findings', 'about', 'drugs and proteins and their interactions'], ['drugs and proteins and their interactions', 'found across', 'multiple MEDLINE abstracts']]","[['MEDHOP', 'has', 'goal']]",[],"[['Dataset', 'has', 'MEDHOP']]",[],[],[],[],[],natural_language_inference,69,30
dataset,"For both datasets we draw upon existing Knowledge Bases ( KBs ) , WIKIDATA and DRUG - BANK , as ground truth , utilizing distant supervision ) to induce the data - similar to and .","[('draw upon', (4, 6))]","[('existing Knowledge Bases ( KBs )', (6, 12)), ('WIKIDATA and DRUG - BANK', (13, 18))]",[],"[['existing Knowledge Bases ( KBs )', 'name', 'WIKIDATA and DRUG - BANK']]","[['Dataset', 'draw upon', 'existing Knowledge Bases ( KBs )']]",[],[],[],[],[],[],natural_language_inference,69,31
baselines,Random Selects a random candidate ; note that the number of candidates differs between samples .,"[('Selects', (1, 2))]","[('Random', (0, 1)), ('random candidate', (3, 5))]","[['Random', 'Selects', 'random candidate']]",[],[],"[['Baselines', 'has', 'Random']]",[],[],[],[],[],natural_language_inference,69,213
baselines,Max- mention,[],"[('Max- mention', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Max- mention']]",[],[],[],[],[],natural_language_inference,69,214
baselines,Predicts the most frequently mentioned candidate in the support documents,"[('Predicts', (0, 1)), ('in', (6, 7))]","[('most frequently mentioned candidate', (2, 6)), ('support documents', (8, 10))]","[['most frequently mentioned candidate', 'in', 'support documents']]",[],[],[],[],"[['Max- mention', 'Predicts', 'most frequently mentioned candidate']]",[],[],[],natural_language_inference,69,215
baselines,Majority - candidate - per-query - type,[],"[('Majority - candidate - per-query - type', (0, 7))]",[],[],[],"[['Baselines', 'has', 'Majority - candidate - per-query - type']]",[],[],[],[],[],natural_language_inference,69,217
baselines,Predicts the candidate c ?,"[('Predicts', (0, 1))]","[('candidate', (2, 3))]",[],[],[],[],[],"[['Majority - candidate - per-query - type', 'Predicts', 'candidate']]",[],[],[],natural_language_inference,69,218
baselines,"C q that was most frequently observed as the true answer in the training set , given the query type of q .","[('that was', (2, 4)), ('as', (7, 8))]","[('most frequently observed', (4, 7)), ('true answer', (9, 11))]","[['most frequently observed', 'as', 'true answer']]",[],[],[],[],"[['candidate', 'that was', 'most frequently observed']]",[],[],[],natural_language_inference,69,219
baselines,TF - IDF,[],"[('TF - IDF', (0, 3))]",[],[],[],"[['Baselines', 'has', 'TF - IDF']]",[],[],[],[],"[['TF - IDF', 'has', 'Retrieval - based models']]",natural_language_inference,69,221
baselines,Retrieval - based models are known to be strong QA baselines if candidate answers are provided .,"[('are', (4, 5)), ('known to be', (5, 8)), ('if', (11, 12))]","[('Retrieval - based models', (0, 4)), ('strong QA baselines', (8, 11)), ('candidate answers', (12, 14)), ('provided', (15, 16))]","[['Retrieval - based models', 'known to be', 'strong QA baselines'], ['strong QA baselines', 'if', 'candidate answers'], ['candidate answers', 'are', 'provided']]",[],[],[],[],[],[],[],[],natural_language_inference,69,222
baselines,"( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the correct candidate is often indicated solely by the presence of certain documents in Sq .","[('that', (11, 12)), ('indicated solely by', (32, 35))]","[('Document - cue', (3, 6)), ('effect', (25, 26)), ('correct candidate', (28, 30)), ('presence of certain documents', (36, 40))]","[['effect', 'that', 'correct candidate'], ['correct candidate', 'indicated solely by', 'presence of certain documents']]","[['Document - cue', 'has', 'effect']]",[],"[['Baselines', 'has', 'Document - cue']]",[],[],[],[],[],natural_language_inference,69,229
results,"The Document - cue baseline can predict more than a third of the samples correctly , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .","[('predict', (6, 7)), ('for', (16, 17)), ('after sub - sampling', (21, 25))]","[('Document - cue baseline', (1, 5)), ('more than a third of the samples', (7, 14)), ('correctly', (14, 15)), ('frequent document - answer pairs', (25, 30)), ('WIKIHOP', (31, 32))]","[['Document - cue baseline', 'predict', 'more than a third of the samples'], ['more than a third of the samples', 'after sub - sampling', 'frequent document - answer pairs'], ['frequent document - answer pairs', 'for', 'WIKIHOP']]","[['more than a third of the samples', 'has', 'correctly']]",[],"[['Results', 'has', 'Document - cue baseline']]",[],[],[],[],[],natural_language_inference,69,261
results,"In the masked setup all baseline models reliant on lexical cues fail in the face of the randomized answer expressions , since the same answer option has different placeholders in different samples .","[('In', (0, 1)), ('reliant on', (7, 9)), ('fail', (11, 12))]","[('masked setup', (2, 4)), ('all baseline models', (4, 7)), ('lexical cues', (9, 11)), ('in the face of the randomized answer expressions', (12, 20))]","[['all baseline models', 'fail', 'in the face of the randomized answer expressions'], ['all baseline models', 'reliant on', 'lexical cues']]","[['masked setup', 'has', 'all baseline models']]","[['Results', 'In', 'masked setup']]",[],[],[],[],[],[],natural_language_inference,69,265
results,Both neural RC models are able to largely retain or even improve their strong performance when answers are masked : they are able to leverage the textual context of the candidate expressions .,"[('are', (4, 5)), ('able to', (5, 7)), ('when', (15, 16))]","[('Both neural RC models', (0, 4)), ('largely retain or even improve', (7, 12)), ('strong performance', (13, 15)), ('answers', (16, 17)), ('masked', (18, 19))]","[['Both neural RC models', 'able to', 'largely retain or even improve'], ['strong performance', 'when', 'answers'], ['answers', 'are', 'masked']]","[['largely retain or even improve', 'has', 'strong performance']]",[],"[['Results', 'has', 'Both neural RC models']]",[],[],[],[],[],natural_language_inference,69,267
results,"In contrast , for the open - domain setting of WIKIHOP , a reduction of the answer vocabulary to 100 random single - token mask expressions clearly helps the model in selecting a candidate span , compared to the multi-token candidate expressions in the unmasked setting .",[],[],"[['open - domain setting', 'of', 'WIKIHOP'], ['reduction', 'of', 'answer vocabulary'], ['answer vocabulary', 'to', '100 random single - token mask expressions'], ['100 random single - token mask expressions', 'helps', 'model'], ['model', 'in selecting', 'candidate span'], ['model', 'compared to', 'multi-token candidate expressions'], ['multi-token candidate expressions', 'in', 'unmasked setting']]","[['open - domain setting', 'has', 'reduction']]","[['Results', 'for', 'open - domain setting']]",[],[],[],[],[],[],natural_language_inference,69,269
research-problem,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,[],"[('EXTRACTIVE QUESTION ANSWERING', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'EXTRACTIVE QUESTION ANSWERING']]",[],[],[],[],natural_language_inference,7,2
research-problem,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",[],"[('answer extraction', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'answer extraction']]",[],[],[],[],natural_language_inference,7,7
model,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .","[('present', (5, 6)), ('called', (10, 11)), ('builds', (13, 14)), ('reusing', (20, 21)), ('for', (23, 24))]","[('novel neural architecture', (7, 10)), ('RASOR', (11, 12)), ('fixed - length span representations', (14, 19)), ('recurrent computations', (21, 23)), ('shared substructures', (24, 26))]","[['novel neural architecture', 'called', 'RASOR'], ['novel neural architecture', 'builds', 'fixed - length span representations'], ['fixed - length span representations', 'reusing', 'recurrent computations'], ['recurrent computations', 'for', 'shared substructures']]",[],"[['Model', 'present', 'novel neural architecture']]",[],[],[],[],[],[],natural_language_inference,7,24
model,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .","[('demonstrate', (1, 2)), ('each of', (5, 7)), ('with', (13, 14)), ('over', (16, 17)), ('leads to', (21, 23)), ('in', (26, 27))]","[('directly classifying', (3, 5)), ('competing spans', (8, 10)), ('training', (12, 13)), ('global normalization', (14, 16)), ('all possible spans', (17, 20)), ('significant increase', (24, 26)), ('performance', (27, 28))]","[['training', 'with', 'global normalization'], ['global normalization', 'over', 'all possible spans'], ['global normalization', 'leads to', 'significant increase'], ['significant increase', 'in', 'performance'], ['directly classifying', 'each of', 'competing spans']]",[],"[['Model', 'demonstrate', 'training'], ['Model', 'demonstrate', 'directly classifying']]",[],[],[],[],[],[],natural_language_inference,7,25
experimental-setup,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,[],[],"[['words', 'using', '300 dimensional GloVe embeddings'], ['300 dimensional GloVe embeddings', 'trained on', 'corpus'], ['corpus', 'of', '840 bn words'], ['words', 'in', 'question and document']]",[],"[['Experimental setup', 'represent', 'words']]",[],[],[],[],[],"[['300 dimensional GloVe embeddings', 'has', 'all out of vocabulary ( OOV ) words']]",natural_language_inference,7,106
experimental-setup,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,"[('cover', (2, 3)), ('projected onto', (16, 18))]","[('200 k words', (3, 6)), ('all out of vocabulary ( OOV ) words', (7, 15)), ('one', (18, 19)), ('1 m randomly initialized 300d embeddings', (20, 26))]","[['all out of vocabulary ( OOV ) words', 'projected onto', 'one']]","[['one', 'of', '1 m randomly initialized 300d embeddings']]",[],[],[],"[['300 dimensional GloVe embeddings', 'cover', '200 k words']]",[],[],[],natural_language_inference,7,107
experimental-setup,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .","[('couple', (1, 2)), ('in', (7, 8)), ('use', (17, 18)), ('to apply', (22, 24)), ('across', (25, 26))]","[('input and forget gates', (3, 7)), ('our LSTMs', (8, 10)), ('single dropout mask', (19, 22)), ('dropout', (24, 25)), ('all LSTM time - steps', (26, 31))]","[['input and forget gates', 'in', 'our LSTMs'], ['single dropout mask', 'to apply', 'dropout'], ['dropout', 'across', 'all LSTM time - steps']]",[],"[['Experimental setup', 'couple', 'input and forget gates'], ['Experimental setup', 'use', 'single dropout mask']]",[],[],[],[],[],[],natural_language_inference,7,108
experimental-setup,Hidden layers in the feed forward neural networks use rectified linear units .,"[('in', (2, 3)), ('use', (8, 9))]","[('Hidden layers', (0, 2)), ('feed forward neural networks', (4, 8)), ('rectified linear units', (9, 12))]","[['Hidden layers', 'in', 'feed forward neural networks'], ['Hidden layers', 'use', 'rectified linear units']]",[],[],"[['Experimental setup', 'has', 'Hidden layers']]",[],[],[],[],[],natural_language_inference,7,109
experimental-setup,Answer candidates are limited to spans with at most 30 words .,"[('limited to', (3, 5)), ('with', (6, 7))]","[('Answer candidates', (0, 2)), ('spans', (5, 6)), ('at most 30 words', (7, 11))]","[['Answer candidates', 'limited to', 'spans'], ['spans', 'with', 'at most 30 words']]",[],[],"[['Experimental setup', 'has', 'Answer candidates']]",[],[],[],[],[],natural_language_inference,7,110
experimental-setup,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",[],[],"[['final model configuration', 'ran', 'grid searches'], ['grid searches', 'over', 'dimensionality'], ['dimensionality', 'of', 'LSTM hidden states'], ['grid searches', 'over', 'width and depth'], ['width and depth', 'of', 'feed forward neural networks'], ['grid searches', 'over', 'dropout'], ['dropout', 'for', 'LSTMs'], ['grid searches', 'over', 'number'], ['number', 'of', 'stacked LSTM layers'], ['grid searches', 'over', 'decay multiplier [ 0.9 , 0.95 , 1.0 ]'], ['decay multiplier [ 0.9 , 0.95 , 1.0 ]', 'multiply', 'learning rate every 10 k steps']]",[],"[['Experimental setup', 'To choose', 'final model configuration']]",[],[],[],[],[],[],natural_language_inference,7,111
experimental-setup,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,[],[],"[['two - layer BiLSTMs', 'for', 'span encoder'], ['two - layer BiLSTMs', 'for', 'passage - independent question representation'], ['learning rate decay', 'of', '5 % every 10 k steps'], ['dropout', 'of', '0.1'], ['best model', 'uses', '50d LSTM states']]",[],[],"[['Experimental setup', 'has', 'two - layer BiLSTMs'], ['Experimental setup', 'has', 'learning rate decay'], ['Experimental setup', 'has', 'dropout'], ['Experimental setup', 'has', 'best model']]",[],[],[],[],[],natural_language_inference,7,112
experimental-setup,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,"[('implemented using', (3, 5)), ('trained on', (8, 10)), ('using', (14, 15)), ('with', (18, 19)), ('of', (22, 23)), ('trained using', (25, 27)), ('on', (31, 32))]","[('models', (1, 2)), ('TensorFlow', (5, 6)), ('SQUAD training set', (11, 14)), ('ADAM optimizer', (16, 18)), ('mini-batch size', (20, 22)), ('4', (23, 24)), ('10 asynchronous training threads', (27, 31)), ('single machine', (33, 35))]","[['models', 'implemented using', 'TensorFlow'], ['models', 'trained on', 'SQUAD training set'], ['SQUAD training set', 'using', 'ADAM optimizer'], ['ADAM optimizer', 'with', 'mini-batch size'], ['mini-batch size', 'of', '4'], ['models', 'trained using', '10 asynchronous training threads'], ['10 asynchronous training threads', 'on', 'single machine']]",[],[],"[['Experimental setup', 'has', 'models']]",[],[],[],[],[],natural_language_inference,7,113
results,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .","[('of', (8, 9)), ('achieves', (13, 14)), ('in terms of', (27, 30)), ('relative to', (35, 37))]","[('RASOR', (12, 13)), ('error reduction', (15, 17)), ('more than 50 %', (18, 22)), ('exact match and F1', (30, 34)), ('human performance upper bound', (38, 42))]","[['RASOR', 'achieves', 'error reduction'], ['error reduction', 'of', 'more than 50 %'], ['more than 50 %', 'relative to', 'human performance upper bound'], ['more than 50 %', 'in terms of', 'exact match and F1']]",[],[],"[['Results', 'has', 'RASOR']]",[],[],[],[],[],natural_language_inference,7,121
results,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .","[('model', (8, 9)), ('leads to', (17, 19)), ('over', (24, 25))]","[('efficiently and explicitly', (5, 8)), ('quadratic number of possible answers', (10, 15)), ('14 % error reduction', (20, 24)), ('best performing Match - LSTM model', (26, 32))]","[['quadratic number of possible answers', 'leads to', '14 % error reduction'], ['14 % error reduction', 'over', 'best performing Match - LSTM model']]","[['efficiently and explicitly', 'has', 'quadratic number of possible answers']]",[],[],[],"[['RASOR', 'model', 'efficiently and explicitly']]",[],[],[],natural_language_inference,7,125
ablation-analysis,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","[('is', (6, 7))]","[('passage - aligned question representation', (1, 6)), ('crucial', (7, 8))]","[['passage - aligned question representation', 'is', 'crucial']]",[],[],"[['Ablation analysis', 'has', 'passage - aligned question representation']]",[],[],[],[],[],natural_language_inference,7,131
ablation-analysis,"First , we observe general improvements when using labels that closely align with the task .","[('observe', (3, 4)), ('when using', (6, 8)), ('that', (9, 10)), ('with', (12, 13))]","[('general improvements', (4, 6)), ('labels', (8, 9)), ('closely align', (10, 12)), ('task', (14, 15))]","[['general improvements', 'when using', 'labels'], ['labels', 'that', 'closely align'], ['closely align', 'with', 'task']]",[],"[['Ablation analysis', 'observe', 'general improvements']]",[],[],[],[],[],[],natural_language_inference,7,157
ablation-analysis,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .","[('using', (12, 13))]","[('interactions between the endpoints', (8, 12)), ('spanlevel FFNN', (14, 16))]","[['interactions between the endpoints', 'using', 'spanlevel FFNN']]",[],[],"[['Ablation analysis', 'observe', 'interactions between the endpoints']]",[],[],[],[],"[['interactions between the endpoints', 'has', 'RASOR']]",natural_language_inference,7,162
ablation-analysis,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .","[('outperforms', (1, 2)), ('by', (6, 7)), ('in', (8, 9))]","[('RASOR', (0, 1)), ('endpoint prediction model', (3, 6)), ('1.1', (7, 8)), ('exact match', (9, 11))]","[['RASOR', 'outperforms', 'endpoint prediction model'], ['endpoint prediction model', 'by', '1.1'], ['1.1', 'in', 'exact match']]",[],[],[],[],[],[],[],[],natural_language_inference,7,163
research-problem,This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .,[],"[('Community Question Answering ( c QA )', (12, 19))]",[],[],[],[],"[['Contribution', 'has research problem', 'Community Question Answering ( c QA )']]",[],[],[],[],natural_language_inference,70,4
research-problem,"In this task , participants are asked to automatically provide good answers in a c QA setting .",[],"[('c QA', (14, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'c QA']]",[],[],[],[],natural_language_inference,70,14
model,All the above subtasks have been modeled as binary classification problems : kernel - based classifiers are trained and the classification score is used to sort the instances and produce the final ranking .,"[('modeled as', (6, 8)), ('are', (16, 17)), ('to sort', (24, 26)), ('produce', (29, 30))]","[('binary classification problems', (8, 11)), ('kernel - based classifiers', (12, 16)), ('trained', (17, 18)), ('classification score', (20, 22)), ('instances', (27, 28)), ('final ranking', (31, 33))]","[['kernel - based classifiers', 'are', 'trained'], ['classification score', 'to sort', 'instances'], ['classification score', 'produce', 'final ranking']]","[['binary classification problems', 'has', 'kernel - based classifiers'], ['kernel - based classifiers', 'has', 'classification score']]","[['Model', 'modeled as', 'binary classification problems']]",[],[],[],[],[],[],natural_language_inference,70,20
model,"All classifiers and kernels have been implemented within the Kernel - based Learning Platform 2 ( KeLP ) , thus determining the team 's name .","[('implemented within', (6, 8))]","[('classifiers and kernels', (1, 4)), ('Kernel - based Learning Platform 2 ( KeLP )', (9, 18))]","[['classifiers and kernels', 'implemented within', 'Kernel - based Learning Platform 2 ( KeLP )']]",[],[],"[['Model', 'has', 'classifiers and kernels']]",[],[],[],[],[],natural_language_inference,70,21
model,"The proposed solution provides three main contributions : ( i ) we employ the approach proposed in , which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees .","[('applies', (19, 20)), ('directly to', (22, 24)), ('modeled as', (28, 30)), ('of', (31, 32))]","[('tree kernels', (20, 22)), ('question and answer texts', (24, 28)), ('pairs', (30, 31)), ('linked syntactic trees', (32, 35))]","[['tree kernels', 'directly to', 'question and answer texts'], ['question and answer texts', 'modeled as', 'pairs'], ['pairs', 'of', 'linked syntactic trees']]",[],"[['Model', 'applies', 'tree kernels']]",[],[],[],[],[],[],natural_language_inference,70,22
model,( iii ) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks .,"[('propose', (4, 5)), ('for', (11, 12)), ('exploit', (16, 17)), ('obtained in', (19, 21))]","[('stacking schema', (6, 8)), ('classifiers', (10, 11)), ('Subtask B and C', (12, 16)), ('inferences', (18, 19)), ('previous subtasks', (22, 24))]","[['classifiers', 'for', 'Subtask B and C'], ['classifiers', 'exploit', 'inferences'], ['inferences', 'obtained in', 'previous subtasks']]","[['stacking schema', 'has', 'classifiers']]","[['Model', 'propose', 'stacking schema']]",[],[],[],[],[],[],natural_language_inference,70,25
experiments,Subtask A,[],"[('Subtask A', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Subtask A']]","[['Subtask A', 'has', 'Results']]",natural_language_inference,70,133
experiments,Results : reports the outcome on Subtask A .,[],"[('Results', (0, 1))]",[],[],[],[],[],[],[],[],[],natural_language_inference,70,139
experiments,"The good results on the 10 fold cross validations are confirmed on the official test set : the model is very accurate and achieved the first position among 12 systems , with the best MAP .","[('achieved', (23, 24)), ('among', (27, 28)), ('with', (31, 32))]","[('first position', (25, 27)), ('12 systems', (28, 30)), ('best MAP', (33, 35))]","[['first position', 'with', 'best MAP'], ['first position', 'among', '12 systems']]",[],[],[],[],"[['Results', 'achieved', 'first position']]",[],[],[],natural_language_inference,70,140
experiments,Subtask B,[],"[('Subtask B', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Subtask B']]","[['Subtask B', 'has', 'Results']]",natural_language_inference,70,143
experiments,"On the official test set , our primary submission achieved the third position w.r.t. MAP among 11 systems .","[('achieved', (9, 10)), ('w.r.t.', (13, 14)), ('among', (15, 16))]","[('our primary submission', (6, 9)), ('third position', (11, 13)), ('MAP', (14, 15)), ('11 systems', (16, 18))]","[['our primary submission', 'achieved', 'third position'], ['third position', 'w.r.t.', 'MAP'], ['third position', 'among', '11 systems']]",[],[],[],[],[],[],"[['Results', 'has', 'our primary submission']]",[],natural_language_inference,70,155
experiments,The primary system achieves the highest F 1 and accuracy on both tuning and test stages .,"[('achieves', (3, 4)), ('on', (10, 11))]","[('primary system', (1, 3)), ('highest F 1 and accuracy', (5, 10)), ('both tuning and test stages', (11, 16))]","[['primary system', 'achieves', 'highest F 1 and accuracy'], ['highest F 1 and accuracy', 'on', 'both tuning and test stages']]",[],[],[],[],[],[],"[['Results', 'has', 'primary system']]",[],natural_language_inference,70,157
experiments,Subtask C Model :,[],"[('Subtask C', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Subtask C']]","[['Subtask C', 'has', 'Results']]",natural_language_inference,70,159
experiments,"Our primary submission achieved the second highest MAP , while our Contrastive 2 is the best result .","[('achieved', (3, 4))]","[('primary submission', (1, 3)), ('second highest MAP', (5, 8))]","[['primary submission', 'achieved', 'second highest MAP']]",[],[],[],[],[],[],"[['Results', 'has', 'primary submission']]",[],natural_language_inference,70,171
experiments,It should be also noted that the F 1 our system is the best among 10 primary submissions .,"[('noted that', (4, 6)), ('is', (11, 12)), ('among', (14, 15))]","[('F 1 our system', (7, 11)), ('best', (13, 14)), ('10 primary submissions', (15, 18))]","[['F 1 our system', 'is', 'best'], ['best', 'among', '10 primary submissions']]",[],[],[],[],"[['Results', 'noted that', 'F 1 our system']]",[],[],[],natural_language_inference,70,172
research-problem,We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .,[],"[('recurrent neural network ( RNN )', (4, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'recurrent neural network ( RNN )']]",[],[],[],[],natural_language_inference,71,4
research-problem,Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) ),[],"[('Recurrent Neural Networks with gating units', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Recurrent Neural Networks with gating units']]",[],[],[],[],natural_language_inference,71,11
research-problem,These works have proven the importance of gating units for Recurrent Neural Networks .,[],"[('gating units for Recurrent Neural Networks', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'gating units for Recurrent Neural Networks']]",[],[],[],[],natural_language_inference,71,13
research-problem,The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .,[],"[('RNNs', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'RNNs']]",[],[],[],[],natural_language_inference,71,14
model,"We propose a new architecture , the Gated Orthogonal Recurrent Unit ( GORU ) , which combines the advantages of the above two frameworks , namely ( i ) the ability to capture long term dependencies by using orthogonal matrices and ( ii ) the ability to "" forget "" by using a GRU structure .",[],[],"[['new architecture', 'combines', 'ability'], ['ability', 'to capture', 'long term dependencies'], ['long term dependencies', 'by using', 'orthogonal matrices'], ['ability', 'to', 'forget'], ['forget', 'by using', 'GRU structure']]","[['new architecture', 'name', 'Gated Orthogonal Recurrent Unit ( GORU )']]","[['Model', 'propose', 'new architecture']]",[],[],[],[],[],[],natural_language_inference,71,29
model,"We demonstrate that GORU is able to learn long term dependencies effectively , even in complicated datasets which require a forgetting ability .","[('demonstrate', (1, 2)), ('able to learn', (5, 8))]","[('GORU', (3, 4)), ('long term dependencies', (8, 11)), ('effectively', (11, 12))]","[['GORU', 'able to learn', 'long term dependencies']]","[['long term dependencies', 'has', 'effectively']]","[['Model', 'demonstrate', 'GORU']]",[],[],[],[],[],[],natural_language_inference,71,30
model,"In this work , we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices .",[],[],"[['implementation', 'of', 'orthogonal transition matrices'], ['orthogonal transition matrices', 'which is', 'subset'], ['subset', 'of', 'unitary matrices']]",[],"[['Model', 'focus on', 'implementation']]",[],[],[],[],[],[],natural_language_inference,71,31
code,"GORU is implemented in Tensorflow , available from https://github.com/jingli9111/GORU-tensorflow",[],"[('https://github.com/jingli9111/GORU-tensorflow', (8, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/jingli9111/GORU-tensorflow']]",[],[],[],[],natural_language_inference,71,103
,The first task we consider is the well known Copying Memory Task .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,105
,"In this experiment , we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 for all models .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,112
,The batch size is set to 128 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,113
,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,115
,The GORU is the only gated - system to successfully solve this task while the GRU and LSTM get stuck at the baseline as shown in .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,120
,Denoise Task,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,121
,"Just as in the previous experiment , we use RM - SProp optimization algorithm with a learning rate of 0.01 and a decay rate of 0.9 for all models .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,128
,The batch size is set to 128 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,129
,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,131
,"EURNN get stuck at the baseline because of lacking forgetting mechanism , while GORU and GRU successfully solve the task .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,132
,Parenthesis Task,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,136
,"In our experiment , the total input length is set to 200 .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,142
,"We used batch size 128 and RMSProp Optimizer with a learning rate 0.001 , decay rate 0.9 on all models .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,143
,"The GORU is able to successfully outperform GRU , LSTM and EURNN in terms of both learning speed and final performances as shown in .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,146
,We also analyzed the activations of the update gates for GORU and GRU .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,147
,Algorithmic Task,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,152
,We used batch size 50 and hidden size 128 for all models .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,156
,The RNNs are trained with RMSProp optimizer with a learning rate of 0.001 and decay rate of 0.9 .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,157
,We found that the GORU performs averagely better than GRU / LSTM and EURNN .,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,71,159
research-problem,CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *,[],"[('Machine Reading Comprehension', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,72,2
research-problem,We present a new dataset for machine comprehension in the medical domain .,[],"[('machine comprehension', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension']]",[],[],[],[],natural_language_inference,72,4
dataset,"For our dataset , we construct queries , answers and supporting passages from BMJ Case Reports , the largest online repository of such documents .","[('construct', (5, 6)), ('from', (12, 13))]","[('queries , answers and supporting passages', (6, 12)), ('BMJ Case Reports', (13, 16))]","[['queries , answers and supporting passages', 'from', 'BMJ Case Reports']]",[],"[['Dataset', 'construct', 'queries , answers and supporting passages']]",[],[],[],[],[],[],natural_language_inference,72,33
dataset,"A case report is a detailed description of a clinical case that focuses on rare diseases , unusual presentation of common conditions and novel treatment methods .","[('is', (3, 4)), ('of', (7, 8)), ('focuses on', (12, 14))]","[('case report', (1, 3)), ('detailed description', (5, 7)), ('clinical case', (9, 11)), ('rare diseases', (14, 16)), ('unusual presentation of common conditions', (17, 22)), ('novel treatment methods', (23, 26))]","[['case report', 'is', 'detailed description'], ['detailed description', 'of', 'clinical case'], ['clinical case', 'focuses on', 'rare diseases'], ['clinical case', 'focuses on', 'unusual presentation of common conditions'], ['clinical case', 'focuses on', 'novel treatment methods']]",[],[],"[['Dataset', 'has', 'case report']]",[],[],[],[],[],natural_language_inference,72,34
dataset,"Each report contains a Learning points section , summarizing the key pieces of information from that report .","[('contains', (2, 3)), ('summarizing', (8, 9)), ('of', (12, 13))]","[('Learning points section', (4, 7)), ('key pieces', (10, 12)), ('information', (13, 14))]","[['Learning points section', 'summarizing', 'key pieces'], ['key pieces', 'of', 'information']]",[],[],[],[],"[['case report', 'contains', 'Learning points section']]",[],[],[],natural_language_inference,72,35
dataset,We use these learning points to create queries by blanking out a medical entity .,"[('use', (1, 2)), ('to create', (5, 7)), ('by blanking out', (8, 11))]","[('learning points', (3, 5)), ('queries', (7, 8)), ('medical entity', (12, 14))]","[['learning points', 'to create', 'queries'], ['queries', 'by blanking out', 'medical entity']]",[],"[['Dataset', 'use', 'learning points']]",[],[],[],[],[],[],natural_language_inference,72,37
dataset,"Our dataset contains around 100,000 queries on 12,000 case reports , has long support passages ( around 1,500 tokens on average ) and includes answers which are single - or multiword medical entities .","[('contains', (2, 3)), ('on', (6, 7)), ('includes', (23, 24)), ('which are', (25, 27))]","[('around 100,000 queries', (3, 6)), ('12,000 case reports', (7, 10)), ('long support passages', (12, 15)), ('answers', (24, 25)), ('single - or multiword medical entities', (27, 33))]","[['long support passages', 'includes', 'answers'], ['answers', 'which are', 'single - or multiword medical entities'], ['around 100,000 queries', 'on', '12,000 case reports']]",[],"[['Dataset', 'contains', 'long support passages'], ['Dataset', 'contains', 'around 100,000 queries']]",[],[],[],[],[],[],natural_language_inference,72,39
baselines,We also include a distance - based method that uses word embeddings ( sim-entity ) .,"[('include', (2, 3)), ('that uses', (8, 10))]","[('distance - based method', (4, 8)), ('word embeddings', (10, 12))]","[['distance - based method', 'that uses', 'word embeddings']]",[],"[['Baselines', 'include', 'distance - based method']]",[],[],[],[],[],[],natural_language_inference,72,149
baselines,We trained a 4 - gram Kneser - Ney model on CliCR training data ( with multi-word entities represented as a single token ) using SRILM .,"[('trained', (1, 2)), ('on', (10, 11)), ('with', (15, 16)), ('represented as', (18, 20)), ('using', (24, 25))]","[('4 - gram Kneser - Ney model', (3, 10)), ('CliCR training data', (11, 14)), ('multi-word entities', (16, 18)), ('single token', (21, 23)), ('SRILM', (25, 26))]","[['4 - gram Kneser - Ney model', 'on', 'CliCR training data'], ['CliCR training data', 'with', 'multi-word entities'], ['multi-word entities', 'represented as', 'single token'], ['CliCR training data', 'using', 'SRILM']]",[],"[['Baselines', 'trained', '4 - gram Kneser - Ney model']]",[],[],[],[],[],[],natural_language_inference,72,157
results,"We see that answer prediction based on contextual representation of queries and passages ( sim -entity ) achieves a strong base performance that is only outperformed by GA 7 In precision , the number of correct words is divided by the number of all predicted words .","[('see that', (1, 3)), ('based on', (5, 7)), ('of', (9, 10)), ('achieves', (17, 18)), ('that is', (22, 24)), ('by', (26, 27))]","[('answer prediction', (3, 5)), ('contextual representation', (7, 9)), ('queries and passages ( sim -entity )', (10, 17)), ('strong base performance', (19, 22)), ('outperformed', (25, 26)), ('GA', (27, 28))]","[['answer prediction', 'based on', 'contextual representation'], ['contextual representation', 'of', 'queries and passages ( sim -entity )'], ['answer prediction', 'achieves', 'strong base performance'], ['strong base performance', 'that is', 'outperformed'], ['outperformed', 'by', 'GA']]",[],"[['Results', 'see that', 'answer prediction']]",[],[],[],[],[],[],natural_language_inference,72,208
results,"The language model performs poorly on EM and F1 , but the embedding - metric score is higher , likely reflecting the fact that the predicted answers - though mostly incorrect - are related to the ground - truth answers .","[('performs', (3, 4)), ('on', (5, 6)), ('is', (16, 17))]","[('language model', (1, 3)), ('poorly', (4, 5)), ('EM and F1', (6, 9)), ('embedding - metric score', (12, 16)), ('higher', (17, 18))]","[['embedding - metric score', 'is', 'higher'], ['language model', 'performs', 'poorly'], ['poorly', 'on', 'EM and F1']]",[],[],"[['Results', 'has', 'embedding - metric score'], ['Results', 'has', 'language model']]",[],[],[],[],[],natural_language_inference,72,211
results,"The GA reader performs well across all entity set - ups , even when the entities are not marked in the passage .","[('performs', (3, 4)), ('across', (5, 6))]","[('GA reader', (1, 3)), ('well', (4, 5)), ('all entity set - ups', (6, 11))]","[['GA reader', 'performs', 'well'], ['well', 'across', 'all entity set - ups']]",[],[],"[['Results', 'has', 'GA reader']]",[],[],[],[],[],natural_language_inference,72,213
results,"Upon inspecting the predicted answers more closely , we have observed that GA - NoEnt tends to predict longer answers than GA - Ent / Anonym .","[('observed that', (10, 12)), ('tends to predict', (15, 18)), ('than', (20, 21))]","[('GA - NoEnt', (12, 15)), ('longer answers', (18, 20)), ('GA - Ent / Anonym', (21, 26))]","[['GA - NoEnt', 'tends to predict', 'longer answers'], ['longer answers', 'than', 'GA - Ent / Anonym']]",[],"[['Results', 'observed that', 'GA - NoEnt']]",[],[],[],[],[],[],natural_language_inference,72,215
results,The results for SA reader are far below the per-formance of GA reader .,"[('are', (5, 6)), ('of', (10, 11))]","[('SA reader', (3, 5)), ('far below', (6, 8)), ('per-formance', (9, 10)), ('GA reader', (11, 13))]","[['SA reader', 'are', 'far below'], ['per-formance', 'of', 'GA reader']]","[['far below', 'has', 'per-formance']]",[],"[['Results', 'has', 'SA reader']]",[],[],[],[],[],natural_language_inference,72,223
results,We also see that it performs much better on anonymized entities than on non-anonymized ones .,"[('performs', (5, 6)), ('on', (8, 9)), ('than on', (11, 13))]","[('much better', (6, 8)), ('anonymized entities', (9, 11)), ('non-anonymized ones', (13, 15))]","[['much better', 'than on', 'non-anonymized ones'], ['much better', 'on', 'anonymized entities']]",[],[],[],[],"[['GA reader', 'performs', 'much better']]",[],[],[],natural_language_inference,72,224
research-problem,Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .,[],"[('attention mechanisms', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'attention mechanisms']]",[],[],[],[],natural_language_inference,73,14
model,"In this paper , we first propose a novel hard attention mechanism called "" reinforced sequence sampling ( RSS ) "" , which selects tokens from an input sequence in parallel , and differs from existing ones in that it is highly parallelizable without any recurrent structure .","[('propose', (6, 7)), ('called', (12, 13)), ('selects', (23, 24)), ('from', (25, 26)), ('in', (29, 30)), ('is', (40, 41)), ('without', (43, 44))]","[('novel hard attention mechanism', (8, 12)), ('reinforced sequence sampling ( RSS )', (14, 20)), ('tokens', (24, 25)), ('input sequence', (27, 29)), ('parallel', (30, 31)), ('highly parallelizable', (41, 43)), ('any recurrent structure', (44, 47))]","[['novel hard attention mechanism', 'selects', 'tokens'], ['tokens', 'from', 'input sequence'], ['input sequence', 'in', 'parallel'], ['novel hard attention mechanism', 'called', 'reinforced sequence sampling ( RSS )'], ['novel hard attention mechanism', 'is', 'highly parallelizable'], ['highly parallelizable', 'without', 'any recurrent structure']]",[],"[['Model', 'propose', 'novel hard attention mechanism']]",[],[],[],[],[],[],natural_language_inference,73,36
model,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the RSS with a soft self - attention .","[('develop', (2, 3)), ('combines', (18, 19)), ('with', (21, 22))]","[('reinforced self - attention ( ReSA )', (7, 14)), ('RSS', (20, 21)), ('soft self - attention', (23, 27))]","[['reinforced self - attention ( ReSA )', 'combines', 'RSS'], ['RSS', 'with', 'soft self - attention']]",[],"[['Model', 'develop', 'reinforced self - attention ( ReSA )']]",[],[],[],[],[],[],natural_language_inference,73,37
model,"In ReSA , two parameter - untied RSS are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .","[('In', (0, 1)), ('applied to', (10, 12)), ('of', (14, 15))]","[('ReSA', (1, 2)), ('two parameter - untied RSS', (3, 8)), ('two copies', (12, 14)), ('input sequence', (16, 18))]","[['two parameter - untied RSS', 'applied to', 'two copies'], ['two copies', 'of', 'input sequence']]","[['ReSA', 'has', 'two parameter - untied RSS']]","[['Model', 'In', 'ReSA']]",[],[],[],[],[],[],natural_language_inference,73,38
model,Re SA only models the sparse dependencies between the head and dependent tokens selected by the two RSS modules .,"[('models', (3, 4)), ('between', (7, 8)), ('selected by', (13, 15))]","[('Re SA', (0, 2)), ('sparse dependencies', (5, 7)), ('head and dependent tokens', (9, 13)), ('two RSS modules', (16, 19))]","[['Re SA', 'models', 'sparse dependencies'], ['sparse dependencies', 'between', 'head and dependent tokens'], ['head and dependent tokens', 'selected by', 'two RSS modules']]",[],[],"[['Model', 'has', 'Re SA']]",[],[],[],[],[],natural_language_inference,73,39
model,"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .","[('build', (3, 4)), ('based on', (21, 23)), ('without', (24, 25))]","[('sentence - encoding model', (5, 9)), ('reinforced self - attention network ( ReSAN )', (11, 19)), ('ReSA', (23, 24)), ('any CNN / RNN structure', (25, 30))]","[['sentence - encoding model', 'based on', 'ReSA'], ['ReSA', 'without', 'any CNN / RNN structure']]","[['sentence - encoding model', 'name', 'reinforced self - attention network ( ReSAN )']]","[['Model', 'build', 'sentence - encoding model']]",[],[],[],[],[],[],natural_language_inference,73,40
code,All the experiments codes are released at https://github.com/ taoshen58/DiSAN /tree/master/ReSAN .,[],"[('https://github.com/ taoshen58/DiSAN /tree/master/ReSAN', (7, 10))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/ taoshen58/DiSAN /tree/master/ReSAN']]",[],[],[],[],natural_language_inference,73,44
experimental-setup,All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti .,"[('conducted in', (3, 5)), ('with', (6, 7)), ('run on', (9, 11))]","[('experiments', (1, 2)), ('Python', (5, 6)), ('Tensorflow', (7, 8)), ('Nvidia GTX 1080 Ti', (12, 16))]","[['experiments', 'conducted in', 'Python'], ['Python', 'with', 'Tensorflow'], ['experiments', 'run on', 'Nvidia GTX 1080 Ti']]",[],[],"[['Experimental setup', 'has', 'experiments']]",[],[],[],[],[],natural_language_inference,73,177
experimental-setup,"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .","[('use', (1, 2)), ('as', (3, 4)), ('performs', (7, 8)), ('than', (10, 11)), ('on', (12, 13))]","[('Adadelta', (2, 3)), ('optimizer', (4, 5)), ('more stable', (8, 10)), ('Adam', (11, 12)), ('ReSAN', (13, 14))]","[['Adadelta', 'performs', 'more stable'], ['more stable', 'than', 'Adam'], ['more stable', 'on', 'ReSAN'], ['Adadelta', 'as', 'optimizer']]",[],"[['Experimental setup', 'use', 'Adadelta']]",[],[],[],[],[],[],natural_language_inference,73,178
experimental-setup,We use 300D Glo Ve 6B pre-trained vectors,[],"[('300D Glo Ve 6B pre-trained vectors', (2, 8))]",[],[],[],"[['Experimental setup', 'use', '300D Glo Ve 6B pre-trained vectors']]",[],[],[],[],[],natural_language_inference,73,180
,Natural Language Inference,[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,73,185
,"Compared to the methods from official leaderboard , ReSAN outperforms all the sentence encoding based methods and achieves the best test accuracy .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,73,193
experiments,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters with better performance .","[('compared to', (2, 4)), ('i.e.', (9, 10)), ('uses', (22, 23)), ('with', (26, 27))]","[('last best models', (5, 8)), ('600D Gumbel TreeLSTM encoders', (11, 15)), ('600D Residual stacked encoders', (16, 20)), ('ReSAN', (21, 22)), ('far fewer parameters', (23, 26)), ('better performance', (27, 29))]","[['ReSAN', 'with', 'better performance'], ['ReSAN', 'uses', 'far fewer parameters'], ['last best models', 'i.e.', '600D Gumbel TreeLSTM encoders'], ['last best models', 'i.e.', '600D Residual stacked encoders']]","[['last best models', 'has', 'ReSAN']]",[],[],[],"[['Results', 'compared to', 'last best models']]",[],[],[],natural_language_inference,73,194
experiments,"Furthermore , ReSAN even outperforms the 300D SPINN - PI encoders by 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .","[('by', (11, 12))]","[('ReSAN', (2, 3)), ('outperforms', (4, 5)), ('300D SPINN - PI encoders', (6, 11))]",[],"[['ReSAN', 'has', 'outperforms'], ['outperforms', 'has', '300D SPINN - PI encoders']]",[],[],[],[],"[['300D SPINN - PI encoders', 'by', '3.1 %']]","[['Results', 'has', 'ReSAN']]",[],natural_language_inference,73,196
experiments,"Compared to the recurrent models ( e.g. , Bi - LSTM and Bi - GRU ) , ReSAN shows better prediction quality and more compelling efficiency due to parallelizable computations .","[('Compared to', (0, 2)), ('e.g.', (6, 7)), ('shows', (18, 19)), ('due to', (26, 28))]","[('recurrent models', (3, 5)), ('Bi - LSTM', (8, 11)), ('Bi - GRU', (12, 15)), ('ReSAN', (17, 18)), ('better prediction quality', (19, 22)), ('more compelling efficiency', (23, 26)), ('parallelizable computations', (28, 30))]","[['recurrent models', 'due to', 'parallelizable computations'], ['ReSAN', 'shows', 'better prediction quality'], ['ReSAN', 'shows', 'more compelling efficiency'], ['recurrent models', 'e.g.', 'Bi - LSTM'], ['recurrent models', 'e.g.', 'Bi - GRU']]","[['parallelizable computations', 'has', 'ReSAN']]",[],[],[],"[['Results', 'Compared to', 'recurrent models']]",[],[],[],natural_language_inference,73,198
experiments,"Compared to the convolutional models ( i.e. , Multiwindow CNN and Hierarchical CNN ) , ReSAN significantly outperforms them by 3.1 % and 2.4 % respectively due to the weakness of CNNs in modeling long - range dependencies .","[('i.e.', (6, 7)), ('by', (19, 20))]","[('convolutional models', (3, 5)), ('Multiwindow CNN', (8, 10)), ('Hierarchical CNN', (11, 13)), ('ReSAN', (15, 16)), ('significantly outperforms', (16, 18)), ('3.1 % and 2.4 %', (20, 25))]","[['significantly outperforms', 'by', '3.1 % and 2.4 %'], ['convolutional models', 'i.e.', 'Multiwindow CNN'], ['convolutional models', 'i.e.', 'Hierarchical CNN']]","[['convolutional models', 'has', 'ReSAN'], ['ReSAN', 'has', 'significantly outperforms']]",[],[],[],[],[],"[['Results', 'Compared to', 'convolutional models']]",[],natural_language_inference,73,199
experiments,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a similar number of parameters with better test performance and less time cost .","[('uses', (14, 15)), ('with', (20, 21))]","[('attention - based models', (3, 7)), ('multi-head attention', (8, 10)), ('DiSAN', (11, 12)), ('ReSAN', (13, 14)), ('similar number of parameters', (16, 20)), ('better test performance', (21, 24)), ('less time cost', (25, 28))]","[['ReSAN', 'with', 'better test performance'], ['ReSAN', 'with', 'less time cost'], ['ReSAN', 'uses', 'similar number of parameters']]","[['attention - based models', 'name', 'multi-head attention'], ['attention - based models', 'name', 'DiSAN'], ['attention - based models', 'has', 'ReSAN']]",[],[],[],[],[],"[['Results', 'Compared to', 'attention - based models']]",[],natural_language_inference,73,200
ablation-analysis,"In terms of prediction quality , the results show that 1 ) the unselected head tokens do contribute to the prediction , bringing 0.2 % improvement ; 2 ) using separate RSS modules to select the head and dependent tokens improves accuracy by 0.5 % ; and 3 ) hard attention and soft self - attention modules improve the accuracy by 0.3 % and 2.9 % respectively .",[],[],"[['prediction quality', 'using', 'hard attention and soft self - attention modules'], ['hard attention and soft self - attention modules', 'improve', 'accuracy'], ['accuracy', 'by', '0.3 % and 2.9 %'], ['prediction quality', 'using', 'separate RSS modules'], ['separate RSS modules', 'improves', 'accuracy'], ['accuracy', 'by', '0.5 %'], ['separate RSS modules', 'to select', 'head and dependent tokens'], ['unselected head tokens', 'contribute to', 'prediction'], ['unselected head tokens', 'bringing', '0.2 % improvement']]","[['prediction quality', 'has', 'unselected head tokens']]","[['Ablation analysis', 'In terms of', 'prediction quality']]",[],[],[],[],[],[],natural_language_inference,73,204
research-problem,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,[],"[('Natural Language Inference', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,74,2
research-problem,"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .",[],"[('Natural Language Inference ( NLI )', (0, 6)), ('Recognizing Textual Entailment ( RTE )', (10, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference ( NLI )'], ['Contribution', 'has research problem', 'Recognizing Textual Entailment ( RTE )']]",[],[],[],[],natural_language_inference,74,4
research-problem,"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .",[],"[('NLI', (33, 34))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,74,6
model,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .","[('propose', (5, 6)), ('for', (11, 12)), ('where', (16, 17)), ('from', (21, 22)), ('to', (34, 35))]","[('Discourse Marker Augmented Network', (7, 11)), ('natural language inference', (12, 15)), ('transfer', (18, 19)), ('knowledge', (20, 21)), ('existing supervised task', (23, 26)), ('Discourse Marker Prediction ( DMP )', (27, 33)), ('integrated NLI model', (36, 39))]","[['Discourse Marker Augmented Network', 'for', 'natural language inference'], ['Discourse Marker Augmented Network', 'where', 'transfer'], ['knowledge', 'from', 'existing supervised task'], ['existing supervised task', 'to', 'integrated NLI model']]","[['transfer', 'has', 'knowledge'], ['existing supervised task', 'name', 'Discourse Marker Prediction ( DMP )']]","[['Model', 'propose', 'Discourse Marker Augmented Network']]",[],[],[],[],[],[],natural_language_inference,74,34
model,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,"[('that learns', (7, 9)), ('of', (11, 12)), ('from', (14, 15)), ('inject', (20, 21)), ('to', (23, 24))]","[('sentence encoder model', (4, 7)), ('representations', (10, 11)), ('sentences', (13, 14)), ('DMP task', (16, 18)), ('encoder', (22, 23)), ('NLI network', (25, 27))]","[['sentence encoder model', 'that learns', 'representations'], ['representations', 'of', 'sentences'], ['sentences', 'from', 'DMP task'], ['representations', 'inject', 'encoder'], ['encoder', 'to', 'NLI network']]",[],[],"[['Model', 'propose', 'sentence encoder model']]",[],[],[],[],[],natural_language_inference,74,35
model,"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model .","[('of', (2, 3)), ('employ', (16, 17)), ('with', (19, 20)), ('defined by', (22, 24)), ('to train', (31, 33))]","[('reinforcement learning', (17, 19)), ('reward', (21, 22)), ('uniformity extent', (25, 27)), ('original labels', (29, 31)), ('model', (34, 35))]","[['reinforcement learning', 'with', 'reward'], ['reward', 'defined by', 'uniformity extent'], ['uniformity extent', 'of', 'original labels'], ['uniformity extent', 'to train', 'model']]",[],"[['Model', 'employ', 'reinforcement learning']]",[],[],[],[],[],[],natural_language_inference,74,37
experimental-setup,We use the Stanford CoreNLP toolkit to tokenize the words and generate POS and NER tags .,"[('use', (1, 2)), ('to tokenize', (6, 8)), ('generate', (11, 12))]","[('Stanford CoreNLP toolkit', (3, 6)), ('words', (9, 10)), ('POS and NER tags', (12, 16))]","[['Stanford CoreNLP toolkit', 'to tokenize', 'words'], ['Stanford CoreNLP toolkit', 'generate', 'POS and NER tags']]",[],"[['Experimental setup', 'use', 'Stanford CoreNLP toolkit']]",[],[],[],[],[],[],natural_language_inference,74,153
experimental-setup,"The word embeddings are initialized by 300d Glove , the dimensions of POS and NER embeddings are 30 and 10 .","[('are', (3, 4)), ('initialized by', (4, 6)), ('of', (11, 12))]","[('word embeddings', (1, 3)), ('300d Glove', (6, 8)), ('dimensions', (10, 11)), ('POS and NER embeddings', (12, 16)), ('30 and 10', (17, 20))]","[['word embeddings', 'initialized by', '300d Glove'], ['dimensions', 'of', 'POS and NER embeddings'], ['POS and NER embeddings', 'are', '30 and 10']]",[],[],"[['Experimental setup', 'has', 'word embeddings'], ['Experimental setup', 'has', 'dimensions']]",[],[],[],[],[],natural_language_inference,74,154
experimental-setup,We apply Tensorflow r 1.3 as our neural network framework .,"[('as', (5, 6))]","[('Tensorflow r 1.3', (2, 5)), ('our neural network framework', (6, 10))]","[['Tensorflow r 1.3', 'as', 'our neural network framework']]",[],[],"[['Experimental setup', 'apply', 'Tensorflow r 1.3']]",[],[],[],[],[],natural_language_inference,74,156
experimental-setup,"We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9 , the decay rate as 0.97 for every 5000 step .",[],[],"[['hidden size', 'as', '300'], ['300', 'for', 'all the LSTM layers'], ['decay rate', 'as', '0.97'], ['0.97', 'for', 'every 5000 step'], ['dropout', 'with', 'initial ratio'], ['initial ratio', 'of', '0.9'], ['dropout', 'between', 'layers']]",[],"[['Experimental setup', 'apply', 'decay rate'], ['Experimental setup', 'apply', 'dropout']]","[['Experimental setup', 'set', 'hidden size']]",[],[],[],[],[],natural_language_inference,74,157
experimental-setup,We use the AdaDelta for optimization as described in with ? as 0.95 and as 1 e - 8 .,"[('for', (4, 5)), ('with', (9, 10))]","[('AdaDelta', (3, 4)), ('optimization', (5, 6)), ('? as 0.95 and as 1 e - 8', (10, 19))]","[['AdaDelta', 'with', '? as 0.95 and as 1 e - 8'], ['AdaDelta', 'for', 'optimization']]",[],[],"[['Experimental setup', 'use', 'AdaDelta']]",[],[],[],[],[],natural_language_inference,74,158
experimental-setup,We set our batch size as 36 and the initial learning rate as 0.6 .,[],[],"[['our batch size', 'as', '36'], ['initial learning rate', 'as', '0.6']]",[],"[['Experimental setup', 'set', 'our batch size'], ['Experimental setup', 'set', 'initial learning rate']]",[],[],[],[],[],[],natural_language_inference,74,159
experimental-setup,"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .",[],[],"[['DMP task', 'use', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'as', '0.1'], ['anneal', 'by', 'half'], ['validation accuracy', 'is', 'lower'], ['lower', 'than', 'previous epoch']]","[['DMP task', 'has', 'anneal'], ['anneal', 'has', 'each time'], ['each time', 'has', 'validation accuracy']]",[],[],[],[],[],[],[],natural_language_inference,74,162
experimental-setup,"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .","[('is', (4, 5)), ('set to', (5, 7))]","[('number of epochs', (1, 4)), ('10', (8, 9)), ('feedforward dropout rate', (12, 15)), ('0.2', (16, 17))]","[['number of epochs', 'set to', '10'], ['feedforward dropout rate', 'is', '0.2']]",[],[],[],[],[],[],"[['DMP task', 'has', 'number of epochs'], ['DMP task', 'has', 'feedforward dropout rate']]",[],natural_language_inference,74,163
results,"Obviously , the performance of most of the integrated methods are better than the sentence encoding based models above .","[('of', (4, 5)), ('are', (10, 11)), ('than', (12, 13))]","[('performance', (3, 4)), ('most of the integrated methods', (5, 10)), ('better', (11, 12)), ('sentence encoding based models', (14, 18))]","[['performance', 'of', 'most of the integrated methods'], ['most of the integrated methods', 'are', 'better'], ['better', 'than', 'sentence encoding based models']]",[],[],"[['Results', 'has', 'performance']]",[],[],[],[],[],natural_language_inference,74,169
results,"The performance of our model achieves 89.6 % on SNLI , 80.3 % on matched MultiNLI and 79.4 % on mismatched MultiNLI , which are all state - of - the - art results .",[],[],"[['our model', 'achieves', '89.6 %'], ['89.6 %', 'on', 'SNLI'], ['our model', 'achieves', '80.3 %'], ['80.3 %', 'on', 'matched MultiNLI'], ['our model', 'achieves', '79.4 %'], ['79.4 %', 'on', 'mismatched MultiNLI'], ['our model', 'achieves', 'all state - of - the - art results']]",[],[],[],[],[],[],"[['performance', 'of', 'our model']]",[],natural_language_inference,74,172
ablation-analysis,"As shown in , we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model .","[('in', (2, 3)), ('conduct', (5, 6)), ('on', (9, 10))]","[('an ablation experiment', (6, 9)), ('SNLI development dataset', (10, 13))]","[['an ablation experiment', 'on', 'SNLI development dataset']]",[],"[['Ablation analysis', 'conduct', 'an ablation experiment']]",[],[],[],[],[],"[['SNLI development dataset', 'has', 'result']]",natural_language_inference,74,174
ablation-analysis,"The result is obviously not satisfactory , which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large - scale datasets .",[],[],"[['result', 'only using', 'sentence embedding'], ['sentence embedding', 'from', 'discourse markers'], ['sentence embedding', 'is', 'not ideal'], ['sentence embedding', 'to predict', 'answer'], ['result', 'is', 'not satisfactory']]","[['not ideal', 'in', 'large - scale datasets']]",[],[],[],[],[],[],[],natural_language_inference,74,176
ablation-analysis,"We then remove the sentence encoder model , which means we do n't use the knowledge transferred from the DMP task and thus the representations r p and r hare set to be zero vectors in the equation ( 6 ) and the equation .","[('remove', (2, 3)), ('to', (31, 32))]","[('sentence encoder model', (4, 7))]",[],[],[],[],[],"[['SNLI development dataset', 'remove', 'sentence encoder model']]",[],[],[],natural_language_inference,74,177
ablation-analysis,"We observe that the performance drops significantly to 87 . 24 % , which is nearly 1.5 % to our DMAN model , which indicates that the discourse markers have deep connections with the logical relations between two sentences they links .","[('observe that', (1, 3))]","[('performance', (4, 5)), ('drops significantly', (5, 7)), ('87 . 24 %', (8, 12))]",[],"[['performance', 'has', 'drops significantly'], ['drops significantly', 'to', '87 . 24 %']]",[],[],[],"[['sentence encoder model', 'observe that', 'performance']]",[],[],[],natural_language_inference,74,178
ablation-analysis,"we remove the character - level embedding and the POS and NER features , the performance drops a lot .","[('remove', (1, 2))]","[('character - level embedding', (3, 7)), ('POS and NER features', (9, 13)), ('performance', (15, 16)), ('drops a lot', (16, 19))]","[['drops a lot', 'remove', 'character - level embedding'], ['drops a lot', 'remove', 'POS and NER features']]","[['performance', 'has', 'drops a lot']]",[],[],[],[],[],"[['SNLI development dataset', 'has', 'performance']]",[],natural_language_inference,74,181
ablation-analysis,The exact match feature also demonstrates its effectiveness in the ablation result .,"[('demonstrates', (5, 6)), ('in', (8, 9))]","[('exact match feature', (1, 4)), ('effectiveness', (7, 8)), ('ablation result', (10, 12))]","[['exact match feature', 'demonstrates', 'effectiveness'], ['effectiveness', 'in', 'ablation result']]",[],[],[],[],[],[],"[['SNLI development dataset', 'has', 'exact match feature']]",[],natural_language_inference,74,183
ablation-analysis,"Finally , we ablate the reinforcement learning part , in other words , we only use the original loss function to optimize the model ( set ? = 1 ) .","[('ablate', (3, 4))]","[('reinforcement learning part', (5, 8))]",[],[],[],[],[],"[['SNLI development dataset', 'ablate', 'reinforcement learning part']]",[],[],"[['reinforcement learning part', 'has', 'result']]",natural_language_inference,74,184
ablation-analysis,"The result drops about 0.5 % , which proves that it is helpful to utilize all the information from the annotators .",[],"[('result', (1, 2)), ('drops about 0.5 %', (2, 6))]",[],"[['result', 'has', 'drops about 0.5 %']]",[],[],[],[],[],[],[],natural_language_inference,74,185
research-problem,Directly reading documents and being able to answer questions from them is an unsolved challenge .,[],"[('Directly reading documents', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Directly reading documents']]",[],[],[],[],natural_language_inference,75,4
research-problem,"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .",[],"[('question answering ( QA )', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,75,5
research-problem,"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .",[],"[('QA', (24, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,75,8
model,"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .","[('propose', (4, 5)), ('generalizes', (23, 24))]","[('Key - Value Memory Network ( KV - MemNN )', (6, 16)), ('new neural network architecture', (18, 22)), ('original Memory Network', (25, 28))]","[['new neural network architecture', 'generalizes', 'original Memory Network']]","[['Key - Value Memory Network ( KV - MemNN )', 'has', 'new neural network architecture']]","[['Model', 'propose', 'Key - Value Memory Network ( KV - MemNN )']]",[],[],[],[],[],[],natural_language_inference,75,25
model,The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,"[('performs', (4, 5)), ('storing', (8, 9)), ('in', (10, 11))]","[('KV - MemNN', (1, 4)), ('QA', (5, 6)), ('facts', (9, 10)), ('value structured memory', (14, 17))]","[['KV - MemNN', 'performs', 'QA'], ['QA', 'storing', 'facts'], ['facts', 'in', 'value structured memory']]",[],[],"[['Model', 'has', 'KV - MemNN']]",[],[],[],[],[],natural_language_inference,75,26
model,"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .","[('is', (2, 3)), ('to use', (9, 11)), ('to address', (12, 14)), ('with respect to', (16, 19))]","[('memory', (1, 2)), ('designed', (3, 4)), ('keys', (11, 12)), ('relevant memories', (14, 16)), ('question', (20, 21))]","[['memory', 'is', 'designed'], ['designed', 'to use', 'keys'], ['keys', 'to address', 'relevant memories'], ['relevant memories', 'with respect to', 'question']]",[],[],"[['Model', 'has', 'memory']]",[],[],[],[],[],natural_language_inference,75,27
model,"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .","[('encode', (6, 7)), ('to leverage', (14, 16)), ('between', (19, 20))]","[('prior knowledge', (7, 9)), ('complex transforms', (17, 19)), ('keys and values', (20, 23))]","[['complex transforms', 'between', 'keys and values']]",[],"[['Model', 'encode', 'prior knowledge'], ['Model', 'to leverage', 'complex transforms']]",[],[],[],[],[],[],natural_language_inference,75,28
results,WikiMovies,[],"[('WikiMovies', (0, 1))]",[],[],[],"[['Results', 'has', 'WikiMovies']]",[],[],[],[],"[['WikiMovies', 'has', 'Key - Value Memory Networks']]",natural_language_inference,75,161
results,"However , Key - Value Memory Networks outperform all other methods on all three data source types .",[],"[('Key - Value Memory Networks', (2, 7)), ('outperform', (7, 8)), ('all other methods', (8, 11))]",[],"[['Key - Value Memory Networks', 'has', 'outperform'], ['outperform', 'has', 'all other methods'], ['Key - Value Memory Networks', 'has', 'outperform']]",[],[],[],[],[],[],[],natural_language_inference,75,170
results,"Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .","[('from', (1, 2))]","[('Reading', (0, 1)), ('Wikipedia documents', (2, 4)), ('outperforms', (8, 9)), ('IE - based KB ( IE )', (10, 17))]","[['Reading', 'from', 'Wikipedia documents']]","[['Wikipedia documents', 'has', 'outperforms'], ['outperforms', 'has', 'IE - based KB ( IE )']]",[],[],[],[],[],"[['WikiMovies', 'has', 'Reading']]",[],natural_language_inference,75,171
results,WikiQA,[],"[('WikiQA', (0, 1))]",[],[],[],"[['Results', 'has', 'WikiQA']]",[],[],[],[],"[['WikiQA', 'has', 'Key - Value Memory Networks']]",natural_language_inference,75,187
results,"Key - Value Memory Networks outperform a large set of other methods , although the results of the L.D.C. method of are very similar .",[],"[('Key - Value Memory Networks', (0, 5)), ('outperform', (5, 6)), ('large set of other methods', (7, 12))]",[],"[['outperform', 'has', 'large set of other methods']]",[],[],[],[],[],[],[],natural_language_inference,75,202
research-problem,"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) .",[],"[('Recognizing textual entailment ( RTE )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Recognizing textual entailment ( RTE )']]",[],[],[],[],natural_language_inference,76,16
research-problem,"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems .",[],"[('RTE', (40, 41))]",[],[],[],[],"[['Contribution', 'has research problem', 'RTE']]",[],[],[],[],natural_language_inference,76,17
model,"In contrast , we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise .",[],[],"[['attentive neural network', 'capable of', 'reasoning'], ['reasoning', 'over', 'entailments'], ['entailments', 'of', 'pairs'], ['pairs', 'of', 'words and phrases'], ['entailments', 'by processing', 'hypothesis'], ['hypothesis', 'conditioned on', 'premise'], ['reasoning', 'over', 'entailments'], ['entailments', 'of', 'pairs'], ['pairs', 'of', 'words and phrases']]",[],"[['Model', 'proposing', 'attentive neural network']]",[],[],[],[],[],[],natural_language_inference,76,25
model,"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .",[],[],"[['neural word - by - word attention mechanism', 'to encourage', 'reasoning'], ['neural model', 'based on', 'LSTMs'], ['neural model', 'reads', 'two sentences'], ['two sentences', 'in', 'one go']]",[],"[['Model', 'extend', 'neural word - by - word attention mechanism'], ['Model', 'present', 'neural model']]",[],[],[],[],[],[],natural_language_inference,76,26
results,We found that processing the hypothesis conditioned on the premise instead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al. 's LSTM .,"[('processing', (3, 4)), ('conditioned on', (6, 8)), ('of', (11, 12)), ('gives', (16, 17)), ('in', (23, 24)), ('over', (25, 26))]","[('hypothesis', (5, 6)), ('premise', (9, 10)), ('improvement', (18, 19)), ('3.3 percentage points', (20, 23)), ('accuracy', (24, 25)), (""Bowman et al. 's LSTM"", (26, 31))]","[['hypothesis', 'conditioned on', 'premise'], ['hypothesis', 'gives', 'improvement'], ['improvement', 'of', '3.3 percentage points'], ['3.3 percentage points', 'over', ""Bowman et al. 's LSTM""], ['3.3 percentage points', 'in', 'accuracy']]",[],"[['Results', 'processing', 'hypothesis']]",[],[],[],[],[],[],natural_language_inference,76,99
results,Our LSTM outperforms a simple lexicalized classifier by 2.7 percentage points .,"[('by', (7, 8))]","[('Our LSTM', (0, 2)), ('outperforms', (2, 3)), ('simple lexicalized classifier', (4, 7)), ('2.7 percentage points', (8, 11))]","[['outperforms', 'by', '2.7 percentage points']]","[['Our LSTM', 'has', 'outperforms'], ['outperforms', 'has', 'simple lexicalized classifier']]",[],"[['Results', 'has', 'Our LSTM']]",[],[],[],[],[],natural_language_inference,76,105
results,"By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159 , and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding ( one for the premise and one for the hypothesis conditioned on the representation of the premise ) .",[],[],"[['attention mechanism', 'found', '0.9 percentage point'], ['improvement', 'over', 'single LSTM'], ['attention mechanism', 'found', '1.4 percentage point'], ['increase', 'over', 'benchmark model'], ['benchmark model', 'uses', 'two LSTMs'], ['two LSTMs', 'for', 'conditional encoding']]","[['0.9 percentage point', 'has', 'improvement'], ['1.4 percentage point', 'has', 'increase']]","[['Results', 'incorporating', 'attention mechanism']]",[],[],[],[],[],[],natural_language_inference,76,108
results,Enabling the model to attend over output vectors of the premise for every word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise .,[],[],"[['model', 'attend', 'output vectors'], ['output vectors', 'of', 'premise'], ['output vectors', 'yields', 'another 1.2 percentage point'], ['improvement', 'compared to', 'attending'], ['attending', 'based only on', 'last output vector'], ['last output vector', 'of', 'premise'], ['output vectors', 'for', 'every word'], ['every word', 'in', 'hypothesis']]","[['another 1.2 percentage point', 'has', 'improvement']]","[['Results', 'Enabling', 'model']]",[],[],[],[],[],[],natural_language_inference,76,114
results,Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE .,"[('Allowing', (0, 1)), ('attend', (5, 6)), ('based on', (9, 11)), ('not', (14, 15)), ('for', (19, 20))]","[('model', (2, 3)), ('hypothesis', (8, 9)), ('premise', (12, 13)), ('improve', (17, 18)), ('performance', (18, 19)), ('RTE', (20, 21))]","[['model', 'attend', 'hypothesis'], ['hypothesis', 'not', 'improve'], ['performance', 'for', 'RTE'], ['hypothesis', 'based on', 'premise']]","[['improve', 'has', 'performance']]","[['Results', 'Allowing', 'model']]",[],[],[],[],[],[],natural_language_inference,76,117
research-problem,Making Neural QA as Simple as Possible but not Simpler,[],"[('Neural QA', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural QA']]",[],[],[],[],natural_language_inference,77,2
research-problem,Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .,[],"[('question answering ( QA )', (6, 11)), ('QA', (25, 26))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )'], ['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,77,4
research-problem,Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .,[],"[('Question answering', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Question answering']]",[],[],[],[],natural_language_inference,77,11
model,"In particular , we develop a simple neural , bag - of - words ( BoW ) - and a recurrent neural network ( RNN ) baseline , namely FastQA .","[('develop', (4, 5)), ('namely', (28, 29))]","[('neural , bag - of - words ( BoW )', (7, 17)), ('recurrent neural network ( RNN )', (20, 26)), ('FastQA', (29, 30))]","[['FastQA', 'develop', 'neural , bag - of - words ( BoW )'], ['FastQA', 'develop', 'recurrent neural network ( RNN )']]",[],"[['Model', 'namely', 'FastQA']]",[],[],[],[],[],[],natural_language_inference,77,28
model,"Crucially , both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level .","[('model', (14, 15)), ('between', (16, 17)), ('through', (21, 22)), ('on', (24, 25))]","[('interaction', (11, 12)), ('question and context', (17, 20)), ('computable features', (22, 24)), ('word level', (26, 28))]","[['interaction', 'between', 'question and context'], ['question and context', 'through', 'computable features'], ['computable features', 'on', 'word level']]",[],"[['Model', 'model', 'interaction']]",[],[],[],[],[],[],natural_language_inference,77,29
experimental-setup,BoW Model,[],"[('BoW Model', (0, 2))]",[],[],[],"[['Experimental setup', 'has', 'BoW Model']]",[],[],[],[],[],natural_language_inference,77,141
experimental-setup,The BoW model is trained on spans up to length 10 to keep the computation tractable .,"[('trained on', (4, 6)), ('up to length', (7, 10))]","[('spans', (6, 7)), ('10', (10, 11))]","[['spans', 'up to length', '10']]",[],[],[],[],"[['BoW Model', 'trained on', 'spans']]",[],[],[],natural_language_inference,77,142
experimental-setup,As pre-processing steps we lowercase all inputs and tokenize it using spacy 4 .,"[('As', (0, 1)), ('using', (10, 11))]","[('pre-processing steps', (1, 3)), ('lowercase', (4, 5)), ('all inputs', (5, 7)), ('tokenize', (8, 9)), ('spacy', (11, 12))]","[['tokenize', 'using', 'spacy']]","[['pre-processing steps', 'has', 'lowercase'], ['lowercase', 'has', 'all inputs'], ['pre-processing steps', 'has', 'tokenize']]",[],[],[],"[['BoW Model', 'As', 'pre-processing steps']]",[],[],[],natural_language_inference,77,144
experimental-setup,The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords .,"[('in', (3, 4)), ('computed on', (7, 9)), ('provided by', (10, 12)), ('restricted to', (14, 16)), ('are not', (19, 21))]","[('binary word', (1, 3)), ('question feature', (4, 6)), ('lemmas', (9, 10)), ('spacy', (12, 13)), ('alphanumeric words', (16, 18)), ('stopwords', (21, 22))]","[['binary word', 'in', 'question feature'], ['binary word', 'computed on', 'lemmas'], ['lemmas', 'provided by', 'spacy'], ['lemmas', 'restricted to', 'alphanumeric words'], ['alphanumeric words', 'are not', 'stopwords'], ['binary word', 'in', 'question feature']]",[],[],[],[],[],[],"[['BoW Model', 'has', 'binary word']]",[],natural_language_inference,77,145
experimental-setup,"Throughout all experiments we use a hidden dimensionality of n = 150 , dropout at the input embeddings with the same mask for all words and a rate of 0.2 and 300 - dimensional fixed word - embeddings from Glove .",[],[],"[['hidden dimensionality', 'of', 'fixed word - embeddings'], ['300 - dimensional', 'from', 'Glove'], ['hidden dimensionality', 'of', 'dropout'], ['dropout', 'with', 'same mask'], ['same mask', 'for', 'all words'], ['dropout', 'at', 'input embeddings'], ['rate', 'of', '0.2'], ['hidden dimensionality', 'of', 'n'], ['n', '=', '150'], ['hidden dimensionality', 'of', 'n'], ['same mask', 'for', 'all words']]","[['fixed word - embeddings', 'has', '300 - dimensional'], ['dropout', 'has', 'rate']]",[],[],[],"[['BoW Model', 'use', 'hidden dimensionality']]",[],[],[],natural_language_inference,77,146
experimental-setup,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between epochs .,"[('employed', (1, 2)), ('for', (3, 4)), ('with', (5, 6)), ('of', (11, 12))]","[('ADAM', (2, 3)), ('optimization', (4, 5)), ('initial learning - rate', (7, 11)), ('10 ?3', (12, 14))]","[['ADAM', 'with', 'initial learning - rate'], ['initial learning - rate', 'of', '10 ?3'], ['ADAM', 'for', 'optimization'], ['ADAM', 'with', 'initial learning - rate'], ['initial learning - rate', 'of', '10 ?3'], ['ADAM', 'for', 'optimization']]",[],[],[],[],"[['BoW Model', 'employed', 'ADAM']]",[],[],[],natural_language_inference,77,147
experimental-setup,We used mini-batches of size 32 .,"[('used', (1, 2)), ('of', (3, 4))]","[('mini-batches', (2, 3)), ('size', (4, 5)), ('32', (5, 6))]","[['mini-batches', 'of', 'size']]","[['size', 'has', '32']]",[],[],[],"[['BoW Model', 'used', 'mini-batches']]",[],[],[],natural_language_inference,77,148
experimental-setup,FastQA,[],"[('FastQA', (0, 1))]",[],[],[],"[['Experimental setup', 'has', 'FastQA']]",[],[],[],[],[],natural_language_inference,77,149
experimental-setup,We tokenize the input on whitespaces ( exclusive ) and non-alphanumeric characters ( inclusive ) .,"[('tokenize', (1, 2)), ('on', (4, 5))]","[('input', (3, 4)), ('whitespaces', (5, 6)), ('non-alphanumeric characters', (10, 12))]","[['input', 'on', 'whitespaces'], ['input', 'on', 'non-alphanumeric characters']]",[],[],[],[],"[['FastQA', 'tokenize', 'input']]",[],[],[],natural_language_inference,77,151
experimental-setup,The binary word in question feature is computed on the words as they appear in context .,[],[],"[['binary word', 'computed on', 'words'], ['words', 'appear in', 'context']]",[],[],[],[],[],[],"[['FastQA', 'has', 'binary word']]",[],natural_language_inference,77,152
experimental-setup,"Throughout all experiments we use a hidden dimensionality of n = 300 , variational dropout at the input embeddings with the same mask for all words and a rate of 0.5 and 300 dimensional fixed word - embeddings from Glove .",[],[],"[['n', '=', '300'], ['variational dropout', 'with', 'same mask'], ['variational dropout', 'at', 'input embeddings'], ['rate', 'of', '0.5'], ['300 dimensional', 'from', 'Glove']]","[['variational dropout', 'has', 'rate'], ['variational dropout', 'has', 'fixed word - embeddings'], ['fixed word - embeddings', 'has', '300 dimensional']]",[],[],[],"[['FastQA', 'use', 'hidden dimensionality'], ['FastQA', 'use', 'variational dropout']]",[],[],[],natural_language_inference,77,153
experimental-setup,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between checkpoints .,[],[],[],[],[],[],[],"[['FastQA', 'employed', 'ADAM']]",[],[],[],natural_language_inference,77,154
results,Our neural BoW baseline achieves good results on both datasets ( Tables 3 and 1 ) 5 .,"[('achieves', (4, 5))]","[('neural BoW baseline', (1, 4)), ('good results', (5, 7))]","[['neural BoW baseline', 'achieves', 'good results']]",[],[],"[['Results', 'has', 'neural BoW baseline']]",[],[],[],[],[],natural_language_inference,77,175
results,"For instance , it outperforms a feature rich logistic - regression baseline on the SQuAD development set and nearly reaches the BiLSTM baseline system ( i.e. , FastQA without character embeddings and features ) .","[('on', (12, 13))]","[('outperforms', (4, 5)), ('feature rich logistic - regression baseline', (6, 12)), ('SQuAD development set', (14, 17)), ('nearly reaches', (18, 20)), ('BiLSTM baseline system', (21, 24))]","[['feature rich logistic - regression baseline', 'on', 'SQuAD development set']]","[['nearly reaches', 'has', 'BiLSTM baseline system'], ['outperforms', 'has', 'feature rich logistic - regression baseline']]",[],"[['Results', 'has', 'nearly reaches'], ['Results', 'has', 'outperforms']]",[],[],[],[],[],natural_language_inference,77,176
results,It is very competitive to previously established stateof - the - art results on the two datasets and even improves those for News QA .,"[('to', (4, 5)), ('for', (21, 22))]","[('very competitive', (2, 4)), ('previously established stateof - the - art results', (5, 13)), ('improves', (19, 20)), ('News QA', (22, 24))]","[['improves', 'for', 'News QA'], ['very competitive', 'to', 'previously established stateof - the - art results']]",[],[],"[['Results', 'has', 'improves'], ['Results', 'has', 'very competitive']]",[],[],[],[],[],natural_language_inference,77,179
research-problem,"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",[],"[('Natural Language Inference', (13, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,78,2
research-problem,This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .,[],"[('Natural Language Inference ( NLI )', (9, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference ( NLI )']]",[],[],[],[],natural_language_inference,78,4
research-problem,"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former .",[],"[('NLI', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,78,14
model,There are several new novel components in our work .,[],"[('several new novel components', (2, 6))]",[],[],[],"[['Model', 'has', 'several new novel components']]",[],[],[],[],[],natural_language_inference,78,26
model,"Firstly , we propose a compare , compress and propagate ( Com Prop ) architecture where compressed alignment features are propagated to upper layers ( such as a RNN - based encoder ) for enhancing representation learning .","[('propose', (3, 4)), ('propagated to', (20, 22)), ('such as', (25, 27)), ('for enhancing', (33, 35))]","[('compare , compress and propagate ( Com Prop ) architecture', (5, 15)), ('compressed alignment features', (16, 19)), ('upper layers', (22, 24)), ('RNN - based encoder', (28, 32)), ('representation learning', (35, 37))]","[['compressed alignment features', 'for enhancing', 'representation learning'], ['compressed alignment features', 'propagated to', 'upper layers'], ['upper layers', 'such as', 'RNN - based encoder']]","[['compare , compress and propagate ( Com Prop ) architecture', 'has', 'compressed alignment features']]","[['Model', 'propose', 'compare , compress and propagate ( Com Prop ) architecture']]",[],[],[],[],[],[],natural_language_inference,78,27
model,"Secondly , in order to achieve an efficient propagation of alignment features , we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature .","[('to', (4, 5)), ('achieve', (5, 6)), ('of', (9, 10)), ('propose', (14, 15)), ('to reduce', (18, 20))]","[('efficient propagation', (7, 9)), ('alignment features', (10, 12)), ('alignment factorization layers', (15, 18)), ('each alignment vector', (20, 23)), ('single scalar valued feature', (25, 29))]","[['efficient propagation', 'of', 'alignment features'], ['efficient propagation', 'propose', 'alignment factorization layers'], ['alignment factorization layers', 'to reduce', 'each alignment vector'], ['each alignment vector', 'to', 'single scalar valued feature']]",[],"[['Model', 'achieve', 'efficient propagation']]",[],[],[],[],[],[],natural_language_inference,78,28
model,"Each scalar valued feature is used to augment the base word representation , allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information .","[('used to', (5, 7))]","[('scalar valued feature', (1, 4)), ('augment', (7, 8)), ('base word representation', (9, 12))]","[['scalar valued feature', 'used to', 'augment']]","[['augment', 'has', 'base word representation']]",[],"[['Model', 'has', 'scalar valued feature']]",[],[],[],[],[],natural_language_inference,78,29
experimental-setup,We implement our model in TensorFlow and train them on Nvidia P100 GPUs .,"[('implement', (1, 2)), ('in', (4, 5)), ('train them on', (7, 10))]","[('our model', (2, 4)), ('TensorFlow', (5, 6)), ('Nvidia P100 GPUs', (10, 13))]","[['our model', 'in', 'TensorFlow']]",[],"[['Experimental setup', 'train them on', 'Nvidia P100 GPUs'], ['Experimental setup', 'implement', 'our model']]",[],[],[],[],[],[],natural_language_inference,78,192
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.0003 .","[('use', (1, 2)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam optimizer', (3, 5)), ('initial learning rate', (14, 17)), ('0.0003', (18, 19))]","[['Adam optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0003']]",[],"[['Experimental setup', 'use', 'Adam optimizer']]",[],[],[],[],[],[],natural_language_inference,78,193
experimental-setup,L2 regularization is set to 10 ?6 .,"[('set to', (3, 5))]","[('L2 regularization', (0, 2)), ('10 ?6', (5, 7))]","[['L2 regularization', 'set to', '10 ?6']]",[],[],"[['Experimental setup', 'has', 'L2 regularization']]",[],[],[],[],[],natural_language_inference,78,194
experimental-setup,"Dropout with a keep probability of 0.8 is applied after each fullyconnected , recurrent or highway layer .","[('with', (1, 2)), ('of', (5, 6)), ('applied after', (8, 10))]","[('Dropout', (0, 1)), ('keep probability', (3, 5)), ('0.8', (6, 7)), ('each fullyconnected , recurrent or highway layer', (10, 17))]","[['Dropout', 'with', 'keep probability'], ['keep probability', 'of', '0.8'], ['keep probability', 'applied after', 'each fullyconnected , recurrent or highway layer']]",[],[],"[['Experimental setup', 'has', 'Dropout']]",[],[],[],[],[],natural_language_inference,78,195
experimental-setup,"The batch size is tuned amongst { 128 , 256 , 512 } .","[('tuned amongst', (4, 6))]","[('batch size', (1, 3)), ('{ 128 , 256 , 512 }', (6, 13))]","[['batch size', 'tuned amongst', '{ 128 , 256 , 512 }']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,78,196
experimental-setup,"The number of latent factors k for the factorization layer is tuned amongst { 5 , 10 , 50 , 100 , 150 } .","[('for', (6, 7)), ('tuned amongst', (11, 13))]","[('number of latent factors k', (1, 6)), ('factorization layer', (8, 10)), ('{ 5 , 10 , 50 , 100 , 150 }', (13, 24))]","[['number of latent factors k', 'for', 'factorization layer'], ['factorization layer', 'tuned amongst', '{ 5 , 10 , 50 , 100 , 150 }']]",[],[],"[['Experimental setup', 'has', 'number of latent factors k']]",[],[],[],[],[],natural_language_inference,78,197
experimental-setup,The size of the hidden layers of the highway network layers are set to 300 .,[],[],"[['size', 'of', 'hidden layers'], ['hidden layers', 'of', 'highway network layers'], ['highway network layers', 'set to', '300']]",[],[],"[['Experimental setup', 'has', 'size']]",[],[],[],[],[],natural_language_inference,78,198
experimental-setup,All parameters are initialized with xavier initialization .,"[('initialized with', (3, 5))]","[('parameters', (1, 2)), ('xavier initialization', (5, 7))]","[['parameters', 'initialized with', 'xavier initialization']]",[],[],"[['Experimental setup', 'has', 'parameters']]",[],[],[],[],[],natural_language_inference,78,199
experimental-setup,Word embeddings are preloaded with 300d Glo Ve embeddings and fixed during training .,"[('preloaded with', (3, 5)), ('fixed during', (10, 12))]","[('Word embeddings', (0, 2)), ('300d Glo Ve embeddings', (5, 9)), ('training', (12, 13))]","[['Word embeddings', 'fixed during', 'training'], ['Word embeddings', 'preloaded with', '300d Glo Ve embeddings']]",[],[],"[['Experimental setup', 'has', 'Word embeddings']]",[],[],[],[],[],natural_language_inference,78,200
experimental-setup,Sequence lengths are padded to batch - wise maximum .,"[('padded to', (3, 5))]","[('Sequence lengths', (0, 2)), ('batch - wise maximum', (5, 9))]","[['Sequence lengths', 'padded to', 'batch - wise maximum']]",[],[],"[['Experimental setup', 'has', 'Sequence lengths']]",[],[],[],[],[],natural_language_inference,78,201
experimental-setup,The batch order is ( randomly ) sorted within buckets following .,"[('( randomly ) sorted within', (4, 9))]","[('batch order', (1, 3)), ('buckets', (9, 10))]","[['batch order', '( randomly ) sorted within', 'buckets']]",[],[],"[['Experimental setup', 'has', 'batch order']]",[],[],[],[],[],natural_language_inference,78,202
results,Table 1 reports our results on the SNLI benchmark .,"[('on', (5, 6))]","[('SNLI benchmark', (7, 9))]",[],[],"[['Results', 'on', 'SNLI benchmark']]",[],[],[],[],[],[],natural_language_inference,78,204
results,"On the cross sentence ( single model setting ) , the performance of our proposed CAFE model is extremely competitive .","[('On', (0, 1)), ('of', (12, 13)), ('is', (17, 18))]","[('cross sentence ( single model setting )', (2, 9)), ('performance', (11, 12)), ('proposed CAFE model', (14, 17)), ('extremely competitive', (18, 20))]","[['performance', 'of', 'proposed CAFE model'], ['performance', 'is', 'extremely competitive']]","[['cross sentence ( single model setting )', 'has', 'performance']]",[],[],[],"[['SNLI benchmark', 'On', 'cross sentence ( single model setting )']]",[],[],[],natural_language_inference,78,205
results,CAFE obtains,"[('obtains', (1, 2))]","[('CAFE', (0, 1))]",[],[],[],[],[],[],"[['CAFE', 'obtains', '88.5 % accuracy']]","[['SNLI benchmark', 'has', 'CAFE']]",[],natural_language_inference,78,207
results,"88.5 % accuracy on the SNLI test set , an extremely competitive score on the extremely popular benchmark .","[('on', (3, 4))]","[('88.5 % accuracy', (0, 3)), ('SNLI test set', (5, 8)), ('extremely competitive score', (10, 13))]","[['88.5 % accuracy', 'on', 'SNLI test set']]","[['88.5 % accuracy', 'has', 'extremely competitive score']]",[],[],[],[],[],[],[],natural_language_inference,78,208
results,"For example , CAFE also achieves 88.3 % and 88.1 % test accuracy with only 3.5 M and 1.5 M parameters","[('achieves', (5, 6)), ('with', (13, 14))]","[('88.3 % and 88.1 % test accuracy', (6, 13)), ('only 3.5 M and 1.5 M parameters', (14, 21))]","[['88.3 % and 88.1 % test accuracy', 'with', 'only 3.5 M and 1.5 M parameters']]",[],[],[],[],"[['CAFE', 'achieves', '88.3 % and 88.1 % test accuracy']]",[],[],[],natural_language_inference,78,210
results,"Due to resource constraints , we did not train CAFE + ELMo ensembles but a single run ( and single model ) of CAFE + ELMo already achieves 89.0 score on SNLI .","[('achieves', (27, 28)), ('on', (30, 31))]","[('CAFE + ELMo', (9, 12)), ('89.0 score', (28, 30)), ('SNLI', (31, 32))]","[['CAFE + ELMo', 'achieves', '89.0 score'], ['89.0 score', 'on', 'SNLI']]",[],[],[],[],[],[],"[['SNLI benchmark', 'has', 'CAFE + ELMo']]",[],natural_language_inference,78,214
results,This outperforms the state - of - theart ESIM and DIIN models with only a fraction of the parameter cost .,"[('with', (12, 13)), ('of', (16, 17))]","[('outperforms', (1, 2)), ('state - of - theart ESIM and DIIN models', (3, 12)), ('fraction', (15, 16)), ('parameter cost', (18, 20))]","[['state - of - theart ESIM and DIIN models', 'with', 'fraction'], ['fraction', 'of', 'parameter cost']]","[['outperforms', 'has', 'state - of - theart ESIM and DIIN models']]",[],[],[],[],[],"[['CAFE + ELMo', 'has', 'outperforms']]",[],natural_language_inference,78,216
results,"Moreover , our lightweight adaptation achieves 87.7 % with only 750K parameters , which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ( 86.8 % ) .","[('achieves', (5, 6))]","[('lightweight adaptation', (3, 5)), ('87.7 %', (6, 8))]","[['lightweight adaptation', 'achieves', '87.7 %']]",[],[],[],[],[],[],"[['CAFE + ELMo', 'has', 'lightweight adaptation']]",[],natural_language_inference,78,218
results,"Finally , an ensemble of 5 CAFE models achieves 89.3 % test accuracy , the best test scores on the SNLI benchmark to date 3 .","[('of', (4, 5)), ('achieves', (8, 9))]","[('ensemble', (3, 4)), ('5 CAFE models', (5, 8)), ('89.3 % test accuracy', (9, 13)), ('best test scores', (15, 18))]","[['ensemble', 'of', '5 CAFE models'], ['5 CAFE models', 'achieves', '89.3 % test accuracy']]","[['89.3 % test accuracy', 'has', 'best test scores']]",[],[],[],[],[],"[['SNLI benchmark', 'has', 'ensemble']]",[],natural_language_inference,78,219
results,"On MultiNLI , CAFE significantly outperforms ESIM , a strong state - of - the - art model on both settings .",[],"[('MultiNLI', (1, 2)), ('CAFE', (3, 4)), ('significantly outperforms', (4, 6)), ('ESIM', (6, 7))]",[],"[['MultiNLI', 'has', 'CAFE'], ['CAFE', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'ESIM']]",[],"[['Results', 'on', 'MultiNLI']]",[],[],[],[],[],natural_language_inference,78,228
results,We also outperform the ESIM + Read model .,[],"[('outperform', (2, 3)), ('ESIM + Read model', (4, 8))]",[],"[['outperform', 'has', 'ESIM + Read model']]",[],[],[],[],[],"[['CAFE', 'has', 'outperform']]",[],natural_language_inference,78,229
results,An ensemble of CAFE models achieve competitive re-sult on the MultiNLI dataset .,"[('achieve', (5, 6))]","[('ensemble of CAFE models', (1, 5)), ('competitive re-sult', (6, 8))]","[['ensemble of CAFE models', 'achieve', 'competitive re-sult']]",[],[],[],[],[],[],"[['MultiNLI', 'has', 'ensemble of CAFE models']]",[],natural_language_inference,78,230
results,"On SciTail , our proposed CAFE model achieves state - of - the - art performance .","[('On', (0, 1)), ('achieves', (7, 8))]","[('SciTail', (1, 2)), ('proposed CAFE model', (4, 7)), ('state - of - the - art performance', (8, 16))]","[['proposed CAFE model', 'achieves', 'state - of - the - art performance']]","[['SciTail', 'has', 'proposed CAFE model']]","[['Results', 'On', 'SciTail']]",[],[],[],[],[],[],natural_language_inference,78,231
results,The performance gain over strong baselines such as DecompAtt and ESIM are ?,"[('over', (3, 4)), ('such as', (6, 8)), ('are', (11, 12))]","[('performance gain', (1, 3)), ('strong baselines', (4, 6)), ('DecompAtt', (8, 9)), ('ESIM', (10, 11))]","[['performance gain', 'over', 'strong baselines'], ['strong baselines', 'such as', 'DecompAtt'], ['strong baselines', 'such as', 'ESIM']]",[],[],[],[],[],"[['performance gain', 'are', '10 % ? 13 %']]","[['SciTail', 'has', 'performance gain']]",[],natural_language_inference,78,232
results,10 % ? 13 % in terms of accuracy .,"[('in terms of', (5, 8))]","[('10 % ? 13 %', (0, 5)), ('accuracy', (8, 9))]","[['10 % ? 13 %', 'in terms of', 'accuracy']]",[],[],[],[],[],[],[],[],natural_language_inference,78,233
results,"CAFE also outperforms DGEM , which uses a graph - based attention for improved performance , by a significant margin of 5 % .","[('by', (16, 17)), ('of', (20, 21))]","[('CAFE', (0, 1)), ('outperforms', (2, 3)), ('DGEM', (3, 4)), ('significant margin', (18, 20)), ('5 %', (21, 23))]","[['outperforms', 'by', 'significant margin'], ['significant margin', 'of', '5 %']]","[['CAFE', 'has', 'outperforms'], ['outperforms', 'has', 'DGEM']]",[],[],[],[],[],"[['SciTail', 'has', 'CAFE']]",[],natural_language_inference,78,234
ablation-analysis,The 1 - layer linear setting performs the best and is therefore reported in .,"[('performs', (6, 7))]","[('1 - layer linear setting', (1, 6)), ('best', (8, 9))]","[['1 - layer linear setting', 'performs', 'best']]",[],[],"[['Ablation analysis', 'has', '1 - layer linear setting']]",[],[],[],[],[],natural_language_inference,78,239
ablation-analysis,Using ReLU seems to be worse than nonlinear FC layers .,"[('Using', (0, 1)), ('seems to be', (2, 5)), ('than', (6, 7))]","[('ReLU', (1, 2)), ('worse', (5, 6)), ('nonlinear FC layers', (7, 10))]","[['ReLU', 'seems to be', 'worse'], ['worse', 'than', 'nonlinear FC layers']]",[],"[['Ablation analysis', 'Using', 'ReLU']]",[],[],[],[],[],[],natural_language_inference,78,240
ablation-analysis,"In , we explore the utility of using character and syntactic embeddings , which we found to have helped CAFE marginally .","[('explore', (3, 4)), ('of using', (6, 8)), ('found to', (15, 17))]","[('utility', (5, 6)), ('character and syntactic embeddings', (8, 12)), ('helped', (18, 19)), ('CAFE', (19, 20))]","[['utility', 'of using', 'character and syntactic embeddings'], ['character and syntactic embeddings', 'found to', 'helped']]","[['helped', 'has', 'CAFE']]","[['Ablation analysis', 'explore', 'utility']]",[],[],[],[],[],[],natural_language_inference,78,243
ablation-analysis,"In ( 4 ) , we remove the inter-attention alignment features , which naturally impact the model performance significantly .","[('remove', (6, 7))]","[('inter-attention alignment features', (8, 11)), ('naturally impact', (13, 15)), ('model performance', (16, 18)), ('significantly', (18, 19))]",[],"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","[['Ablation analysis', 'remove', 'inter-attention alignment features']]",[],[],[],[],[],[],natural_language_inference,78,244
ablation-analysis,We observe that both highway layers have marginally helped the over all performance .,"[('observe', (1, 2)), ('have', (6, 7))]","[('both highway layers', (3, 6)), ('marginally helped', (7, 9)), ('over all performance', (10, 13))]","[['both highway layers', 'have', 'marginally helped']]","[['marginally helped', 'has', 'over all performance']]","[['Ablation analysis', 'observe', 'both highway layers']]",[],[],[],[],[],[],natural_language_inference,78,246
ablation-analysis,We observe that the Sub and Concat compositions were more important than the Mul composition .,"[('were', (8, 9)), ('than', (11, 12))]","[('Sub and Concat compositions', (4, 8)), ('more important', (9, 11)), ('Mul composition', (13, 15))]","[['Sub and Concat compositions', 'were', 'more important'], ['more important', 'than', 'Mul composition']]",[],[],"[['Ablation analysis', 'observe', 'Sub and Concat compositions']]",[],[],[],[],[],natural_language_inference,78,248
ablation-analysis,"Finally , in ( 10 ) , we replace the LSTM encoder with a BiLSTM , observing that adding bi-directionality did not improve performance for our model .","[('replace', (8, 9)), ('with', (12, 13)), ('observing', (16, 17)), ('for', (24, 25))]","[('LSTM encoder', (10, 12)), ('BiLSTM', (14, 15)), ('adding bi-directionality', (18, 20)), ('did not improve', (20, 23)), ('performance', (23, 24)), ('our model', (25, 27))]","[['LSTM encoder', 'with', 'BiLSTM'], ['LSTM encoder', 'observing', 'adding bi-directionality'], ['performance', 'for', 'our model']]","[['adding bi-directionality', 'has', 'did not improve'], ['did not improve', 'has', 'performance']]","[['Ablation analysis', 'replace', 'LSTM encoder']]",[],[],[],[],[],[],natural_language_inference,78,250
research-problem,Distributed Representations of Sentences and Documents,[],"[('Distributed Representations of Sentences and Documents', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Distributed Representations of Sentences and Documents']]",[],[],[],[],natural_language_inference,79,2
model,"In this paper , we propose Paragraph Vector , an unsupervised framework that learns continuous distributed vector representations for pieces of texts .","[('propose', (5, 6)), ('that learns', (12, 14)), ('for', (18, 19))]","[('Paragraph Vector', (6, 8)), ('unsupervised framework', (10, 12)), ('continuous distributed vector representations', (14, 18)), ('pieces of texts', (19, 22))]","[['unsupervised framework', 'that learns', 'continuous distributed vector representations'], ['continuous distributed vector representations', 'for', 'pieces of texts']]","[['Paragraph Vector', 'has', 'unsupervised framework']]","[['Model', 'propose', 'Paragraph Vector']]",[],[],[],[],[],[],natural_language_inference,79,23
model,"The name Paragraph Vector is to emphasize the fact that the method can be applied to variable - length pieces of texts , anything from a phrase or sentence to a large document .","[('can be applied to', (12, 16))]","[('Paragraph Vector', (2, 4)), ('variable - length pieces of texts', (16, 22))]","[['Paragraph Vector', 'can be applied to', 'variable - length pieces of texts']]",[],[],"[['Model', 'name', 'Paragraph Vector']]",[],[],[],[],[],natural_language_inference,79,25
model,"In our model , the vector representation is trained to be useful for predicting words in a paragraph .","[('trained to be', (8, 11)), ('for predicting', (12, 14)), ('in', (15, 16))]","[('vector representation', (5, 7)), ('useful', (11, 12)), ('words', (14, 15)), ('paragraph', (17, 18))]","[['vector representation', 'trained to be', 'useful'], ['useful', 'for predicting', 'words'], ['words', 'in', 'paragraph']]",[],[],"[['Model', 'has', 'vector representation']]",[],[],[],[],[],natural_language_inference,79,26
model,"More precisely , we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context .","[('concatenate', (4, 5)), ('with', (8, 9)), ('from', (12, 13)), ('predict', (16, 17)), ('in', (20, 21))]","[('paragraph vector', (6, 8)), ('several word vectors', (9, 12)), ('paragraph', (14, 15)), ('following word', (18, 20)), ('given context', (22, 24))]","[['paragraph vector', 'with', 'several word vectors'], ['several word vectors', 'from', 'paragraph'], ['paragraph vector', 'predict', 'following word'], ['following word', 'in', 'given context']]",[],"[['Model', 'concatenate', 'paragraph vector']]",[],[],[],[],[],[],natural_language_inference,79,27
model,Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,"[('trained by', (7, 9))]","[('Both word vectors and paragraph vectors', (0, 6)), ('stochastic gradient descent', (10, 13)), ('backpropagation', (14, 15))]","[['Both word vectors and paragraph vectors', 'trained by', 'stochastic gradient descent'], ['Both word vectors and paragraph vectors', 'trained by', 'backpropagation']]",[],[],"[['Model', 'has', 'Both word vectors and paragraph vectors']]",[],[],[],[],[],natural_language_inference,79,28
model,"While paragraph vectors are unique among paragraphs , the word vectors are shared .",[],[],"[['paragraph vectors', 'are', 'unique'], ['unique', 'among', 'paragraphs'], ['word vectors', 'are', 'shared']]",[],[],"[['Model', 'has', 'paragraph vectors'], ['Model', 'has', 'word vectors']]",[],[],[],[],[],natural_language_inference,79,29
model,"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .","[('At', (0, 1)), ('inferred by', (8, 10)), ('training', (15, 16)), ('until', (20, 21))]","[('prediction time', (1, 3)), ('paragraph vectors', (5, 7)), ('fixing', (10, 11)), ('word vectors', (12, 14)), ('new paragraph vector', (17, 20)), ('convergence', (21, 22))]","[['prediction time', 'training', 'new paragraph vector'], ['new paragraph vector', 'until', 'convergence'], ['paragraph vectors', 'inferred by', 'fixing']]","[['prediction time', 'has', 'paragraph vectors'], ['fixing', 'has', 'word vectors']]","[['Model', 'At', 'prediction time']]",[],[],[],[],[],[],natural_language_inference,79,30
experimental-setup,"We learn the word vectors and paragraph vectors using 75,000 training documents ( 25,000 labeled and 50,000 unlabeled instances ) .","[('learn', (1, 2)), ('using', (8, 9))]","[('word vectors and paragraph vectors', (3, 8)), ('75,000 training documents', (9, 12)), ('25,000 labeled', (13, 15)), ('50,000 unlabeled instances', (16, 19))]","[['word vectors and paragraph vectors', 'using', '75,000 training documents']]","[['75,000 training documents', 'has', '25,000 labeled'], ['75,000 training documents', 'has', '50,000 unlabeled instances']]","[['Experimental setup', 'learn', 'word vectors and paragraph vectors']]",[],[],[],[],[],[],natural_language_inference,79,163
experimental-setup,"The paragraph vectors for the 25,000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment .",[],[],"[['paragraph vectors', 'fed through', 'neural network'], ['neural network', 'with', 'one hidden layer'], ['one hidden layer', 'with', '50 units'], ['paragraph vectors', 'fed through', 'logistic classifier'], ['logistic classifier', 'to predict', 'sentiment'], ['paragraph vectors', 'for', '25,000 labeled instances']]",[],[],"[['Experimental setup', 'has', 'paragraph vectors']]",[],[],[],[],[],natural_language_inference,79,164
experimental-setup,"In particular , we cross validate the window size , and the optimal window size is 10 words .","[('is', (15, 16))]","[('optimal window size', (12, 15)), ('10 words', (16, 18))]","[['optimal window size', 'is', '10 words']]",[],[],"[['Experimental setup', 'has', 'optimal window size']]",[],[],[],[],[],natural_language_inference,79,169
experimental-setup,"The vector presented to the classifier is a concatenation of two vectors , one from PV - DBOW and one from PV - DM .","[('presented to', (2, 4)), ('concatenation of', (8, 10))]","[('vector', (1, 2)), ('classifier', (5, 6)), ('PV - DBOW', (15, 18)), ('PV - DM', (21, 24))]","[['vector', 'presented to', 'classifier'], ['classifier', 'concatenation of', 'PV - DBOW'], ['classifier', 'concatenation of', 'PV - DM']]",[],[],"[['Experimental setup', 'has', 'vector']]",[],[],[],[],"[['PV - DBOW', 'has', 'learned vector representations'], ['PV - DM', 'has', 'learned vector representations']]",natural_language_inference,79,170
experimental-setup,"In PV - DBOW , the learned vector representations have 400 dimensions .","[('have', (9, 10))]","[('learned vector representations', (6, 9)), ('400 dimensions', (10, 12))]","[['learned vector representations', 'have', '400 dimensions'], ['learned vector representations', 'have', '400 dimensions']]",[],[],[],[],[],[],[],[],natural_language_inference,79,171
experimental-setup,"In PV - DM , the learned vector representations have 400 dimensions for both words and documents .",[],[],"[['400 dimensions', 'for', 'words and documents']]",[],[],[],[],[],[],[],[],natural_language_inference,79,172
experimental-setup,"To predict the 10 - th word , we concatenate the paragraph vectors and word vectors .","[('To predict', (0, 2)), ('concatenate', (9, 10))]","[('10 - th word', (3, 7)), ('paragraph vectors', (11, 13)), ('word vectors', (14, 16))]","[['10 - th word', 'concatenate', 'paragraph vectors'], ['10 - th word', 'concatenate', 'word vectors']]",[],"[['Experimental setup', 'To predict', '10 - th word']]",[],[],[],[],[],[],natural_language_inference,79,173
experimental-setup,"Special characters such as , .!?","[('such as', (2, 4))]","[('Special characters', (0, 2)), (', .!?', (4, 6))]","[['Special characters', 'such as', ', .!?']]",[],[],"[['Experimental setup', 'has', 'Special characters']]",[],[],[],[],[],natural_language_inference,79,174
experimental-setup,are treated as a normal word .,"[('treated as', (1, 3))]","[('normal word', (4, 6))]",[],[],[],[],[],"[[', .!?', 'treated as', 'normal word']]",[],[],[],natural_language_inference,79,175
results,"As can be seen from the for long documents , bag - of - words models perform quite well and it is difficult to improve upon them using word vectors .","[('for', (6, 7)), ('perform', (16, 17)), ('using', (27, 28))]","[('long documents', (7, 9)), ('bag - of - words models', (10, 16)), ('quite well', (17, 19)), ('difficult to improve', (22, 25)), ('word vectors', (28, 30))]","[['bag - of - words models', 'perform', 'quite well'], ['difficult to improve', 'using', 'word vectors']]","[['long documents', 'has', 'bag - of - words models'], ['bag - of - words models', 'has', 'difficult to improve']]","[['Results', 'for', 'long documents']]",[],[],[],[],[],[],natural_language_inference,79,179
results,The combination of two models yields an improvement approximately 1.5 % in terms of error rates .,"[('yields', (5, 6)), ('in terms of', (11, 14))]","[('combination of two models', (1, 5)), ('improvement', (7, 8)), ('approximately 1.5 %', (8, 11)), ('error rates', (14, 16))]","[['combination of two models', 'yields', 'improvement'], ['approximately 1.5 %', 'in terms of', 'error rates']]","[['improvement', 'has', 'approximately 1.5 %']]",[],"[['Results', 'has', 'combination of two models']]",[],[],[],[],[],natural_language_inference,79,181
results,The method described in this paper is the only approach that goes significantly beyond the barrier of 10 % error rate .,"[('is', (6, 7)), ('goes', (11, 12)), ('of', (16, 17))]","[('method described', (1, 3)), ('only approach', (8, 10)), ('significantly beyond the barrier', (12, 16)), ('10 % error rate', (17, 21))]","[['method described', 'is', 'only approach'], ['only approach', 'goes', 'significantly beyond the barrier'], ['significantly beyond the barrier', 'of', '10 % error rate']]",[],[],"[['Results', 'has', 'method described']]",[],[],[],[],[],natural_language_inference,79,184
results,It achieves 7.42 % which is another 1.3 % absolute improvement ( or 15 % relative improvement ) over the best previous result of ..,"[('achieves', (1, 2)), ('which is', (4, 6)), ('over', (18, 19))]","[('7.42 %', (2, 4)), ('another 1.3 % absolute improvement', (6, 11)), ('15 % relative improvement', (13, 17)), ('best previous result', (20, 23))]","[['7.42 %', 'over', 'best previous result'], ['7.42 %', 'which is', 'another 1.3 % absolute improvement'], ['7.42 %', 'which is', '15 % relative improvement']]",[],[],[],[],"[['method described', 'achieves', '7.42 %']]",[],[],[],natural_language_inference,79,185
research-problem,Deep Learning for Answer Sentence Selection,[],"[('Answer Sentence Selection', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Answer Sentence Selection']]",[],[],[],[],natural_language_inference,8,2
model,"In this paper , we show that a neural network - based sentence model can be applied to the task of answer sentence selection .","[('show', (5, 6)), ('can be applied to', (14, 18))]","[('neural network - based sentence model', (8, 14)), ('task of answer sentence selection', (19, 24))]","[['neural network - based sentence model', 'can be applied to', 'task of answer sentence selection']]",[],"[['Model', 'show', 'neural network - based sentence model']]",[],[],[],[],[],[],natural_language_inference,8,36
model,"We construct two distributional sentence models ; first a bag - of - words model , and second , a bigram model based on a convolutional neural network .","[('construct', (1, 2)), ('first', (7, 8)), ('second', (17, 18)), ('based on', (22, 24))]","[('two distributional sentence models', (2, 6)), ('bag - of - words model', (9, 15)), ('bigram model', (20, 22)), ('convolutional neural network', (25, 28))]","[['two distributional sentence models', 'first', 'bag - of - words model'], ['two distributional sentence models', 'second', 'bigram model'], ['bigram model', 'based on', 'convolutional neural network']]",[],"[['Model', 'construct', 'two distributional sentence models']]",[],[],[],[],[],[],natural_language_inference,8,37
model,"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .","[('Assuming a set of', (0, 4)), ('train', (10, 11)), ('to learn', (14, 16)), ('between', (19, 20))]","[('pre-trained semantic word embeddings', (4, 8)), ('supervised model', (12, 14)), ('semantic matching', (17, 19)), ('question and answer pairs', (20, 24))]","[['supervised model', 'to learn', 'semantic matching'], ['semantic matching', 'Assuming a set of', 'pre-trained semantic word embeddings'], ['semantic matching', 'between', 'question and answer pairs']]",[],"[['Model', 'train', 'supervised model']]",[],[],[],[],[],[],natural_language_inference,8,38
model,"We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .","[('present', (2, 3)), ('of', (6, 7)), ('combines', (11, 12)), ('with', (19, 20))]","[('enhanced version', (4, 6)), ('signal', (13, 14)), ('distributed matching algorithm', (16, 19)), ('two simple word matching features', (20, 25))]","[['enhanced version', 'combines', 'signal'], ['signal', 'of', 'distributed matching algorithm'], ['distributed matching algorithm', 'with', 'two simple word matching features']]",[],"[['Model', 'present', 'enhanced version']]",[],[],[],[],[],[],natural_language_inference,8,40
experimental-setup,We used word embeddings ( d = 50 ) that were computed using Collobert and Weston 's neural language model and provided by Turian et al ..,"[('used', (1, 2)), ('computed using', (11, 13))]","[('word embeddings ( d = 50 )', (2, 9)), (""Collobert and Weston 's neural language model"", (13, 20))]","[['word embeddings ( d = 50 )', 'computed using', ""Collobert and Weston 's neural language model""]]",[],"[['Experimental setup', 'used', 'word embeddings ( d = 50 )']]",[],[],[],[],[],[],natural_language_inference,8,150
experimental-setup,"The other model weights were randomly intitialised using a Gaussian distribution ( = 0 , ? = 0.01 ) .","[('using', (7, 8))]","[('other model weights', (1, 4)), ('randomly intitialised', (5, 7)), ('Gaussian distribution ( = 0 , ? = 0.01 )', (9, 19))]","[['randomly intitialised', 'using', 'Gaussian distribution ( = 0 , ? = 0.01 )']]","[['other model weights', 'has', 'randomly intitialised']]",[],"[['Experimental setup', 'has', 'other model weights']]",[],[],[],[],[],natural_language_inference,8,152
experimental-setup,All hyperparameters were optimised via grid search on the MAP score on the development data .,"[('optimised via', (3, 5)), ('on', (7, 8))]","[('All hyperparameters', (0, 2)), ('grid search', (5, 7)), ('MAP score on the development data', (9, 15))]","[['All hyperparameters', 'optimised via', 'grid search'], ['grid search', 'on', 'MAP score on the development data']]",[],[],"[['Experimental setup', 'has', 'All hyperparameters']]",[],[],[],[],[],natural_language_inference,8,153
experimental-setup,We use the AdaGrad algorithm for training .,"[('use', (1, 2)), ('for', (5, 6))]","[('AdaGrad algorithm', (3, 5)), ('training', (6, 7))]","[['AdaGrad algorithm', 'for', 'training']]",[],"[['Experimental setup', 'use', 'AdaGrad algorithm']]",[],[],[],[],[],[],natural_language_inference,8,154
experimental-setup,"L - BFGS was used to train the logistic regression classifier , with L2 regulariser of 0.01 .","[('to train', (5, 7)), ('with', (12, 13)), ('of', (15, 16))]","[('L - BFGS', (0, 3)), ('logistic regression classifier', (8, 11)), ('L2 regulariser', (13, 15)), ('0.01', (16, 17))]","[['L - BFGS', 'to train', 'logistic regression classifier'], ['logistic regression classifier', 'with', 'L2 regulariser'], ['L2 regulariser', 'of', '0.01']]",[],[],"[['Experimental setup', 'has', 'L - BFGS']]",[],[],[],[],[],natural_language_inference,8,159
results,"As can be seen , the bigram model performs better than the unigram model and the addition of the IDF - weighted word count features significantly improve performance for both models by 10 % - 15 % .","[('performs', (8, 9)), ('than', (10, 11)), ('addition of', (16, 18)), ('for', (28, 29)), ('by', (31, 32))]","[('bigram model', (6, 8)), ('better', (9, 10)), ('unigram model', (12, 14)), ('IDF - weighted word count features', (19, 25)), ('significantly improve', (25, 27)), ('performance', (27, 28)), ('both models', (29, 31)), ('10 % - 15 %', (32, 37))]","[['bigram model', 'performs', 'better'], ['better', 'than', 'unigram model'], ['bigram model', 'addition of', 'IDF - weighted word count features'], ['significantly improve', 'by', '10 % - 15 %'], ['performance', 'for', 'both models']]","[['IDF - weighted word count features', 'has', 'significantly improve'], ['significantly improve', 'has', 'performance']]",[],"[['Results', 'has', 'bigram model']]",[],[],[],[],[],natural_language_inference,8,161
results,"As can be seen in , our best models ( bigram + count ) outperform all baselines and prior work on MAP and are very close to the best model proposed by Yih et al. on MRR .","[('outperform', (14, 15))]","[('best models ( bigram + count )', (7, 14)), ('all baselines', (15, 17))]","[['best models ( bigram + count )', 'outperform', 'all baselines']]",[],[],"[['Results', 'has', 'best models ( bigram + count )']]",[],[],[],[],[],natural_language_inference,8,169
research-problem,DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,[],"[('Natural Language Inference', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,80,2
research-problem,We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,[],"[('natural language inference ( NLI )', (10, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,80,4
research-problem,"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .",[],"[('NLI', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,80,12
model,We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,"[('propose', (1, 2))]","[('dependent reading bidirectional LSTM ( DR - BiLSTM ) model', (3, 13))]",[],[],"[['Model', 'propose', 'dependent reading bidirectional LSTM ( DR - BiLSTM ) model']]",[],[],[],[],[],[],natural_language_inference,80,30
model,"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .","[('first encodes', (11, 13)), ('considering', (14, 15)), ('on', (16, 17))]","[('premise u and a hypothesis v', (2, 8)), ('dependency', (15, 16)), ('each other', (17, 19))]","[['premise u and a hypothesis v', 'considering', 'dependency'], ['dependency', 'on', 'each other']]",[],"[['Model', 'first encodes', 'premise u and a hypothesis v']]",[],[],[],[],[],[],natural_language_inference,80,31
model,"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .","[('employs', (4, 5)), ('to extract', (9, 11)), ('from', (13, 14))]","[('soft attention mechanism', (6, 9)), ('relevant information', (11, 13)), ('these encodings', (14, 16))]","[['soft attention mechanism', 'to extract', 'relevant information'], ['relevant information', 'from', 'these encodings']]",[],"[['Model', 'employs', 'soft attention mechanism']]",[],[],[],[],[],[],natural_language_inference,80,32
model,"The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .","[('passed to', (6, 8)), ('uses', (13, 14)), ('in', (19, 20))]","[('augmented sentence representations', (1, 4)), ('inference stage', (9, 11)), ('similar dependent reading strategy', (15, 19)), ('both directions', (20, 22))]","[['augmented sentence representations', 'passed to', 'inference stage'], ['inference stage', 'uses', 'similar dependent reading strategy'], ['similar dependent reading strategy', 'in', 'both directions']]",[],[],"[['Model', 'has', 'augmented sentence representations']]",[],[],[],[],[],natural_language_inference,80,33
model,"Finally , a decision is made through a multi - layer perceptron ( MLP ) based on the aggregated information .","[('made through', (5, 7)), ('based on', (15, 17))]","[('decision', (3, 4)), ('multi - layer perceptron ( MLP )', (8, 15)), ('aggregated information', (18, 20))]","[['decision', 'made through', 'multi - layer perceptron ( MLP )'], ['multi - layer perceptron ( MLP )', 'based on', 'aggregated information']]",[],[],"[['Model', 'has', 'decision']]",[],[],[],[],[],natural_language_inference,80,34
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize our word embedding vectors .,"[('use', (1, 2)), ('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('our word embedding vectors', (11, 15))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'our word embedding vectors']]",[],"[['Hyperparameters', 'use', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],[],[],[],natural_language_inference,80,128
hyperparameters,All hidden states of BiLSTMs during input encoding and inference have 450 dimensions ( r = 300 and d = 450 ) .,"[('of', (3, 4)), ('during', (5, 6)), ('have', (10, 11))]","[('All hidden states', (0, 3)), ('BiLSTMs', (4, 5)), ('input encoding and inference', (6, 10)), ('450 dimensions', (11, 13))]","[['All hidden states', 'of', 'BiLSTMs'], ['BiLSTMs', 'during', 'input encoding and inference'], ['input encoding and inference', 'have', '450 dimensions']]",[],[],"[['Hyperparameters', 'has', 'All hidden states']]",[],[],[],[],[],natural_language_inference,80,129
hyperparameters,"The weights are learned by minimizing the log - loss on the training data via the Adam optimizer ( Kingma and Ba , 2014 ) .","[('learned by', (3, 5)), ('on', (10, 11)), ('via', (14, 15))]","[('weights', (1, 2)), ('minimizing', (5, 6)), ('log - loss', (7, 10)), ('training data', (12, 14)), ('Adam optimizer', (16, 18))]","[['weights', 'learned by', 'minimizing'], ['log - loss', 'on', 'training data'], ['minimizing', 'via', 'Adam optimizer']]","[['minimizing', 'has', 'log - loss']]",[],"[['Hyperparameters', 'has', 'weights']]",[],[],[],[],[],natural_language_inference,80,130
hyperparameters,The initial learning rate is 0.0004 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.0004', (5, 6))]","[['initial learning rate', 'is', '0.0004']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],[],[],[],[],natural_language_inference,80,131
hyperparameters,"To avoid overfitting , we use dropout with the rate of 0.4 for regularization , which is applied to all feedforward connections .","[('To avoid', (0, 2)), ('use', (5, 6)), ('with', (7, 8)), ('of', (10, 11)), ('for', (12, 13)), ('applied to', (17, 19))]","[('overfitting', (2, 3)), ('dropout', (6, 7)), ('rate', (9, 10)), ('0.4', (11, 12)), ('regularization', (13, 14)), ('all feedforward connections', (19, 22))]","[['overfitting', 'use', 'dropout'], ['dropout', 'with', 'rate'], ['rate', 'of', '0.4'], ['dropout', 'for', 'regularization'], ['dropout', 'applied to', 'all feedforward connections']]",[],"[['Hyperparameters', 'To avoid', 'overfitting']]",[],[],[],[],[],[],natural_language_inference,80,132
hyperparameters,"During training , the word embeddings are updated to learn effective representations for the NLI task .","[('During', (0, 1)), ('updated to learn', (7, 10)), ('for', (12, 13))]","[('training', (1, 2)), ('word embeddings', (4, 6)), ('effective representations', (10, 12)), ('NLI task', (14, 16))]","[['word embeddings', 'updated to learn', 'effective representations'], ['effective representations', 'for', 'NLI task']]","[['training', 'has', 'word embeddings']]","[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,80,133
hyperparameters,We use a fairly small batch size of 32 to provide more exploration power to the model .,"[('of', (7, 8))]","[('fairly small batch size', (3, 7)), ('32', (8, 9))]","[['fairly small batch size', 'of', '32']]",[],[],"[['Hyperparameters', 'use', 'fairly small batch size']]",[],[],[],[],[],natural_language_inference,80,134
results,DR - BiLSTM ( Single ) achieves 88.5 % accuracy on the test set which is noticeably the best reported result among the existing single models for this task .,"[('achieves', (6, 7)), ('on', (10, 11)), ('is', (15, 16)), ('among', (21, 22))]","[('DR - BiLSTM ( Single )', (0, 6)), ('88.5 % accuracy', (7, 10)), ('test set', (12, 14)), ('best reported result', (18, 21)), ('existing single models', (23, 26))]","[['DR - BiLSTM ( Single )', 'achieves', '88.5 % accuracy'], ['88.5 % accuracy', 'is', 'best reported result'], ['best reported result', 'among', 'existing single models'], ['88.5 % accuracy', 'on', 'test set']]",[],[],"[['Results', 'has', 'DR - BiLSTM ( Single )']]",[],[],[],[],[],natural_language_inference,80,174
results,"DR - BiLSTM ( Ensemble ) achieves the accuracy of 89.3 % , the best result observed on SNLI , while DR - BiLSTM ( Single ) obtains the accuracy of 88.5 % , which considerably outperforms the previous non-ensemble models .",[],[],"[['accuracy', 'of', '88.5 %'], ['DR - BiLSTM ( Ensemble )', 'achieves', 'accuracy'], ['accuracy', 'of', '89.3 %'], ['best result', 'observed on', 'SNLI']]","[['88.5 %', 'has', 'considerably outperforms'], ['considerably outperforms', 'has', 'previous non-ensemble models'], ['89.3 %', 'has', 'best result']]",[],"[['Results', 'has', 'DR - BiLSTM ( Ensemble )']]",[],"[['DR - BiLSTM ( Single )', 'obtains', 'accuracy']]",[],[],[],natural_language_inference,80,177
results,"Also , utilizing a trivial preprocessing step yields to further improvements of 0.4 % and 0.3 % for single and ensemble DR - BiLSTM models respectively .",[],[],"[['trivial preprocessing step', 'yields to', 'further improvements'], ['further improvements', 'for', 'single and ensemble DR - BiLSTM models']]",[],"[['Results', 'utilizing', 'trivial preprocessing step']]",[],[],[],[],[],[],natural_language_inference,80,178
results,Our ensemble model considerably outperforms the current state - of - the - art by obtaining 89.3 % accuracy .,"[('by obtaining', (14, 16))]","[('ensemble model', (1, 3)), ('considerably outperforms', (3, 5)), ('current state - of - the - art', (6, 14)), ('89.3 % accuracy', (16, 19))]","[['current state - of - the - art', 'by obtaining', '89.3 % accuracy']]","[['ensemble model', 'has', 'considerably outperforms'], ['considerably outperforms', 'has', 'current state - of - the - art']]",[],"[['Results', 'has', 'ensemble model']]",[],[],[],[],[],natural_language_inference,80,183
results,We can see that our preprocessing mechanism leads to further improvements of 0.4 % and 0.3 % on the SNLI test set for our single and ensemble models respectively .,"[('leads to', (7, 9)), ('of', (11, 12)), ('on', (17, 18)), ('for', (22, 23))]","[('preprocessing mechanism', (5, 7)), ('further improvements', (9, 11)), ('0.4 % and 0.3 %', (12, 17)), ('SNLI test set', (19, 22)), ('our single and ensemble models', (23, 28))]","[['further improvements', 'of', '0.4 % and 0.3 %'], ['preprocessing mechanism', 'leads to', 'further improvements'], ['further improvements', 'of', '0.4 % and 0.3 %'], ['0.4 % and 0.3 %', 'for', 'our single and ensemble models'], ['0.4 % and 0.3 %', 'on', 'SNLI test set']]",[],[],"[['Results', 'has', 'preprocessing mechanism']]",[],[],[],[],[],natural_language_inference,80,186
results,"In fact , our single model ( "" DR - BiLSTM ( Single ) + Process "" ) obtains the state - of - the - art performance over both reported single and ensemble models by performing a simple preprocessing step .","[('obtains', (18, 19)), ('over', (28, 29)), ('by performing', (35, 37))]","[('our single model', (3, 6)), ('DR - BiLSTM ( Single ) + Process', (8, 16)), ('state - of - the - art performance', (20, 28)), ('reported single and ensemble models', (30, 35)), ('simple preprocessing step', (38, 41))]","[['our single model', 'obtains', 'state - of - the - art performance'], ['state - of - the - art performance', 'over', 'reported single and ensemble models'], ['reported single and ensemble models', 'by performing', 'simple preprocessing step']]","[['our single model', 'name', 'DR - BiLSTM ( Single ) + Process']]",[],"[['Results', 'has', 'our single model']]",[],[],[],[],[],natural_language_inference,80,187
results,"Furthermore , "" DR - BiLSTM ( Ensem . ) + Process "" outperforms the existing state - of - the - art remarkably ( 0.7 % improvement ) .",[],"[('DR - BiLSTM ( Ensem . ) + Process', (3, 12)), ('outperforms', (13, 14)), ('existing state - of - the - art', (15, 23)), ('remarkably', (23, 24)), ('0.7 % improvement', (25, 28))]",[],"[['DR - BiLSTM ( Ensem . ) + Process', 'has', 'outperforms'], ['outperforms', 'has', 'existing state - of - the - art'], ['existing state - of - the - art', 'has', 'remarkably'], ['existing state - of - the - art', 'has', '0.7 % improvement']]",[],"[['Results', 'has', 'DR - BiLSTM ( Ensem . ) + Process']]",[],[],[],[],[],natural_language_inference,80,188
ablation-analysis,We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of < 0.001 over Chi square test .,"[('see that', (2, 4)), ('lead to', (6, 8))]","[('all modifications', (4, 6)), ('new model', (9, 11))]","[['all modifications', 'lead to', 'new model']]",[],"[['Ablation analysis', 'see that', 'all modifications']]",[],[],[],[],[],[],natural_language_inference,80,195
ablation-analysis,"Among all components , three of them have noticeable influences : max pooling , difference in the attention stage , and dependent reading .","[('Among', (0, 1)), ('have', (7, 8))]","[('all components', (1, 3)), ('three of them', (4, 7)), ('noticeable influences', (8, 10)), ('max pooling', (11, 13)), ('difference in the attention stage', (14, 19)), ('dependent reading', (21, 23))]","[['three of them', 'have', 'noticeable influences']]","[['all components', 'has', 'three of them'], ['three of them', 'has', 'max pooling'], ['three of them', 'has', 'difference in the attention stage'], ['three of them', 'has', 'dependent reading']]","[['Ablation analysis', 'Among', 'all components']]",[],[],[],[],[],[],natural_language_inference,80,197
ablation-analysis,"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement , specifically in the encoding stage .","[('illustrate', (1, 2)), ('of', (4, 5)), ('leads to', (11, 13)), ('in', (17, 18))]","[('importance', (3, 4)), ('our proposed dependent reading strategy', (5, 10)), ('significant improvement', (13, 15)), ('encoding stage', (19, 21))]","[['importance', 'of', 'our proposed dependent reading strategy'], ['our proposed dependent reading strategy', 'leads to', 'significant improvement'], ['significant improvement', 'in', 'encoding stage']]",[],"[['Ablation analysis', 'illustrate', 'importance']]",[],[],[],[],[],[],natural_language_inference,80,199
ablation-analysis,demonstrates that we achieve the best performance with 450 - dimensional BiLSTMs .,"[('demonstrates', (0, 1)), ('with', (7, 8))]","[('best performance', (5, 7)), ('450 - dimensional BiLSTMs', (8, 12))]","[['best performance', 'with', '450 - dimensional BiLSTMs']]",[],"[['Ablation analysis', 'demonstrates', 'best performance']]",[],[],[],[],[],[],natural_language_inference,80,203
research-problem,COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING,[],"[('MULTI - EVIDENCE QUESTION ANSWERING', (11, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'MULTI - EVIDENCE QUESTION ANSWERING']]",[],[],[],[],natural_language_inference,81,2
research-problem,"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document .",[],"[('question answering', (12, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering']]",[],[],[],[],natural_language_inference,81,4
research-problem,A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .,[],"[('question answering ( QA )', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,81,42
research-problem,"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .",[],"[('neural question answering', (13, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural question answering']]",[],[],[],[],natural_language_inference,81,43
model,"Our multi-evidence QA model , the Coarse - grain Fine - grain Coattention Network ( CFC ) , selects among a set of candidate answers given a set of support documents and a query .",[],[],"[['Our multi-evidence QA model', 'selects among', 'set'], ['set', 'given', 'set'], ['set', 'of', 'support documents'], ['set', 'of', 'query'], ['set', 'of', 'candidate answers']]","[['Our multi-evidence QA model', 'name', 'Coarse - grain Fine - grain Coattention Network ( CFC )']]",[],"[['Model', 'has', 'Our multi-evidence QA model']]",[],[],[],[],[],natural_language_inference,81,46
model,The CFC is inspired by coarse - grain reasoning and fine - grain reasoning .,"[('inspired by', (3, 5))]","[('CFC', (1, 2)), ('coarse - grain reasoning', (5, 9)), ('fine - grain reasoning', (10, 14))]","[['CFC', 'inspired by', 'coarse - grain reasoning'], ['CFC', 'inspired by', 'fine - grain reasoning']]",[],[],"[['Model', 'has', 'CFC']]",[],[],[],[],[],natural_language_inference,81,47
model,"In coarse - grain reasoning , the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available , then scores each candidate .","[('In', (0, 1)), ('builds', (8, 9)), ('of', (12, 13)), ('conditioned on', (15, 17))]","[('coarse - grain reasoning', (1, 5)), ('model', (7, 8)), ('coarse summary', (10, 12)), ('support documents', (13, 15)), ('query', (18, 19))]","[['model', 'builds', 'coarse summary'], ['coarse summary', 'of', 'support documents'], ['coarse summary', 'conditioned on', 'query']]","[['coarse - grain reasoning', 'has', 'model']]","[['Model', 'In', 'coarse - grain reasoning']]",[],[],[],[],[],[],natural_language_inference,81,48
model,"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .","[('matches', (8, 9)), ('to gauge', (23, 25)), ('of', (27, 28))]","[('fine - grain reasoning', (1, 5)), ('model', (7, 8)), ('specific finegrain contexts', (9, 12)), ('candidate', (15, 16)), ('relevance', (26, 27))]","[['model', 'matches', 'specific finegrain contexts'], ['specific finegrain contexts', 'to gauge', 'relevance'], ['relevance', 'of', 'candidate']]","[['fine - grain reasoning', 'has', 'model']]",[],"[['Model', 'In', 'fine - grain reasoning']]",[],[],[],[],[],natural_language_inference,81,49
model,Each module employs a novel hierarchical attention - a hierarchy of coattention and self - attention - to combine information from the support documents conditioned on the query and candidates .,"[('employs', (2, 3)), ('of', (10, 11)), ('to combine', (17, 19)), ('from', (20, 21)), ('conditioned on', (24, 26))]","[('Each module', (0, 2)), ('novel hierarchical attention', (4, 7)), ('hierarchy', (9, 10)), ('coattention', (11, 12)), ('self - attention', (13, 16)), ('information', (19, 20)), ('support documents', (22, 24)), ('query', (27, 28)), ('candidates', (29, 30))]","[['Each module', 'employs', 'novel hierarchical attention'], ['hierarchy', 'to combine', 'information'], ['information', 'conditioned on', 'query'], ['information', 'conditioned on', 'candidates'], ['information', 'from', 'support documents'], ['hierarchy', 'of', 'coattention'], ['hierarchy', 'of', 'self - attention']]","[['novel hierarchical attention', 'has', 'hierarchy']]",[],"[['Model', 'has', 'Each module']]",[],[],[],[],[],natural_language_inference,81,51
experiments,MULTI - EVIDENCE QUESTION ANSWERING ON WIKIHOP,"[('ON', (5, 6))]","[('MULTI - EVIDENCE QUESTION ANSWERING', (0, 5)), ('WIKIHOP', (6, 7))]","[['MULTI - EVIDENCE QUESTION ANSWERING', 'ON', 'WIKIHOP']]",[],[],[],[],[],[],"[['Tasks', 'has', 'MULTI - EVIDENCE QUESTION ANSWERING']]","[['MULTI - EVIDENCE QUESTION ANSWERING', 'has', 'Hyperparameters']]",natural_language_inference,81,111
experiments,We tokenize the data using Stanford CoreNLP .,"[('using', (4, 5))]","[('tokenize', (1, 2)), ('Stanford CoreNLP', (5, 7))]","[['tokenize', 'using', 'Stanford CoreNLP']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'tokenize']]",[],natural_language_inference,81,121
experiments,We use fixed Glo Ve embeddings as well as character ngram embeddings .,"[('use', (1, 2)), ('as well as', (6, 9))]","[('fixed Glo Ve embeddings', (2, 6)), ('character ngram embeddings', (9, 12))]","[['fixed Glo Ve embeddings', 'as well as', 'character ngram embeddings']]",[],[],[],[],"[['Hyperparameters', 'use', 'fixed Glo Ve embeddings']]",[],[],[],natural_language_inference,81,122
experiments,We split symbolic query relations into words .,"[('split', (1, 2)), ('into', (5, 6))]","[('symbolic query relations', (2, 5)), ('words', (6, 7))]","[['symbolic query relations', 'into', 'words']]",[],[],[],[],"[['Hyperparameters', 'split', 'symbolic query relations']]",[],[],[],natural_language_inference,81,123
experiments,All models are trained using ADAM .,"[('trained using', (3, 5))]","[('ADAM', (5, 6))]",[],[],[],[],[],"[['Hyperparameters', 'trained using', 'ADAM']]",[],[],[],natural_language_inference,81,124
experiments,The CFC achieves state - of - the - art results on both the masked and unmasked versions of WikiHop .,"[('achieves', (2, 3)), ('on', (11, 12)), ('of', (18, 19))]","[('CFC', (1, 2)), ('state - of - the - art results', (3, 11)), ('masked and unmasked versions', (14, 18)), ('WikiHop', (19, 20))]","[['CFC', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'on', 'masked and unmasked versions'], ['masked and unmasked versions', 'of', 'WikiHop']]",[],[],[],[],[],[],"[['Results', 'has', 'CFC']]",[],natural_language_inference,81,127
experiments,"In particular , on the blind , held - out WikiHop test set , the CFC achieves a new best accuracy of 70.6 % .","[('on', (3, 4)), ('achieves', (16, 17)), ('of', (21, 22))]","[('blind , held - out WikiHop test set', (5, 13)), ('CFC', (15, 16)), ('new best accuracy', (18, 21)), ('70.6 %', (22, 24))]","[['CFC', 'achieves', 'new best accuracy'], ['new best accuracy', 'of', '70.6 %']]","[['blind , held - out WikiHop test set', 'has', 'CFC']]",[],[],[],"[['Results', 'on', 'blind , held - out WikiHop test set']]",[],[],[],natural_language_inference,81,128
experiments,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,"[('ON', (4, 5))]","[('RERANKING EXTRACTIVE QUESTION ANSWERING', (0, 4)), ('TRIVIAQA', (5, 6))]","[['RERANKING EXTRACTIVE QUESTION ANSWERING', 'ON', 'TRIVIAQA']]",[],[],[],[],[],[],"[['Tasks', 'has', 'RERANKING EXTRACTIVE QUESTION ANSWERING']]",[],natural_language_inference,81,134
experiments,Our experimental results in show that reranking using the CFC provides consistent performance gains over only using the span extraction question answering model .,"[('show', (4, 5)), ('using', (7, 8)), ('provides', (10, 11)), ('over only using', (14, 17))]","[('Our experimental results', (0, 3)), ('reranking', (6, 7)), ('CFC', (9, 10)), ('consistent performance gains', (11, 14)), ('span extraction question answering model', (18, 23))]","[['Our experimental results', 'show', 'reranking'], ['reranking', 'using', 'CFC'], ['reranking', 'provides', 'consistent performance gains'], ['consistent performance gains', 'over only using', 'span extraction question answering model'], ['reranking', 'using', 'CFC'], ['reranking', 'using', 'CFC']]",[],[],[],[],[],[],"[['RERANKING EXTRACTIVE QUESTION ANSWERING', 'has', 'Our experimental results']]",[],natural_language_inference,81,141
experiments,"In particular , reranking using the CFC improves performance regardless of whether the candidate answer set obtained from the span extraction model contains correct answers .",[],[],[],"[['reranking', 'has', 'improves'], ['improves', 'has', 'performance']]",[],[],[],[],[],"[['RERANKING EXTRACTIVE QUESTION ANSWERING', 'has', 'reranking']]",[],natural_language_inference,81,142
experiments,"On the whole Trivia QA dev set , reranking using the CFC results in again of 3.1 % EM and 3.0 % F1 , which suggests that the CFC can be used to further refine the outputs produced by span extraction question answering models .",[],[],"[['reranking', 'results in', 'again'], ['again', 'of', '3.1 % EM and 3.0 % F1']]","[['whole Trivia QA dev set', 'has', 'reranking']]",[],[],[],"[['RERANKING EXTRACTIVE QUESTION ANSWERING', 'On', 'whole Trivia QA dev set']]",[],[],[],natural_language_inference,81,143
ablation-analysis,Both the coarse - grain module and the fine - grain module significantly contribute to model performance .,"[('contribute to', (13, 15))]","[('Both the coarse - grain module and the fine - grain module', (0, 12)), ('model performance', (15, 17))]","[['Both the coarse - grain module and the fine - grain module', 'contribute to', 'model performance']]",[],[],"[['Ablation analysis', 'has', 'Both the coarse - grain module and the fine - grain module']]",[],[],[],[],[],natural_language_inference,81,145
ablation-analysis,Replacing selfattention layers with mean - pooling and the bidirectional GRUs with unidirectional GRUs result in less performance degradation .,[],[],"[['less performance degradation', 'Replacing', 'bidirectional GRUs'], ['bidirectional GRUs', 'with', 'unidirectional GRUs'], ['less performance degradation', 'Replacing', 'selfattention layers'], ['selfattention layers', 'with', 'mean - pooling']]",[],"[['Ablation analysis', 'result in', 'less performance degradation']]",[],[],[],[],[],[],natural_language_inference,81,146
ablation-analysis,"Replacing the encoder with a projection over word embeddings result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .","[('Replacing', (0, 1)), ('with', (3, 4)), ('over', (6, 7)), ('result in', (9, 11))]","[('encoder', (2, 3)), ('projection', (5, 6)), ('word embeddings', (7, 9)), ('significant performance drop', (11, 14))]","[['encoder', 'with', 'projection'], ['projection', 'over', 'word embeddings'], ['projection', 'result in', 'significant performance drop']]",[],"[['Ablation analysis', 'Replacing', 'encoder']]",[],[],[],[],[],[],natural_language_inference,81,147
ablation-analysis,The fine - grain - only model under-performs the coarse - grain - only model consistently across almost all length measures .,"[('across', (16, 17))]","[('fine - grain - only model', (1, 7)), ('under-performs', (7, 8)), ('coarse - grain - only model', (9, 15)), ('consistently', (15, 16)), ('almost all length measures', (17, 21))]","[['consistently', 'across', 'almost all length measures']]","[['fine - grain - only model', 'has', 'under-performs'], ['under-performs', 'has', 'coarse - grain - only model'], ['under-performs', 'has', 'consistently']]",[],"[['Ablation analysis', 'has', 'fine - grain - only model']]",[],[],[],[],[],natural_language_inference,81,149
ablation-analysis,"However , the fine - grain - only model matches or outperforms the coarse - grain - only model on examples with a large number of support documents or with long support documents .","[('on', (19, 20)), ('with', (21, 22)), ('of', (25, 26))]","[('matches or outperforms', (9, 12)), ('coarse - grain - only model', (13, 19)), ('examples', (20, 21)), ('large number', (23, 25)), ('support documents', (26, 28)), ('long support documents', (30, 33))]","[['coarse - grain - only model', 'on', 'examples'], ['examples', 'with', 'large number'], ['large number', 'of', 'support documents'], ['examples', 'with', 'long support documents']]","[['matches or outperforms', 'has', 'coarse - grain - only model']]",[],[],[],[],[],"[['fine - grain - only model', 'has', 'matches or outperforms']]",[],natural_language_inference,81,151
research-problem,Dynamic Entity Representation with Max - pooling Improves Machine Reading,[],"[('Machine Reading', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading']]",[],[],[],[],natural_language_inference,82,2
code,Our code for the model is available at https://github.com/soskek/der-network,[],"[('https://github.com/soskek/der-network', (8, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/soskek/der-network']]",[],[],[],[],natural_language_inference,82,7
model,"We , however , take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity , by gathering and accumulating information on that entity as it reads a document ( Section 2 ) .","[('implement', (11, 12)), ('dynamically builds', (15, 17)), ('for', (19, 20)), ('by gathering and accumulating', (23, 27)), ('on', (28, 29)), ('as it reads', (31, 34))]","[('reader', (13, 14)), ('meaning representations', (17, 19)), ('each entity', (20, 22)), ('information', (27, 28)), ('entity', (30, 31)), ('document', (35, 36))]","[['reader', 'dynamically builds', 'meaning representations'], ['meaning representations', 'for', 'each entity'], ['meaning representations', 'by gathering and accumulating', 'information'], ['information', 'as it reads', 'document'], ['information', 'on', 'entity']]",[],"[['Model', 'implement', 'reader']]",[],[],[],[],[],[],natural_language_inference,82,23
hyperparameters,"For preprocessing , we segment sentences at punctuation marks "" . "" , "" ! "" , and "" ? "" .","[('For', (0, 1)), ('segment', (4, 5))]","[('preprocessing', (1, 2)), ('sentences', (5, 6)), ('punctuation marks', (7, 9)), ('.', (10, 11)), ('!', (14, 15)), ('?', (19, 20))]","[['preprocessing', 'segment', 'sentences'], ['preprocessing', 'segment', 'punctuation marks']]","[['punctuation marks', 'has', '.'], ['punctuation marks', 'has', '!'], ['punctuation marks', 'has', '?']]","[['Hyperparameters', 'For', 'preprocessing']]",[],[],[],[],[],[],natural_language_inference,82,76
hyperparameters,"We train our model 8 with hyper - parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model .","[('train', (1, 2)), ('with', (5, 6)), ('on', (11, 12))]","[('model', (3, 4)), ('hyper - parameters', (6, 9)), ('lightly tuned', (9, 11)), ('validation set', (13, 15))]","[['model', 'with', 'hyper - parameters'], ['lightly tuned', 'on', 'validation set']]","[['hyper - parameters', 'has', 'lightly tuned']]","[['Hyperparameters', 'train', 'model']]",[],[],[],[],[],[],natural_language_inference,82,78
results,"As shown in , Max - pooling described in Section 2.2 drastically improves performance , showing the effect of accumulating information on entities .",[],"[('Max - pooling', (4, 7)), ('drastically improves', (11, 13)), ('performance', (13, 14))]",[],"[['Max - pooling', 'has', 'drastically improves'], ['drastically improves', 'has', 'performance']]",[],"[['Results', 'has', 'Max - pooling']]",[],[],[],[],[],natural_language_inference,82,80
results,"Further , we note that initializing our model with pre-trained word vectors 10 is helpful , though world knowledge of entities has been prevented by the anonymization process .","[('note that', (3, 5)), ('with', (8, 9)), ('is', (13, 14))]","[('initializing', (5, 6)), ('our model', (6, 8)), ('pre-trained word vectors', (9, 12)), ('helpful', (14, 15))]","[['our model', 'with', 'pre-trained word vectors'], ['pre-trained word vectors', 'is', 'helpful']]","[['initializing', 'has', 'our model']]","[['Results', 'note that', 'initializing']]",[],[],[],[],[],[],natural_language_inference,82,100
results,"Finally , we note that our model , full DER Network , shows the best results compared to several previous reader models , endorsing our approach as promising .","[('shows', (12, 13)), ('compared to', (16, 18))]","[('our model', (5, 7)), ('full DER Network', (8, 11)), ('best results', (14, 16)), ('several previous reader models', (18, 22))]","[['our model', 'shows', 'best results'], ['best results', 'compared to', 'several previous reader models']]","[['our model', 'name', 'full DER Network']]",[],"[['Results', 'note that', 'our model']]",[],[],[],[],[],natural_language_inference,82,102
results,"The 99 % confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [ 0.700 , 0.740 ] and [ 0.708 , 0.749 ] , respectively ( measured by bootstrap tests ) .",[],[],"[['99 % confidence intervals', 'of', 'results'], ['results', 'of', 'full DER Network and the one initialized by word2vec'], ['full DER Network and the one initialized by word2vec', 'were', '[ 0.700 , 0.740 ]'], ['full DER Network and the one initialized by word2vec', 'were', '[ 0.708 , 0.749 ]'], ['full DER Network and the one initialized by word2vec', 'on', 'test set']]",[],[],"[['Results', 'has', '99 % confidence intervals']]",[],[],[],[],[],natural_language_inference,82,103
research-problem,Story Comprehension for Predicting What Happens Next,[],"[('Story Comprehension', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Story Comprehension']]",[],[],[],[],natural_language_inference,83,2
research-problem,"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense .",[],"[('Automatic story comprehension', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic story comprehension']]",[],[],[],[],natural_language_inference,83,4
research-problem,"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists .",[],"[('automatically understanding stories', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'automatically understanding stories']]",[],[],[],[],natural_language_inference,83,14
research-problem,"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation .",[],"[('story - cloze', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'story - cloze']]",[],[],[],[],natural_language_inference,83,29
model,"In this paper we explore three semantic aspects of story understanding : ( i ) the sequence of events described in the story , ( ii ) the evolution of sentiment and emotional trajectories , and ( iii ) topical consistency .",[],[],"[['three semantic aspects', 'of', 'story understanding'], ['sequence of events', 'described in', 'story'], ['evolution', 'of', 'sentiment and emotional trajectories']]","[['three semantic aspects', 'has', 'sequence of events'], ['three semantic aspects', 'has', 'evolution'], ['three semantic aspects', 'has', 'topical consistency']]","[['Model', 'explore', 'three semantic aspects']]",[],[],[],[],[],[],natural_language_inference,83,35
model,"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .",[],[],"[['first aspect', 'motivated from', 'semantic script induction'], ['first aspect', 'evaluates if', 'events'], ['events', 'likely to', 'occur'], ['occur', 'within', 'sequence of events'], ['sequence of events', 'described in', 'preceding context'], ['events', 'described in', 'ending - alternative']]",[],[],"[['Model', 'has', 'first aspect']]",[],[],[],[],[],natural_language_inference,83,36
model,Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story .,"[('evaluating if', (5, 7)), ('described in', (9, 11)), ('considering', (16, 17)), ('of', (19, 20))]","[('sentiment', (8, 9)), ('ending option', (12, 14)), ('makes sense', (14, 16)), ('context', (18, 19)), ('story', (21, 22))]","[['makes sense', 'considering', 'context'], ['context', 'of', 'story'], ['sentiment', 'described in', 'ending option']]","[['sentiment', 'has', 'makes sense']]","[['Model', 'evaluating if', 'sentiment']]",[],[],[],[],[],[],natural_language_inference,83,42
model,Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context .,"[('analyzing if', (6, 8)), ('of', (10, 11)), ('consistent with', (15, 17))]","[('topic', (9, 10)), ('ending option', (12, 14)), ('preceding context', (18, 20))]","[['topic', 'of', 'ending option'], ['topic', 'consistent with', 'preceding context']]",[],"[['Model', 'analyzing if', 'topic']]",[],[],[],[],[],[],natural_language_inference,83,45
model,We present a log - linear model that is used to weigh the various aspects of the story using a hidden variable .,"[('present', (1, 2)), ('used to', (9, 11)), ('of', (15, 16)), ('using', (18, 19))]","[('log - linear model', (3, 7)), ('weigh', (11, 12)), ('various aspects', (13, 15)), ('story', (17, 18)), ('hidden variable', (20, 22))]","[['log - linear model', 'used to', 'weigh'], ['weigh', 'using', 'hidden variable'], ['various aspects', 'of', 'story']]","[['weigh', 'has', 'various aspects']]","[['Model', 'present', 'log - linear model']]",[],[],[],[],[],[],natural_language_inference,83,46
baselines,DSSM : It trains two deep neural networks to project the context and the ending - options into the same vector space .,"[('trains', (3, 4)), ('to project', (8, 10)), ('into', (17, 18))]","[('DSSM', (0, 1)), ('two deep neural networks', (4, 8)), ('context and the ending - options', (11, 17)), ('same vector space', (19, 22))]","[['DSSM', 'trains', 'two deep neural networks'], ['two deep neural networks', 'to project', 'context and the ending - options'], ['context and the ending - options', 'into', 'same vector space']]",[],[],"[['Baselines', 'has', 'DSSM']]",[],[],[],[],[],natural_language_inference,83,173
baselines,Msap :,[],"[('Msap', (0, 1))]",[],[],[],"[['Baselines', 'has', 'Msap']]",[],[],[],[],[],natural_language_inference,83,175
baselines,It trains a logistic regression based on stylistic and languagemodel based features .,"[('trains', (1, 2)), ('based on', (5, 7))]","[('logistic regression', (3, 5)), ('stylistic and languagemodel based features', (7, 12))]","[['logistic regression', 'based on', 'stylistic and languagemodel based features']]",[],[],[],[],"[['Msap', 'trains', 'logistic regression']]",[],[],[],natural_language_inference,83,177
baselines,LR : Our next baseline is a simple logistic regression model which is agnostic to the fact that there are multiple types of aspects .,"[('is', (5, 6)), ('agnostic to', (13, 15))]","[('LR', (0, 1)), ('simple logistic regression model', (7, 11)), ('multiple types of aspects', (20, 24))]","[['LR', 'is', 'simple logistic regression model'], ['simple logistic regression model', 'agnostic to', 'multiple types of aspects']]",[],[],"[['Baselines', 'has', 'LR']]",[],[],[],[],[],natural_language_inference,83,178
baselines,Majority Vote :,[],"[('Majority Vote', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Majority Vote']]",[],[],[],[],"[['Majority Vote', 'has', 'ensemble method']]",natural_language_inference,83,180
baselines,"This ensemble method uses the features extracted for each of the K = 3 aspects , to train K separate logistic regression models .","[('uses', (3, 4)), ('extracted for', (6, 8)), ('to train', (16, 18))]","[('ensemble method', (1, 3)), ('features', (5, 6)), ('K = 3 aspects', (11, 15)), ('K separate logistic regression models', (18, 23))]","[['ensemble method', 'uses', 'features'], ['features', 'extracted for', 'K = 3 aspects'], ['features', 'to train', 'K separate logistic regression models']]",[],[],[],[],[],[],[],[],natural_language_inference,83,181
baselines,Soft Voting :,[],"[('Soft Voting', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Soft Voting']]",[],[],[],[],[],natural_language_inference,83,183
baselines,This baseline also learns K different aspect - specific classifiers .,"[('learns', (3, 4))]","[('K different aspect - specific classifiers', (4, 10))]",[],[],[],[],[],"[['Soft Voting', 'learns', 'K different aspect - specific classifiers']]",[],[],[],natural_language_inference,83,184
baselines,Aspect - aware Ensemble :,[],"[('Aspect - aware Ensemble', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Aspect - aware Ensemble']]",[],[],[],[],[],natural_language_inference,83,189
baselines,"Like the voting methods , this baseline also trains K different aspectspecific classifiers .","[('trains', (8, 9))]","[('K different aspectspecific classifiers', (9, 13))]",[],[],[],[],[],"[['Aspect - aware Ensemble', 'trains', 'K different aspectspecific classifiers']]",[],[],[],natural_language_inference,83,190
ablation-analysis,shows the performance of a logistic regression model trained using all the features ( All ) and then using individual feature - groups .,"[('shows', (0, 1)), ('of', (3, 4)), ('using', (9, 10))]","[('performance', (2, 3)), ('logistic regression model', (5, 8)), ('trained', (8, 9)), ('all the features ( All )', (10, 16)), ('individual feature - groups', (19, 23))]","[['performance', 'of', 'logistic regression model'], ['trained', 'using', 'all the features ( All )'], ['trained', 'using', 'individual feature - groups']]","[['logistic regression model', 'has', 'trained']]","[['Ablation analysis', 'shows', 'performance']]",[],[],[],[],[],[],natural_language_inference,83,197
ablation-analysis,"We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .","[('see that', (2, 4)), ('extracted from', (6, 8)), ('analyzing', (10, 11)), ('have', (15, 16)), ('followed by', (21, 23))]","[('features', (5, 6)), ('aspect', (9, 10)), ('event - sequence', (12, 15)), ('strongest predictive power', (17, 20)), ('characterizing', (24, 25)), ('Sentiment - trajectory', (25, 28))]","[['features', 'extracted from', 'aspect'], ['aspect', 'analyzing', 'event - sequence'], ['event - sequence', 'have', 'strongest predictive power'], ['strongest predictive power', 'followed by', 'characterizing']]","[['characterizing', 'has', 'Sentiment - trajectory']]","[['Ablation analysis', 'see that', 'features']]",[],[],[],[],[],[],natural_language_inference,83,198
ablation-analysis,The features measuring top - ical consistency result in lowest accuracy but they still perform better than random on the task .,"[('measuring', (2, 3)), ('result in', (7, 9)), ('than', (16, 17))]","[('features', (1, 2)), ('top - ical consistency', (3, 7)), ('lowest accuracy', (9, 11)), ('perform better', (14, 16)), ('random', (17, 18))]","[['features', 'measuring', 'top - ical consistency'], ['top - ical consistency', 'result in', 'lowest accuracy'], ['perform better', 'than', 'random']]","[['top - ical consistency', 'has', 'perform better']]",[],"[['Ablation analysis', 'has', 'features']]",[],[],[],[],[],natural_language_inference,83,199
research-problem,LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY,[],"[('COMPUTE WORD EMBEDDINGS ON THE FLY', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'COMPUTE WORD EMBEDDINGS ON THE FLY']]",[],[],[],[],natural_language_inference,84,2
research-problem,"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well .",[],"[('Learning representations for rare words', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Learning representations for rare words']]",[],[],[],[],natural_language_inference,84,12
model,"In this paper we propose a new method for computing embeddings "" on the fly "" , which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution .","[('propose', (4, 5)), ('for computing', (8, 10)), ('jointly addresses', (18, 20)), ('for learning', (29, 31)), ('in', (32, 33))]","[('new method', (6, 8)), ('embeddings "" on the fly ""', (10, 16)), ('large vocabulary problem', (21, 24)), ('paucity of data', (26, 29)), ('representations', (31, 32)), ('long tail of the Zipfian distribution', (34, 40))]","[['new method', 'for computing', 'embeddings "" on the fly ""'], ['embeddings "" on the fly ""', 'jointly addresses', 'large vocabulary problem'], ['embeddings "" on the fly ""', 'jointly addresses', 'paucity of data'], ['paucity of data', 'for learning', 'representations'], ['representations', 'in', 'long tail of the Zipfian distribution']]",[],"[['Model', 'propose', 'new method']]",[],[],[],[],[],[],natural_language_inference,84,20
model,"This method , which we illustrate in , can be summarized as follows : instead of directly learning separate representations for all words in a potentially unbounded vocabulary , we train a network to predict the representations of words based on auxiliary data .","[('of', (15, 16)), ('train', (30, 31)), ('to predict', (33, 35)), ('based on', (39, 41))]","[('representations', (19, 20)), ('words', (22, 23)), ('network', (32, 33)), ('auxiliary data', (41, 43))]","[['network', 'to predict', 'representations'], ['representations', 'based on', 'auxiliary data'], ['representations', 'of', 'words']]",[],"[['Model', 'train', 'network']]",[],[],[],[],[],[],natural_language_inference,84,21
model,Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .,"[('used', (7, 8)), ('as', (9, 10)), ('to', (11, 12)), ('compute', (17, 18))]","[('Several sources of auxiliary data', (0, 5)), ('simultaneously', (8, 9)), ('input', (10, 11)), ('neural network', (13, 15)), ('combined representation', (19, 21))]","[['Several sources of auxiliary data', 'as', 'input'], ['input', 'to', 'neural network'], ['neural network', 'compute', 'combined representation'], ['Several sources of auxiliary data', 'used', 'simultaneously']]",[],[],"[['Model', 'has', 'Several sources of auxiliary data']]",[],[],[],[],[],natural_language_inference,84,25
model,"These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .","[('used for', (5, 7)), ('combined with', (15, 17)), ('directly trained on', (20, 23)), ('pretrained from', (28, 30))]","[('out - of - vocabulary words', (7, 13)), ('withinvocabulary word embeddings', (17, 20)), ('task of interest', (24, 27)), ('external data source', (31, 34))]","[['withinvocabulary word embeddings', 'directly trained on', 'task of interest'], ['withinvocabulary word embeddings', 'pretrained from', 'external data source']]",[],[],[],[],"[['combined representation', 'used for', 'out - of - vocabulary words'], ['combined representation', 'combined with', 'withinvocabulary word embeddings']]",[],[],[],natural_language_inference,84,26
model,"Importantly , the auxiliary data encoders are trained jointly with the objective , ensuring the preservation of semantic alignment with representations of within - vocabulary words .",[],[],"[['auxiliary data encoders', 'ensuring', 'preservation'], ['preservation', 'of', 'semantic alignment'], ['semantic alignment', 'with', 'representations'], ['representations', 'of', 'within - vocabulary words'], ['auxiliary data encoders', 'trained jointly with', 'objective']]",[],[],"[['Model', 'has', 'auxiliary data encoders']]",[],[],[],[],[],natural_language_inference,84,27
experiments,QUESTION ANSWERING,[],"[('QUESTION ANSWERING', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'QUESTION ANSWERING']]","[['QUESTION ANSWERING', 'has', 'Results']]",natural_language_inference,84,99
experiments,Looking at the results one can see that adding any external information results in a significant improvement over the baseline model ( B ) ( 3.7 - 10.5 points ) .,"[('adding', (8, 9)), ('results in', (12, 14)), ('over', (17, 18))]","[('any external information', (9, 12)), ('significant improvement', (15, 17)), ('baseline model', (19, 21)), ('3.7 - 10.5 points', (25, 29))]","[['any external information', 'results in', 'significant improvement'], ['significant improvement', 'over', 'baseline model']]","[['significant improvement', 'has', '3.7 - 10.5 points']]",[],[],[],"[['Results', 'adding', 'any external information']]",[],[],[],natural_language_inference,84,129
experiments,"When the dictionary alone is used , mean pooling ( D3 ) performs similarly to LSTM ( D4 ) .","[('performs', (12, 13)), ('to', (14, 15))]","[('dictionary alone', (2, 4)), ('mean pooling', (7, 9)), ('similarly', (13, 14)), ('LSTM', (15, 16))]","[['mean pooling', 'performs', 'similarly'], ['similarly', 'to', 'LSTM']]","[['dictionary alone', 'has', 'mean pooling']]",[],[],[],[],[],"[['Results', 'has', 'dictionary alone']]",[],natural_language_inference,84,130
experiments,"We found that adding the spelling ( S ) helps more than adding a dictionary ( D ) ( 3 points difference ) , possibly due to relatively lower coverage of our dictionary .",[],[],"[['helps more', 'than', 'adding']]","[['spelling', 'has', 'helps more'], ['adding', 'has', 'dictionary'], ['helps more', 'has', '3 points difference']]",[],[],[],"[['Results', 'adding', 'spelling']]",[],[],[],natural_language_inference,84,133
experiments,"However , the model that uses both ( SD ) has a 1.1 point advantage over the model that uses just the spelling ( S ) , demonstrating that combining several forms of auxiliary data allows the model to exploit the complementary information they provide .",[],[],"[['1.1 point advantage', 'over', 'model'], ['model', 'uses', 'just the spelling']]","[['SD', 'has', '1.1 point advantage']]",[],[],[],"[['Results', 'uses', 'SD']]",[],[],[],natural_language_inference,84,134
experiments,"The model with GLoVe embeddings ( G ) is still ahead with a 1.1 point margin , but the gap has been shrunk .","[('is', (8, 9)), ('with', (11, 12))]","[('model with GLoVe embeddings ( G )', (1, 8)), ('still ahead', (9, 11)), ('1.1 point margin', (13, 16))]","[['model with GLoVe embeddings ( G )', 'is', 'still ahead'], ['still ahead', 'with', '1.1 point margin']]",[],[],[],[],[],[],"[['Results', 'has', 'model with GLoVe embeddings ( G )']]",[],natural_language_inference,84,135
experiments,ENTAILMENT PREDICTION,[],"[('ENTAILMENT PREDICTION', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'ENTAILMENT PREDICTION']]","[['ENTAILMENT PREDICTION', 'has', 'Results']]",natural_language_inference,84,155
experiments,"Compared to the SQuAD results , an important difference is that spelling was not as useful on SNLI and MultiNLI .","[('was', (12, 13)), ('on', (16, 17))]","[('spelling', (11, 12)), ('not as useful', (13, 16)), ('SNLI and MultiNLI', (17, 20))]","[['spelling', 'was', 'not as useful'], ['not as useful', 'on', 'SNLI and MultiNLI']]",[],[],[],[],[],[],"[['Results', 'has', 'spelling']]",[],natural_language_inference,84,172
experiments,"We also note that we tried using fixed random embeddings for OOV words as proposed by , and that this method did not bring a significant advantage over the baseline .","[('using', (6, 7)), ('for', (10, 11)), ('over', (27, 28))]","[('fixed random embeddings', (7, 10)), ('OOV words', (11, 13)), ('did not bring a significant advantage', (21, 27)), ('baseline', (29, 30))]","[['fixed random embeddings', 'for', 'OOV words'], ['did not bring a significant advantage', 'over', 'baseline']]","[['fixed random embeddings', 'has', 'did not bring a significant advantage']]",[],[],[],"[['Results', 'using', 'fixed random embeddings']]",[],[],[],natural_language_inference,84,173
experiments,"shows that , as expected , dictionary - enabled models significantly outperform baseline models for sentences containing rare words .","[('for', (14, 15)), ('containing', (16, 17))]","[('dictionary - enabled models', (6, 10)), ('significantly outperform', (10, 12)), ('baseline models', (12, 14)), ('sentences', (15, 16)), ('rare words', (17, 19))]","[['significantly outperform', 'for', 'sentences'], ['sentences', 'containing', 'rare words']]","[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]",[],[],[],[],[],"[['Results', 'has', 'dictionary - enabled models']]",[],natural_language_inference,84,178
experiments,LANGUAGE MODELLING,[],"[('LANGUAGE MODELLING', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'LANGUAGE MODELLING']]","[['LANGUAGE MODELLING', 'has', 'Results']]",natural_language_inference,84,179
experiments,"Similarly to our other experiments , using external information to compute embeddings of unknown words helps in all cases .","[('using', (6, 7)), ('to compute', (9, 11)), ('of', (12, 13)), ('in', (16, 17))]","[('external information', (7, 9)), ('embeddings', (11, 12)), ('unknown words', (13, 15)), ('helps', (15, 16)), ('all cases', (17, 19))]","[['external information', 'to compute', 'embeddings'], ['embeddings', 'of', 'unknown words'], ['helps', 'in', 'all cases']]","[['external information', 'has', 'helps']]",[],[],[],"[['Results', 'using', 'external information']]",[],[],[],natural_language_inference,84,199
experiments,"We note that lemma + lowercase performs worse than any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .","[('note that', (1, 3)), ('performs', (6, 7)), ('than', (8, 9)), ('with', (11, 12))]","[('lemma + lowercase', (3, 6)), ('worse', (7, 8)), ('any model', (9, 11)), ('dictionary', (13, 14))]","[['lemma + lowercase', 'performs', 'worse'], ['worse', 'than', 'any model'], ['any model', 'with', 'dictionary']]",[],[],[],[],"[['Results', 'note that', 'lemma + lowercase']]",[],[],[],natural_language_inference,84,201
experiments,Adding spelling consistently helps more than adding dictionary definitions .,"[('Adding', (0, 1)), ('than', (5, 6))]","[('spelling', (1, 2)), ('consistently helps more', (2, 5)), ('adding', (6, 7)), ('dictionary definitions', (7, 9))]","[['consistently helps more', 'than', 'adding']]","[['spelling', 'has', 'consistently helps more'], ['adding', 'has', 'dictionary definitions']]",[],[],[],"[['Results', 'Adding', 'spelling']]",[],[],[],natural_language_inference,84,202
experiments,"Using both dictionary and spelling is consistently slightly better than using just spelling , and the improvement is more pronounced in the restricted setting .","[('Using', (0, 1)), ('than', (9, 10))]","[('dictionary and spelling', (2, 5)), ('consistently slightly better', (6, 9)), ('just spelling', (11, 13))]","[['consistently slightly better', 'than', 'just spelling']]","[['dictionary and spelling', 'has', 'consistently slightly better']]",[],[],[],"[['Results', 'Using', 'dictionary and spelling']]",[],[],[],natural_language_inference,84,204
experiments,Using Glo Ve embeddings results in the best perplexity .,"[('results in', (4, 6))]","[('Glo Ve embeddings', (1, 4)), ('best perplexity', (7, 9))]","[['Glo Ve embeddings', 'results in', 'best perplexity']]",[],[],[],[],[],[],"[['Results', 'Using', 'Glo Ve embeddings']]",[],natural_language_inference,84,205
research-problem,Enhanced LSTM for Natural Language Inference,[],"[('Natural Language Inference', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,85,2
research-problem,Reasoning and inference are central to human and artificial intelligence .,[],"[('Reasoning and inference', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reasoning and inference']]",[],[],[],[],natural_language_inference,85,4
research-problem,"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise .",[],"[('natural language inference ( NLI )', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,85,15
model,"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .","[('enhancing', (30, 31)), ('based on', (34, 36))]","[('sequential inference models', (31, 34)), ('chain models', (36, 38))]","[['sequential inference models', 'based on', 'chain models']]",[],"[['Model', 'enhancing', 'sequential inference models']]",[],[],[],[],[],[],natural_language_inference,85,24
research-problem,Exploring syntax for NLI is very attractive to us .,[],"[('NLI', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,85,26
model,"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .","[('explicitly encoding', (4, 6)), ('with', (8, 9)), ('in', (11, 12))]","[('parsing information', (6, 8)), ('recursive networks', (9, 11)), ('local inference modeling', (13, 16)), ('inference composition', (17, 19))]","[['parsing information', 'with', 'recursive networks'], ['parsing information', 'in', 'local inference modeling'], ['parsing information', 'in', 'inference composition']]",[],"[['Model', 'explicitly encoding', 'parsing information']]",[],[],[],[],[],[],natural_language_inference,85,30
hyperparameters,"We use the Adam method ( Kingma and Ba , 2014 ) for optimization .","[('use', (1, 2)), ('for', (12, 13))]","[('Adam method', (3, 5)), ('optimization', (13, 14))]","[['Adam method', 'for', 'optimization']]",[],"[['Hyperparameters', 'use', 'Adam method']]",[],[],[],[],[],[],natural_language_inference,85,167
hyperparameters,The first momentum is set to be 0.9 and the second 0.999 .,"[('set to be', (4, 7))]","[('first momentum', (1, 3)), ('0.9', (7, 8)), ('second', (10, 11)), ('0.999', (11, 12))]","[['first momentum', 'set to be', '0.9']]","[['second', 'has', '0.999']]",[],"[['Hyperparameters', 'has', 'first momentum'], ['Hyperparameters', 'has', 'second']]",[],[],[],[],[],natural_language_inference,85,168
hyperparameters,The initial learning rate is 0.0004 and the batch size is 32 .,[],[],"[['initial learning rate', 'is', '0.0004'], ['batch size', 'is', '32']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate'], ['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,85,169
hyperparameters,"All hidden states of LSTMs , tree - LSTMs , and word embeddings have 300 dimensions .","[('of', (3, 4)), ('have', (13, 14))]","[('hidden states', (1, 3)), ('LSTMs', (4, 5)), ('tree - LSTMs', (6, 9)), ('word embeddings', (11, 13)), ('300 dimensions', (14, 16))]","[['hidden states', 'of', 'LSTMs'], ['hidden states', 'of', 'tree - LSTMs'], ['hidden states', 'of', 'word embeddings'], ['hidden states', 'have', '300 dimensions']]",[],[],"[['Hyperparameters', 'has', 'hidden states']]",[],[],[],[],[],natural_language_inference,85,170
hyperparameters,"We use dropout with a rate of 0.5 , which is applied to all feedforward connections .","[('with', (3, 4)), ('of', (6, 7)), ('applied to', (11, 13))]","[('dropout', (2, 3)), ('rate', (5, 6)), ('0.5', (7, 8)), ('all feedforward connections', (13, 16))]","[['dropout', 'with', 'rate'], ['rate', 'of', '0.5'], ['dropout', 'applied to', 'all feedforward connections']]",[],[],"[['Hyperparameters', 'use', 'dropout']]",[],[],[],[],[],natural_language_inference,85,171
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"[('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('our word embeddings', (11, 14))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'our word embeddings']]",[],[],"[['Hyperparameters', 'use', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],[],[],natural_language_inference,85,172
hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('initialized', (10, 11)), ('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('randomly', (11, 12)), ('Gaussian samples', (13, 15))]","[['Out - of - vocabulary ( OOV ) words', 'initialized', 'randomly'], ['randomly', 'with', 'Gaussian samples']]",[],[],"[['Hyperparameters', 'has', 'Out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],natural_language_inference,85,173
results,"Our final model achieves the accuracy of 88.6 % , the best result observed on SNLI , while our enhanced sequential encoding model attains an accuracy of 88.0 % , which also outperform the previous models .",[],[],"[['accuracy', 'of', '88.6 %'], ['accuracy', 'of', '88.0 %'], ['Our final model', 'achieves', 'accuracy'], ['accuracy', 'of', '88.6 %'], ['best result', 'observed on', 'SNLI'], ['our enhanced sequential encoding model', 'attains', 'accuracy'], ['accuracy', 'of', '88.0 %']]","[['88.6 %', 'has', 'best result'], ['88.0 %', 'has', 'outperform'], ['outperform', 'has', 'previous models']]",[],"[['Results', 'has', 'Our final model'], ['Results', 'has', 'our enhanced sequential encoding model']]",[],[],[],[],[],natural_language_inference,85,185
results,"In general , adding intra-sentence attention yields further improvement , which is not very surprising as it could help align the relevant text spans between premise and hypothesis .","[('adding', (3, 4)), ('yields', (6, 7))]","[('intra-sentence attention', (4, 6)), ('further improvement', (7, 9))]","[['intra-sentence attention', 'yields', 'further improvement']]",[],"[['Results', 'adding', 'intra-sentence attention']]",[],[],[],[],[],[],natural_language_inference,85,191
results,"The table shows that our ESIM model achieves an accuracy of 88.0 % , which has already outperformed all the previous models , including those using much more complicated network architectures .",[],[],"[['our ESIM model', 'achieves', 'accuracy'], ['previous models', 'including', 'more complicated network architectures']]","[['our ESIM model', 'has', 'outperformed'], ['outperformed', 'has', 'previous models']]",[],"[['Results', 'has', 'our ESIM model']]",[],[],[],[],[],natural_language_inference,85,195
results,"We ensemble our ESIM model with syntactic tree - LSTMs based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM , attaining an accuracy of 88.6 % .",[],[],"[['our ESIM model', 'with', 'syntactic tree - LSTMs'], ['syntactic tree - LSTMs', 'based on', 'syntactic parse trees'], ['our ESIM model', 'achieve', 'significant improvement'], ['significant improvement', 'over', 'our best sequential encoding model ESIM'], ['significant improvement', 'attaining', 'accuracy']]",[],"[['Results', 'ensemble', 'our ESIM model']]",[],[],[],[],[],[],natural_language_inference,85,196
ablation-analysis,Each tree node is implemented with a tree - LSTM block same as in model .,"[('implemented with', (4, 6))]","[('Each tree node', (0, 3)), ('tree - LSTM block', (7, 11))]","[['Each tree node', 'implemented with', 'tree - LSTM block']]",[],[],"[['Ablation analysis', 'has', 'Each tree node']]",[],[],[],[],[],natural_language_inference,85,204
ablation-analysis,"shows that with this replacement , the performance drops to 88.2 % .","[('performance', (7, 8)), ('to', (9, 10))]","[('drops', (8, 9)), ('88.2 %', (10, 12))]","[['drops', 'to', '88.2 %']]",[],[],[],[],"[['tree - LSTM block', 'performance', 'drops']]",[],[],[],natural_language_inference,85,205
ablation-analysis,"If we remove the pooling layer in inference composition and replace it with summation as in , the accuracy drops to 87.1 % .","[('remove', (2, 3)), ('in', (6, 7)), ('replace it with', (10, 13)), ('to', (20, 21))]","[('pooling layer', (4, 6)), ('inference composition', (7, 9)), ('summation', (13, 14)), ('accuracy', (18, 19)), ('drops', (19, 20)), ('87.1 %', (21, 23))]","[['pooling layer', 'in', 'inference composition'], ['pooling layer', 'replace it with', 'summation'], ['drops', 'to', '87.1 %']]","[['accuracy', 'has', 'drops'], ['summation', 'has', 'accuracy'], ['accuracy', 'has', 'drops'], ['accuracy', 'has', 'drops']]","[['Ablation analysis', 'remove', 'pooling layer']]",[],[],[],[],[],[],natural_language_inference,85,207
ablation-analysis,"If we remove the difference and elementwise product from the local inference enhancement layer , the accuracy drops to 87.0 % .","[('from', (8, 9)), ('to', (18, 19))]","[('difference and elementwise product', (4, 8)), ('local inference enhancement layer', (10, 14)), ('accuracy', (16, 17)), ('drops', (17, 18)), ('87.0 %', (19, 21))]","[['difference and elementwise product', 'from', 'local inference enhancement layer'], ['drops', 'to', '87.0 %']]","[['local inference enhancement layer', 'has', 'accuracy']]",[],"[['Ablation analysis', 'remove', 'difference and elementwise product']]",[],[],[],[],[],natural_language_inference,85,208
ablation-analysis,"To provide some detailed comparison with , replacing bidirectional LSTMs in inference composition and also input encoding with feedforward neural network reduces the accuracy to 87.3 % and 86.3 % respectively .","[('with', (5, 6)), ('replacing', (7, 8)), ('in', (10, 11)), ('to', (24, 25))]","[('bidirectional LSTMs', (8, 10)), ('inference composition', (11, 13)), ('input encoding', (15, 17)), ('feedforward neural network', (18, 21)), ('reduces', (21, 22)), ('accuracy', (23, 24)), ('87.3 % and 86.3 %', (25, 30))]","[['bidirectional LSTMs', 'in', 'inference composition'], ['accuracy', 'to', '87.3 % and 86.3 %'], ['input encoding', 'with', 'feedforward neural network']]","[['reduces', 'has', 'accuracy']]","[['Ablation analysis', 'replacing', 'bidirectional LSTMs'], ['Ablation analysis', 'replacing', 'reduces'], ['Ablation analysis', 'replacing', 'input encoding']]",[],[],[],[],[],[],natural_language_inference,85,209
ablation-analysis,"If we remove the premise - based attention from ESIM ( model 23 ) , the accuracy drops to 87.2 % on the test set .","[('from', (8, 9)), ('to', (18, 19)), ('on', (21, 22))]","[('premise - based attention', (4, 8)), ('ESIM', (9, 10)), ('accuracy', (16, 17)), ('drops', (17, 18)), ('87.2 %', (19, 21)), ('test set', (23, 25))]","[['premise - based attention', 'from', 'ESIM'], ['drops', 'to', '87.2 %'], ['drops', 'on', 'test set']]","[['premise - based attention', 'has', 'accuracy']]",[],"[['Ablation analysis', 'remove', 'premise - based attention']]",[],[],[],[],[],natural_language_inference,85,213
ablation-analysis,"Removing the hypothesis - based attention ( model 24 ) decrease the accuracy to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .","[('Removing', (0, 1)), ('to', (13, 14))]","[('hypothesis - based attention ( model 24 )', (2, 10)), ('decrease', (10, 11)), ('accuracy', (12, 13)), ('86.5 %', (14, 16))]","[['accuracy', 'to', '86.5 %']]","[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","[['Ablation analysis', 'Removing', 'hypothesis - based attention ( model 24 )']]",[],[],[],[],[],[],natural_language_inference,85,215
research-problem,Multi - Perspective Context Matching for Machine Comprehension,[],"[('Machine Comprehension', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,86,2
research-problem,"Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques .",[],"[('machine comprehension ( MC )', (1, 6)), ('MC', (30, 31))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension ( MC )'], ['Contribution', 'has research problem', 'MC']]",[],[],[],[],natural_language_inference,86,4
model,"In this work , we focus on the SQuAD dataset and propose an end - to - end deep neural network model for machine comprehension .","[('propose', (11, 12)), ('for', (22, 23))]","[('end - to - end deep neural network model', (13, 22)), ('machine comprehension', (23, 25))]","[['end - to - end deep neural network model', 'for', 'machine comprehension']]",[],"[['Model', 'propose', 'end - to - end deep neural network model']]",[],[],[],[],[],[],natural_language_inference,86,27
model,"Based on this assumption , we design a Multi - Perspective Context Matching ( MPCM ) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives .","[('design', (6, 7)), ('to identify', (17, 19)), ('by matching', (22, 24)), ('of', (26, 27)), ('in', (29, 30)), ('with', (32, 33)), ('from', (35, 36))]","[('Multi - Perspective Context Matching ( MPCM ) model', (8, 17)), ('answer span', (20, 22)), ('context', (25, 26)), ('each point', (27, 29)), ('passage', (31, 32)), ('question', (34, 35)), ('multiple perspectives', (36, 38))]","[['Multi - Perspective Context Matching ( MPCM ) model', 'to identify', 'answer span'], ['answer span', 'by matching', 'context'], ['context', 'with', 'question'], ['question', 'from', 'multiple perspectives'], ['context', 'of', 'each point'], ['each point', 'in', 'passage']]",[],"[['Model', 'design', 'Multi - Perspective Context Matching ( MPCM ) model']]",[],[],[],[],[],[],natural_language_inference,86,29
model,"Instead of enumerating all the possible spans explicitly and ranking them , our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage .","[('identifies', (14, 15)), ('by predicting', (18, 20)), ('with', (26, 27)), ('across', (31, 32))]","[('answer span', (16, 18)), ('beginning and ending points', (21, 25)), ('individually', (25, 26)), ('globally normalized probability distributions', (27, 31)), ('whole passage', (33, 35))]","[['answer span', 'by predicting', 'beginning and ending points'], ['beginning and ending points', 'with', 'globally normalized probability distributions'], ['globally normalized probability distributions', 'across', 'whole passage']]","[['beginning and ending points', 'has', 'individually']]","[['Model', 'identifies', 'answer span']]",[],[],[],[],[],[],natural_language_inference,86,30
experimental-setup,We process the corpus with the tokenizer from Stanford CorNLP .,"[('process', (1, 2)), ('with', (4, 5)), ('from', (7, 8))]","[('corpus', (3, 4)), ('tokenizer', (6, 7)), ('Stanford CorNLP', (8, 10))]","[['corpus', 'with', 'tokenizer'], ['tokenizer', 'from', 'Stanford CorNLP']]",[],"[['Experimental setup', 'process', 'corpus']]",[],[],[],[],[],[],natural_language_inference,86,102
experimental-setup,"To initialize the word embeddings in the word representation layer , we use the 300 - dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus .","[('initialize', (1, 2)), ('in', (5, 6)), ('use', (12, 13)), ('pre-trained from', (20, 22))]","[('word embeddings', (3, 5)), ('word representation layer', (7, 10)), ('300 - dimensional GloVe word vectors', (14, 20)), ('840B Common Crawl corpus', (23, 27))]","[['word embeddings', 'in', 'word representation layer'], ['word embeddings', 'use', '300 - dimensional GloVe word vectors'], ['300 - dimensional GloVe word vectors', 'pre-trained from', '840B Common Crawl corpus']]",[],"[['Experimental setup', 'initialize', 'word embeddings']]",[],[],[],[],[],[],natural_language_inference,86,104
experimental-setup,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","[('For', (0, 1)), ('initialize', (13, 14))]","[('out - of - vocabulary ( OOV ) words', (2, 11)), ('word embeddings', (15, 17)), ('randomly', (17, 18))]","[['out - of - vocabulary ( OOV ) words', 'initialize', 'word embeddings']]","[['word embeddings', 'has', 'randomly']]","[['Experimental setup', 'For', 'out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],[],natural_language_inference,86,105
experimental-setup,"We set the hidden size as 100 for all the LSTM layers , and set the number of perspectives l of our multiperspective matching function ( Equation ( 5 ) ) as 50 .",[],[],"[['number of perspectives l', 'as', '50'], ['number of perspectives l', 'of', 'our multiperspective matching function'], ['hidden size', 'as', '100'], ['hidden size', 'for', 'all the LSTM layers']]",[],"[['Experimental setup', 'set', 'number of perspectives l'], ['Experimental setup', 'set', 'hidden size']]",[],[],[],[],[],[],natural_language_inference,86,106
experimental-setup,"We apply dropout to every layers in , and set the dropout ratio as 0.2 .","[('apply', (1, 2)), ('to', (3, 4)), ('as', (13, 14))]","[('dropout', (2, 3)), ('every layers', (4, 6)), ('dropout ratio', (11, 13)), ('0.2', (14, 15))]","[['dropout ratio', 'as', '0.2'], ['dropout', 'to', 'every layers']]",[],"[['Experimental setup', 'apply', 'dropout']]","[['Experimental setup', 'set', 'dropout ratio']]",[],[],[],[],[],natural_language_inference,86,107
experimental-setup,"To train the model , we minimize the cross entropy of the be - ginning and end points , and use the ADAM optimizer to update parameters .","[('minimize', (6, 7)), ('of', (10, 11)), ('use', (20, 21)), ('to update', (24, 26))]","[('cross entropy', (8, 10)), ('be - ginning and end points', (12, 18)), ('ADAM optimizer', (22, 24)), ('parameters', (26, 27))]","[['cross entropy', 'of', 'be - ginning and end points'], ['ADAM optimizer', 'to update', 'parameters']]",[],"[['Experimental setup', 'minimize', 'cross entropy'], ['Experimental setup', 'use', 'ADAM optimizer']]",[],[],[],[],[],[],natural_language_inference,86,108
experimental-setup,We set the learning rate as 0.0001 .,"[('as', (5, 6))]","[('learning rate', (3, 5)), ('0.0001', (6, 7))]","[['learning rate', 'as', '0.0001']]",[],[],"[['Experimental setup', 'set', 'learning rate']]",[],[],[],[],[],natural_language_inference,86,109
ablation-analysis,We can see that removing any components from the MPCM model decreases the performance significantly .,"[('removing', (4, 5)), ('from', (7, 8))]","[('any components', (5, 7)), ('MPCM model', (9, 11)), ('decreases', (11, 12)), ('performance', (13, 14)), ('significantly', (14, 15))]","[['any components', 'from', 'MPCM model']]","[['any components', 'has', 'decreases'], ['decreases', 'has', 'performance'], ['performance', 'has', 'significantly']]","[['Ablation analysis', 'removing', 'any components']]",[],[],[],[],[],[],natural_language_inference,86,126
ablation-analysis,"Among all the layers , the Aggregation Layer is the most crucial layer .","[('Among', (0, 1)), ('is', (8, 9))]","[('all the layers', (1, 4)), ('Aggregation Layer', (6, 8)), ('most crucial layer', (10, 13))]","[['Aggregation Layer', 'is', 'most crucial layer']]","[['all the layers', 'has', 'Aggregation Layer']]","[['Ablation analysis', 'Among', 'all the layers']]",[],[],[],[],[],[],natural_language_inference,86,127
ablation-analysis,"Among all the matching strategies , Maxpooling - Matching has the biggest effect .",[],"[('all the matching strategies', (1, 5)), ('Maxpooling - Matching', (6, 9)), ('biggest effect', (11, 13))]",[],"[['all the matching strategies', 'has', 'Maxpooling - Matching'], ['Maxpooling - Matching', 'has', 'biggest effect']]",[],"[['Ablation analysis', 'Among', 'all the matching strategies']]",[],[],[],[],[],natural_language_inference,86,128
research-problem,SG - Net : Syntax - Guided Machine Reading Comprehension,[],"[('Machine Reading Comprehension', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,87,2
research-problem,"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering .",[],"[('machine reading comprehension ( MRC )', (22, 28))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading comprehension ( MRC )']]",[],[],[],[],natural_language_inference,87,13
research-problem,We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,[],"[('MRC', (6, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'MRC']]",[],[],[],[],natural_language_inference,87,17
model,"In this paper , we extend the self - attention mechanism with syntax - guided constraint , to capture syntax related parts with each concerned word .",[],[],"[['self - attention mechanism', 'with', 'syntax - guided constraint'], ['self - attention mechanism', 'to capture', 'syntax related parts'], ['syntax related parts', 'with', 'each concerned word']]",[],"[['Model', 'extend', 'self - attention mechanism']]",[],[],[],[],[],[],natural_language_inference,87,27
model,"Specifically , we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence , namely syntactic dependency of interest ( SDOI ) , by regarding each word as a child node and the SDOI consists all its ancestor nodes and itself in the dependency parsing tree .",[],[],"[['pre-trained dependency syntactic parse tree structure', 'namely', 'syntactic dependency of interest ( SDOI )'], ['pre-trained dependency syntactic parse tree structure', 'to produce', 'related nodes'], ['related nodes', 'for', 'each word'], ['each word', 'in', 'sentence'], ['each word', 'regarding', 'each word'], ['each word', 'as', 'child node']]",[],"[['Model', 'adopt', 'pre-trained dependency syntactic parse tree structure']]",[],[],[],[],[],[],natural_language_inference,87,28
model,"To effectively accommodate such SDOI information , we propose a novel syntax - guided network ( SG - Net ) , which fuses the original SAN and SDOI - SAN , to provide more linguistically inspired representation for challenging reading comprehension tasks 1 .","[('accommodate', (2, 3)), ('propose', (8, 9)), ('fuses', (22, 23)), ('to provide', (31, 33)), ('for', (37, 38))]","[('SDOI information', (4, 6)), ('novel syntax - guided network ( SG - Net )', (10, 20)), ('original SAN and SDOI - SAN', (24, 30)), ('more linguistically inspired representation', (33, 37)), ('reading comprehension tasks', (39, 42))]","[['SDOI information', 'propose', 'novel syntax - guided network ( SG - Net )'], ['novel syntax - guided network ( SG - Net )', 'to provide', 'more linguistically inspired representation'], ['more linguistically inspired representation', 'for', 'reading comprehension tasks'], ['novel syntax - guided network ( SG - Net )', 'fuses', 'original SAN and SDOI - SAN']]",[],"[['Model', 'accommodate', 'SDOI information']]",[],[],[],[],[],[],natural_language_inference,87,30
hyperparameters,We adopt the Whole Word Masking BERT as the baseline 6 .,"[('adopt', (1, 2))]","[('Whole Word Masking BERT', (3, 7))]",[],[],"[['Hyperparameters', 'adopt', 'Whole Word Masking BERT']]",[],[],[],[],[],[],natural_language_inference,87,129
hyperparameters,"The initial learning rate is set in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .",[],[],"[['initial learning rate', 'with', 'warm - up rate'], ['warm - up rate', 'of', '0.1'], ['initial learning rate', 'with', 'L2 weight decay'], ['L2 weight decay', 'of', '0.01'], ['initial learning rate', 'set in', '{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],[],[],[],[],natural_language_inference,87,130
hyperparameters,"The batch size is selected in { 16 , 20 , 32 } .","[('selected in', (4, 6))]","[('batch size', (1, 3)), ('{ 16 , 20 , 32 }', (6, 13))]","[['batch size', 'selected in', '{ 16 , 20 , 32 }']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,87,131
hyperparameters,The maximum number of epochs is set to 3 or 10 depending on tasks .,"[('set to', (6, 8))]","[('maximum number of epochs', (1, 5)), ('3 or 10', (8, 11))]","[['maximum number of epochs', 'set to', '3 or 10']]",[],[],"[['Hyperparameters', 'has', 'maximum number of epochs']]",[],[],[],[],[],natural_language_inference,87,132
hyperparameters,The weight ?,[],"[('weight', (1, 2))]",[],[],[],"[['Hyperparameters', 'has', 'weight']]",[],[],[],[],[],natural_language_inference,87,133
hyperparameters,in the dual context aggregation is 0.5 .,"[('in', (0, 1)), ('is', (5, 6))]","[('dual context aggregation', (2, 5)), ('0.5', (6, 7))]",[],[],[],[],[],"[['weight', 'in', 'dual context aggregation'], ['weight', 'is', '0.5']]",[],[],[],natural_language_inference,87,134
hyperparameters,"All the texts are tokenized using wordpieces , and the maximum input length is set to 384 for both of SQuAD and RACE .","[('are', (3, 4)), ('using', (5, 6)), ('set to', (14, 16)), ('for', (17, 18))]","[('texts', (2, 3)), ('tokenized', (4, 5)), ('wordpieces', (6, 7)), ('maximum input length', (10, 13)), ('384', (16, 17)), ('SQuAD and RACE', (20, 23))]","[['maximum input length', 'set to', '384'], ['384', 'for', 'SQuAD and RACE'], ['texts', 'are', 'tokenized'], ['tokenized', 'using', 'wordpieces']]",[],[],"[['Hyperparameters', 'has', 'maximum input length'], ['Hyperparameters', 'has', 'texts']]",[],[],[],[],[],natural_language_inference,87,135
results,It also outperforms all the published works and achieves the 2nd place on the leaderboard when submitting SG - NET .,"[('achieves', (8, 9)), ('on', (12, 13)), ('when submitting', (15, 17))]","[('outperforms', (2, 3)), ('all the published works', (3, 7)), ('2nd place', (10, 12)), ('leaderboard', (14, 15)), ('SG - NET', (17, 20))]","[['outperforms', 'achieves', '2nd place'], ['2nd place', 'when submitting', 'SG - NET'], ['2nd place', 'on', 'leaderboard']]","[['outperforms', 'has', 'all the published works']]",[],"[['Results', 'has', 'outperforms']]",[],[],[],[],[],natural_language_inference,87,143
results,"We also find that adding an extra answer verifier module could yield better result , which is pre-trained only to determine whether question is answerable or not with the same training data as SG - Net .","[('adding', (4, 5)), ('yield', (11, 12))]","[('extra answer verifier module', (6, 10)), ('better result', (12, 14))]","[['extra answer verifier module', 'yield', 'better result']]",[],"[['Results', 'adding', 'extra answer verifier module']]",[],[],[],[],[],[],natural_language_inference,87,144
research-problem,Long Short - Term Memory - Networks for Machine Reading,[],"[('Machine Reading', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading']]",[],[],[],[],natural_language_inference,88,2
model,The idea is to use multiple memory slots outside the recurrence to piece - wise store representations of the input ; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller .,"[('to', (3, 4)), ('use', (4, 5)), ('outside', (8, 9)), ('of', (17, 18)), ('for', (25, 26)), ('modeled as', (30, 32)), ('with', (35, 36))]","[('multiple memory slots', (5, 8)), ('recurrence', (10, 11)), ('piece - wise store representations', (12, 17)), ('input', (19, 20)), ('read and write operations', (21, 25)), ('each slot', (26, 28)), ('attention mechanism', (33, 35)), ('recurrent controller', (37, 39))]","[['multiple memory slots', 'outside', 'recurrence'], ['recurrence', 'to', 'piece - wise store representations'], ['piece - wise store representations', 'of', 'input'], ['read and write operations', 'for', 'each slot'], ['each slot', 'modeled as', 'attention mechanism'], ['attention mechanism', 'with', 'recurrent controller']]",[],"[['Model', 'use', 'multiple memory slots'], ['Model', 'use', 'read and write operations']]",[],[],[],[],[],[],natural_language_inference,88,42
model,We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens .,"[('leverage', (2, 3)), ('to empower', (6, 8)), ('with', (11, 12)), ('to discover', (20, 22)), ('among', (23, 24))]","[('memory and attention', (3, 6)), ('recurrent network', (9, 11)), ('stronger memorization capability', (12, 15)), ('ability', (19, 20)), ('relations', (22, 23)), ('tokens', (24, 25))]","[['memory and attention', 'to empower', 'recurrent network'], ['recurrent network', 'with', 'stronger memorization capability'], ['recurrent network', 'with', 'ability'], ['ability', 'to discover', 'relations'], ['relations', 'among', 'tokens']]",[],"[['Model', 'leverage', 'memory and attention']]",[],[],[],[],[],[],natural_language_inference,88,43
model,This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing .,"[('realized by', (2, 4)), ('in', (9, 10)), ('of', (12, 13)), ('together with', (16, 18)), ('for', (19, 20))]","[('inserting', (4, 5)), ('memory network module', (6, 9)), ('update', (11, 12)), ('recurrent network', (14, 16)), ('attention', (18, 19)), ('memory addressing', (20, 22))]","[['memory network module', 'together with', 'attention'], ['attention', 'for', 'memory addressing'], ['memory network module', 'in', 'update'], ['update', 'of', 'recurrent network']]","[['inserting', 'has', 'memory network module']]","[['Model', 'realized by', 'inserting']]",[],[],[],[],[],[],natural_language_inference,88,44
model,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a reading simulator that can be used for sequence processing tasks .","[('term', (6, 7)), ('is', (18, 19)), ('used for', (25, 27))]","[('Long Short - Term Memory - Network ( LSTMN )', (7, 17)), ('reading simulator', (20, 22)), ('sequence processing tasks', (27, 30))]","[['Long Short - Term Memory - Network ( LSTMN )', 'is', 'reading simulator'], ['reading simulator', 'used for', 'sequence processing tasks']]",[],"[['Model', 'term', 'Long Short - Term Memory - Network ( LSTMN )']]",[],[],[],[],[],[],natural_language_inference,88,47
model,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,"[('processes', (2, 3)), ('while', (5, 6)), ('past tokens', (8, 10)), ('relate to', (18, 20)), ('being', (23, 24))]","[('text', (3, 4)), ('incrementally', (4, 5)), ('learning', (6, 7)), ('in the memory', (10, 13)), ('to what extent', (14, 17)), ('current token', (21, 23)), ('processed', (24, 25))]","[['text', 'while', 'learning'], ['learning', 'past tokens', 'in the memory'], ['learning', 'past tokens', 'to what extent'], ['to what extent', 'relate to', 'current token'], ['current token', 'being', 'processed']]","[['text', 'has', 'incrementally']]","[['Model', 'processes', 'text']]",[],[],[],[],[],[],natural_language_inference,88,49
model,"As a result , the model induces undirected relations among tokens as an intermediate step of learning representations .","[('induces', (6, 7)), ('among', (9, 10)), ('as an', (11, 13)), ('of learning', (15, 17))]","[('undirected relations', (7, 9)), ('tokens', (10, 11)), ('intermediate step', (13, 15)), ('representations', (17, 18))]","[['undirected relations', 'as an', 'intermediate step'], ['intermediate step', 'of learning', 'representations'], ['undirected relations', 'among', 'tokens']]",[],"[['Model', 'induces', 'undirected relations']]",[],[],[],[],[],[],natural_language_inference,88,50
code,Our code is available at https://github.com/cheng6076/,[],"[('https://github.com/cheng6076/', (5, 6))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/cheng6076/']]",[],[],[],[],natural_language_inference,88,144
experiments,Language Modeling,[],"[('Language Modeling', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Language Modeling']]","[['Language Modeling', 'has', 'Hyperparameters']]",natural_language_inference,88,146
experiments,"We used stochastic gradient descent for optimization with an initial learning rate of 0.65 , which decays by a factor of 0.85 per epoch if no significant improvement has been observed on the validation set .",[],[],"[['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.65'], ['stochastic gradient descent', 'for', 'optimization'], ['decays', 'by', 'factor'], ['factor', 'of', '0.85'], ['factor', 'per', 'epoch']]","[['stochastic gradient descent', 'has', 'decays']]",[],[],[],"[['Hyperparameters', 'used', 'stochastic gradient descent']]",[],[],[],natural_language_inference,88,152
experiments,We renormalize the gradient if its norm is greater than 5 .,"[('if', (4, 5)), ('greater than', (8, 10))]","[('renormalize', (1, 2)), ('gradient', (3, 4)), ('norm', (6, 7)), ('5', (10, 11))]","[['gradient', 'if', 'norm'], ['norm', 'greater than', '5']]","[['renormalize', 'has', 'gradient']]",[],[],[],[],[],"[['Hyperparameters', 'has', 'renormalize']]",[],natural_language_inference,88,153
experiments,The mini - batch size was set to 40 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('40', (8, 9))]","[['mini - batch size', 'set to', '40']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'mini - batch size']]",[],natural_language_inference,88,154
experiments,The dimensions of the word embeddings were set to 150 for all models .,"[('of', (2, 3)), ('set to', (7, 9)), ('for', (10, 11))]","[('dimensions', (1, 2)), ('word embeddings', (4, 6)), ('150', (9, 10)), ('all models', (11, 13))]","[['dimensions', 'of', 'word embeddings'], ['dimensions', 'set to', '150'], ['150', 'for', 'all models']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'dimensions']]",[],natural_language_inference,88,155
experiments,The first one is a Kneser - Ney 5 - gram language model ( KN5 ) which generally serves as a non-neural baseline for the language modeling task .,"[('serves as', (18, 20)), ('for', (23, 24))]","[('Kneser - Ney 5 - gram language model ( KN5 )', (5, 16)), ('non-neural baseline', (21, 23)), ('language modeling task', (25, 28))]","[['Kneser - Ney 5 - gram language model ( KN5 )', 'serves as', 'non-neural baseline'], ['non-neural baseline', 'for', 'language modeling task']]",[],[],[],[],[],[],"[['Baselines', 'has', 'Kneser - Ney 5 - gram language model ( KN5 )']]",[],natural_language_inference,88,157
experiments,The gated - feedback LSTM has feedback gates connecting the hidden states across multiple time steps as an adaptive control of the information flow .,"[('connecting', (8, 9)), ('across', (12, 13)), ('as', (16, 17)), ('of', (20, 21))]","[('gated - feedback LSTM', (1, 5)), ('feedback gates', (6, 8)), ('hidden states', (10, 12)), ('multiple time steps', (13, 16)), ('adaptive control', (18, 20)), ('information flow', (22, 24))]","[['feedback gates', 'connecting', 'hidden states'], ['hidden states', 'across', 'multiple time steps'], ['hidden states', 'as', 'adaptive control'], ['adaptive control', 'of', 'information flow']]","[['gated - feedback LSTM', 'has', 'feedback gates']]",[],[],[],[],[],"[['Baselines', 'has', 'gated - feedback LSTM']]",[],natural_language_inference,88,160
experiments,The depth - gated LSTM uses a depth gate to connect memory cells of vertically adjacent layers .,"[('uses', (5, 6)), ('to connect', (9, 11)), ('of', (13, 14))]","[('depth - gated LSTM', (1, 5)), ('depth gate', (7, 9)), ('memory cells', (11, 13)), ('vertically adjacent layers', (14, 17))]","[['depth - gated LSTM', 'uses', 'depth gate'], ['depth gate', 'to connect', 'memory cells'], ['memory cells', 'of', 'vertically adjacent layers']]",[],[],[],[],[],[],"[['Baselines', 'has', 'depth - gated LSTM']]",[],natural_language_inference,88,161
experiments,"Amongst all deep architectures , the three - layer LSTMN also performs best .","[('Amongst', (0, 1)), ('performs', (11, 12))]","[('all deep architectures', (1, 4)), ('three - layer LSTMN', (6, 10)), ('best', (12, 13))]","[['three - layer LSTMN', 'performs', 'best']]","[['all deep architectures', 'has', 'three - layer LSTMN']]",[],[],[],"[['Results', 'Amongst', 'all deep architectures']]",[],[],[],natural_language_inference,88,172
experiments,Sentiment Analysis,[],"[('Sentiment Analysis', (0, 2))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Sentiment Analysis']]","[['Sentiment Analysis', 'has', 'Baselines']]",natural_language_inference,88,179
experiments,We used pretrained 300 - D Glove 840B vectors to initialize the word embeddings .,"[('used', (1, 2)), ('to initialize', (9, 11))]","[('pretrained 300 - D Glove 840B vectors', (2, 9)), ('word embeddings', (12, 14))]","[['pretrained 300 - D Glove 840B vectors', 'to initialize', 'word embeddings']]",[],[],[],[],"[['Hyperparameters', 'used', 'pretrained 300 - D Glove 840B vectors']]",[],[],[],natural_language_inference,88,193
experiments,"The gradient for words with Glove embeddings , was scaled by 0.35 in the first epoch after which all word embeddings were updated normally .","[('for', (2, 3)), ('with', (4, 5)), ('scaled by', (9, 11)), ('in', (12, 13)), ('after', (16, 17)), ('updated', (22, 23))]","[('gradient', (1, 2)), ('words', (3, 4)), ('Glove embeddings', (5, 7)), ('0.35', (11, 12)), ('first epoch', (14, 16)), ('all word embeddings', (18, 21)), ('normally', (23, 24))]","[['first epoch', 'after', 'all word embeddings'], ['all word embeddings', 'updated', 'normally'], ['gradient', 'for', 'words'], ['words', 'with', 'Glove embeddings'], ['gradient', 'scaled by', '0.35'], ['0.35', 'in', 'first epoch'], ['first epoch', 'after', 'all word embeddings'], ['all word embeddings', 'updated', 'normally']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'gradient']]",[],natural_language_inference,88,194
experiments,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively .","[('for', (10, 11)), ('with', (12, 13)), ('set to', (17, 19))]","[('Adam ( Kingma and Ba , 2015 )', (2, 10)), ('optimization', (11, 12)), ('two momentum parameters', (14, 17)), ('0.9 and 0.999', (19, 22))]","[['two momentum parameters', 'set to', '0.9 and 0.999'], ['Adam ( Kingma and Ba , 2015 )', 'for', 'optimization'], ['Adam ( Kingma and Ba , 2015 )', 'for', 'optimization'], ['optimization', 'with', 'two momentum parameters'], ['two momentum parameters', 'set to', '0.9 and 0.999']]",[],[],[],[],[],[],"[['Hyperparameters', 'used', 'Adam ( Kingma and Ba , 2015 )']]",[],natural_language_inference,88,195
experiments,The initial learning rate was set to 2E - 3 .,"[('set to', (5, 7))]","[('initial learning rate', (1, 4)), ('2E - 3', (7, 10))]","[['initial learning rate', 'set to', '2E - 3']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],natural_language_inference,88,196
experiments,The regularization constant was 1E - 4 and the mini-batch size was 5 .,[],[],"[['regularization constant', 'was', '1E - 4'], ['mini-batch size', 'was', '5']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'regularization constant'], ['Hyperparameters', 'has', 'mini-batch size']]",[],natural_language_inference,88,197
experiments,A dropout rate of 0.5 was applied to the neural network classifier .,"[('of', (3, 4)), ('applied to', (6, 8))]","[('dropout rate', (1, 3)), ('0.5', (4, 5)), ('neural network classifier', (9, 12))]","[['dropout rate', 'of', '0.5'], ['dropout rate', 'applied to', 'neural network classifier']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'dropout rate']]",[],natural_language_inference,88,198
experiments,"Most of these models ( including ours ) are LSTM variants ( third block in , recursive neural networks ( first block ) , or convolutional neural networks ( CNNs ; second block ) .","[('are', (8, 9))]","[('LSTM variants', (9, 11))]",[],[],[],[],[],"[['Baselines', 'are', 'LSTM variants']]",[],[],[],natural_language_inference,88,200
experiments,"For comparison , we also report the performance of the paragraph vector model ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .","[('report', (5, 6)), ('of', (8, 9))]","[('performance', (7, 8)), ('paragraph vector model', (10, 13))]","[['performance', 'of', 'paragraph vector model']]",[],[],[],[],"[['Baselines', 'report', 'performance']]",[],[],[],natural_language_inference,88,203
experiments,The results in show that both 1 - and 2 - layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art .,"[('show that', (3, 5))]","[('both 1 - and 2 - layer LSTMNs', (5, 13)), ('outperform', (13, 14)), ('LSTM baselines', (15, 17))]",[],"[['both 1 - and 2 - layer LSTMNs', 'has', 'outperform'], ['outperform', 'has', 'LSTM baselines']]",[],[],[],"[['Results', 'show that', 'both 1 - and 2 - layer LSTMNs']]",[],[],[],natural_language_inference,88,204
experiments,On the fine - grained and binary classification tasks our 2 - layer LSTMN performs close to the best system T -. shows examples of intra-attention for sentiment words .,"[('On', (0, 1)), ('performs', (14, 15)), ('to', (16, 17))]","[('fine - grained and binary classification tasks', (2, 9)), ('our 2 - layer LSTMN', (9, 14)), ('close', (15, 16)), ('best system', (18, 20))]","[['our 2 - layer LSTMN', 'performs', 'close'], ['close', 'to', 'best system']]","[['fine - grained and binary classification tasks', 'has', 'our 2 - layer LSTMN']]",[],[],[],"[['Results', 'On', 'fine - grained and binary classification tasks']]",[],[],[],natural_language_inference,88,206
experiments,Natural Language Inference,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Natural Language Inference']]","[['Natural Language Inference', 'has', 'Experimental setup']]",natural_language_inference,88,208
experiments,We used pre-trained 300 - D Glove 840B vectors to initialize the word embeddings .,"[('used', (1, 2)), ('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('word embeddings', (12, 14))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'word embeddings']]",[],[],[],[],"[['Experimental setup', 'used', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],natural_language_inference,88,220
experiments,"Out - of - vocabulary ( OOV ) words were initialized randomly with Gaussian samples ( = 0 , ?= 1 ) .","[('initialized', (10, 11)), ('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('randomly', (11, 12)), ('Gaussian samples', (13, 15))]","[['Out - of - vocabulary ( OOV ) words', 'initialized', 'randomly'], ['randomly', 'with', 'Gaussian samples']]",[],[],[],[],[],[],"[['Experimental setup', 'used', 'Out - of - vocabulary ( OOV ) words']]",[],natural_language_inference,88,221
experiments,"We only updated OOV vectors in the first epoch , after which all word embeddings were updated normally .",[],[],"[['OOV vectors', 'in', 'first epoch']]",[],[],[],[],"[['Experimental setup', 'updated', 'OOV vectors']]",[],[],[],natural_language_inference,88,222
experiments,"The dropout rate was selected from [ 0.1 , 0.2 , 0.3 , 0.4 ] .","[('selected from', (4, 6))]","[('dropout rate', (1, 3)), ('[ 0.1 , 0.2 , 0.3 , 0.4 ]', (6, 15))]","[['dropout rate', 'selected from', '[ 0.1 , 0.2 , 0.3 , 0.4 ]']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'dropout rate']]",[],natural_language_inference,88,223
experiments,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively , and the initial learning rate set to 1E - 3 .",[],[],"[['Adam ( Kingma and Ba , 2015 )', 'with', 'initial learning rate'], ['initial learning rate', 'set to', '1E - 3'], ['Adam ( Kingma and Ba , 2015 )', 'with', 'two momentum parameters']]",[],[],[],[],[],[],"[['Experimental setup', 'used', 'Adam ( Kingma and Ba , 2015 )']]",[],natural_language_inference,88,224
experiments,The mini- batch size was set to 16 or 32 .,"[('set to', (5, 7))]","[('mini- batch size', (1, 4)), ('16 or 32', (7, 10))]","[['mini- batch size', 'set to', '16 or 32']]",[],[],[],[],[],[],"[['Experimental setup', 'used', 'mini- batch size']]",[],natural_language_inference,88,225
experiments,"Specifically , these include a model which encodes the premise and hypothesis independently with two LSTMs , a shared LSTM ( Rocktschel et al. , 2016 ) , a word - by - word attention model , and a matching LSTM ( m LSTM ; ) .",[],"[('shared LSTM ( Rocktschel et al. , 2016 )', (18, 27)), ('word - by - word attention model', (29, 36)), ('matching LSTM ( m LSTM ; )', (39, 46))]",[],[],[],[],[],[],[],"[['Baselines', 'has', 'shared LSTM ( Rocktschel et al. , 2016 )'], ['Baselines', 'has', 'word - by - word attention model'], ['Baselines', 'has', 'matching LSTM ( m LSTM ; )']]",[],natural_language_inference,88,228
experiments,We also compared our models with a bag - of - words baseline which averages the pre-trained embeddings for the words in each sentence and concatenates them to create features for a logistic regression classifier ( first block in ) .,"[('compared', (2, 3))]","[('bag - of - words baseline', (7, 13))]",[],[],[],[],[],"[['Baselines', 'compared', 'bag - of - words baseline']]",[],[],[],natural_language_inference,88,230
experiments,LSTMNs achieve better performance compared Models,"[('achieve', (1, 2))]","[('LSTMNs', (0, 1)), ('better performance', (2, 4))]","[['LSTMNs', 'achieve', 'better performance']]",[],[],[],[],[],[],"[['Results', 'has', 'LSTMNs']]",[],natural_language_inference,88,231
experiments,"We also observe that fusion is generally beneficial , and that deep fusion slightly improves over shallow fusion .","[('observe', (2, 3)), ('is', (5, 6)), ('over', (15, 16))]","[('fusion', (4, 5)), ('generally beneficial', (6, 8)), ('deep fusion', (11, 13)), ('slightly improves', (13, 15)), ('shallow fusion', (16, 18))]","[['fusion', 'is', 'generally beneficial'], ['slightly improves', 'over', 'shallow fusion']]","[['deep fusion', 'has', 'slightly improves']]",[],[],[],"[['Results', 'observe', 'fusion'], ['Results', 'observe', 'deep fusion']]",[],[],[],natural_language_inference,88,233
experiments,"With standard training , our deep fusion yields the state - of - the - art performance in this task .","[('With', (0, 1)), ('yields', (7, 8))]","[('standard training', (1, 3)), ('deep fusion', (5, 7)), ('state - of - the - art performance', (9, 17))]","[['deep fusion', 'yields', 'state - of - the - art performance']]","[['standard training', 'has', 'deep fusion']]",[],[],[],"[['Results', 'With', 'standard training']]",[],[],[],natural_language_inference,88,235
research-problem,Read + Verify : Machine Reading Comprehension with Unanswerable Questions,[],"[('Machine Reading Comprehension', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,89,2
model,"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .","[('propose', (7, 8)), ('aims to be', (16, 19)), ('to', (20, 21))]","[('read - then - verify system', (9, 15)), ('robust', (19, 20)), ('unanswerable questions', (21, 23))]","[['read - then - verify system', 'aims to be', 'robust'], ['robust', 'to', 'unanswerable questions']]",[],"[['Model', 'propose', 'read - then - verify system']]",[],[],[],[],[],[],natural_language_inference,89,26
model,"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .","[('consists of', (6, 8)), ('for extracting', (17, 19)), ('detecting', (22, 23)), ('for deciding', (33, 35)), ('is', (41, 42))]","[('our system', (4, 6)), ('two components', (8, 10)), ('no-answer reader', (15, 17)), ('candidate answers', (19, 21)), ('unanswerable questions', (23, 25)), ('answer verifier', (31, 33)), ('extracted candidate', (39, 41)), ('legitimate', (42, 43))]","[['our system', 'consists of', 'two components'], ['no-answer reader', 'for extracting', 'candidate answers'], ['no-answer reader', 'detecting', 'unanswerable questions'], ['answer verifier', 'for deciding', 'extracted candidate'], ['extracted candidate', 'is', 'legitimate']]","[['two components', 'has', 'no-answer reader'], ['two components', 'has', 'answer verifier']]",[],"[['Model', 'has', 'our system']]",[],[],[],[],[],natural_language_inference,89,27
model,"First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .","[('augment', (3, 4)), ('with', (6, 7))]","[('existing readers', (4, 6)), ('two auxiliary losses', (7, 10))]","[['existing readers', 'with', 'two auxiliary losses']]",[],"[['Model', 'augment', 'existing readers']]",[],[],[],[],[],[],natural_language_inference,89,29
model,We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .,"[('introducing', (5, 6)), ('aims to', (11, 13)), ('on', (14, 15))]","[('independent span loss', (7, 10)), ('concentrate', (13, 14)), ('answer extraction task', (16, 19))]","[['independent span loss', 'aims to', 'concentrate'], ['concentrate', 'on', 'answer extraction task']]",[],"[['Model', 'introducing', 'independent span loss']]",[],[],[],[],[],[],natural_language_inference,89,32
model,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .","[('with', (5, 6)), ('leverage', (12, 13)), ('to generate', (17, 19)), ('where', (25, 26)), ('is', (28, 29)), ('used for', (39, 41))]","[('multi-head pointer network', (14, 17)), ('two pairs of span scores', (19, 24)), ('one pair', (26, 28)), ('normalized', (29, 30)), ('no -answer score', (32, 35)), ('other', (37, 38)), ('auxiliary loss', (42, 44))]","[['multi-head pointer network', 'to generate', 'two pairs of span scores'], ['two pairs of span scores', 'where', 'other'], ['other', 'used for', 'auxiliary loss'], ['two pairs of span scores', 'where', 'one pair'], ['one pair', 'is', 'normalized'], ['normalized', 'with', 'no -answer score']]",[],"[['Model', 'leverage', 'multi-head pointer network']]",[],[],[],[],[],[],natural_language_inference,89,33
model,"Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .","[('present', (3, 4)), ('to further alleviate', (8, 11)), ('focusing on', (15, 17)), ('without', (21, 22)), ('of', (26, 27))]","[('another independent noanswer loss', (4, 8)), ('confliction', (12, 13)), ('no-answer detection task', (18, 21)), ('shared normalization', (24, 26)), ('answer extraction', (27, 29))]","[['another independent noanswer loss', 'to further alleviate', 'confliction'], ['another independent noanswer loss', 'focusing on', 'no-answer detection task'], ['no-answer detection task', 'without', 'shared normalization'], ['shared normalization', 'of', 'answer extraction']]",[],"[['Model', 'present', 'another independent noanswer loss']]",[],[],[],[],[],[],natural_language_inference,89,34
model,"Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .","[('introduce', (11, 12)), ('aims at', (19, 21)), ('supports', (25, 26))]","[('additional answer verifying phase', (13, 17)), ('finding', (21, 22)), ('local entailment', (22, 24)), ('answer', (27, 28))]","[['additional answer verifying phase', 'aims at', 'finding'], ['local entailment', 'supports', 'answer']]","[['finding', 'has', 'local entailment']]","[['Model', 'introduce', 'additional answer verifying phase']]",[],[],[],[],[],[],natural_language_inference,89,35
model,"Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .","[('investigate', (13, 14)), ('for', (17, 18))]","[('three different architectures', (14, 17)), ('answer verifying task', (19, 22))]","[['three different architectures', 'for', 'answer verifying task']]",[],"[['Model', 'investigate', 'three different architectures']]",[],[],[],[],[],[],natural_language_inference,89,39
model,"The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .",[],[],"[['first one', 'is', 'sequential model'], ['sequential model', 'takes', 'two sentences'], ['two sentences', 'as', 'along sequence'], ['second one', 'capture', 'interactions'], ['interactions', 'between', 'two sentences']]",[],[],[],[],[],[],"[['three different architectures', 'has', 'first one'], ['three different architectures', 'has', 'second one']]",[],natural_language_inference,89,40
model,The last one is a hybrid model that combines the above two models to test if the performance can be further improved .,"[('is', (3, 4)), ('that combines', (7, 9))]","[('last one', (1, 3)), ('hybrid model', (5, 7)), ('above two models', (10, 13))]","[['last one', 'is', 'hybrid model'], ['hybrid model', 'that combines', 'above two models']]",[],[],[],[],[],[],"[['three different architectures', 'has', 'last one']]",[],natural_language_inference,89,41
experimental-setup,"We run a grid search on ? and ? among [ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ] .","[('run', (1, 2)), ('among', (9, 10))]","[('grid search', (3, 5)), ('[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]', (10, 23))]","[['grid search', 'among', '[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]']]",[],"[['Experimental setup', 'run', 'grid search']]",[],[],[],[],[],[],natural_language_inference,89,177
experimental-setup,"As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .",[],[],"[['Adam optimizer ( Kingma and Ba 2014 )', 'with', 'learning rate'], ['learning rate', 'of', '0.0008'], ['dropout', 'of', '0.3'], ['0.3', 'applied for', 'preventing overfitting'], ['hidden size', 'set as', '300']]","[['Model - II', 'has', 'Adam optimizer ( Kingma and Ba 2014 )'], ['Model - II', 'has', 'dropout'], ['Model - II', 'has', 'hidden size']]","[['Experimental setup', 'For', 'Model - II']]",[],[],[],[],[],[],natural_language_inference,89,179
experimental-setup,"The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .",[],[],"[['batch size', 'is', '48'], ['48', 'for', 'reader'], ['batch size', 'is', '64'], ['64', 'for', 'Model - II'], ['batch size', 'is', '32'], ['32', 'for', 'Model - I'], ['32', 'for', 'Model - III']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,89,180
experimental-setup,"We use the Glo Ve 100D embeddings for the reader , and 300D embeddings for Model - II and Model - III .",[],[],"[['300D embeddings', 'for', 'Model - II'], ['300D embeddings', 'for', 'Model - III'], ['Glo Ve 100D embeddings', 'for', 'reader']]",[],"[['Experimental setup', 'use', '300D embeddings'], ['Experimental setup', 'use', 'Glo Ve 100D embeddings']]",[],[],[],[],[],[],natural_language_inference,89,181
experimental-setup,"We utilize the nltk tokenizer 3 to preprocess passages and questions , as well as split sentences .","[('utilize', (1, 2)), ('to preprocess', (6, 8)), ('split', (15, 16))]","[('nltk tokenizer', (3, 5)), ('passages and questions', (8, 11)), ('sentences', (16, 17))]","[['nltk tokenizer', 'split', 'sentences'], ['nltk tokenizer', 'to preprocess', 'passages and questions']]",[],"[['Experimental setup', 'utilize', 'nltk tokenizer']]",[],[],[],[],[],[],natural_language_inference,89,182
results,"As we can see , our system obtains state - of the - art results by achieving an EM score of 71.7 and a F 1 score of 74.2 on the test set .",[],[],"[['our system', 'obtains', 'state - of the - art results'], ['state - of the - art results', 'on', 'test set'], ['test set', 'by achieving', 'F 1 score'], ['F 1 score', 'of', '74.2'], ['test set', 'by achieving', 'EM score'], ['EM score', 'of', '71.7']]",[],[],"[['Results', 'has', 'our system']]",[],[],[],[],[],natural_language_inference,89,187
results,Notice that SLQA + has reached a comparable result compared to our approach .,"[('Notice that', (0, 2)), ('reached', (5, 6)), ('compared to', (9, 11))]","[('SLQA +', (2, 4)), ('comparable result', (7, 9)), ('our approach', (11, 13))]","[['SLQA +', 'reached', 'comparable result'], ['comparable result', 'compared to', 'our approach']]",[],"[['Results', 'Notice that', 'SLQA +']]",[],[],[],[],[],[],natural_language_inference,89,188
ablation-analysis,"Removing the independent span loss ( indep - I ) results in a performance drop for all answerable questions ( HasAns ) , indicating that this loss helps the model in better identifying the answer boundary .","[('Removing', (0, 1)), ('results in', (10, 12)), ('for', (15, 16))]","[('independent span loss ( indep - I )', (2, 10)), ('performance drop', (13, 15)), ('all answerable questions ( HasAns )', (16, 22))]","[['independent span loss ( indep - I )', 'results in', 'performance drop'], ['performance drop', 'for', 'all answerable questions ( HasAns )']]",[],"[['Ablation analysis', 'Removing', 'independent span loss ( indep - I )']]",[],[],[],[],[],[],natural_language_inference,89,193
ablation-analysis,"Ablating independent no - answer loss ( indep - II ) , on the other hand , causes little influence on HasAns , but leads to a severe decline on no - answer accuracy ( NoAns ACC ) .",[],[],"[['independent no - answer loss ( indep - II )', 'leads to', 'severe decline'], ['severe decline', 'on', 'no - answer accuracy ( NoAns ACC )'], ['independent no - answer loss ( indep - II )', 'causes', 'little influence'], ['little influence', 'on', 'HasAns']]",[],"[['Ablation analysis', 'Ablating', 'independent no - answer loss ( indep - II )']]",[],[],[],[],[],[],natural_language_inference,89,194
ablation-analysis,"Finally , deleting both of two losses causes a degradation of more than 1.5 points on the over all performance in terms of F1 , with or without ELMo embeddings .","[('deleting', (2, 3)), ('causes', (7, 8)), ('of', (10, 11)), ('on', (15, 16)), ('in terms of', (20, 23)), ('with or without', (25, 28))]","[('both of two losses', (3, 7)), ('degradation', (9, 10)), ('more than 1.5 points', (11, 15)), ('over all performance', (17, 20)), ('F1', (23, 24)), ('ELMo embeddings', (28, 30))]","[['both of two losses', 'causes', 'degradation'], ['degradation', 'of', 'more than 1.5 points'], ['more than 1.5 points', 'on', 'over all performance'], ['over all performance', 'with or without', 'ELMo embeddings'], ['over all performance', 'in terms of', 'F1']]",[],"[['Ablation analysis', 'deleting', 'both of two losses']]",[],[],[],[],[],[],natural_language_inference,89,196
ablation-analysis,"Adding ELMo embeddings , however , does not boost the performance .","[('Adding', (0, 1)), ('does not', (6, 8))]","[('ELMo embeddings', (1, 3)), ('boost', (8, 9)), ('performance', (10, 11))]","[['ELMo embeddings', 'does not', 'boost']]","[['boost', 'has', 'performance']]","[['Ablation analysis', 'Adding', 'ELMo embeddings']]",[],[],[],[],[],[],natural_language_inference,89,200
ablation-analysis,We find that the improvement on noanswer accuracy is significant .,"[('find that', (1, 3)), ('on', (5, 6)), ('is', (8, 9))]","[('improvement', (4, 5)), ('noanswer accuracy', (6, 8)), ('significant', (9, 10))]","[['improvement', 'is', 'significant'], ['improvement', 'on', 'noanswer accuracy']]",[],"[['Ablation analysis', 'find that', 'improvement']]",[],[],[],[],[],[],natural_language_inference,89,204
ablation-analysis,We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80 .,"[('observe', (1, 2)), ('achieves', (8, 9)), ('when', (12, 13)), ('is', (15, 16))]","[('RMR + ELMo + Verifier', (3, 8)), ('best precision', (10, 12)), ('recall', (14, 15)), ('less than 80', (16, 19))]","[['RMR + ELMo + Verifier', 'achieves', 'best precision'], ['best precision', 'when', 'recall'], ['recall', 'is', 'less than 80']]",[],"[['Ablation analysis', 'observe', 'RMR + ELMo + Verifier']]",[],[],[],[],[],[],natural_language_inference,89,210
ablation-analysis,"Ablating two auxiliary losses , however , leads to an over all degradation on the curve , but it still outperforms the baseline by a large margin .","[('leads to', (7, 9)), ('on', (13, 14)), ('by', (23, 24))]","[('two auxiliary losses', (1, 4)), ('over all degradation', (10, 13)), ('curve', (15, 16)), ('outperforms', (20, 21)), ('baseline', (22, 23)), ('large margin', (25, 27))]","[['two auxiliary losses', 'leads to', 'over all degradation'], ['over all degradation', 'on', 'curve'], ['two auxiliary losses', 'leads to', 'outperforms'], ['outperforms', 'by', 'large margin']]","[['outperforms', 'has', 'baseline']]",[],"[['Ablation analysis', 'Ablating', 'two auxiliary losses']]",[],[],[],[],[],natural_language_inference,89,212
research-problem,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,[],"[('QUESTION ANSWERING', (13, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'QUESTION ANSWERING']]",[],[],[],[],natural_language_inference,9,2
research-problem,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",[],"[('question answering when reasoning over multiple facts', (9, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering when reasoning over multiple facts']]",[],[],[],[],natural_language_inference,9,4
model,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",[],[],"[['Query - Reduction Network 1 ( QRN )', 'is', 'single recurrent unit'], ['single recurrent unit', 'that addresses', 'long - term dependency problem'], ['long - term dependency problem', 'by simplifying', 'recurrent update'], ['long - term dependency problem', 'of', 'most RNN - based models'], ['long - term dependency problem', 'taking', 'advantage'], ['advantage', 'of', ""RNN 's capability""], [""RNN 's capability"", 'to model', 'sequential data']]",[],[],"[['Model', 'has', 'Query - Reduction Network 1 ( QRN )']]",[],[],[],[],[],natural_language_inference,9,24
model,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .","[('considers', (1, 2)), ('as', (5, 6)), ('of', (8, 9)), ('transforms', (15, 16)), ('to', (22, 23)), ('observes', (29, 30))]","[('context sentences', (3, 5)), ('sequence', (7, 8)), ('state - changing triggers', (9, 13)), ('original query', (20, 22)), ('more informed query', (24, 27)), ('through time', (32, 34))]","[['context sentences', 'as', 'sequence'], ['sequence', 'of', 'state - changing triggers'], ['state - changing triggers', 'observes', 'through time'], ['through time', 'transforms', 'original query'], ['original query', 'to', 'more informed query']]",[],[],[],[],"[['Query - Reduction Network 1 ( QRN )', 'considers', 'context sentences']]",[],[],[],natural_language_inference,9,25
model,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .","[('better encodes', (9, 11)), ('performed', (34, 35))]","[('locality information', (11, 13)), ('query updates', (31, 33)), ('locally', (35, 36))]","[['query updates', 'performed', 'locally']]","[['locality information', 'has', 'query updates']]",[],[],[],"[['Query - Reduction Network 1 ( QRN )', 'better encodes', 'locality information']]",[],[],[],natural_language_inference,9,33
hyperparameters,We withhold 10 % of the training for development .,"[('withhold', (1, 2)), ('of', (4, 5)), ('for', (7, 8))]","[('10 %', (2, 4)), ('training', (6, 7)), ('development', (8, 9))]","[['10 %', 'of', 'training'], ['10 %', 'for', 'development']]",[],"[['Hyperparameters', 'withhold', '10 %']]",[],[],[],[],[],[],natural_language_inference,9,204
hyperparameters,We use the hidden state size of 50 by deafult .,"[('use', (1, 2)), ('of', (6, 7)), ('by', (8, 9))]","[('hidden state size', (3, 6)), ('50', (7, 8)), ('deafult', (9, 10))]","[['hidden state size', 'of', '50'], ['50', 'by', 'deafult']]",[],"[['Hyperparameters', 'use', 'hidden state size']]",[],[],[],[],[],[],natural_language_inference,9,205
hyperparameters,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",[],[],"[['Batch sizes', 'of', '128'], ['128', 'for', 'bAbI QA 10 k'], ['Batch sizes', 'of', '32'], ['32', 'for', 'bAbI story - based QA 1k'], ['32', 'for', 'bAb I dialog'], ['32', 'for', 'DSTC2 dialog']]",[],[],"[['Hyperparameters', 'has', 'Batch sizes']]",[],[],[],[],[],natural_language_inference,9,206
hyperparameters,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,"[('in', (2, 3)), ('initialized with', (9, 11))]","[('weights', (1, 2)), ('input and output modules', (4, 8)), ('zero mean', (11, 13))]","[['weights', 'in', 'input and output modules'], ['input and output modules', 'initialized with', 'zero mean']]",[],[],"[['Hyperparameters', 'has', 'weights']]",[],[],"[['input and output modules', 'initialized with', 'standard deviation of 1 / ? d']]",[],[],natural_language_inference,9,207
hyperparameters,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,"[('of', (2, 3)), ('used for', (5, 7))]","[('Forget bias', (0, 2)), ('2.5', (3, 4)), ('update gates', (7, 9))]","[['Forget bias', 'of', '2.5'], ['2.5', 'used for', 'update gates']]",[],[],"[['Hyperparameters', 'has', 'Forget bias']]",[],[],[],[],[],natural_language_inference,9,209
hyperparameters,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,"[('of', (3, 4)), ('used for', (13, 15))]","[('L2 weight decay', (0, 3)), ('0.001 ( 0.0005 for QA 10 k )', (4, 12)), ('all weights', (15, 17))]","[['L2 weight decay', 'of', '0.001 ( 0.0005 for QA 10 k )'], ['0.001 ( 0.0005 for QA 10 k )', 'used for', 'all weights']]",[],[],"[['Hyperparameters', 'has', 'L2 weight decay']]",[],[],[],[],[],natural_language_inference,9,210
hyperparameters,The loss function is the cross entropy between v and the one - hot vector of the true answer .,"[('is', (3, 4)), ('between', (7, 8)), ('of', (15, 16))]","[('loss function', (1, 3)), ('cross entropy', (5, 7)), ('v and the one - hot vector', (8, 15)), ('true answer', (17, 19))]","[['loss function', 'is', 'cross entropy'], ['cross entropy', 'between', 'v and the one - hot vector'], ['v and the one - hot vector', 'of', 'true answer']]",[],[],"[['Hyperparameters', 'has', 'loss function']]",[],[],[],[],[],natural_language_inference,9,211
hyperparameters,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",[],[],"[['loss', 'minimized by', 'stochastic gradient descent'], ['stochastic gradient descent', 'for', 'maximally 500 epochs'], ['training', 'is', 'early stopped'], ['early stopped', 'if', 'loss'], ['not decrease', 'for', '50 epochs'], ['loss', 'on', 'development data']]","[['loss', 'has', 'not decrease']]",[],"[['Hyperparameters', 'has', 'loss'], ['Hyperparameters', 'has', 'training']]",[],[],[],[],[],natural_language_inference,9,212
hyperparameters,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,"[('controlled by', (4, 6)), ('with', (7, 8)), ('of', (12, 13))]","[('learning rate', (1, 3)), ('AdaGrad', (6, 7)), ('initial learning rate', (9, 12)), ('0.5 ( 0.1 for QA 10 k )', (13, 21))]","[['learning rate', 'controlled by', 'AdaGrad'], ['AdaGrad', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.5 ( 0.1 for QA 10 k )']]",[],[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],natural_language_inference,9,213
hyperparameters,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .","[('repeat', (11, 12)), ('with', (24, 25)), ('of', (29, 30))]","[('each training procedure', (12, 15)), ('10 times ( 50 times for 10 k )', (15, 24)), ('new random initialization', (26, 29)), ('weights', (31, 32))]","[['10 times ( 50 times for 10 k )', 'with', 'new random initialization'], ['new random initialization', 'of', 'weights']]","[['each training procedure', 'has', '10 times ( 50 times for 10 k )']]","[['Hyperparameters', 'repeat', 'each training procedure']]",[],[],[],[],[],[],natural_language_inference,9,214
baselines,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .","[('include', (1, 2))]","[('LSTM', (2, 3)), ('End - to - end Memory Networks ( N2N )', (4, 14)), ('Dynamic Memory Networks ( DMN + )', (15, 22)), ('Gated End - to - end Memory Networks ( GMe m N2N )', (23, 36)), ('Differentiable Neural Computer ( DNC )', (38, 44))]",[],[],"[['Baselines', 'include', 'LSTM'], ['Baselines', 'include', 'End - to - end Memory Networks ( N2N )'], ['Baselines', 'include', 'Dynamic Memory Networks ( DMN + )'], ['Baselines', 'include', 'Gated End - to - end Memory Networks ( GMe m N2N )'], ['Baselines', 'include', 'Differentiable Neural Computer ( DNC )']]",[],[],[],[],[],[],natural_language_inference,9,217
results,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .","[('In', (0, 1)), ('outperforms', (21, 22)), ('by', (25, 26))]","[('1 k data', (1, 4)), (""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )"", (5, 21)), ('all other models', (22, 25)), ('large margin ( 2.8 + % )', (27, 34))]","[[""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )"", 'outperforms', 'all other models'], ['all other models', 'by', 'large margin ( 2.8 + % )']]","[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","[['Results', 'In', '1 k data']]",[],[],[],[],[],[],natural_language_inference,9,220
results,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",[],[],"[['average accuracy', 'of', ""QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model""], [""QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model"", 'outperforms', 'all previous models'], ['all previous models', 'achieving', 'nearly perfect score'], ['nearly perfect score', 'of', '99.7 %'], ['all previous models', 'by', 'large margin ( 2.5 + % )']]","[['10 k dataset', 'has', 'average accuracy']]",[],"[['Results', 'In', '10 k dataset']]",[],[],[],[],[],natural_language_inference,9,221
results,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,"[('outperforms', (1, 2)), ('by', (4, 5))]","[('QRN', (0, 1)), ('previous work', (2, 4)), ('large margin ( 2.0 + % )', (6, 13))]","[['QRN', 'outperforms', 'previous work'], ['previous work', 'by', 'large margin ( 2.0 + % )']]",[],[],"[['Results', 'has', 'QRN']]",[],[],[],[],[],natural_language_inference,9,225
ablation-analysis,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .","[('When', (13, 14)), ('is', (18, 19)), ('lacks', (24, 25))]","[('number of layers', (15, 18)), ('only one', (19, 21)), ('model', (23, 24)), ('reasoning capability', (25, 27))]","[['number of layers', 'is', 'only one'], ['model', 'lacks', 'reasoning capability']]","[['number of layers', 'has', 'model']]","[['Ablation analysis', 'When', 'number of layers']]",[],[],[],[],[],[],natural_language_inference,9,229
ablation-analysis,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .","[('In the case of', (0, 4)), ('when', (8, 9)), ('correctly training', (20, 22)), ('becomes', (24, 25))]","[('1 k dataset', (4, 7)), ('too many layers', (11, 14)), ('model', (23, 24)), ('increasingly difficult', (25, 27))]","[['1 k dataset', 'when', 'too many layers'], ['too many layers', 'correctly training', 'model'], ['too many layers', 'becomes', 'increasingly difficult']]",[],"[['Ablation analysis', 'In the case of', '1 k dataset']]",[],[],[],[],[],[],natural_language_inference,9,230
ablation-analysis,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .","[('helps', (19, 20))]","[('10 k dataset', (4, 7)), ('many layers ( 6 ) and hidden dimensions ( 200 )', (8, 19)), ('reasoning', (20, 21))]","[['many layers ( 6 ) and hidden dimensions ( 200 )', 'helps', 'reasoning']]","[['10 k dataset', 'has', 'many layers ( 6 ) and hidden dimensions ( 200 )']]",[],"[['Ablation analysis', 'In the case of', '10 k dataset']]",[],[],[],[],[],natural_language_inference,9,231
ablation-analysis,( b ) Adding the reset gate helps .,"[('Adding', (3, 4))]","[('reset gate', (5, 7)), ('helps', (7, 8))]",[],"[['reset gate', 'has', 'helps']]","[['Ablation analysis', 'Adding', 'reset gate']]",[],[],[],[],[],[],natural_language_inference,9,232
ablation-analysis,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","[('Including', (3, 4)), ('in', (7, 8))]","[('vector gates', (4, 6)), ('hurts', (6, 7)), ('1 k datasets', (8, 11))]","[['hurts', 'in', '1 k datasets']]","[['vector gates', 'has', 'hurts']]","[['Ablation analysis', 'Including', 'vector gates']]",[],[],[],[],[],[],natural_language_inference,9,233
ablation-analysis,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .","[('in', (7, 8)), ('sometimes', (16, 17))]","[('vector gates', (5, 7)), ('bAbI story - based QA 10 k dataset', (8, 16)), ('help', (17, 18))]","[['vector gates', 'in', 'bAbI story - based QA 10 k dataset'], ['bAbI story - based QA 10 k dataset', 'sometimes', 'help']]",[],[],"[['Ablation analysis', 'has', 'vector gates']]",[],[],[],[],[],natural_language_inference,9,234
ablation-analysis,"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .","[('Increasing', (3, 4)), ('of', (6, 7)), ('to', (10, 11)), ('in', (12, 13))]","[('dimension', (5, 6)), ('hidden state', (8, 10)), ('100', (11, 12)), (""dialog 's Task 6 ( DSTC2 )"", (14, 21)), ('helps', (21, 22))]","[['dimension', 'of', 'hidden state'], ['hidden state', 'in', ""dialog 's Task 6 ( DSTC2 )""], ['hidden state', 'to', '100']]","[[""dialog 's Task 6 ( DSTC2 )"", 'has', 'helps']]","[['Ablation analysis', 'Increasing', 'dimension']]",[],[],[],[],[],[],natural_language_inference,9,235
ablation-analysis,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,"[('hypothesized that', (3, 5)), ('required for', (10, 12))]","[('larger hidden state', (6, 9)), ('real data', (12, 14))]","[['larger hidden state', 'required for', 'real data']]",[],"[['Ablation analysis', 'hypothesized that', 'larger hidden state']]",[],[],[],[],[],[],natural_language_inference,9,236
research-problem,A Decomposable Attention Model for Natural Language Inference,[],"[('Natural Language Inference', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,90,2
research-problem,Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .,[],"[('Natural language inference ( NLI )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,90,9
research-problem,NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .,[],"[('NLI', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,90,10
model,"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .","[('relies on', (9, 11)), ('with respect to', (17, 20))]","[('alignment', (11, 12)), ('fully computationally decomposable', (14, 17)), ('input text', (21, 23))]","[['fully computationally decomposable', 'with respect to', 'input text']]",[],"[['Model', 'relies on', 'alignment']]","[['Model', 'has', 'fully computationally decomposable']]",[],[],[],[],[],natural_language_inference,90,24
model,"Given two sentences , where each word is repre-sented by an embedding vector , we first create a soft alignment matrix using neural attention .",[],[],"[['two sentences', 'create', 'soft alignment matrix'], ['soft alignment matrix', 'using', 'neural attention'], ['each word', 'repre-sented by', 'embedding vector']]","[['two sentences', 'has', 'each word']]",[],[],[],[],[],[],[],natural_language_inference,90,26
model,We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,"[('use', (2, 3)), ('to decompose', (8, 10)), ('into', (12, 13))]","[('( soft ) alignment', (4, 8)), ('task', (11, 12)), ('subproblems', (13, 14))]","[['( soft ) alignment', 'to decompose', 'task'], ['task', 'into', 'subproblems']]",[],[],[],[],"[['two sentences', 'use', '( soft ) alignment']]",[],[],[],natural_language_inference,90,27
model,"Finally , the results of these subproblems are merged to produce the final classification .","[('of', (4, 5)), ('to produce', (9, 11))]","[('results', (3, 4)), ('subproblems', (6, 7)), ('merged', (8, 9)), ('final classification', (12, 14))]","[['results', 'of', 'subproblems'], ['merged', 'to produce', 'final classification']]","[['results', 'has', 'merged']]",[],[],[],[],[],"[['two sentences', 'has', 'results']]",[],natural_language_inference,90,28
model,"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .","[('apply', (5, 6)), ('to endow', (8, 10)), ('with', (12, 13)), ('of', (16, 17)), ('prior to', (18, 20))]","[('intra-sentence attention', (6, 8)), ('model', (11, 12)), ('richer encoding', (14, 16)), ('substructures', (17, 18)), ('alignment step', (21, 23))]","[['intra-sentence attention', 'to endow', 'model'], ['model', 'with', 'richer encoding'], ['richer encoding', 'prior to', 'alignment step'], ['richer encoding', 'of', 'substructures']]",[],"[['Model', 'apply', 'intra-sentence attention']]",[],[],[],[],[],[],natural_language_inference,90,29
experimental-setup,The method was implemented in TensorFlow .,"[('implemented in', (3, 5))]","[('TensorFlow', (5, 6))]",[],[],"[['Experimental setup', 'implemented in', 'TensorFlow']]",[],[],[],[],[],[],natural_language_inference,90,112
experimental-setup,We use 300 dimensional GloVe embeddings to represent words .,"[('use', (1, 2)), ('to represent', (6, 8))]","[('300 dimensional GloVe embeddings', (2, 6)), ('words', (8, 9))]","[['300 dimensional GloVe embeddings', 'to represent', 'words']]",[],"[['Experimental setup', 'use', '300 dimensional GloVe embeddings']]",[],[],[],[],[],[],natural_language_inference,90,119
experimental-setup,"Each embedding vector was normalized to have 2 norm of 1 and projected down to 200 dimensions , a number determined via hyperparameter tuning .","[('to have', (5, 7)), ('to', (14, 15))]","[('Each embedding vector', (0, 3)), ('normalized', (4, 5)), ('2 norm of 1', (7, 11)), ('projected down', (12, 14)), ('200 dimensions', (15, 17))]","[['projected down', 'to', '200 dimensions'], ['normalized', 'to have', '2 norm of 1']]","[['Each embedding vector', 'has', 'projected down'], ['Each embedding vector', 'has', 'normalized']]",[],"[['Experimental setup', 'has', 'Each embedding vector']]",[],[],[],[],[],natural_language_inference,90,120
experimental-setup,Out - of - vocabulary ( OOV ) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1 .,"[('hashed to', (10, 12)), ('initialized to', (18, 20))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('one of 100 random embeddings', (12, 17)), ('mean 0', (20, 22)), ('standard deviation 1', (23, 26))]","[['Out - of - vocabulary ( OOV ) words', 'hashed to', 'one of 100 random embeddings'], ['one of 100 random embeddings', 'initialized to', 'mean 0'], ['one of 100 random embeddings', 'initialized to', 'standard deviation 1']]",[],[],"[['Experimental setup', 'has', 'Out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],natural_language_inference,90,121
experimental-setup,All other parameter weights ( hidden layers etc. ) were initialized from random Gaussians with mean 0 and standard deviation 0.01 .,"[('initialized from', (10, 12)), ('with', (14, 15))]","[('other parameter weights ( hidden layers etc. )', (1, 9)), ('random Gaussians', (12, 14)), ('mean 0', (15, 17)), ('standard deviation 0.01', (18, 21))]","[['other parameter weights ( hidden layers etc. )', 'initialized from', 'random Gaussians'], ['random Gaussians', 'with', 'mean 0'], ['random Gaussians', 'with', 'standard deviation 0.01']]",[],[],"[['Experimental setup', 'has', 'other parameter weights ( hidden layers etc. )']]",[],[],[],[],[],natural_language_inference,90,123
experimental-setup,"Each hyperparameter setting was run on a single machine with 10 asynchronous gradient - update threads , using Adagrad for optimization with the default initial accumulator value of 0.1 .",[],[],"[['Each hyperparameter setting', 'using', 'Adagrad'], ['Adagrad', 'with', 'default initial accumulator value'], ['default initial accumulator value', 'of', '0.1'], ['Adagrad', 'for', 'optimization'], ['Each hyperparameter setting', 'run on', 'single machine'], ['single machine', 'with', '10 asynchronous gradient - update threads']]",[],[],"[['Experimental setup', 'has', 'Each hyperparameter setting']]",[],[],[],[],[],natural_language_inference,90,124
experimental-setup,"Dropout regularization was used for all ReLU layers , but not for the final linear layer .","[('used for', (3, 5)), ('not for', (10, 12))]","[('Dropout regularization', (0, 2)), ('all ReLU layers', (5, 8)), ('final linear layer', (13, 16))]","[['Dropout regularization', 'used for', 'all ReLU layers'], ['all ReLU layers', 'not for', 'final linear layer']]",[],[],"[['Experimental setup', 'has', 'Dropout regularization']]",[],[],[],[],[],natural_language_inference,90,125
experimental-setup,"We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - vanilla , 0.025 - intra-attention ) .",[],"[('dropout ratio', (14, 16)), ('0.2', (17, 18)), ('learning rate', (20, 22)), ('0.05', (23, 24)), ('vanilla', (25, 26)), ('0.025', (27, 28)), ('intra-attention', (29, 30))]",[],"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]",[],"[['Experimental setup', 'has', 'dropout ratio'], ['Experimental setup', 'has', 'learning rate']]",[],[],[],[],[],natural_language_inference,90,126
results,Our vanilla approach achieves state - of - theart results with almost an order of magnitude fewer parameters than the LSTMN of .,"[('achieves', (3, 4)), ('with', (10, 11)), ('than', (18, 19))]","[('Our vanilla approach', (0, 3)), ('state - of - theart results', (4, 10)), ('almost an order of magnitude fewer parameters', (11, 18)), ('LSTMN', (20, 21))]","[['Our vanilla approach', 'achieves', 'state - of - theart results'], ['state - of - theart results', 'with', 'almost an order of magnitude fewer parameters'], ['almost an order of magnitude fewer parameters', 'than', 'LSTMN']]",[],[],"[['Results', 'has', 'Our vanilla approach']]",[],[],[],[],[],natural_language_inference,90,130
results,Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art .,"[('Adding', (0, 1)), ('gives', (3, 4)), ('of', (7, 8)), ('over', (11, 12))]","[('intra-sentence attention', (1, 3)), ('considerable improvement', (5, 7)), ('0.5 percentage points', (8, 11)), ('existing state of the art', (13, 18))]","[['intra-sentence attention', 'gives', 'considerable improvement'], ['considerable improvement', 'of', '0.5 percentage points'], ['0.5 percentage points', 'over', 'existing state of the art']]",[],"[['Results', 'Adding', 'intra-sentence attention']]",[],[],[],[],[],[],natural_language_inference,90,131
research-problem,A Fast Unified Model for Parsing and Sentence Understanding,[],"[('Parsing', (5, 6)), ('Sentence Understanding', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Parsing'], ['Contribution', 'has research problem', 'Sentence Understanding']]",[],[],[],[],natural_language_inference,91,2
model,"This paper introduces a new model to address both these issues : the Stack - augmented Parser - Interpreter Neural Network , or SPINN , shown in .","[('introduces', (2, 3))]","[('Stack - augmented Parser - Interpreter Neural Network', (13, 21)), ('SPINN', (23, 24))]",[],"[['Stack - augmented Parser - Interpreter Neural Network', 'name', 'SPINN']]","[['Model', 'introduces', 'Stack - augmented Parser - Interpreter Neural Network']]",[],[],[],[],[],[],natural_language_inference,91,21
model,"SPINN executes the computations of a tree - structured model in a linearized sequence , and can incorporate a neural network parser that produces the required parse structure on the fly .","[('executes', (1, 2)), ('of', (4, 5)), ('in', (10, 11)), ('incorporate', (17, 18)), ('that produces', (22, 24))]","[('SPINN', (0, 1)), ('computations', (3, 4)), ('tree - structured model', (6, 10)), ('linearized sequence', (12, 14)), ('neural network parser', (19, 22)), ('required parse structure', (25, 28))]","[['SPINN', 'executes', 'computations'], ['computations', 'in', 'linearized sequence'], ['computations', 'of', 'tree - structured model'], ['SPINN', 'incorporate', 'neural network parser'], ['neural network parser', 'that produces', 'required parse structure']]",[],[],"[['Model', 'has', 'SPINN']]",[],[],[],[],[],natural_language_inference,91,22
model,This design improves upon the TreeRNN architecture in three ways :,"[('improves upon', (2, 4))]","[('TreeRNN architecture', (5, 7))]",[],[],"[['Model', 'improves upon', 'TreeRNN architecture']]",[],[],[],[],[],[],natural_language_inference,91,23
model,"At test time , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .","[('At', (0, 1)), ('can simultaneously', (5, 7)), ('removing', (13, 14)), ('on', (16, 17))]","[('test time', (1, 3)), ('parse', (7, 8)), ('interpret', (9, 10)), ('unparsed sentences', (10, 12)), ('dependence', (15, 16)), ('external parser', (18, 20))]","[['test time', 'can simultaneously', 'parse'], ['test time', 'can simultaneously', 'interpret'], ['test time', 'removing', 'dependence'], ['dependence', 'on', 'external parser']]","[['interpret', 'has', 'unparsed sentences']]",[],[],[],"[['TreeRNN architecture', 'At', 'test time']]",[],[],[],natural_language_inference,91,24
model,"Secondly , it supports batched computation for both parsed and unparsed sentences , yielding dramatic speedups over standard TreeRNNs .","[('supports', (3, 4)), ('for', (6, 7)), ('yielding', (13, 14)), ('over', (16, 17))]","[('batched computation', (4, 6)), ('parsed and unparsed sentences', (8, 12)), ('dramatic speedups', (14, 16)), ('standard TreeRNNs', (17, 19))]","[['batched computation', 'yielding', 'dramatic speedups'], ['dramatic speedups', 'over', 'standard TreeRNNs'], ['batched computation', 'for', 'parsed and unparsed sentences']]",[],[],[],[],"[['TreeRNN architecture', 'supports', 'batched computation']]",[],[],[],natural_language_inference,91,25
model,"Finally , it supports a novel tree - sequence hybrid architecture for handling local linear context in sentence interpretation .","[('for handling', (11, 13)), ('in', (16, 17))]","[('novel tree - sequence hybrid architecture', (5, 11)), ('local linear context', (13, 16)), ('sentence interpretation', (17, 19))]","[['novel tree - sequence hybrid architecture', 'for handling', 'local linear context'], ['local linear context', 'in', 'sentence interpretation']]",[],[],[],[],[],[],"[['TreeRNN architecture', 'supports', 'novel tree - sequence hybrid architecture']]",[],natural_language_inference,91,26
experimental-setup,Our optimized C ++/ CUDA models and the Theano source code for the full SPINN are available at https://github.com / stanfordnlp/spinn. 30 tokens or fewer .,[],"[('Our optimized C ++/ CUDA models', (0, 6)), ('Theano source code', (8, 11)), ('full SPINN', (13, 15)), ('https://github.com / stanfordnlp/spinn.', (18, 21))]",[],"[['full SPINN', 'has', 'Our optimized C ++/ CUDA models'], ['full SPINN', 'has', 'Theano source code']]",[],"[['Experimental setup', 'has', 'full SPINN']]","[['Contribution', 'Code', 'https://github.com / stanfordnlp/spinn.']]",[],[],[],[],natural_language_inference,91,143
experimental-setup,We fix the model dimension D and the word embedding dimension at 300 .,"[('at', (11, 12))]","[('fix', (1, 2)), ('model dimension D', (3, 6)), ('word embedding dimension', (8, 11)), ('300', (12, 13))]","[['fix', 'at', '300']]","[['300', 'has', 'model dimension D'], ['300', 'has', 'word embedding dimension']]",[],"[['Experimental setup', 'has', 'fix']]",[],[],[],[],[],natural_language_inference,91,144
experimental-setup,We run the CPU performance test on a 2.20 GHz 16 core Intel Xeon E5-2660 processor with hyperthreading enabled .,"[('on', (6, 7)), ('with', (16, 17))]","[('CPU performance test', (3, 6)), ('2.20 GHz 16 core Intel Xeon E5-2660 processor', (8, 16)), ('hyperthreading', (17, 18))]","[['CPU performance test', 'on', '2.20 GHz 16 core Intel Xeon E5-2660 processor'], ['2.20 GHz 16 core Intel Xeon E5-2660 processor', 'with', 'hyperthreading']]",[],[],"[['Experimental setup', 'has', 'CPU performance test']]",[],[],[],[],[],natural_language_inference,91,145
experimental-setup,We test our thin - stack implementation and the RNN model on an NVIDIA Titan X GPU .,"[('test', (1, 2)), ('on', (11, 12))]","[('thin - stack implementation', (3, 7)), ('RNN model', (9, 11)), ('NVIDIA Titan X GPU', (13, 17))]","[['NVIDIA Titan X GPU', 'test', 'thin - stack implementation'], ['NVIDIA Titan X GPU', 'test', 'RNN model']]",[],"[['Experimental setup', 'on', 'NVIDIA Titan X GPU']]",[],[],[],[],[],[],natural_language_inference,91,146
results,"We find that the bare SPINN - PI - NT model performs little better than the RNN baseline , but that SPINN - PI with the added tracking LSTM performs well .",[],[],"[['SPINN - PI', 'with', 'added tracking LSTM'], ['SPINN - PI', 'performs', 'well'], ['bare SPINN - PI - NT model', 'performs', 'little better'], ['little better', 'than', 'RNN baseline']]",[],"[['Results', 'find that', 'SPINN - PI'], ['Results', 'find that', 'bare SPINN - PI - NT model']]",[],[],[],[],[],[],natural_language_inference,91,189
results,"The full SPINN model with its relatively weak internal parser performs slightly less well , but nonetheless robustly exceeds the performance of the RNN baseline .","[('with', (4, 5)), ('performs', (10, 11)), ('of', (21, 22))]","[('full SPINN model', (1, 4)), ('relatively weak internal parser', (6, 10)), ('slightly less well', (11, 14)), ('robustly exceeds', (17, 19)), ('performance', (20, 21)), ('RNN baseline', (23, 25))]","[['full SPINN model', 'with', 'relatively weak internal parser'], ['full SPINN model', 'performs', 'slightly less well'], ['performance', 'of', 'RNN baseline']]","[['full SPINN model', 'has', 'robustly exceeds'], ['robustly exceeds', 'has', 'performance']]",[],"[['Results', 'has', 'full SPINN model']]",[],[],[],[],[],natural_language_inference,91,191
results,Both SPINN - PI and the full SPINN significantly outperform all previous sentence - encoding models .,[],"[('SPINN - PI and the full SPINN', (1, 8)), ('significantly outperform', (8, 10)), ('previous sentence - encoding models', (11, 16))]",[],"[['SPINN - PI and the full SPINN', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'previous sentence - encoding models']]",[],"[['Results', 'has', 'SPINN - PI and the full SPINN']]",[],[],[],[],[],natural_language_inference,91,192
results,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .","[('uses', (15, 16)), ('for', (20, 21)), ('to build', (30, 32))]","[('outperform', (5, 6)), ('tree - based CNN', (7, 11)), ('tree - structured composition', (16, 20)), ('local feature extraction', (21, 24)), ('simpler pooling techniques', (27, 30)), ('sentence features', (32, 34))]","[['tree - based CNN', 'uses', 'tree - structured composition'], ['tree - structured composition', 'for', 'local feature extraction'], ['tree - based CNN', 'uses', 'simpler pooling techniques'], ['simpler pooling techniques', 'to build', 'sentence features']]","[['outperform', 'has', 'tree - based CNN']]",[],[],[],[],[],"[['SPINN - PI and the full SPINN', 'has', 'outperform']]",[],natural_language_inference,91,193
results,"Our results show that a model that uses tree - structured composition fully ( SPINN ) outper - forms one which uses it only partially ( tree - based CNN ) , which in turn outperforms one which does not use it at all ( RNN ) .","[('show that', (2, 4)), ('that uses', (6, 8)), ('which', (20, 21))]","[('model', (5, 6)), ('tree - structured composition fully ( SPINN )', (8, 16)), ('outper - forms', (16, 19)), ('one', (19, 20)), ('only partially ( tree - based CNN )', (23, 31)), ('outperforms', (35, 36)), ('not use it at all ( RNN )', (39, 47))]","[['model', 'that uses', 'tree - structured composition fully ( SPINN )'], ['one', 'which', 'not use it at all ( RNN )']]","[['tree - structured composition fully ( SPINN )', 'has', 'outper - forms'], ['outper - forms', 'has', 'only partially ( tree - based CNN )'], ['only partially ( tree - based CNN )', 'has', 'outperforms'], ['outperforms', 'has', 'one']]","[['Results', 'show that', 'model']]",[],[],[],[],[],[],natural_language_inference,91,194
results,"The full SPINN performed moderately well at reproducing the Stanford Parser 's parses of the SNLI data at a transition - by - transition level , with 92.4 % accuracy at test time .",[],[],"[['full SPINN', 'performed', 'moderately well'], ['moderately well', 'at reproducing', ""Stanford Parser 's parses""], [""Stanford Parser 's parses"", 'at', 'transition - by - transition level'], [""Stanford Parser 's parses"", 'of', 'SNLI data'], ['moderately well', 'with', '92.4 % accuracy'], ['92.4 % accuracy', 'at', 'test time']]",[],[],"[['Results', 'has', 'full SPINN']]",[],[],[],[],[],natural_language_inference,91,195
research-problem,A Discrete Hard EM Approach for Weakly Supervised Question Answering,[],"[('Weakly Supervised Question Answering', (6, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Weakly Supervised Question Answering']]",[],[],[],[],natural_language_inference,92,2
research-problem,Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .,[],"[('question answering ( QA )', (1, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],natural_language_inference,92,4
research-problem,"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them .",[],"[('QA', (15, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],natural_language_inference,92,8
model,"In this paper , we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent - variable learning problems .","[('formulate', (10, 11)), ('as', (19, 20))]","[('wide range of weakly supervised QA tasks', (12, 19)), ('discrete latent - variable learning problems', (20, 26))]","[['wide range of weakly supervised QA tasks', 'as', 'discrete latent - variable learning problems']]",[],"[['Model', 'formulate', 'wide range of weakly supervised QA tasks']]",[],[],[],[],[],[],natural_language_inference,92,20
,"We demonstrate that for many recently introduced tasks , which we group into three categories as given in , it is relatively easy to precompute a discrete , task - specific set of possible solutions that contains the correct solution along with a modest number of spurious options .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,92,25
model,"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .","[('is', (3, 4)), ('to determine', (5, 7)), ('in', (9, 10)), ('while estimating', (17, 19))]","[('learning challenge', (1, 3)), ('which solution', (7, 9)), ('set', (11, 12)), ('correct one', (14, 16)), ('complete QA model', (20, 23))]","[['learning challenge', 'to determine', 'which solution'], ['which solution', 'in', 'set'], ['which solution', 'is', 'correct one'], ['correct one', 'while estimating', 'complete QA model']]",[],[],"[['Model', 'has', 'learning challenge']]",[],[],[],[],[],natural_language_inference,92,26
model,"We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .","[('model', (1, 2)), ('as', (7, 8)), ('develop', (14, 15)), ('uses', (19, 20))]","[('set of possible solutions', (3, 7)), ('discrete latent variable', (9, 12)), ('learning strategy', (16, 18)), ('hard - EM - style parameter updates', (20, 27))]","[['set of possible solutions', 'as', 'discrete latent variable'], ['learning strategy', 'uses', 'hard - EM - style parameter updates']]",[],"[['Model', 'model', 'set of possible solutions'], ['Model', 'develop', 'learning strategy']]",[],[],[],[],[],[],natural_language_inference,92,27
model,"This algorithm repeatedly ( i ) predicts the most likely solution according to the current model from the precomputed set , and ( ii ) updates the model parameters to further encourage its own prediction .","[('predicts', (6, 7)), ('according to', (11, 13)), ('from', (16, 17)), ('updates', (25, 26)), ('to further encourage', (29, 32))]","[('most likely solution', (8, 11)), ('current model', (14, 16)), ('precomputed set', (18, 20)), ('model parameters', (27, 29)), ('own prediction', (33, 35))]","[['model parameters', 'to further encourage', 'own prediction'], ['most likely solution', 'according to', 'current model'], ['current model', 'from', 'precomputed set']]",[],"[['Model', 'updates', 'model parameters'], ['Model', 'predicts', 'most likely solution']]",[],[],[],[],[],[],natural_language_inference,92,28
model,"Intuitively , these hard updates more strongly enforce our prior beliefs that there is a single correct solution .","[('there is', (12, 14))]","[('hard updates', (3, 5)), ('strongly enforce', (6, 8)), ('prior beliefs', (9, 11)), ('single correct solution', (15, 18))]","[['prior beliefs', 'there is', 'single correct solution']]","[['hard updates', 'has', 'strongly enforce'], ['strongly enforce', 'has', 'prior beliefs']]",[],"[['Model', 'has', 'hard updates']]",[],[],[],[],[],natural_language_inference,92,29
experiments,Multi-mention Reading Comprehension,[],"[('Multi-mention Reading Comprehension', (0, 3))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'Multi-mention Reading Comprehension']]","[['Multi-mention Reading Comprehension', 'has', 'Experimental setup']]",natural_language_inference,92,155
experiments,We use uncased version of BERT base .,"[('use', (1, 2)), ('of', (4, 5))]","[('uncased version', (2, 4)), ('BERT base', (5, 7))]","[['uncased version', 'of', 'BERT base']]",[],[],[],[],"[['Experimental setup', 'use', 'uncased version']]",[],[],[],natural_language_inference,92,164
experiments,We use batch size of 20 for two reading comprehension tasks and 192 for two open - domain QA tasks .,[],[],"[['batch size', 'of', '192'], ['192', 'for', 'two open - domain QA tasks'], ['batch size', 'of', '20'], ['20', 'for', 'two reading comprehension tasks']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'batch size']]",[],natural_language_inference,92,166
experiments,"For opendomain QA tasks , we retrieve 50 Wikipedia articles through TF - IDF ( Chen et al. , 2017 ) and further run to retrieve 20 ( for train ) or 80 ( for development and test ) paragraphs .","[('For', (0, 1)), ('retrieve', (6, 7)), ('through', (10, 11))]","[('opendomain QA tasks', (1, 4)), ('50 Wikipedia articles', (7, 10)), ('TF - IDF', (11, 14))]","[['opendomain QA tasks', 'retrieve', '50 Wikipedia articles'], ['50 Wikipedia articles', 'through', 'TF - IDF']]",[],[],[],[],"[['Experimental setup', 'For', 'opendomain QA tasks']]",[],[],[],natural_language_inference,92,168
experiments,"We try 10 , 20 , 40 and 80 paragraphs on the development set to choose the number of paragraphs to use on the test set .","[('try', (1, 2)), ('on', (10, 11)), ('to choose', (14, 16)), ('to use on', (20, 23))]","[('10 , 20 , 40 and 80 paragraphs', (2, 10)), ('development set', (12, 14)), ('number of paragraphs', (17, 20)), ('test set', (24, 26))]","[['10 , 20 , 40 and 80 paragraphs', 'to choose', 'number of paragraphs'], ['number of paragraphs', 'to use on', 'test set'], ['10 , 20 , 40 and 80 paragraphs', 'on', 'development set']]",[],[],[],[],"[['Experimental setup', 'try', '10 , 20 , 40 and 80 paragraphs']]",[],[],[],natural_language_inference,92,169
experiments,"To avoid local optima , we perform annealing : at training step t , the model optimizes on MML objective with a probability of min ( t / ? , 1 ) and otherwise use our objective , where ?","[('To avoid', (0, 2)), ('perform', (6, 7))]","[('local optima', (2, 4)), ('annealing', (7, 8))]","[['local optima', 'perform', 'annealing']]",[],[],[],[],"[['Experimental setup', 'To avoid', 'local optima']]",[],[],[],natural_language_inference,92,170
experiments,"6 First of all , we observe that First - Only is a strong baseline across all the datasets .","[('observe', (6, 7)), ('is', (11, 12))]","[('First - Only', (8, 11)), ('strong baseline', (13, 15))]","[['First - Only', 'is', 'strong baseline']]",[],[],[],[],"[['Results', 'observe', 'First - Only']]",[],[],[],natural_language_inference,92,176
experiments,"Second , while MML achieves comparable result to the First - Only baseline , our learning method outperforms others by 2 + F1 / ROUGE - L / EM consistently on all datasets .","[('achieves', (4, 5)), ('to', (7, 8)), ('by', (19, 20)), ('on', (30, 31))]","[('MML', (3, 4)), ('comparable result', (5, 7)), ('First - Only baseline', (9, 13)), ('our learning method', (14, 17)), ('outperforms', (17, 18)), ('2 + F1 / ROUGE - L / EM', (20, 29)), ('all datasets', (31, 33))]","[['MML', 'achieves', 'comparable result'], ['comparable result', 'to', 'First - Only baseline'], ['outperforms', 'by', '2 + F1 / ROUGE - L / EM'], ['outperforms', 'on', 'all datasets']]","[['our learning method', 'has', 'outperforms']]",[],[],[],[],[],"[['Results', 'has', 'MML'], ['Results', 'has', 'our learning method']]",[],natural_language_inference,92,178
experiments,"Lastly , our method achieves the new state - of the - art on NARRATIVEQA , TRIVIAQA - OPEN and NATURALQUESTIONS - OPEN , and is comparable to the state - of - the - art on TRIVIAQA , despite our aggressive truncation of documents .",[],[],"[['our method', 'achieves', 'new state - of the - art'], ['new state - of the - art', 'on', 'NARRATIVEQA'], ['new state - of the - art', 'on', 'TRIVIAQA - OPEN'], ['new state - of the - art', 'on', 'NATURALQUESTIONS - OPEN'], ['our method', 'achieves', 'comparable'], ['comparable', 'to', 'state - of - the - art'], ['comparable', 'on', 'TRIVIAQA']]",[],[],[],[],[],[],"[['Results', 'has', 'our method']]",[],natural_language_inference,92,179
research-problem,Gated Self - Matching Networks for Reading Comprehension and Question Answering,[],"[('Reading Comprehension and Question Answering', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension and Question Answering']]",[],[],[],[],natural_language_inference,93,2
research-problem,"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .",[],"[('reading comprehension style question answering', (11, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'reading comprehension style question answering']]",[],[],[],[],natural_language_inference,93,4
model,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .","[('introduce', (4, 5)), ('for', (24, 25))]","[('gated self - matching network', (6, 11)), ('end - to - end neural network model', (16, 24)), ('reading comprehension and question answering', (25, 30))]","[['end - to - end neural network model', 'for', 'reading comprehension and question answering']]","[['gated self - matching network', 'has', 'end - to - end neural network model']]","[['Model', 'introduce', 'gated self - matching network']]",[],[],[],[],[],[],natural_language_inference,93,22
model,Our model consists of four parts :,"[('consists of', (2, 4))]","[('four parts', (4, 6))]",[],[],"[['Model', 'consists of', 'four parts']]",[],[],[],[],[],"[['four parts', 'has', 'recurrent network encoder'], ['four parts', 'has', 'gated matching layer'], ['four parts', 'has', 'self - matching layer'], ['four parts', 'has', 'pointernetwork based answer boundary prediction layer']]",natural_language_inference,93,23
model,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .","[('to build', (6, 8)), ('for', (9, 10)), ('to match', (21, 23)), ('to aggregate', (35, 37)), ('from', (38, 39))]","[('recurrent network encoder', (3, 6)), ('representation', (8, 9)), ('questions and passages', (10, 13)), ('gated matching layer', (18, 21)), ('question and passage', (24, 27)), ('self - matching layer', (31, 35)), ('information', (37, 38)), ('whole passage', (40, 42)), ('pointernetwork based answer boundary prediction layer', (47, 53))]","[['recurrent network encoder', 'to build', 'representation'], ['representation', 'for', 'questions and passages'], ['gated matching layer', 'to match', 'question and passage'], ['self - matching layer', 'to aggregate', 'information'], ['information', 'from', 'whole passage']]",[],[],[],[],[],[],[],[],natural_language_inference,93,24
experimental-setup,We use the tokenizer from Stanford CoreNLP to preprocess each passage and question .,"[('use', (1, 2)), ('from', (4, 5)), ('to preprocess', (7, 9))]","[('tokenizer', (3, 4)), ('Stanford CoreNLP', (5, 7)), ('each passage and question', (9, 13))]","[['tokenizer', 'from', 'Stanford CoreNLP'], ['tokenizer', 'to preprocess', 'each passage and question']]",[],"[['Experimental setup', 'use', 'tokenizer']]",[],[],[],[],[],[],natural_language_inference,93,108
experimental-setup,The Gated Recurrent Unit variant of LSTM is used throughout our model .,"[('of', (5, 6)), ('used throughout', (8, 10))]","[('Gated Recurrent Unit variant', (1, 5)), ('LSTM', (6, 7)), ('our model', (10, 12))]","[['Gated Recurrent Unit variant', 'of', 'LSTM'], ['Gated Recurrent Unit variant', 'used throughout', 'our model']]",[],[],"[['Experimental setup', 'has', 'Gated Recurrent Unit variant']]",[],[],[],[],[],natural_language_inference,93,109
experimental-setup,"For word embedding , we use pretrained case - sensitive GloVe embeddings 2 ( Pennington et al. , 2014 ) for both questions and passages , and it is fixed during training ; We use zero vectors to represent all out - of - vocab words .","[('For', (0, 1)), ('use', (5, 6)), ('for', (20, 21)), ('during', (30, 31)), ('to represent', (37, 39))]","[('word embedding', (1, 3)), ('pretrained case - sensitive GloVe embeddings', (6, 12)), ('questions and passages', (22, 25)), ('fixed', (29, 30)), ('training', (31, 32)), ('zero vectors', (35, 37)), ('all out - of - vocab words', (39, 46))]","[['word embedding', 'use', 'zero vectors'], ['zero vectors', 'to represent', 'all out - of - vocab words'], ['word embedding', 'use', 'pretrained case - sensitive GloVe embeddings'], ['pretrained case - sensitive GloVe embeddings', 'for', 'questions and passages'], ['fixed', 'during', 'training']]","[['pretrained case - sensitive GloVe embeddings', 'has', 'fixed']]","[['Experimental setup', 'For', 'word embedding']]",[],[],[],[],[],[],natural_language_inference,93,110
experimental-setup,"We utilize 1 layer of bi-directional GRU to compute character - level embeddings and 3 layers of bi-directional GRU to encode questions and passages , the gated attention - based recurrent network for question and passage matching is also encoded bidirectionally in our experiment .","[('utilize', (1, 2)), ('to compute', (7, 9)), ('to encode', (19, 21)), ('for', (32, 33)), ('encoded', (39, 40))]","[('1 layer of bi-directional GRU', (2, 7)), ('character - level embeddings', (9, 13)), ('3 layers of bi-directional GRU', (14, 19)), ('questions and passages', (21, 24)), ('gated attention - based recurrent network', (26, 32)), ('question and passage matching', (33, 37)), ('bidirectionally', (40, 41))]","[['3 layers of bi-directional GRU', 'to encode', 'questions and passages'], ['gated attention - based recurrent network', 'for', 'question and passage matching'], ['gated attention - based recurrent network', 'encoded', 'bidirectionally'], ['1 layer of bi-directional GRU', 'to compute', 'character - level embeddings']]",[],"[['Experimental setup', 'utilize', '3 layers of bi-directional GRU'], ['Experimental setup', 'utilize', 'gated attention - based recurrent network'], ['Experimental setup', 'utilize', '1 layer of bi-directional GRU']]",[],[],[],[],[],[],natural_language_inference,93,111
experimental-setup,The hidden vector length is set to 75 for all layers .,"[('set to', (5, 7)), ('for', (8, 9))]","[('hidden vector length', (1, 4)), ('75', (7, 8)), ('all layers', (9, 11))]","[['hidden vector length', 'set to', '75'], ['75', 'for', 'all layers']]",[],[],"[['Experimental setup', 'use', 'hidden vector length']]",[],[],[],[],[],natural_language_inference,93,112
experimental-setup,The hidden size used to compute attention scores is also 75 .,"[('to compute', (4, 6)), ('is', (8, 9))]","[('hidden size', (1, 3)), ('attention scores', (6, 8)), ('75', (10, 11))]","[['hidden size', 'to compute', 'attention scores'], ['attention scores', 'is', '75']]",[],[],"[['Experimental setup', 'use', 'hidden size']]",[],[],[],[],[],natural_language_inference,93,113
experimental-setup,We also apply dropout between layers with a dropout rate of 0.2 .,"[('apply', (2, 3)), ('between', (4, 5)), ('with', (6, 7)), ('of', (10, 11))]","[('dropout', (3, 4)), ('layers', (5, 6)), ('dropout rate', (8, 10)), ('0.2', (11, 12))]","[['dropout', 'with', 'dropout rate'], ['dropout rate', 'of', '0.2'], ['dropout', 'between', 'layers']]",[],"[['Experimental setup', 'apply', 'dropout']]",[],[],[],[],[],[],natural_language_inference,93,114
experimental-setup,"The model is optimized with AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 1 .","[('optimized with', (3, 5)), ('with', (11, 12)), ('of', (16, 17))]","[('model', (1, 2)), ('AdaDelta ( Zeiler , 2012 )', (5, 11)), ('initial learning rate', (13, 16)), ('1', (17, 18))]","[['model', 'optimized with', 'AdaDelta ( Zeiler , 2012 )'], ['AdaDelta ( Zeiler , 2012 )', 'with', 'initial learning rate'], ['initial learning rate', 'of', '1']]",[],[],"[['Experimental setup', 'has', 'model']]",[],[],[],[],[],natural_language_inference,93,115
experimental-setup,The ? and used in AdaDelta are 0.95 and 1e ? 6 respectively .,"[('used in', (3, 5)), ('are', (6, 7))]","[('AdaDelta', (5, 6)), ('0.95 and 1e ? 6', (7, 12))]","[['AdaDelta', 'are', '0.95 and 1e ? 6']]",[],"[['Experimental setup', 'used in', 'AdaDelta']]",[],[],[],[],[],[],natural_language_inference,93,116
ablation-analysis,attention - based recurrent network ( GARNN ) and self - matching attention mechanism positively contribute to the final results of gated self - matching networks .,"[('to', (16, 17)), ('of', (20, 21))]","[('attention - based recurrent network ( GARNN )', (0, 8)), ('self - matching attention mechanism', (9, 14)), ('positively contribute', (14, 16)), ('final results', (18, 20)), ('gated self - matching networks', (21, 26))]","[['positively contribute', 'to', 'final results'], ['final results', 'of', 'gated self - matching networks']]","[['positively contribute', 'has', 'attention - based recurrent network ( GARNN )'], ['positively contribute', 'has', 'self - matching attention mechanism']]",[],"[['Ablation analysis', 'has', 'positively contribute']]",[],[],[],[],[],natural_language_inference,93,132
ablation-analysis,"Removing self - matching results in 3.5 point EM drop , which reveals that information in the passage plays an important role .","[('Removing', (0, 1)), ('results in', (4, 6))]","[('self - matching', (1, 4)), ('3.5 point EM drop', (6, 10))]","[['self - matching', 'results in', '3.5 point EM drop']]",[],"[['Ablation analysis', 'Removing', 'self - matching']]",[],[],[],[],[],[],natural_language_inference,93,133
ablation-analysis,Characterlevel embeddings contribute towards the model 's performance since it can better handle out - ofvocab or rare words .,"[('contribute towards', (2, 4)), ('can', (10, 11))]","[('Characterlevel embeddings', (0, 2)), (""model 's performance"", (5, 8)), ('better handle', (11, 13)), ('out - ofvocab or rare words', (13, 19))]","[['Characterlevel embeddings', 'can', 'better handle'], ['Characterlevel embeddings', 'contribute towards', ""model 's performance""]]","[['better handle', 'has', 'out - ofvocab or rare words']]",[],"[['Ablation analysis', 'has', 'Characterlevel embeddings']]",[],[],[],[],[],natural_language_inference,93,134
ablation-analysis,Character - level embeddings are not utilized .,"[('not', (5, 6))]","[('Character - level embeddings', (0, 4)), ('utilized', (6, 7))]","[['Character - level embeddings', 'not', 'utilized']]",[],[],"[['Ablation analysis', 'has', 'Character - level embeddings']]",[],[],[],[],[],natural_language_inference,93,137
ablation-analysis,"As shown in , the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset .","[('introduced in', (6, 8)), ('for', (15, 16))]","[('gate', (5, 6)), ('question and passage matching layer', (8, 13)), ('helpful', (14, 15)), ('GRU and LSTM', (17, 20))]","[['gate', 'introduced in', 'question and passage matching layer'], ['helpful', 'for', 'GRU and LSTM']]","[['question and passage matching layer', 'has', 'helpful']]",[],"[['Ablation analysis', 'has', 'gate']]",[],[],[],[],[],natural_language_inference,93,138
research-problem,Commonsense for Generative Multi - Hop Question Answering Tasks,[],"[('Generative Multi - Hop Question Answering', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generative Multi - Hop Question Answering']]",[],[],[],[],natural_language_inference,94,2
research-problem,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",[],"[('Reading comprehension QA', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading comprehension QA']]",[],[],[],[],natural_language_inference,94,4
research-problem,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",[],[],[],[],[],[],[],[],[],[],[],natural_language_inference,94,14
research-problem,"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA .",[],"[('machine reading comprehension ( MRC ) based QA', (9, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine reading comprehension ( MRC ) based QA']]",[],[],[],[],natural_language_inference,94,24
code,https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,[],"[('https://github.com/yicheng-w/CommonSenseMultiHopQA', (0, 1))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/yicheng-w/CommonSenseMultiHopQA']]",[],[],[],[],natural_language_inference,94,27
research-problem,"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context .",[],"[('reasoning - based MRC - QA', (6, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'reasoning - based MRC - QA']]",[],[],[],[],natural_language_inference,94,28
model,"In this paper , we first propose the Multi - Hop Pointer - Generator Model ( MHPGM ) , a strong baseline model that uses multiple hops of bidirectional attention , self - attention , and a pointer - generator decoder to effectively read and reason within along passage and synthesize a coherent response .","[('first propose', (5, 7)), ('uses', (24, 25)), ('of', (27, 28)), ('to effectively', (41, 43)), ('within', (46, 47))]","[('Multi - Hop Pointer - Generator Model ( MHPGM )', (8, 18)), ('strong baseline model', (20, 23)), ('multiple hops', (25, 27)), ('bidirectional attention', (28, 30)), ('self - attention', (31, 34)), ('pointer - generator decoder', (37, 41)), ('read and reason', (43, 46)), ('along passage', (47, 49)), ('synthesize', (50, 51)), ('coherent response', (52, 54))]","[['strong baseline model', 'uses', 'multiple hops'], ['multiple hops', 'of', 'bidirectional attention'], ['multiple hops', 'of', 'self - attention'], ['multiple hops', 'of', 'pointer - generator decoder'], ['multiple hops', 'to effectively', 'read and reason'], ['read and reason', 'within', 'along passage'], ['multiple hops', 'to effectively', 'synthesize']]","[['Multi - Hop Pointer - Generator Model ( MHPGM )', 'has', 'strong baseline model'], ['synthesize', 'has', 'coherent response']]","[['Model', 'first propose', 'Multi - Hop Pointer - Generator Model ( MHPGM )']]",[],[],[],[],[],[],natural_language_inference,94,37
model,"Next , to address the issue that understanding human - generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense ( background ) knowledge , we present an algorithm for selecting useful , grounded multi-hop relational knowledge paths from ConceptNet ) via a pointwise mutual information ( PMI ) and term - frequency - based scoring function .","[('present', (34, 35)), ('for selecting', (37, 39)), ('from', (46, 47)), ('via', (49, 50))]","[('algorithm', (36, 37)), ('useful , grounded multi-hop relational knowledge paths', (39, 46)), ('ConceptNet', (47, 48)), ('pointwise mutual information ( PMI )', (51, 57)), ('term - frequency - based scoring function', (58, 65))]","[['algorithm', 'for selecting', 'useful , grounded multi-hop relational knowledge paths'], ['useful , grounded multi-hop relational knowledge paths', 'from', 'ConceptNet'], ['useful , grounded multi-hop relational knowledge paths', 'via', 'pointwise mutual information ( PMI )'], ['useful , grounded multi-hop relational knowledge paths', 'via', 'term - frequency - based scoring function']]",[],"[['Model', 'present', 'algorithm']]",[],[],[],[],[],[],natural_language_inference,94,39
model,"We then present a novel method of inserting these selected commonsense paths between the hops of document - context reasoning within our model , via the Necessary and Optional Information Cell ( NOIC ) , which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference .",[],[],"[['novel method', 'of', 'inserting'], ['selected commonsense paths', 'between', 'hops'], ['hops', 'of', 'document - context reasoning'], ['document - context reasoning', 'within', 'our model'], ['selected commonsense paths', 'via', 'Necessary and Optional Information Cell ( NOIC )'], ['Necessary and Optional Information Cell ( NOIC )', 'employs', 'selectivelygated attention mechanism'], ['selectivelygated attention mechanism', 'that utilizes', 'commonsense information'], ['commonsense information', 'to effectively fill in', 'gaps of inference']]","[['inserting', 'has', 'selected commonsense paths']]",[],"[['Model', 'present', 'novel method']]",[],[],[],[],[],natural_language_inference,94,40
results,"We see empirically that our model outperforms all generative models on NarrativeQA , and is competitive with the top span prediction models .","[('see empirically that', (1, 4)), ('on', (10, 11)), ('with', (16, 17))]","[('our model', (4, 6)), ('outperforms', (6, 7)), ('all generative models', (7, 10)), ('NarrativeQA', (11, 12)), ('competitive', (15, 16)), ('top span prediction models', (18, 22))]","[['competitive', 'with', 'top span prediction models'], ['all generative models', 'on', 'NarrativeQA']]","[['our model', 'has', 'competitive'], ['our model', 'has', 'outperforms'], ['outperforms', 'has', 'all generative models']]","[['Results', 'see empirically that', 'our model']]",[],[],[],[],[],[],natural_language_inference,94,214
results,"Furthermore , with the NOIC commonsense integration , we were able to further improve performance ( p < 0.001 on all metrics 5 ) , establishing a new state - of - the - art for the task .","[('with', (2, 3)), ('able to', (10, 12)), ('establishing', (25, 26)), ('for', (35, 36))]","[('NOIC commonsense integration', (4, 7)), ('further improve', (12, 14)), ('performance', (14, 15)), ('new state - of - the - art', (27, 35)), ('task', (37, 38))]","[['NOIC commonsense integration', 'able to', 'further improve'], ['performance', 'establishing', 'new state - of - the - art'], ['new state - of - the - art', 'for', 'task']]","[['further improve', 'has', 'performance']]","[['Results', 'with', 'NOIC commonsense integration']]",[],[],[],[],[],[],natural_language_inference,94,215
results,"We also see that our model performs reasonably well on WikiHop , and further achieves promising initial improvements via the addition of commonsense , hinting at the generalizability of our approaches .",[],[],"[['our model', 'performs', 'reasonably well'], ['reasonably well', 'on', 'WikiHop'], ['our model', 'achieves', 'promising initial improvements'], ['promising initial improvements', 'via', 'addition'], ['addition', 'of', 'commonsense'], ['commonsense', 'hinting at', 'generalizability'], ['generalizability', 'of', 'our approaches']]",[],"[['Results', 'see that', 'our model']]",[],[],[],[],[],[],natural_language_inference,94,216
results,"We speculate that the improvement is smaller on Wikihop because only approximately 11 % of WikiHop data points require commonsense and because WikiHop data requires more fact - based commonsense ( e.g. , from Freebase ) as opposed to semantics - based commonsense ( e.g. , from Con-ceptNet ( Speer and Havasi , 2012 ) ) .","[('speculate', (1, 2)), ('on', (7, 8))]","[('improvement', (4, 5)), ('smaller', (6, 7)), ('Wikihop', (8, 9))]","[['smaller', 'on', 'Wikihop']]","[['improvement', 'has', 'smaller']]","[['Results', 'speculate', 'improvement']]",[],[],[],[],[],[],natural_language_inference,94,217
ablation-analysis,Experiment 1 and 5 are our models presented in were also important for the model 's performance and that self - attention is able to contribute significantly to performance on top of other components of the model .,"[('able to contribute', (23, 26)), ('to', (27, 28)), ('on top of', (29, 32))]","[('performance', (16, 17)), ('self - attention', (19, 22)), ('significantly', (26, 27)), ('other components of the model', (32, 37))]","[['self - attention', 'able to contribute', 'significantly'], ['significantly', 'to', 'performance'], ['performance', 'on top of', 'other components of the model']]",[],[],"[['Ablation analysis', 'has', 'self - attention']]",[],[],[],[],[],natural_language_inference,94,224
ablation-analysis,"Finally , we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline .","[('see that', (3, 5)), ('via', (9, 10)), ('on top of', (21, 24))]","[('effectively introducing', (5, 7)), ('external knowledge', (7, 9)), ('our commonsense selection algorithm', (10, 14)), ('NOIC', (15, 16)), ('improve', (17, 18)), ('performance', (18, 19)), ('our strong baseline', (24, 27))]","[['performance', 'on top of', 'our strong baseline'], ['performance', 'via', 'our commonsense selection algorithm'], ['performance', 'via', 'NOIC']]","[['effectively introducing', 'has', 'external knowledge'], ['external knowledge', 'has', 'improve'], ['improve', 'has', 'performance']]","[['Ablation analysis', 'see that', 'effectively introducing']]",[],[],[],[],[],[],natural_language_inference,94,225
ablation-analysis,"The results of these are shown in , where we see that neither NumberBatch nor random - relationships nor single - hop common - sense offer statistically significant improvements 7 , whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics .","[('across', (41, 42))]","[('our commonsense selection and incorporation mechanism', (32, 38)), ('improves', (38, 39)), ('performance significantly', (39, 41)), ('all metrics', (42, 44))]","[['performance significantly', 'across', 'all metrics']]","[['our commonsense selection and incorporation mechanism', 'has', 'improves'], ['improves', 'has', 'performance significantly']]",[],"[['Ablation analysis', 'see that', 'our commonsense selection and incorporation mechanism']]",[],[],[],[],[],natural_language_inference,94,232
research-problem,Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification,[],"[('Multi - Passage Machine Reading Comprehension', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi - Passage Machine Reading Comprehension']]",[],[],[],[],natural_language_inference,95,2
research-problem,Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine reading comprehension ( MRC )']]",[],[],[],[],natural_language_inference,95,4
research-problem,"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .",[],"[('MRC', (2, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'MRC']]",[],[],[],[],natural_language_inference,95,5
model,"The over all framework of our model is demonstrated in , which consists of three modules .","[('of', (4, 5)), ('consists of', (12, 14))]","[('three modules', (14, 16))]",[],[],"[['Model', 'consists of', 'three modules']]",[],[],[],[],[],[],natural_language_inference,95,51
model,"First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .","[('follow', (3, 4)), ('to find', (10, 12)), ('for', (15, 16)), ('by identifying', (18, 20))]","[('boundary - based MRC models', (5, 10)), ('answer candidate', (13, 15)), ('each passage', (16, 18)), ('start and end position', (21, 25)), ('answer', (27, 28))]","[['boundary - based MRC models', 'to find', 'answer candidate'], ['answer candidate', 'by identifying', 'start and end position'], ['answer candidate', 'for', 'each passage']]","[['start and end position', 'of', 'answer']]",[],[],[],"[['three modules', 'follow', 'boundary - based MRC models']]",[],[],[],natural_language_inference,95,52
model,"Second , we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective .",[],[],"[['content scores', 'to measure', 'quality'], ['quality', 'of', 'candidates'], ['meanings', 'of', 'answer candidates'], ['answer candidates', 'extracted from', 'those passages']]",[],[],[],[],"[['three modules', 'use', 'content scores'], ['three modules', 'model', 'meanings']]",[],[],[],natural_language_inference,95,53
model,"Third , we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations .","[('conduct', (3, 4)), ('by enabling', (7, 9)), ('to attend', (12, 14)), ('based on', (18, 20))]","[('answer verification', (5, 7)), ('each answer candidate', (9, 12)), ('other candidates', (16, 18)), ('their representations', (20, 22))]","[['answer verification', 'by enabling', 'each answer candidate'], ['each answer candidate', 'to attend', 'other candidates'], ['other candidates', 'based on', 'their representations']]",[],[],[],[],"[['three modules', 'conduct', 'answer verification']]",[],[],[],natural_language_inference,95,54
model,"Therefore , the final answer is determined by three factors : the boundary , the content and the crosspassage answer verification .","[('determined by', (6, 8))]","[('final answer', (3, 5)), ('three factors', (8, 10)), ('boundary', (12, 13)), ('content', (15, 16)), ('crosspassage answer verification', (18, 21))]","[['final answer', 'determined by', 'three factors']]","[['three factors', 'has', 'boundary'], ['three factors', 'has', 'content'], ['three factors', 'has', 'crosspassage answer verification']]",[],"[['Model', 'has', 'final answer']]",[],[],[],[],[],natural_language_inference,95,56
model,"The three steps are modeled using different modules , which can be jointly trained in our end - to - end framework .","[('modeled using', (4, 6)), ('can be', (10, 12)), ('in', (14, 15))]","[('different modules', (6, 8)), ('jointly trained', (12, 14)), ('our end - to - end framework', (15, 22))]","[['different modules', 'can be', 'jointly trained'], ['jointly trained', 'in', 'our end - to - end framework']]",[],[],[],[],"[['three factors', 'modeled using', 'different modules']]",[],[],[],natural_language_inference,95,57
experimental-setup,"For MS - MARCO , we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP and we choose the span that achieves the highest ROUGE - L score with the reference answers as the gold span for training .","[('For', (0, 1)), ('preprocess', (6, 7)), ('with', (9, 10)), ('from', (13, 14))]","[('MS - MARCO', (1, 4)), ('corpus', (8, 9)), ('reversible tokenizer', (11, 13)), ('Stanford CoreNLP', (14, 16))]","[['MS - MARCO', 'preprocess', 'corpus'], ['corpus', 'with', 'reversible tokenizer'], ['reversible tokenizer', 'from', 'Stanford CoreNLP']]",[],"[['Experimental setup', 'For', 'MS - MARCO']]",[],[],[],[],[],[],natural_language_inference,95,145
experimental-setup,We employ the 300 - D pre-trained Glove embeddings and keep it fixed during training .,"[('employ', (1, 2)), ('during', (13, 14))]","[('300 - D pre-trained Glove embeddings', (3, 9)), ('fixed', (12, 13)), ('training', (14, 15))]","[['fixed', 'during', 'training']]","[['300 - D pre-trained Glove embeddings', 'has', 'fixed']]",[],[],[],"[['MS - MARCO', 'employ', '300 - D pre-trained Glove embeddings']]",[],[],[],natural_language_inference,95,146
experimental-setup,The character embeddings are randomly initialized with its dimension as 30 .,"[('are', (3, 4)), ('with', (6, 7)), ('as', (9, 10))]","[('character embeddings', (1, 3)), ('randomly initialized', (4, 6)), ('dimension', (8, 9)), ('30', (10, 11))]","[['character embeddings', 'with', 'dimension'], ['dimension', 'as', '30'], ['character embeddings', 'are', 'randomly initialized']]",[],[],[],[],[],[],"[['MS - MARCO', 'has', 'character embeddings']]",[],natural_language_inference,95,147
results,Results on DuReader,"[('on', (1, 2))]","[('DuReader', (2, 3))]",[],[],"[['Results', 'on', 'DuReader']]",[],[],[],[],[],[],natural_language_inference,95,161
results,We can see that this paragraph ranking can boost the BiDAF baseline significantly .,"[('see that', (2, 4)), ('boost', (8, 9))]","[('paragraph ranking', (5, 7)), ('BiDAF baseline', (10, 12)), ('significantly', (12, 13))]","[['paragraph ranking', 'boost', 'BiDAF baseline'], ['paragraph ranking', 'boost', 'significantly']]",[],[],[],[],"[['DuReader', 'see that', 'paragraph ranking']]",[],[],[],natural_language_inference,95,165
results,"Finally , we implement our system based on this new strategy , and our system ( single model ) achieves further improvement by a large margin .","[('achieves', (19, 20)), ('by', (22, 23))]","[('our system ( single model )', (13, 19)), ('further improvement', (20, 22)), ('large margin', (24, 26))]","[['our system ( single model )', 'achieves', 'further improvement'], ['further improvement', 'by', 'large margin']]",[],[],[],[],[],[],"[['DuReader', 'has', 'our system ( single model )']]",[],natural_language_inference,95,166
ablation-analysis,"From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .","[('see that', (4, 6)), ('makes', (9, 10)), ('to', (13, 14)), ('confirms', (20, 21)), ('that', (23, 24)), ('for', (31, 32))]","[('answer verification', (7, 9)), ('great contribution', (11, 13)), ('over all improvement', (15, 18)), ('our hypothesis', (21, 23)), ('cross - passage answer verification', (24, 29)), ('useful', (30, 31)), ('multi-passage MRC', (33, 35))]","[['answer verification', 'makes', 'great contribution'], ['great contribution', 'confirms', 'our hypothesis'], ['our hypothesis', 'that', 'cross - passage answer verification'], ['useful', 'for', 'multi-passage MRC'], ['great contribution', 'to', 'over all improvement']]","[['cross - passage answer verification', 'has', 'useful']]","[['Ablation analysis', 'see that', 'answer verification']]",[],[],[],[],[],[],natural_language_inference,95,186
ablation-analysis,"For the ablation of the content model , we analyze that it will not only affect the content score itself , but also violate the verification model since the content probabilities are necessary for the answer representation , which will be further analyzed in Section 4.3 .","[('For', (0, 1)), ('of', (3, 4)), ('not only affect', (13, 16)), ('violate', (23, 24))]","[('ablation', (2, 3)), ('content model', (5, 7)), ('analyze', (9, 10)), ('content score itself', (17, 20)), ('verification model', (25, 27))]","[['ablation', 'of', 'content model'], ['analyze', 'violate', 'verification model'], ['analyze', 'not only affect', 'content score itself']]","[['content model', 'has', 'analyze']]","[['Ablation analysis', 'For', 'ablation']]",[],[],[],[],[],[],natural_language_inference,95,187
ablation-analysis,"Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","[('jointly training', (4, 6)), ('can', (9, 10)), ('provide', (10, 11)), ('shows that', (15, 17)), ('are', (20, 21)), ('with', (29, 30)), ('at', (32, 33))]","[('three models', (7, 9)), ('great benefits', (11, 13)), ('three tasks', (18, 20)), ('closely related', (22, 24)), ('boost', (26, 27)), ('each other', (27, 29)), ('shared representations', (30, 32)), ('bottom layers', (33, 35))]","[['three models', 'provide', 'great benefits'], ['great benefits', 'shows that', 'three tasks'], ['three tasks', 'can', 'boost'], ['each other', 'with', 'shared representations'], ['shared representations', 'at', 'bottom layers'], ['three tasks', 'are', 'closely related']]","[['boost', 'has', 'each other']]","[['Ablation analysis', 'jointly training', 'three models']]",[],[],[],[],[],[],natural_language_inference,95,188
ablation-analysis,"At last , comparing our method with the baseline , we achieve an improvement of nearly 3 points without the yes / no classification .","[('achieve', (11, 12)), ('of', (14, 15)), ('without', (18, 19))]","[('improvement', (13, 14)), ('nearly 3 points', (15, 18)), ('yes / no classification', (20, 24))]","[['improvement', 'of', 'nearly 3 points'], ['nearly 3 points', 'without', 'yes / no classification']]",[],"[['Ablation analysis', 'achieve', 'improvement']]",[],[],[],[],[],[],natural_language_inference,95,189
research-problem,Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference,[],"[('Grounded Commonsense Inference', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Grounded Commonsense Inference']]",[],[],[],[],natural_language_inference,96,2
dataset,We use this method to construct Swag : an adversarial dataset with 113 k multiple - choice questions .,"[('construct', (5, 6)), ('with', (11, 12))]","[('Swag', (6, 7)), ('adversarial dataset', (9, 11)), ('113 k multiple - choice questions', (12, 18))]","[['adversarial dataset', 'with', '113 k multiple - choice questions']]","[['Swag', 'has', 'adversarial dataset']]","[['Dataset', 'construct', 'Swag']]",[],[],[],[],[],[],natural_language_inference,96,36
dataset,"We start with pairs of temporally adjacent video captions , each with a context and a follow - up event that we know is physically possible .","[('start with', (1, 3)), ('of', (4, 5)), ('each with', (10, 12)), ('know', (22, 23))]","[('pairs', (3, 4)), ('temporally adjacent video captions', (5, 9)), ('context', (13, 14)), ('follow - up event', (16, 20)), ('physically possible', (24, 26))]","[['pairs', 'of', 'temporally adjacent video captions'], ['temporally adjacent video captions', 'each with', 'context'], ['temporally adjacent video captions', 'each with', 'follow - up event'], ['follow - up event', 'know', 'physically possible']]",[],"[['Dataset', 'start with', 'pairs']]",[],[],[],[],[],[],natural_language_inference,96,37
dataset,We then use a state - of - theart language model fine - tuned on this data to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .,"[('use', (2, 3)), ('fine - tuned on', (11, 15)), ('to massively oversample', (17, 20)), ('of', (23, 24))]","[('state - of - theart language model', (4, 11)), ('data', (16, 17)), ('diverse set', (21, 23)), ('possible negative sentence endings ( or counterfactuals )', (24, 32))]","[['state - of - theart language model', 'to massively oversample', 'diverse set'], ['diverse set', 'of', 'possible negative sentence endings ( or counterfactuals )'], ['state - of - theart language model', 'fine - tuned on', 'data']]",[],"[['Dataset', 'use', 'state - of - theart language model']]",[],[],[],[],[],[],natural_language_inference,96,38
dataset,"Next , we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones .","[('filter', (3, 4)), ('using', (10, 11)), ('to obtain', (16, 18)), ('of', (20, 21)), ('with', (23, 24)), ('to', (27, 28))]","[('aggressively and adversarially', (7, 10)), ('committee of trained models', (12, 16)), ('population', (19, 20)), ('de-biased endings', (21, 23)), ('similar stylistic features', (24, 27)), ('real ones', (29, 31))]","[['aggressively and adversarially', 'using', 'committee of trained models'], ['committee of trained models', 'to obtain', 'population'], ['population', 'of', 'de-biased endings'], ['de-biased endings', 'with', 'similar stylistic features'], ['similar stylistic features', 'to', 'real ones']]",[],[],[],[],"[['possible negative sentence endings ( or counterfactuals )', 'filter', 'aggressively and adversarially']]",[],[],[],natural_language_inference,96,39
dataset,"Finally , these filtered counterfactuals are validated by crowd workers to further ensure data quality .","[('validated by', (6, 8)), ('to further ensure', (10, 13))]","[('filtered counterfactuals', (3, 5)), ('crowd workers', (8, 10)), ('data quality', (13, 15))]","[['filtered counterfactuals', 'validated by', 'crowd workers'], ['crowd workers', 'to further ensure', 'data quality']]",[],[],"[['Dataset', 'has', 'filtered counterfactuals']]",[],[],[],[],[],natural_language_inference,96,40
results,"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings , which obtains 43.6 % .","[('only uses', (4, 6)), ('is', (8, 9)), ('with', (13, 14)), ('obtains', (18, 19))]","[('best model', (1, 3)), ('ending', (7, 8)), ('LSTM sequence model', (10, 13)), ('ELMo embeddings', (14, 16)), ('43.6 %', (19, 21))]","[['best model', 'only uses', 'ending'], ['ending', 'is', 'LSTM sequence model'], ['LSTM sequence model', 'obtains', '43.6 %'], ['LSTM sequence model', 'with', 'ELMo embeddings']]",[],[],"[['Results', 'has', 'best model']]",[],[],[],[],"[['LSTM sequence model', 'has', 'greatly improves']]",natural_language_inference,96,236
results,"This model , as with most models studied , greatly improves with more context : by 3.1 % when given the initial noun phrase , and by an ad-ditional 4 % when also given the first sentence .",[],[],"[['greatly improves', 'with', 'more context'], ['greatly improves', 'by', 'ad-ditional 4 %'], ['ad-ditional 4 %', 'given', 'first sentence'], ['greatly improves', 'by', '3.1 %'], ['3.1 %', 'given', 'initial noun phrase']]",[],[],[],[],[],[],[],[],natural_language_inference,96,237
results,Further improvement is gained from models that compute pairwise representations of the inputs .,"[('gained from', (3, 5)), ('that compute', (6, 8)), ('of', (10, 11))]","[('Further improvement', (0, 2)), ('models', (5, 6)), ('pairwise representations', (8, 10)), ('inputs', (12, 13))]","[['Further improvement', 'gained from', 'models'], ['models', 'that compute', 'pairwise representations'], ['pairwise representations', 'of', 'inputs']]",[],[],"[['Results', 'has', 'Further improvement']]",[],[],[],[],[],natural_language_inference,96,238
results,"While the simplest such model , Dual - BoW , obtains only 35.1 % accuracy , combining In - fer Sent sentence representations gives 40.5 % accuracy ( InferSent - Bilinear ) .","[('obtains', (10, 11)), ('combining', (16, 17)), ('gives', (23, 24))]","[('simplest such model', (2, 5)), ('Dual - BoW', (6, 9)), ('only 35.1 % accuracy', (11, 15)), ('In - fer Sent sentence representations', (17, 23)), ('40.5 % accuracy', (24, 27)), ('InferSent - Bilinear', (28, 31))]","[['simplest such model', 'obtains', 'only 35.1 % accuracy'], ['simplest such model', 'combining', 'In - fer Sent sentence representations'], ['In - fer Sent sentence representations', 'gives', '40.5 % accuracy']]","[['simplest such model', 'name', 'Dual - BoW'], ['In - fer Sent sentence representations', 'name', 'InferSent - Bilinear']]",[],"[['Results', 'has', 'simplest such model']]",[],[],[],[],[],natural_language_inference,96,239
results,"The best results come from pairwise NLI models : when fully trained on Swag , ESIM + ELMo obtains 59.2 % accuracy .","[('come from', (3, 5)), ('fully trained on', (10, 13)), ('obtains', (18, 19))]","[('best results', (1, 3)), ('pairwise NLI models', (5, 8)), ('Swag , ESIM + ELMo', (13, 18)), ('59.2 % accuracy', (19, 22))]","[['best results', 'come from', 'pairwise NLI models'], ['pairwise NLI models', 'fully trained on', 'Swag , ESIM + ELMo'], ['Swag , ESIM + ELMo', 'obtains', '59.2 % accuracy']]",[],[],"[['Results', 'has', 'best results']]",[],[],[],[],[],natural_language_inference,96,240
research-problem,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,[],"[('Machine Comprehension', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension']]",[],[],[],[],natural_language_inference,97,2
research-problem,Understanding unstructured text is a major goal within natural language processing .,[],"[('Understanding unstructured text', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Understanding unstructured text']]",[],[],[],[],natural_language_inference,97,4
research-problem,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,[],"[('Machine comprehension ( MC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine comprehension ( MC )']]",[],[],[],[],natural_language_inference,97,16
model,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,"[('present', (1, 2)), ('to', (7, 8)), ('designed to', (10, 12)), ('in', (14, 15))]","[('parallel - hierarchical approach', (3, 7)), ('machine comprehension', (8, 10)), ('work well', (12, 14)), ('data - limited setting', (16, 20))]","[['parallel - hierarchical approach', 'designed to', 'work well'], ['work well', 'in', 'data - limited setting'], ['parallel - hierarchical approach', 'to', 'machine comprehension']]",[],"[['Model', 'present', 'parallel - hierarchical approach']]",[],[],[],[],[],[],natural_language_inference,97,21
model,Our model learns to comprehend at a high level even when data is sparse .,"[('learns to', (2, 4)), ('at', (5, 6)), ('even when', (9, 11)), ('is', (12, 13))]","[('comprehend', (4, 5)), ('high level', (7, 9)), ('data', (11, 12)), ('sparse', (13, 14))]","[['comprehend', 'at', 'high level'], ['comprehend', 'even when', 'data'], ['data', 'is', 'sparse']]",[],"[['Model', 'learns to', 'comprehend']]",[],[],[],[],[],[],natural_language_inference,97,26
model,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,"[('to', (2, 3)), ('compares', (8, 9)), ('using', (17, 18))]","[('question and answer candidates', (10, 14)), ('text', (16, 17)), ('several distinct perspectives', (18, 21))]","[['question and answer candidates', 'using', 'several distinct perspectives'], ['question and answer candidates', 'to', 'text']]",[],"[['Model', 'compares', 'question and answer candidates']]",[],[],[],[],[],[],natural_language_inference,97,27
model,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","[('compares', (3, 4)), ('to', (6, 7)), ('in', (8, 9)), ('viewed as', (11, 13)), ('represented using', (22, 24)), ('of', (28, 29))]","[('semantic perspective', (1, 3)), ('hypothesis', (5, 6)), ('sentences', (7, 8)), ('text', (10, 11)), ('single , self - contained thoughts', (13, 19)), ('sum and transformation', (25, 28)), ('word embedding vectors', (29, 32))]","[['semantic perspective', 'compares', 'hypothesis'], ['hypothesis', 'to', 'sentences'], ['sentences', 'in', 'text'], ['sentences', 'viewed as', 'single , self - contained thoughts'], ['semantic perspective', 'represented using', 'sum and transformation'], ['sum and transformation', 'of', 'word embedding vectors']]",[],[],"[['Model', 'has', 'semantic perspective']]",[],[],[],[],[],natural_language_inference,97,29
model,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .","[('focuses on', (7, 9)), ('between', (11, 12)), ('from', (14, 15)), ('at', (19, 20))]","[('word - by - word perspective', (1, 7)), ('similarity matches', (9, 11)), ('individual words', (12, 14)), ('hypothesis and text', (15, 18)), ('various scales', (20, 22))]","[['word - by - word perspective', 'focuses on', 'similarity matches'], ['similarity matches', 'at', 'various scales'], ['similarity matches', 'between', 'individual words'], ['individual words', 'from', 'hypothesis and text']]",[],[],"[['Model', 'has', 'word - by - word perspective']]",[],[],[],[],[],natural_language_inference,97,30
model,"As in the semantic perspective , we consider matches over complete sentences .","[('consider', (7, 8)), ('over', (9, 10))]","[('matches', (8, 9)), ('complete sentences', (10, 12))]","[['matches', 'over', 'complete sentences']]",[],[],[],[],"[['word - by - word perspective', 'consider', 'matches']]",[],[],[],natural_language_inference,97,31
model,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .","[('use', (2, 3)), ('acting on', (6, 8))]","[('sliding window', (4, 6)), ('subsentential scale', (9, 11))]","[['sliding window', 'acting on', 'subsentential scale']]",[],[],[],[],"[['word - by - word perspective', 'use', 'sliding window']]",[],[],[],natural_language_inference,97,32
experimental-setup,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .","[('For', (0, 1)), ('use', (4, 5)), ('trained with', (11, 13)), ('on', (14, 15))]","[('word vectors', (1, 3)), (""Google 's publicly available embeddings"", (5, 10)), ('word2vec', (13, 14)), ('100 - billion - word News corpus', (16, 23))]","[['word vectors', 'use', ""Google 's publicly available embeddings""], [""Google 's publicly available embeddings"", 'trained with', 'word2vec'], ['word2vec', 'on', '100 - billion - word News corpus']]",[],"[['Experimental setup', 'For', 'word vectors']]",[],[],[],[],[],[],natural_language_inference,97,218
experimental-setup,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .","[('kept', (3, 4)), ('throughout', (5, 6))]","[('fixed', (4, 5)), ('training', (6, 7))]","[['fixed', 'throughout', 'training']]",[],[],[],[],"[['word vectors', 'kept', 'fixed']]",[],[],[],natural_language_inference,97,219
experimental-setup,The vectors are 300 - dimensional ( d = 300 ) .,"[('are', (2, 3))]","[('300 - dimensional', (3, 6))]",[],[],[],[],[],"[['word vectors', 'are', '300 - dimensional']]",[],[],[],natural_language_inference,97,220
experimental-setup,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .","[('used', (19, 20)), ('as', (21, 22))]","[('0.5', (20, 21)), ('dropout probability', (23, 25))]","[['0.5', 'as', 'dropout probability']]",[],"[['Experimental setup', 'used', '0.5']]",[],[],[],[],[],[],natural_language_inference,97,231
experimental-setup,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .","[('occurs after', (1, 3)), ('allowed to', (13, 15)), ('with', (16, 17))]","[('Dropout', (0, 1)), ('all neural - network transformations', (3, 8)), ('change', (15, 16)), ('training', (17, 18))]","[['Dropout', 'occurs after', 'all neural - network transformations'], ['all neural - network transformations', 'allowed to', 'change'], ['change', 'with', 'training']]",[],[],"[['Experimental setup', 'has', 'Dropout']]",[],[],[],[],[],natural_language_inference,97,232
experimental-setup,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","[('with', (5, 6)), ('of', (20, 21))]","[('Adam optimizer', (3, 5)), ('standard settings', (7, 9)), ('learning rate', (18, 20)), ('0.003', (21, 22))]","[['learning rate', 'of', '0.003'], ['Adam optimizer', 'with', 'standard settings']]",[],[],"[['Experimental setup', 'used', 'learning rate'], ['Experimental setup', 'used', 'Adam optimizer']]",[],[],[],[],[],natural_language_inference,97,235
results,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ?",[],[],"[['latter two', 'on', 'multi questions ( ? 0.3 % )'], ['these methods', 'on', 'single questions ( > 2 % )']]","[['MCTest - 500', 'has', 'Parallel Hierarchical model'], ['Parallel Hierarchical model', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'latter two'], ['Parallel Hierarchical model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'these methods']]","[['Results', 'On', 'MCTest - 500']]",[],[],[],[],[],[],natural_language_inference,97,247
results,The method of achieves the best over all result on MCTest - 160 .,"[('achieves', (3, 4)), ('on', (9, 10))]","[('method', (1, 2)), ('best over all result', (5, 9)), ('MCTest - 160', (10, 13))]","[['method', 'achieves', 'best over all result'], ['best over all result', 'on', 'MCTest - 160']]",[],[],"[['Results', 'has', 'method']]",[],[],[],[],[],natural_language_inference,97,249
results,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,"[('by', (8, 9)), ('across', (12, 13))]","[('our model', (3, 5)), ('outperforming', (5, 6)), ('alternatives', (7, 8)), ('large margin', (10, 12)), ('board ( > 15 % )', (14, 20))]","[['outperforming', 'by', 'large margin'], ['large margin', 'across', 'board ( > 15 % )']]","[['our model', 'has', 'outperforming'], ['outperforming', 'has', 'alternatives']]",[],"[['Results', 'has', 'our model']]",[],[],[],[],[],natural_language_inference,97,253
ablation-analysis,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .","[('is', (6, 7)), ('contributing', (9, 10))]","[('n-gram functionality', (4, 6)), ('important', (7, 8)), ('almost 5 % accuracy improvement', (10, 15))]","[['n-gram functionality', 'contributing', 'almost 5 % accuracy improvement'], ['n-gram functionality', 'is', 'important']]",[],[],"[['Ablation analysis', 'has', 'n-gram functionality']]",[],[],[],[],[],natural_language_inference,97,260
ablation-analysis,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .","[('contributes', (4, 5)), ('to', (7, 8))]","[('top N function', (1, 4)), ('very little', (5, 7)), ('over all performance', (9, 12))]","[['top N function', 'contributes', 'very little'], ['very little', 'to', 'over all performance']]",[],[],"[['Ablation analysis', 'has', 'top N function']]",[],[],[],[],[],natural_language_inference,97,263
ablation-analysis,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .","[('Ablating', (0, 1)), ('made', (4, 5)), ('reducing', (10, 11)), ('by', (12, 13))]","[('sentential component', (2, 4)), ('most significant difference', (6, 9)), ('performance', (11, 12)), ('more than 5 %', (13, 17))]","[['sentential component', 'made', 'most significant difference'], ['sentential component', 'reducing', 'performance'], ['performance', 'by', 'more than 5 %']]",[],"[['Ablation analysis', 'Ablating', 'sentential component']]",[],[],[],[],[],[],natural_language_inference,97,264
ablation-analysis,Simple word - by - word matching is obviously useful on MCTest .,"[('on', (10, 11))]","[('Simple word - by - word matching', (0, 7)), ('useful', (9, 10)), ('MCTest', (11, 12))]","[['useful', 'on', 'MCTest']]","[['Simple word - by - word matching', 'has', 'useful']]",[],"[['Ablation analysis', 'has', 'Simple word - by - word matching']]",[],[],[],[],[],natural_language_inference,97,265
ablation-analysis,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .","[('makes', (4, 5))]","[('sequential sliding window', (1, 4)), ('3 % contribution', (6, 9))]","[['sequential sliding window', 'makes', '3 % contribution']]",[],[],"[['Ablation analysis', 'has', 'sequential sliding window']]",[],[],[],[],[],natural_language_inference,97,266
ablation-analysis,"On the other hand , the dependency - based sliding window makes only a minor contribution .","[('makes', (11, 12))]","[('dependency - based sliding window', (6, 11)), ('minor contribution', (14, 16))]","[['dependency - based sliding window', 'makes', 'minor contribution']]",[],[],"[['Ablation analysis', 'has', 'dependency - based sliding window']]",[],[],[],[],[],natural_language_inference,97,267
ablation-analysis,"Finally , the exogenous word weights make a significant contribution of almost 5 % .","[('make', (6, 7)), ('of', (10, 11))]","[('exogenous word weights', (3, 6)), ('significant contribution', (8, 10)), ('almost 5 %', (11, 14))]","[['exogenous word weights', 'make', 'significant contribution'], ['significant contribution', 'of', 'almost 5 %']]",[],[],"[['Ablation analysis', 'has', 'exogenous word weights']]",[],[],[],[],[],natural_language_inference,97,270
research-problem,Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference,[],"[('Natural Language Inference', (11, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Natural Language Inference']]",[],[],[],[],natural_language_inference,98,2
research-problem,"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .",[],"[('natural language inference ( NLI )', (11, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language inference ( NLI )']]",[],[],[],[],natural_language_inference,98,14
research-problem,"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.",[],"[('NLI', (2, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLI']]",[],[],[],[],natural_language_inference,98,16
model,"We present here the proposed natural language inference networks which are composed of the following major components : word embedding , sequence encoder , composition layer , and the toplayer classifier .","[('proposed', (4, 5)), ('composed of', (11, 13))]","[('natural language inference networks', (5, 9)), ('word embedding', (18, 20)), ('sequence encoder', (21, 23)), ('composition layer', (24, 26)), ('toplayer classifier', (29, 31))]","[['natural language inference networks', 'composed of', 'word embedding'], ['natural language inference networks', 'composed of', 'sequence encoder'], ['natural language inference networks', 'composed of', 'composition layer'], ['natural language inference networks', 'composed of', 'toplayer classifier']]",[],"[['Model', 'proposed', 'natural language inference networks']]",[],[],[],[],[],[],natural_language_inference,98,29
model,Word Embedding,[],"[('Word Embedding', (0, 2))]",[],[],[],"[['Model', 'has', 'Word Embedding']]",[],[],[],[],[],natural_language_inference,98,31
model,We concatenate embeddings learned at two different levels to represent each word in the sentence : the character composition and holistic word - level embedding .,"[('concatenate', (1, 2)), ('learned at', (3, 5)), ('to represent', (8, 10)), ('in', (12, 13))]","[('embeddings', (2, 3)), ('two different levels', (5, 8)), ('each word', (10, 12)), ('sentence', (14, 15)), ('character composition', (17, 19)), ('holistic word - level embedding', (20, 25))]","[['embeddings', 'learned at', 'two different levels'], ['embeddings', 'to represent', 'each word'], ['each word', 'in', 'sentence']]","[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",[],[],[],"[['Word Embedding', 'concatenate', 'embeddings']]",[],[],[],natural_language_inference,98,33
model,Sequence Encoder,[],"[('Sequence Encoder', (0, 2))]",[],[],[],"[['Model', 'has', 'Sequence Encoder']]",[],[],[],[],"[['Sequence Encoder', 'has', 'sentence pairs']]",natural_language_inference,98,40
model,"To represent words and their context in a premise and hypothesis , sentence pairs are fed into sentence encoders to obtain hidden vectors ( h p and h h ) .","[('fed into', (15, 17)), ('to obtain', (19, 21))]","[('sentence pairs', (12, 14)), ('sentence encoders', (17, 19)), ('hidden vectors ( h p and h h )', (21, 30))]","[['sentence pairs', 'fed into', 'sentence encoders'], ['sentence encoders', 'to obtain', 'hidden vectors ( h p and h h )']]",[],[],[],[],[],[],[],[],natural_language_inference,98,41
model,We use stacked bidirectional LSTMs ( BiL - STM ) as the encoders .,"[('use', (1, 2)), ('as', (10, 11))]","[('stacked bidirectional LSTMs ( BiL - STM )', (2, 10)), ('encoders', (12, 13))]","[['stacked bidirectional LSTMs ( BiL - STM )', 'as', 'encoders']]",[],[],[],[],"[['Sequence Encoder', 'use', 'stacked bidirectional LSTMs ( BiL - STM )']]",[],[],[],natural_language_inference,98,42
model,Composition Layer,[],"[('Composition Layer', (0, 2))]",[],[],[],"[['Model', 'has', 'Composition Layer']]",[],[],[],[],[],natural_language_inference,98,53
model,"To transform sentences into fixed - length vector representations and reason using those representations , we need to compose the hidden vectors obtained by the sequence encoder layer ( h p and h h ) .","[('To transform', (0, 2)), ('into', (3, 4)), ('compose', (18, 19)), ('obtained by', (22, 24))]","[('sentences', (2, 3)), ('fixed - length vector representations', (4, 9)), ('hidden vectors', (20, 22)), ('sequence encoder layer ( h p and h h )', (25, 35))]","[['hidden vectors', 'obtained by', 'sequence encoder layer ( h p and h h )'], ['sequence encoder layer ( h p and h h )', 'To transform', 'sentences'], ['sentences', 'into', 'fixed - length vector representations']]",[],[],[],[],"[['Composition Layer', 'compose', 'hidden vectors']]",[],[],[],natural_language_inference,98,54
model,We propose intra-sentence gated - attention to obtain a fixed - length vector .,"[('propose', (1, 2)), ('to obtain', (6, 8))]","[('intra-sentence gated - attention', (2, 6)), ('fixed - length vector', (9, 13))]","[['intra-sentence gated - attention', 'to obtain', 'fixed - length vector']]",[],[],[],[],"[['Composition Layer', 'propose', 'intra-sentence gated - attention']]",[],[],[],natural_language_inference,98,55
model,Top - layer Classifier,[],"[('Top - layer Classifier', (0, 4))]",[],[],[],"[['Model', 'has', 'Top - layer Classifier']]",[],[],[],[],"[['Top - layer Classifier', 'has', 'Our inference model']]",natural_language_inference,98,67
model,Our inference model feeds the resulting vectors obtained above to the final classifier to determine the over all inference relationship .,"[('feeds', (3, 4)), ('to', (9, 10)), ('to determine', (13, 15))]","[('Our inference model', (0, 3)), ('resulting vectors', (5, 7)), ('final classifier', (11, 13)), ('over all inference relationship', (16, 20))]","[['Our inference model', 'feeds', 'resulting vectors'], ['resulting vectors', 'to determine', 'over all inference relationship'], ['resulting vectors', 'to', 'final classifier']]",[],[],[],[],[],[],[],[],natural_language_inference,98,68
code,"To help replicate our results , we publish our code at https : //github.com/lukecq1231/enc_nli",[],"[('https : //github.com/lukecq1231/enc_nli', (11, 14))]",[],[],[],[],"[['Contribution', 'Code', 'https : //github.com/lukecq1231/enc_nli']]",[],[],[],[],natural_language_inference,98,82
experimental-setup,"We use the Adam ( Kingma and Ba , 2014 ) for optimization .","[('use', (1, 2)), ('for', (11, 12))]","[('Adam ( Kingma and Ba , 2014 )', (3, 11)), ('optimization', (12, 13))]","[['Adam ( Kingma and Ba , 2014 )', 'for', 'optimization']]",[],"[['Experimental setup', 'use', 'Adam ( Kingma and Ba , 2014 )']]",[],[],[],[],[],[],natural_language_inference,98,84
experimental-setup,"Stacked BiLSTM has 3 layers , and all hidden states of BiLSTMs and MLP have 600 dimensions .","[('of', (10, 11)), ('have', (14, 15))]","[('Stacked BiLSTM', (0, 2)), ('3 layers', (3, 5)), ('all hidden states', (7, 10)), ('BiLSTMs and MLP', (11, 14)), ('600 dimensions', (15, 17))]","[['all hidden states', 'of', 'BiLSTMs and MLP'], ['BiLSTMs and MLP', 'have', '600 dimensions']]","[['Stacked BiLSTM', 'has', '3 layers']]",[],"[['Experimental setup', 'has', 'Stacked BiLSTM'], ['Experimental setup', 'has', 'all hidden states']]",[],[],[],[],[],natural_language_inference,98,85
experimental-setup,"The character embedding has 15 dimensions , and CNN filters length is [ 1 , 3 , 5 ] , each of those is 100 dimensions .","[('is', (11, 12))]","[('character embedding', (1, 3)), ('15 dimensions', (4, 6)), ('CNN filters length', (8, 11)), ('[ 1 , 3 , 5 ]', (12, 19)), ('100 dimensions', (24, 26))]","[['CNN filters length', 'is', '[ 1 , 3 , 5 ]']]","[['100 dimensions', 'has', 'CNN filters length'], ['100 dimensions', 'has', 'character embedding'], ['character embedding', 'has', '15 dimensions']]",[],"[['Experimental setup', 'has', '100 dimensions']]",[],[],[],[],[],natural_language_inference,98,86
experimental-setup,We use pretrained GloVe - 840B - 300D vectors as our word - level embeddings and fix these embeddings during the training process .,"[('as', (9, 10)), ('during', (19, 20))]","[('pretrained GloVe - 840B - 300D vectors', (2, 9)), ('our word - level embeddings', (10, 15)), ('fix', (16, 17)), ('training process', (21, 23))]","[['pretrained GloVe - 840B - 300D vectors', 'as', 'our word - level embeddings'], ['fix', 'during', 'training process']]","[['pretrained GloVe - 840B - 300D vectors', 'has', 'fix']]",[],"[['Experimental setup', 'use', 'pretrained GloVe - 840B - 300D vectors']]",[],[],[],[],[],natural_language_inference,98,87
experimental-setup,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('initialized randomly', (10, 12)), ('Gaussian samples', (13, 15))]","[['initialized randomly', 'with', 'Gaussian samples']]","[['Out - of - vocabulary ( OOV ) words', 'has', 'initialized randomly']]",[],"[['Experimental setup', 'has', 'Out - of - vocabulary ( OOV ) words']]",[],[],[],[],[],natural_language_inference,98,88
results,"In addition , we also use our implementation of ESIM , which achieves an accuracy of 76.8 % in the in - domain test set , and 75.8 % in the cross - domain test set , which presents the state - of - the - art results .",[],[],"[['our implementation of ESIM', 'achieves', 'accuracy'], ['accuracy', 'of', '75.8 %'], ['75.8 %', 'in', 'cross - domain test set'], ['accuracy', 'of', '76.8 %'], ['76.8 %', 'in', 'in - domain test set'], ['our implementation of ESIM', 'presents', 'state - of - the - art results']]",[],[],"[['Results', 'has', 'our implementation of ESIM']]",[],[],[],[],[],natural_language_inference,98,95
results,"After removing the cross - sentence attention and adding our gated - attention model , we achieve accuracies of 73.5 % and 73.6 % , which ranks first in the cross - domain test set and ranks second in the in - domain test set among the single models .",[],[],"[['accuracies', 'of', '73.5 % and 73.6 %'], ['73.5 % and 73.6 %', 'among', 'single models'], ['single models', 'ranks', 'first'], ['first', 'in', 'cross - domain test set'], ['single models', 'ranks', 'second'], ['second', 'in', 'in - domain test set'], ['73.5 % and 73.6 %', 'adding', 'our gated - attention model'], ['73.5 % and 73.6 %', 'After removing', 'cross - sentence attention']]",[],"[['Results', 'achieve', 'accuracies']]",[],[],[],[],[],[],natural_language_inference,98,96
results,"When ensembling our models , we obtain accuracies 74.9 % and 74.9 % , which ranks first in both test sets .","[('When', (0, 1)), ('our models', (2, 4)), ('accuracies', (7, 8)), ('74.9 % and 74.9 %', (8, 13)), ('first in', (16, 18))]","[('ensembling', (1, 2)), ('obtain', (6, 7)), ('ranks', (15, 16)), ('both test sets', (18, 21))]","[['ensembling', 'our models', 'obtain'], ['ranks', 'first in', 'both test sets']]",[],"[['Results', 'When', 'ensembling']]",[],[],"[['has', '74.9 % and 74.9 %', 'ranks']]","[['obtain', 'accuracies', 'has']]",[],[],natural_language_inference,98,97
ablation-analysis,"If we remove the gated - attention , the accuracies drop to 72.8 % and 73.6 % .","[('remove', (2, 3)), ('to', (11, 12))]","[('gated - attention', (4, 7)), ('accuracies', (9, 10)), ('drop', (10, 11)), ('72.8 % and 73.6 %', (12, 17))]","[['drop', 'to', '72.8 % and 73.6 %']]","[['accuracies', 'has', 'drop'], ['gated - attention', 'has', 'accuracies'], ['accuracies', 'has', 'drop'], ['accuracies', 'has', 'drop']]","[['Ablation analysis', 'remove', 'gated - attention']]",[],[],[],[],[],[],natural_language_inference,98,102
ablation-analysis,"If we remove charactercomposition vector , the accuracies drop to 72.9 % and 73.5 % .","[('to', (9, 10))]","[('charactercomposition vector', (3, 5)), ('accuracies', (7, 8)), ('drop', (8, 9)), ('72.9 % and 73.5 %', (10, 15))]","[['drop', 'to', '72.9 % and 73.5 %']]","[['charactercomposition vector', 'has', 'accuracies']]",[],"[['Ablation analysis', 'remove', 'charactercomposition vector']]",[],[],[],[],[],natural_language_inference,98,103
ablation-analysis,"If we remove word - level embedding , the accuracies drop to 65.6 % and 66.0 % .","[('to', (11, 12))]","[('word - level embedding', (3, 7)), ('accuracies', (9, 10)), ('drop', (10, 11)), ('65.6 % and 66.0 %', (12, 17))]","[['drop', 'to', '65.6 % and 66.0 %']]","[['word - level embedding', 'has', 'accuracies']]",[],"[['Ablation analysis', 'remove', 'word - level embedding']]",[],[],[],[],[],natural_language_inference,98,104
research-problem,"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question .",[],"[('Machine Comprehension ( MC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine Comprehension ( MC )']]",[],[],[],[],natural_language_inference,99,4
research-problem,"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon .",[],"[('MC', (4, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'MC']]",[],[],[],[],natural_language_inference,99,5
research-problem,Recently machine comprehension task accumulates much concern among NLP researchers .,[],"[('machine comprehension', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'machine comprehension']]",[],[],[],[],natural_language_inference,99,13
model,"In this paper , we propose the novel framework named Smarnet with the hope that it can become as smart as humans .","[('propose', (5, 6)), ('named', (9, 10))]","[('novel framework', (7, 9)), ('Smarnet', (10, 11))]","[['novel framework', 'named', 'Smarnet']]",[],"[['Model', 'propose', 'novel framework']]",[],[],[],[],[],[],natural_language_inference,99,28
model,"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .","[('introduce', (4, 5)), ('exploits', (9, 10)), ('with', (15, 16))]","[('Smarnet framework', (6, 8)), ('fine - grained word understanding', (10, 15)), ('various attribution discriminations', (16, 19))]","[['Smarnet framework', 'exploits', 'fine - grained word understanding'], ['fine - grained word understanding', 'with', 'various attribution discriminations']]",[],"[['Model', 'introduce', 'Smarnet framework']]",[],[],[],[],[],[],natural_language_inference,99,30
model,We then develop the interactive attention with memory network to mimic human reading procedure .,"[('develop', (2, 3)), ('with', (6, 7)), ('to mimic', (9, 11))]","[('interactive attention', (4, 6)), ('memory network', (7, 9)), ('human reading procedure', (11, 14))]","[['interactive attention', 'with', 'memory network'], ['interactive attention', 'to mimic', 'human reading procedure']]",[],"[['Model', 'develop', 'interactive attention']]",[],[],[],[],[],[],natural_language_inference,99,31
model,We also add a checking layer on the answer refining in order to ensure the accuracy .,"[('add', (2, 3)), ('on', (6, 7)), ('to ensure', (12, 14))]","[('checking layer', (4, 6)), ('answer refining', (8, 10)), ('accuracy', (15, 16))]","[['checking layer', 'to ensure', 'accuracy'], ['checking layer', 'on', 'answer refining']]",[],"[['Model', 'add', 'checking layer']]",[],[],[],[],[],[],natural_language_inference,99,32
experimental-setup,"We preprocess each passage and question using the library of nltk and exploit the popular pretrained word embedding GloVe with 100 - dimensional vectors ( Pennington , Socher , and Manning 2014 ) for both questions and passages .","[('preprocess', (1, 2)), ('using', (6, 7)), ('exploit', (12, 13)), ('with', (19, 20)), ('for', (33, 34))]","[('each passage and question', (2, 6)), ('library of nltk', (8, 11)), ('popular pretrained word embedding GloVe', (14, 19)), ('100 - dimensional vectors', (20, 24)), ('questions and passages', (35, 38))]","[['each passage and question', 'using', 'library of nltk'], ['popular pretrained word embedding GloVe', 'with', '100 - dimensional vectors'], ['popular pretrained word embedding GloVe', 'for', 'questions and passages']]",[],"[['Experimental setup', 'preprocess', 'each passage and question'], ['Experimental setup', 'exploit', 'popular pretrained word embedding GloVe']]",[],[],[],[],[],[],natural_language_inference,99,190
experimental-setup,The size of char - level embedding is also set as 100 - dimensional and is obtained by CNN filters under the instruction of ( Kim 2014 ) .,"[('of', (2, 3)), ('set as', (9, 11)), ('obtained by', (16, 18))]","[('size', (1, 2)), ('char - level embedding', (3, 7)), ('100 - dimensional', (11, 14)), ('CNN filters', (18, 20))]","[['size', 'set as', '100 - dimensional'], ['size', 'of', 'char - level embedding'], ['char - level embedding', 'obtained by', 'CNN filters']]",[],[],"[['Experimental setup', 'has', 'size']]",[],[],[],[],[],natural_language_inference,99,191
experimental-setup,We adopt the AdaDelta ( Zeiler 2012 ) optimizer for training with an initial learning rate of 0.0005 .,"[('adopt', (1, 2)), ('for', (9, 10)), ('with', (11, 12)), ('of', (16, 17))]","[('AdaDelta ( Zeiler 2012 ) optimizer', (3, 9)), ('training', (10, 11)), ('initial learning rate', (13, 16)), ('0.0005', (17, 18))]","[['AdaDelta ( Zeiler 2012 ) optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0005'], ['AdaDelta ( Zeiler 2012 ) optimizer', 'for', 'training']]",[],"[['Experimental setup', 'adopt', 'AdaDelta ( Zeiler 2012 ) optimizer']]",[],[],[],[],[],[],natural_language_inference,99,193
experimental-setup,The batch size is set to be 48 for both the SQuAD and TriviaQA datasets .,"[('set to be', (4, 7)), ('for', (8, 9))]","[('batch size', (1, 3)), ('48', (7, 8)), ('SQuAD and TriviaQA datasets', (11, 15))]","[['batch size', 'set to be', '48'], ['48', 'for', 'SQuAD and TriviaQA datasets']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],natural_language_inference,99,194
experimental-setup,We also apply dropout ( Srivastava et al. 2014 ) between layers with a dropout rate of 0.2 .,"[('apply', (2, 3)), ('between', (10, 11)), ('with', (12, 13)), ('of', (16, 17))]","[('dropout ( Srivastava et al. 2014 )', (3, 10)), ('layers', (11, 12)), ('dropout rate', (14, 16)), ('0.2', (17, 18))]","[['dropout ( Srivastava et al. 2014 )', 'with', 'dropout rate'], ['dropout rate', 'of', '0.2'], ['dropout ( Srivastava et al. 2014 )', 'between', 'layers']]",[],"[['Experimental setup', 'apply', 'dropout ( Srivastava et al. 2014 )']]",[],[],[],[],[],[],natural_language_inference,99,195
experimental-setup,"For the multi-hop reasoning , we set the number of hops as 2 which is imitating human reading procedure on skimming and scanning .","[('For', (0, 1)), ('set', (6, 7)), ('as', (11, 12)), ('on', (19, 20))]","[('multi-hop reasoning', (2, 4)), ('number of hops', (8, 11)), ('2', (12, 13)), ('imitating', (15, 16)), ('human reading procedure', (16, 19)), ('skimming and scanning', (20, 23))]","[['multi-hop reasoning', 'set', 'number of hops'], ['number of hops', 'as', '2'], ['imitating', 'on', 'skimming and scanning']]","[['2', 'has', 'imitating'], ['imitating', 'has', 'human reading procedure']]","[['Experimental setup', 'For', 'multi-hop reasoning']]",[],[],[],[],[],[],natural_language_inference,99,196
experimental-setup,"During training , we set the moving averages of all weights as the exponential decay rate of 0.999 .",[],[],"[['training', 'set', 'moving averages'], ['moving averages', 'as', 'exponential decay rate'], ['exponential decay rate', 'of', '0.999'], ['moving averages', 'of', 'all weights']]",[],"[['Experimental setup', 'During', 'training']]",[],[],[],[],[],[],natural_language_inference,99,197
results,"From the tables 1 and 2 we can see our single model achieves an EM score of 71.415 % and a F1 score of 80.160 % and the ensemble model improves to EM 75.989 % and F1 83. 475 % , which are both only after the r-net method at the time of submission .",[],[],"[['improves', 'to', 'EM'], ['improves', 'to', 'F1'], ['our single model', 'achieves', 'EM score'], ['EM score', 'of', '71.415 %'], ['our single model', 'achieves', 'F1 score'], ['F1 score', 'of', '80.160 %']]","[['ensemble model', 'has', 'improves'], ['EM', 'has', '75.989 %'], ['F1', 'has', '83. 475 %']]","[['Results', 'can see', 'ensemble model'], ['Results', 'can see', 'our single model']]",[],[],[],[],[],[],natural_language_inference,99,209
results,We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the test set of Trivia QA .,"[('on', (5, 6)), ('of', (20, 21))]","[('test set', (18, 20)), ('Trivia QA', (21, 23))]","[['test set', 'of', 'Trivia QA']]",[],"[['Results', 'on', 'test set']]",[],[],[],[],[],[],natural_language_inference,99,211
results,We can see our Smarnet model outperforms the other baselines on both wikipedia domain and web domain .,"[('can see', (1, 3)), ('on', (10, 11))]","[('our Smarnet model', (3, 6)), ('outperforms', (6, 7)), ('other baselines', (8, 10)), ('wikipedia domain', (12, 14)), ('web domain', (15, 17))]","[['other baselines', 'on', 'wikipedia domain'], ['other baselines', 'on', 'web domain']]","[['our Smarnet model', 'has', 'outperforms'], ['outperforms', 'has', 'other baselines']]",[],[],[],"[['Trivia QA', 'can see', 'our Smarnet model']]",[],[],[],natural_language_inference,99,212
ablation-analysis,"We see the full features integration obtain the best performance , which demonstrates the necessity of combining all the features into consideration .","[('see', (1, 2)), ('obtain', (6, 7))]","[('full features integration', (3, 6)), ('best performance', (8, 10))]","[['full features integration', 'obtain', 'best performance']]",[],"[['Ablation analysis', 'see', 'full features integration']]",[],[],[],[],[],[],natural_language_inference,99,216
ablation-analysis,"Among all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features drop much more than the other features , which shows the importance of these three features .","[('Among', (0, 1)), ('much more than', (19, 22))]","[('all the feature ablations', (1, 5)), ('Part - Of - Speech', (7, 12)), ('Exact Match', (13, 15)), ('Qtype features', (16, 18)), ('drop', (18, 19)), ('other features', (23, 25))]","[['drop', 'much more than', 'other features']]","[['all the feature ablations', 'has', 'drop'], ['drop', 'has', 'Part - Of - Speech'], ['drop', 'has', 'Exact Match'], ['drop', 'has', 'Qtype features']]","[['Ablation analysis', 'Among', 'all the feature ablations']]",[],[],[],[],[],[],natural_language_inference,99,217
ablation-analysis,"As for the final ablation of POS and NER , we can see the performance decays over 3 % point , which clearly proves the usefulness of the comprehensive lexical information .","[('of', (5, 6)), ('can see', (11, 13)), ('over', (16, 17))]","[('final ablation', (3, 5)), ('POS and NER', (6, 9)), ('performance decays', (14, 16)), ('3 % point', (17, 20))]","[['final ablation', 'of', 'POS and NER'], ['POS and NER', 'can see', 'performance decays'], ['performance decays', 'over', '3 % point']]",[],[],"[['Ablation analysis', 'has', 'final ablation']]",[],[],[],[],[],natural_language_inference,99,219
ablation-analysis,"We first replace our input gate mechanism into simplified feature concatenation strategy , the performance drops nearly 2.3 % on the EM score , which proves the effectiveness of our proposed dynamic input gating mechanism .","[('replace', (2, 3)), ('into', (7, 8)), ('on', (19, 20))]","[('our input gate mechanism', (3, 7)), ('simplified feature concatenation strategy', (8, 12)), ('performance', (14, 15)), ('drops', (15, 16)), ('nearly 2.3 %', (16, 19)), ('EM score', (21, 23))]","[['our input gate mechanism', 'into', 'simplified feature concatenation strategy'], ['nearly 2.3 %', 'on', 'EM score']]","[['our input gate mechanism', 'has', 'performance'], ['performance', 'has', 'drops'], ['drops', 'has', 'nearly 2.3 %']]","[['Ablation analysis', 'replace', 'our input gate mechanism']]",[],[],[],[],[],[],natural_language_inference,99,221
ablation-analysis,The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3 % on the EM score .,[],[],"[['question influence', 'on', 'passage encoding'], ['up to 1.3 %', 'on', 'EM score']]","[['passage encoding', 'has', 'boost'], ['boost', 'has', 'result'], ['boost', 'has', 'up to 1.3 %']]","[['Ablation analysis', 'employing', 'question influence']]",[],[],[],[],[],[],natural_language_inference,99,223
research-problem,NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution,[],"[('Negation Detection and Scope Resolution', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Negation Detection and Scope Resolution']]",[],[],[],[],negation_scope_resolution,0,2
approach,"Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .","[('apply', (9, 10)), ('to', (11, 12))]","[('BERT', (10, 11)), ('negation detection and scope resolution', (12, 17))]","[['BERT', 'to', 'negation detection and scope resolution']]",[],"[['Approach', 'apply', 'BERT']]",[],[],[],[],[],[],negation_scope_resolution,0,34
approach,"We explore the set of design choices involved , and experiment on all 3 public datasets available : the BioScope Corpus ( Abstracts and Full Papers ) , the Sherlock Dataset and the SFU Review Corpus .","[('explore', (1, 2)), ('experiment on', (10, 12))]","[('design choices', (5, 7)), ('3 public datasets available', (13, 17)), ('BioScope Corpus ( Abstracts and Full Papers )', (19, 27)), ('Sherlock Dataset', (29, 31)), ('SFU Review Corpus', (33, 36))]",[],"[['3 public datasets available', 'name', 'BioScope Corpus ( Abstracts and Full Papers )'], ['3 public datasets available', 'name', 'Sherlock Dataset'], ['3 public datasets available', 'name', 'SFU Review Corpus']]","[['Approach', 'explore', 'design choices'], ['Approach', 'experiment on', '3 public datasets available']]",[],[],[],[],[],[],negation_scope_resolution,0,35
experimental-setup,We use Google 's BERT as the base model to generate contextual embeddings for the sentence .,"[('use', (1, 2)), ('as', (5, 6)), ('to generate', (9, 11)), ('for', (13, 14))]","[(""Google 's BERT"", (2, 5)), ('base model', (7, 9)), ('contextual embeddings', (11, 13)), ('sentence', (15, 16))]","[[""Google 's BERT"", 'as', 'base model'], ['base model', 'to generate', 'contextual embeddings'], ['contextual embeddings', 'for', 'sentence']]",[],"[['Experimental setup', 'use', ""Google 's BERT""]]",[],[],[],[],[],[],negation_scope_resolution,0,189
experimental-setup,The input to the BERT model is a sequence of tokenized and encoded tokens of a sentence .,"[('input to', (1, 3)), ('is', (6, 7)), ('of', (14, 15))]","[('BERT model', (4, 6)), ('sequence of tokenized and encoded tokens', (8, 14)), ('sentence', (16, 17))]","[['BERT model', 'is', 'sequence of tokenized and encoded tokens'], ['sequence of tokenized and encoded tokens', 'of', 'sentence']]",[],"[['Experimental setup', 'input to', 'BERT model']]",[],[],[],[],[],[],negation_scope_resolution,0,190
experimental-setup,"We then use a vector of dimension R H x N_C to compute scores per token , for the classification task at hand .","[('of', (5, 6)), ('to compute', (11, 13))]","[('vector', (4, 5)), ('dimension R H x N_C', (6, 11)), ('scores per token', (13, 16))]","[['vector', 'of', 'dimension R H x N_C'], ['dimension R H x N_C', 'to compute', 'scores per token']]",[],[],"[['Experimental setup', 'use', 'vector']]",[],[],[],[],[],negation_scope_resolution,0,191
experimental-setup,"BERT outputs a vector of size R H per token of the input , which we feed to a common classification layer of dimen-sion R Hx5 for cue detection and R Hx2 for scope resolution .",[],[],"[['BERT', 'outputs', 'vector'], ['vector', 'of', 'size R H per token of the input'], ['vector', 'feed to', 'common classification layer'], ['common classification layer', 'of', 'dimen-sion R Hx5'], ['dimen-sion R Hx5', 'for', 'cue detection'], ['common classification layer', 'of', 'R Hx2'], ['R Hx2', 'for', 'scope resolution']]",[],[],"[['Experimental setup', 'has', 'BERT']]",[],[],[],[],[],negation_scope_resolution,0,192
experimental-setup,"We use early stopping on dev data for 6 epochs as tolerance and F 1 score as the early stopping metric , use the Adam optimizer with an initial learning rate of 3 e - 5 , and the Categorical Cross Entropy Loss with class weights as described above to avoid training on the padded label outputs .",[],[],"[['F 1 score', 'as', 'early stopping metric'], ['early stopping', 'on', 'dev data'], ['dev data', 'for', '6 epochs'], ['6 epochs', 'as', 'tolerance'], ['Adam optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '3 e - 5'], ['Categorical Cross Entropy Loss', 'to avoid', 'training'], ['training', 'on', 'padded label outputs']]",[],[],"[['Experimental setup', 'use', 'F 1 score'], ['Experimental setup', 'use', 'early stopping'], ['Experimental setup', 'use', 'Adam optimizer'], ['Experimental setup', 'use', 'Categorical Cross Entropy Loss']]",[],[],[],[],[],negation_scope_resolution,0,193
experimental-setup,"We perform cue detection and scope resolution for all 3 datasets , and train on 1 and test on all datasets .","[('perform', (1, 2)), ('for', (7, 8)), ('train on', (13, 15)), ('test on', (17, 19))]","[('cue detection and scope resolution', (2, 7)), ('all 3 datasets', (8, 11)), ('1', (15, 16)), ('all datasets', (19, 21))]","[['cue detection and scope resolution', 'for', 'all 3 datasets'], ['cue detection and scope resolution', 'train on', '1'], ['cue detection and scope resolution', 'test on', 'all datasets']]",[],"[['Experimental setup', 'perform', 'cue detection and scope resolution']]",[],[],[],[],[],[],negation_scope_resolution,0,194
experimental-setup,"For all other corpuses , we use a default 70 - 15 - 15 split for the train - dev - test data .","[('for', (15, 16))]","[('default 70 - 15 - 15 split', (8, 15)), ('train - dev - test data', (17, 23))]","[['default 70 - 15 - 15 split', 'for', 'train - dev - test data']]",[],[],"[['Experimental setup', 'use', 'default 70 - 15 - 15 split']]",[],[],[],[],[],negation_scope_resolution,0,197
experimental-setup,"We trained the models on free GPUs available via Google Colaboratory , the training scripts are publicly available .","[('trained', (1, 2)), ('on', (4, 5)), ('available via', (7, 9))]","[('models', (3, 4)), ('free GPUs', (5, 7)), ('Google Colaboratory', (9, 11))]","[['models', 'on', 'free GPUs'], ['free GPUs', 'available via', 'Google Colaboratory']]",[],"[['Experimental setup', 'trained', 'models']]",[],[],[],[],[],[],negation_scope_resolution,0,198
results,"For cue detection , on the Sherlock dataset test data , we see that we outperform the best system [ FBK Chowdhury ] by 0.6 F1 measure .","[('For', (0, 1)), ('on', (4, 5)), ('see that', (12, 14)), ('by', (23, 24))]","[('cue detection', (1, 3)), ('Sherlock dataset test data', (6, 10)), ('outperform', (15, 16)), ('best system', (17, 19)), ('0.6 F1 measure', (24, 27))]","[['cue detection', 'on', 'Sherlock dataset test data'], ['Sherlock dataset test data', 'see that', 'outperform'], ['outperform', 'by', '0.6 F1 measure']]","[['outperform', 'has', 'best system']]","[['Results', 'For', 'cue detection']]",[],[],[],[],[],[],negation_scope_resolution,0,201
results,"On the BioScope Abstracts , we perform reasonably well .","[('On', (0, 1)), ('perform', (6, 7))]","[('BioScope Abstracts', (2, 4)), ('reasonably well', (7, 9))]","[['BioScope Abstracts', 'perform', 'reasonably well']]",[],[],[],[],"[['cue detection', 'On', 'BioScope Abstracts']]",[],[],[],negation_scope_resolution,0,202
results,"On the BioScope Full papers , we are able to achieve 90.43 F1 when training on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .","[('achieve', (10, 11)), ('when', (13, 14)), ('on', (15, 16))]","[('BioScope Full papers', (2, 5)), ('90.43 F1', (11, 13)), ('training', (14, 15)), ('same data', (17, 19))]","[['BioScope Full papers', 'achieve', '90.43 F1'], ['90.43 F1', 'when', 'training'], ['training', 'on', 'same data']]",[],[],[],[],[],[],"[['cue detection', 'On', 'BioScope Full papers']]",[],negation_scope_resolution,0,204
results,"On the SFU Review Corpus , we achieve an F1 of 87.08 .","[('achieve', (7, 8)), ('of', (10, 11))]","[('SFU Review Corpus', (2, 5)), ('F1', (9, 10)), ('87.08', (11, 12))]","[['SFU Review Corpus', 'achieve', 'F1'], ['F1', 'of', '87.08']]",[],[],[],[],[],[],"[['cue detection', 'On', 'SFU Review Corpus']]",[],negation_scope_resolution,0,205
results,For scope resolution :,[],"[('scope resolution', (1, 3))]",[],[],[],"[['Results', 'For', 'scope resolution']]",[],[],[],[],[],negation_scope_resolution,0,208
results,"On the Sherlock dataset , we achieve an F1 of 92.36 , outperforming the previous State of the Art by a significant margin ( almost 3.0 F1 ) .","[('On', (0, 1)), ('achieve', (6, 7)), ('of', (9, 10)), ('outperforming', (12, 13)), ('by', (19, 20))]","[('Sherlock dataset', (2, 4)), ('F1', (8, 9)), ('92.36', (10, 11)), ('previous State of the Art', (14, 19)), ('significant margin ( almost 3.0 F1 )', (21, 28))]","[['Sherlock dataset', 'achieve', 'F1'], ['F1', 'of', '92.36'], ['92.36', 'outperforming', 'previous State of the Art'], ['previous State of the Art', 'by', 'significant margin ( almost 3.0 F1 )']]",[],[],[],[],"[['scope resolution', 'On', 'Sherlock dataset']]",[],[],[],negation_scope_resolution,0,209
results,"On the BioScope Abstracts , we achieve an F1 of 95.68 , outperforming the best architecture by 3.57 F1 .","[('achieve', (6, 7)), ('of', (9, 10)), ('outperforming', (12, 13)), ('by', (16, 17))]","[('BioScope Abstracts', (2, 4)), ('F1', (8, 9)), ('95.68', (10, 11)), ('best architecture', (14, 16)), ('3.57 F1', (17, 19))]","[['BioScope Abstracts', 'achieve', 'F1'], ['F1', 'of', '95.68'], ['95.68', 'outperforming', 'best architecture'], ['best architecture', 'by', '3.57 F1']]",[],[],[],[],[],[],"[['scope resolution', 'On', 'BioScope Abstracts']]",[],negation_scope_resolution,0,210
results,"On the Bioscope Full Papers , we outperform the best architecture by 2.64 F1 when training on the same dataset","[('outperform', (7, 8)), ('by', (11, 12)), ('training on', (15, 17))]","[('Bioscope Full Papers', (2, 5)), ('best architecture', (9, 11)), ('2.64 F1', (12, 14)), ('same dataset', (18, 20))]","[['Bioscope Full Papers', 'outperform', 'best architecture'], ['best architecture', 'training on', 'same dataset'], ['best architecture', 'by', '2.64 F1']]",[],[],[],[],[],[],"[['scope resolution', 'On', 'Bioscope Full Papers']]",[],negation_scope_resolution,0,211
results,"On the SFU Review Corpus , we outperform the best system to date by 1.02 F1 .","[('outperform', (7, 8)), ('by', (13, 14))]","[('SFU Review Corpus', (2, 5)), ('best system', (9, 11)), ('1.02 F1', (14, 16))]","[['SFU Review Corpus', 'outperform', 'best system'], ['best system', 'by', '1.02 F1']]",[],[],[],[],[],[],"[['scope resolution', 'On', 'SFU Review Corpus']]",[],negation_scope_resolution,0,212
results,"For negation cue detection , we observe a significant gap between our model , NegBERT , and the current state - of the - art systems , while we outperform the baseline systems .","[('observe', (6, 7)), ('between', (10, 11)), ('outperform', (29, 30))]","[('negation cue detection', (1, 4)), ('significant gap', (8, 10)), ('our model , NegBERT , and the current state - of the - art systems', (11, 26)), ('baseline systems', (31, 33))]","[['negation cue detection', 'outperform', 'baseline systems'], ['negation cue detection', 'observe', 'significant gap'], ['significant gap', 'between', 'our model , NegBERT , and the current state - of the - art systems']]",[],[],"[['Results', 'For', 'negation cue detection']]",[],[],[],[],[],negation_scope_resolution,0,213
results,"When we trained on BioScope Abstracts and tested on the BioScope Full Papers , we surprisingly observed a stateof - the - art result of 91.24 ( a gain of 3.89 F1 points over training on BioScope Full Papers ) , which is far beyond the achievable results on training and evaluating on the Bio-Medical sub corpora .","[('trained on', (2, 4)), ('tested on', (7, 9)), ('observed', (16, 17)), ('of', (24, 25))]","[('BioScope Abstracts', (4, 6)), ('BioScope Full Papers', (10, 13)), ('stateof - the - art result', (18, 24)), ('91.24', (25, 26))]","[['BioScope Abstracts', 'tested on', 'BioScope Full Papers'], ['BioScope Full Papers', 'observed', 'stateof - the - art result'], ['stateof - the - art result', 'of', '91.24']]",[],[],[],[],"[['negation cue detection', 'trained on', 'BioScope Abstracts']]",[],[],[],negation_scope_resolution,0,217
research-problem,Learning Semantic Sentence Embeddings using Pair- wise Discriminator,[],"[('Learning Semantic Sentence Embeddings', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Learning Semantic Sentence Embeddings']]",[],[],[],[],paraphrase_generation,0,2
research-problem,"In this paper , we propose a method for obtaining sentence - level embeddings .",[],"[('obtaining sentence - level embeddings', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'obtaining sentence - level embeddings']]",[],[],[],[],paraphrase_generation,0,4
model,Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .,"[('consists of', (2, 4)), ('is', (10, 11)), ('using', (13, 14))]","[('sequential encoder - decoder', (5, 9)), ('further trained', (11, 13)), ('pairwise discriminator', (15, 17))]","[['sequential encoder - decoder', 'is', 'further trained'], ['further trained', 'using', 'pairwise discriminator']]",[],"[['Model', 'consists of', 'sequential encoder - decoder']]",[],[],[],[],[],[],paraphrase_generation,0,22
model,The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .,"[('widely used for', (7, 10))]","[('encoder - decoder architecture', (1, 5)), ('machine translation', (10, 12)), ('machine comprehension', (13, 15))]","[['encoder - decoder architecture', 'widely used for', 'machine translation'], ['encoder - decoder architecture', 'widely used for', 'machine comprehension']]",[],[],"[['Model', 'has', 'encoder - decoder architecture']]",[],[],[],[],[],paraphrase_generation,0,23
model,"In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .","[('ensures', (5, 6)), ('incurred for', (13, 15))]","[(""' local ' loss"", (7, 11)), ('each recurrent unit cell', (15, 19))]","[[""' local ' loss"", 'incurred for', 'each recurrent unit cell']]",[],"[['Model', 'ensures', ""' local ' loss""]]",[],[],[],[],[],[],paraphrase_generation,0,24
model,"To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .",[],[],"[['whole sentence', 'is', 'correctly encoded'], ['correctly encoded', 'make further use of', 'pair - wise discriminator'], ['pair - wise discriminator', 'that encodes', 'whole sentence'], ['pair - wise discriminator', 'obtains', 'embedding']]",[],"[['Model', 'To ensure', 'whole sentence']]",[],[],[],[],[],[],paraphrase_generation,0,27
model,We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .,"[('further ensure that', (1, 4)), ('to', (7, 8)), ('from', (17, 18))]","[('close', (6, 7)), ('desired ground - truth embeddings', (9, 14)), ('far', (16, 17)), ('other ( sentences in the corpus ) embeddings', (18, 26))]","[['far', 'from', 'other ( sentences in the corpus ) embeddings'], ['close', 'to', 'desired ground - truth embeddings']]",[],[],[],[],"[['correctly encoded', 'further ensure that', 'far'], ['correctly encoded', 'further ensure that', 'close']]",[],[],[],paraphrase_generation,0,28
model,This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .,"[('provides', (3, 4)), ('ensures', (10, 11)), ('close to', (18, 20))]","[(""' global ' loss"", (5, 9)), ('sentence embedding', (12, 14)), ('other semantically related sentence embeddings', (20, 25))]","[[""' global ' loss"", 'ensures', 'sentence embedding'], ['sentence embedding', 'close to', 'other semantically related sentence embeddings']]",[],"[['Model', 'provides', ""' global ' loss""]]",[],[],[],[],[],[],paraphrase_generation,0,29
baselines,We start with baseline model which we take as a simple encoder and decoder network with only the local loss ( ED - Local ) .,"[('start with', (1, 3)), ('take as', (7, 9)), ('with', (15, 16))]","[('baseline model', (3, 5)), ('simple encoder and decoder network', (10, 15)), ('only the local loss ( ED - Local )', (16, 25))]","[['baseline model', 'take as', 'simple encoder and decoder network'], ['simple encoder and decoder network', 'with', 'only the local loss ( ED - Local )']]",[],"[['Baselines', 'start with', 'baseline model']]",[],[],[],[],[],[],paraphrase_generation,0,148
baselines,Further we have experimented with encoder - decoder and a discriminator network with only global loss ( EDD - Global ) to distinguish the ground truth paraphrase with the predicted one .,[],[],"[['encoder - decoder and a discriminator network', 'with', 'only global loss ( EDD - Global )'], ['only global loss ( EDD - Global )', 'to distinguish', 'ground truth paraphrase'], ['ground truth paraphrase', 'with', 'predicted one']]",[],"[['Baselines', 'experimented with', 'encoder - decoder and a discriminator network']]",[],[],[],[],[],[],paraphrase_generation,0,149
baselines,Another variation of our model is used both the global and local loss ( EDD - LG ) .,"[('used', (6, 7))]","[('variation of our model', (1, 5)), ('both the global and local loss ( EDD - LG )', (7, 18))]","[['variation of our model', 'used', 'both the global and local loss ( EDD - LG )']]",[],[],"[['Baselines', 'has', 'variation of our model']]",[],[],[],[],[],paraphrase_generation,0,150
baselines,"Finally , we make the discriminator share weights with the encoder and train this network with both the losses ( EDD - LG ( shared ) ) .",[],[],"[['discriminator', 'share', 'weights'], ['weights', 'with', 'encoder'], ['discriminator', 'train', 'network'], ['network', 'with', 'both the losses ( EDD - LG ( shared ) )']]",[],"[['Baselines', 'make', 'discriminator']]",[],[],[],[],[],[],paraphrase_generation,0,152
ablation-analysis,"Among the ablations , the proposed EDD - LG ( shared ) method works way better than the other variants in terms of BLEU and METEOR metrics by achieving an improvement of 8 % and 6 % in the scores respectively over the baseline method for 50 K dataset and an improvement of 10 % and 7 % in the scores respectively for 100 K dataset .",[],[],"[['proposed EDD - LG ( shared ) method', 'works', 'way better'], ['way better', 'achieving', 'improvement'], ['improvement', 'of', '10 % and 7 %'], ['10 % and 7 %', 'in', 'scores'], ['10 % and 7 %', 'for', '100 K dataset'], ['improvement', 'of', '8 % and 6 %'], ['8 % and 6 %', 'in', 'scores'], ['8 % and 6 %', 'for', '50 K dataset'], ['way better', 'than', 'other variants'], ['way better', 'in terms of', 'BLEU and METEOR metrics']]",[],[],"[['Ablation analysis', 'has', 'proposed EDD - LG ( shared ) method']]",[],[],[],[],[],paraphrase_generation,0,155
research-problem,A Deep Generative Framework for Paraphrase Generation,[],"[('Paraphrase Generation', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Paraphrase Generation']]",[],[],[],[],paraphrase_generation,1,2
research-problem,"In this paper , we address the problem of generating paraphrases automatically .",[],"[('generating paraphrases automatically', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'generating paraphrases automatically']]",[],[],[],[],paraphrase_generation,1,5
model,"In this paper , we present a deep generative framework for automatically generating paraphrases , given a sentence .","[('present', (5, 6)), ('for automatically generating', (10, 13)), ('given', (15, 16))]","[('deep generative framework', (7, 10)), ('paraphrases', (13, 14)), ('sentence', (17, 18))]","[['deep generative framework', 'for automatically generating', 'paraphrases'], ['paraphrases', 'given', 'sentence']]",[],"[['Model', 'present', 'deep generative framework']]",[],[],[],[],[],[],paraphrase_generation,1,25
model,"Our framework combines the power of sequenceto - sequence models , specifically the long short - term memory ( LSTM ) , and deep generative models , specifically the variational autoencoder ( VAE ) , to develop a novel , end - to - end deep learning architecture for the task of paraphrase generation .",[],[],"[['framework', 'combines', 'power'], ['power', 'of', 'sequenceto - sequence models'], ['sequenceto - sequence models', 'specifically', 'long short - term memory ( LSTM )'], ['power', 'of', 'deep generative models'], ['deep generative models', 'specifically', 'variational autoencoder ( VAE )'], ['power', 'to develop', 'novel , end - to - end deep learning architecture'], ['novel , end - to - end deep learning architecture', 'for', 'task'], ['task', 'of', 'paraphrase generation']]",[],[],"[['Model', 'has', 'framework']]",[],[],[],[],[],paraphrase_generation,1,26
model,"To address this limitation , we present a mechanism to condition our VAE model on the original sentence for which we wish to generate the paraphrases .","[('to condition', (9, 11)), ('on', (14, 15)), ('to generate', (22, 24))]","[('mechanism', (8, 9)), ('our VAE model', (11, 14)), ('original sentence', (16, 18)), ('paraphrases', (25, 26))]","[['mechanism', 'to condition', 'our VAE model'], ['our VAE model', 'to generate', 'paraphrases'], ['our VAE model', 'on', 'original sentence']]",[],[],"[['Model', 'present', 'mechanism']]",[],[],[],[],[],paraphrase_generation,1,30
model,"Unlike these methods where number of classes are finite , and do not require any intermediate representation , our method conditions both the sides ( i.e. encoder and decoder ) of VAE on the intermediate representation of the input question obtained through LSTM .",[],[],"[['our method', 'conditions', 'both the sides'], ['both the sides', 'of', 'VAE'], ['VAE', 'on', 'intermediate representation'], ['intermediate representation', 'of', 'input question'], ['input question', 'obtained through', 'LSTM'], ['both the sides', 'i.e.', 'encoder and decoder']]",[],[],"[['Model', 'has', 'our method']]",[],[],[],[],[],paraphrase_generation,1,32
model,"In contrast , our deep generative model enjoys a simple , modular architecture , and can generate not just a single but multiple , semantically sensible , paraphrases for any given sentence .","[('enjoys', (7, 8)), ('can generate', (15, 17))]","[('deep generative model', (4, 7)), ('simple , modular architecture', (9, 13)), ('single', (20, 21)), ('multiple , semantically sensible , paraphrases', (22, 28))]","[['deep generative model', 'enjoys', 'simple , modular architecture'], ['deep generative model', 'can generate', 'single'], ['deep generative model', 'can generate', 'multiple , semantically sensible , paraphrases']]",[],[],"[['Model', 'has', 'deep generative model']]",[],[],[],[],[],paraphrase_generation,1,35
model,"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .","[('where', (8, 9)), ('of', (13, 14)), ('are', (19, 20)), ('generated based on', (27, 30)), ('sampled from', (32, 34))]","[('proposed method', (6, 8)), ('all variations', (9, 11)), ('relatively better quality', (14, 17)), ('top beam - search result', (21, 26)), ('different z', (30, 32)), ('latent space', (35, 37))]","[['proposed method', 'where', 'all variations'], ['all variations', 'of', 'relatively better quality'], ['relatively better quality', 'generated based on', 'different z'], ['different z', 'sampled from', 'latent space'], ['relatively better quality', 'are', 'top beam - search result']]",[],[],"[['Model', 'has', 'proposed method']]",[],[],[],[],[],paraphrase_generation,1,38
baselines,Residual LSTM is also the current state - of - the - art on the MSCOCO dataset .,"[('is', (2, 3)), ('on', (13, 14))]","[('Residual LSTM', (0, 2)), ('current state - of - the - art', (5, 13)), ('MSCOCO dataset', (15, 17))]","[['Residual LSTM', 'is', 'current state - of - the - art'], ['current state - of - the - art', 'on', 'MSCOCO dataset']]",[],[],"[['Baselines', 'has', 'Residual LSTM']]",[],[],[],[],[],paraphrase_generation,1,122
baselines,"For the Quora dataset , there were no known baseline results , so we compare our model with ( 1 ) standard VAE model i.e. , the unsupervised version , and ( 2 ) a "" supervised "" variant VAE - S of the unsupervised model .","[('compare', (14, 15)), ('i.e.', (24, 25)), ('of', (42, 43))]","[('standard VAE model', (21, 24)), ('unsupervised version', (27, 29)), ('"" supervised "" variant VAE - S', (35, 42)), ('unsupervised model', (44, 46))]","[['standard VAE model', 'i.e.', 'unsupervised version'], ['"" supervised "" variant VAE - S', 'of', 'unsupervised model']]",[],"[['Baselines', 'compare', 'standard VAE model'], ['Baselines', 'compare', '"" supervised "" variant VAE - S']]",[],[],[],[],[],[],paraphrase_generation,1,123
hyperparameters,"The dimension of the embedding vector is set to 300 , the dimension of both encoder and decoder is 600 , and the latent space dimension is 1100 .",[],[],"[['dimension', 'of', 'embedding vector'], ['embedding vector', 'set to', '300'], ['dimension', 'of', 'both encoder and decoder'], ['both encoder and decoder', 'is', '600'], ['latent space dimension', 'is', '1100']]",[],[],"[['Hyperparameters', 'has', 'dimension'], ['Hyperparameters', 'has', 'latent space dimension']]",[],[],[],[],[],paraphrase_generation,1,131
hyperparameters,The number of layers in the encoder is 1 and in decoder,"[('in', (4, 5)), ('is', (7, 8))]","[('number of layers', (1, 4)), ('encoder', (6, 7)), ('1', (8, 9)), ('decoder', (11, 12))]","[['number of layers', 'in', 'decoder'], ['number of layers', 'in', 'encoder'], ['encoder', 'is', '1']]",[],[],"[['Hyperparameters', 'has', 'number of layers']]",[],[],[],[],"[['decoder', 'has', '2']]",paraphrase_generation,1,132
hyperparameters,2 . Models are trained with stochastic gradient descent with learning rate fixed at a value of 5 10 ? 5 with dropout rate of 30 % .,"[('trained with', (4, 6)), ('with', (9, 10)), ('fixed at', (12, 14)), ('of', (24, 25))]","[('2', (0, 1)), ('stochastic gradient descent', (6, 9)), ('learning rate', (10, 12)), ('value of 5 10 ? 5', (15, 21)), ('dropout rate', (22, 24)), ('30 %', (25, 27))]","[['stochastic gradient descent', 'with', 'learning rate'], ['learning rate', 'fixed at', 'value of 5 10 ? 5'], ['stochastic gradient descent', 'with', 'dropout rate'], ['dropout rate', 'of', '30 %']]",[],"[['Hyperparameters', 'trained with', 'stochastic gradient descent']]",[],[],[],[],[],[],paraphrase_generation,1,133
hyperparameters,Batch size is kept at 32 .,"[('kept at', (3, 5))]","[('Batch size', (0, 2)), ('32', (5, 6))]","[['Batch size', 'kept at', '32']]",[],[],"[['Hyperparameters', 'has', 'Batch size']]",[],[],[],[],[],paraphrase_generation,1,134
hyperparameters,"Models are trained for a predefined number of iterations , rather than a fixed number of epochs .","[('trained for', (2, 4)), ('rather than', (10, 12))]","[('predefined number of iterations', (5, 9)), ('fixed number of epochs', (13, 17))]","[['predefined number of iterations', 'rather than', 'fixed number of epochs']]",[],"[['Hyperparameters', 'trained for', 'predefined number of iterations']]",[],[],[],[],[],[],paraphrase_generation,1,135
hyperparameters,Number of units in LSTM are set to be the maximum length of the sequence in the training data .,[],[],"[['Number of units', 'in', 'LSTM'], ['LSTM', 'set to be', 'maximum length'], ['maximum length', 'of', 'sequence'], ['sequence', 'in', 'training data']]",[],[],"[['Hyperparameters', 'has', 'Number of units']]",[],[],[],[],[],paraphrase_generation,1,139
results,"Furthermore , the paraphrases generated by our system are well - formed , semantically sensible , and grammatically correct for the most part .","[('generated by', (4, 6)), ('are', (8, 9))]","[('paraphrases', (3, 4)), ('our system', (6, 8)), ('well - formed', (9, 12)), ('semantically sensible', (13, 15)), ('grammatically correct', (17, 19))]","[['paraphrases', 'are', 'well - formed'], ['paraphrases', 'are', 'semantically sensible'], ['paraphrases', 'are', 'grammatically correct'], ['paraphrases', 'generated by', 'our system']]",[],[],"[['Results', 'has', 'paraphrases']]",[],[],[],[],[],paraphrase_generation,1,172
results,"Those numbers are reported in the Measure column with row best - BLEU / best - METEOR . , we report the results for MSCOCO dataset .","[('for', (23, 24))]","[('MSCOCO dataset', (24, 26))]",[],[],"[['Results', 'for', 'MSCOCO dataset']]",[],[],[],[],[],[],paraphrase_generation,1,187
results,"As we can see , we have a significant improvement w.r.t. the baselines .","[('have', (6, 7)), ('w.r.t.', (10, 11))]","[('significant improvement', (8, 10)), ('baselines', (12, 13))]","[['significant improvement', 'w.r.t.', 'baselines']]",[],[],[],[],"[['MSCOCO dataset', 'have', 'significant improvement']]",[],[],[],paraphrase_generation,1,189
results,"Both variations of our supervised model i.e. , VAE - SVG and VAE - SVG - eq perform better than the state - of - the - art with VAE - SVG performing slightly better than VAE - SVG - eq .",[],[],"[['Both variations', 'of', 'supervised model'], ['supervised model', 'perform', 'better'], ['better', 'with', 'VAE - SVG'], ['VAE - SVG', 'performing', 'slightly better'], ['slightly better', 'than', 'VAE - SVG - eq'], ['better', 'than', 'state - of - the - art'], ['supervised model', 'i.e.', 'VAE - SVG'], ['supervised model', 'i.e.', 'VAE - SVG - eq']]",[],[],[],[],[],[],"[['MSCOCO dataset', 'has', 'Both variations']]",[],paraphrase_generation,1,190
results,"When comparing our results with the state - of - the - art baseline , the average metric of the VAE - SVG model is able to give a 10 % absolute point performance improvement for the TER metric , a significant number with respect to the difference between the best and second best baseline which only stands at 2 % absolute point .","[('of', (8, 9)), ('able to give', (25, 28)), ('for', (35, 36))]","[('average metric', (16, 18)), ('VAE - SVG model', (20, 24)), ('10 % absolute point performance improvement', (29, 35)), ('TER metric', (37, 39))]","[['average metric', 'of', 'VAE - SVG model'], ['VAE - SVG model', 'able to give', '10 % absolute point performance improvement'], ['10 % absolute point performance improvement', 'for', 'TER metric']]",[],[],"[['Results', 'has', 'average metric']]",[],[],[],[],[],paraphrase_generation,1,196
results,"For the BLEU and METEOR , our best results are 4.7 % and 4 % absolute point improvement over the state - of - the - art .","[('For', (0, 1)), ('are', (9, 10)), ('over', (18, 19))]","[('BLEU and METEOR', (2, 5)), ('best results', (7, 9)), ('4.7 % and 4 % absolute point improvement', (10, 18)), ('state - of - the - art', (20, 27))]","[['best results', 'are', '4.7 % and 4 % absolute point improvement'], ['4.7 % and 4 % absolute point improvement', 'over', 'state - of - the - art']]","[['BLEU and METEOR', 'has', 'best results']]",[],[],[],"[['MSCOCO dataset', 'For', 'BLEU and METEOR']]",[],[],[],paraphrase_generation,1,197
results,"In , we report results for the Quora dataset .",[],"[('Quora dataset', (7, 9))]",[],[],[],"[['Results', 'for', 'Quora dataset']]",[],[],[],[],"[['Quora dataset', 'has', 'both variations']]",paraphrase_generation,1,198
results,"As we can see , both variations of our model perform significantly better than unsupervised VAE and VAE - S , which is not surprising .","[('of', (7, 8)), ('perform', (10, 11)), ('than', (13, 14))]","[('both variations', (5, 7)), ('model', (9, 10)), ('significantly better', (11, 13)), ('unsupervised VAE', (14, 16)), ('VAE - S', (17, 20))]","[['both variations', 'of', 'model'], ['both variations', 'perform', 'significantly better'], ['significantly better', 'than', 'unsupervised VAE'], ['significantly better', 'than', 'VAE - S']]",[],[],[],[],[],[],[],[],paraphrase_generation,1,199
results,"We also report the results on different training sizes , and as expected , as we increase the training data size , results improve .","[('increase', (16, 17))]","[('results', (4, 5)), ('training data size', (18, 21)), ('improve', (23, 24))]",[],"[['training data size', 'has', 'results'], ['results', 'has', 'improve']]",[],[],[],"[['Quora dataset', 'increase', 'training data size']]",[],[],[],paraphrase_generation,1,200
results,"Comparing the results across different variants of supervised model , VAE - SVG - eq performs the best .","[('Comparing', (0, 1)), ('across', (3, 4)), ('of', (6, 7)), ('performs', (15, 16))]","[('results', (2, 3)), ('different variants', (4, 6)), ('supervised model', (7, 9)), ('VAE - SVG - eq', (10, 15)), ('best', (17, 18))]","[['results', 'across', 'different variants'], ['different variants', 'of', 'supervised model'], ['VAE - SVG - eq', 'performs', 'best']]","[['different variants', 'has', 'VAE - SVG - eq']]",[],[],[],"[['Quora dataset', 'Comparing', 'results']]",[],[],[],paraphrase_generation,1,201
results,"We also experimented with generating paraphrases through beam - search , and , unlike MSCOCO , it turns out that beam search improves the results significantly .","[('experimented with', (2, 4)), ('through', (6, 7)), ('turns out that', (17, 20)), ('improves', (22, 23))]","[('generating paraphrases', (4, 6)), ('beam - search', (7, 10)), ('beam search', (20, 22)), ('results', (24, 25)), ('significantly', (25, 26))]","[['generating paraphrases', 'through', 'beam - search'], ['generating paraphrases', 'turns out that', 'beam search'], ['beam search', 'improves', 'results']]","[['results', 'has', 'significantly']]",[],[],[],"[['Quora dataset', 'experimented with', 'generating paraphrases']]",[],[],[],paraphrase_generation,1,203
results,"When comparing the best variant of our model with unsupervised model ( VAE ) , we are able to get more than 27 % absolute point ( more than 3 times ) boost in BLEU score , and more than 19 % absolute point ( more than 2 times ) boost in METEOR ; and when comparing with VAE - S , we are able to get a boost of almost 19 % absolute points in BLEU ( 2 times ) and more than 10 % absolute points in METEOR ( 1.5 times ) .",[],[],"[['best variant of our model', 'with', 'unsupervised model ( VAE )'], ['best variant of our model', 'able to get', 'more than 19 % absolute point ( more than 2 times ) boost'], ['more than 19 % absolute point ( more than 2 times ) boost', 'in', 'METEOR'], ['best variant of our model', 'able to get', 'more than 27 % absolute point ( more than 3 times ) boost'], ['more than 27 % absolute point ( more than 3 times ) boost', 'in', 'BLEU score'], ['VAE - S', 'able to get', 'more than 10 % absolute points'], ['more than 10 % absolute points', 'in', 'METEOR ( 1.5 times )'], ['VAE - S', 'able to get', 'boost of almost 19 % absolute points'], ['boost of almost 19 % absolute points', 'in', 'BLEU ( 2 times )']]",[],[],[],[],"[['Quora dataset', 'comparing', 'best variant of our model'], ['Quora dataset', 'comparing', 'VAE - S']]",[],[],[],paraphrase_generation,1,205
research-problem,Robust Multilingual Part - of - Speech Tagging via Adversarial Training,[],"[('Part - of - Speech Tagging', (2, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Part - of - Speech Tagging']]",[],[],[],[],part-of-speech_tagging,0,2
research-problem,"In this paper , we propose and analyze a neural POS tagging model that exploits AT .",[],"[('neural POS tagging', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural POS tagging']]",[],[],[],[],part-of-speech_tagging,0,6
model,"In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .","[('propose and carefully analyze', (15, 19)), ('that exploits', (31, 33))]","[('neural part - of - speech ( POS ) tagging model', (20, 31)), ('adversarial training', (33, 35))]","[['neural part - of - speech ( POS ) tagging model', 'that exploits', 'adversarial training']]",[],"[['Model', 'propose and carefully analyze', 'neural part - of - speech ( POS ) tagging model']]",[],[],[],[],[],[],part-of-speech_tagging,0,32
model,"With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .","[('With', (0, 1)), ('as', (6, 7)), ('apply', (13, 14)), ('considering', (17, 18)), ('to', (19, 20))]","[('BiLSTM - CRF model', (2, 6)), ('baseline POS tagger', (8, 11)), ('adversarial training', (14, 16)), ('perturbations', (18, 19)), ('input word / character embeddings', (20, 25))]","[['BiLSTM - CRF model', 'as', 'baseline POS tagger'], ['BiLSTM - CRF model', 'apply', 'adversarial training'], ['adversarial training', 'considering', 'perturbations'], ['perturbations', 'to', 'input word / character embeddings']]",[],"[['Model', 'With', 'BiLSTM - CRF model']]",[],[],[],[],[],[],part-of-speech_tagging,0,33
hyperparameters,"We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .","[('train', (1, 2)), ('by', (10, 11)), ('with', (19, 20))]","[('model parameters and word / character embeddings', (3, 10)), ('mini-batch stochastic gradient descent ( SGD )', (12, 19)), ('batch size', (20, 22)), ('10', (22, 23)), ('momentum', (24, 25)), ('0.9', (25, 26)), ('initial learning rate', (27, 30)), ('0.01', (30, 31)), ('decay rate', (32, 34)), ('0.05', (34, 35))]","[['model parameters and word / character embeddings', 'by', 'mini-batch stochastic gradient descent ( SGD )'], ['mini-batch stochastic gradient descent ( SGD )', 'with', 'initial learning rate'], ['mini-batch stochastic gradient descent ( SGD )', 'with', 'decay rate'], ['mini-batch stochastic gradient descent ( SGD )', 'with', 'batch size'], ['mini-batch stochastic gradient descent ( SGD )', 'with', 'momentum']]","[['initial learning rate', 'has', '0.01'], ['decay rate', 'has', '0.05'], ['batch size', 'has', '10'], ['momentum', 'has', '0.9']]","[['Hyperparameters', 'train', 'model parameters and word / character embeddings']]",[],[],[],[],[],[],part-of-speech_tagging,0,132
hyperparameters,We also use a gradient clipping of 5.0 .,"[('use', (2, 3)), ('of', (6, 7))]","[('gradient clipping', (4, 6)), ('5.0', (7, 8))]","[['gradient clipping', 'of', '5.0']]",[],"[['Hyperparameters', 'use', 'gradient clipping']]",[],[],[],[],[],[],part-of-speech_tagging,0,133
hyperparameters,The models are trained with early stopping ) based on the development performance .,"[('trained with', (3, 5)), ('based on', (8, 10))]","[('early stopping', (5, 7)), ('development performance', (11, 13))]","[['early stopping', 'based on', 'development performance']]",[],"[['Hyperparameters', 'trained with', 'early stopping']]",[],[],[],[],[],[],part-of-speech_tagging,0,134
results,PTB - WSJ dataset .,[],"[('PTB - WSJ dataset', (0, 4))]",[],[],[],"[['Results', 'has', 'PTB - WSJ dataset']]",[],[],[],[],"[['PTB - WSJ dataset', 'has', 'baseline ( BiLSTM - CRF ) model']]",part-of-speech_tagging,0,139
results,"As expected , our baseline ( BiLSTM - CRF ) model ( accuracy 97.54 % ) performs on par with other state - of - the - art systems .","[('performs', (16, 17)), ('with', (19, 20))]","[('baseline ( BiLSTM - CRF ) model', (4, 11)), ('accuracy', (12, 13)), ('97.54 %', (13, 15)), ('on par', (17, 19)), ('other state - of - the - art systems', (20, 29))]","[['baseline ( BiLSTM - CRF ) model', 'performs', 'on par'], ['on par', 'with', 'other state - of - the - art systems']]","[['baseline ( BiLSTM - CRF ) model', 'has', 'accuracy'], ['accuracy', 'has', '97.54 %']]",[],[],[],[],[],[],[],part-of-speech_tagging,0,141
results,"Built upon this baseline , our adversarial training ( AT ) model reaches accuracy 97.58 % thanks to its regularization power , outperforming recent POS taggers except .","[('Built upon', (0, 2)), ('reaches', (12, 13)), ('outperforming', (22, 23))]","[('adversarial training ( AT ) model', (6, 12)), ('accuracy', (13, 14)), ('97.58 %', (14, 16)), ('recent POS taggers', (23, 26))]","[['adversarial training ( AT ) model', 'reaches', 'accuracy'], ['adversarial training ( AT ) model', 'outperforming', 'recent POS taggers']]","[['accuracy', 'has', '97.58 %']]",[],[],[],"[['baseline ( BiLSTM - CRF ) model', 'Built upon', 'adversarial training ( AT ) model']]",[],[],[],part-of-speech_tagging,0,142
research-problem,Learning Better Internal Structure of Words for Sequence Labeling,[],"[('Sequence Labeling', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sequence Labeling']]",[],[],[],[],part-of-speech_tagging,1,2
model,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their characters .","[('propose', (3, 4)), ('for learning', (14, 16)), ('by composing', (21, 23))]","[('IntNet', (4, 5)), ('funnel - shaped wide convolutional neural network', (7, 14)), ('internal structure of words', (17, 21)), ('characters', (24, 25))]","[['funnel - shaped wide convolutional neural network', 'for learning', 'internal structure of words'], ['internal structure of words', 'by composing', 'characters']]","[['IntNet', 'has', 'funnel - shaped wide convolutional neural network']]","[['Model', 'propose', 'IntNet']]",[],[],[],[],[],[],part-of-speech_tagging,1,43
model,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning character - to - word representations from limited supervised training corpora .","[('explores', (14, 15)), ('with no', (19, 21)), ('for learning', (24, 26)), ('from', (32, 33))]","[('funnel - shaped Int - Net', (8, 14)), ('deeper and wider architecture', (15, 19)), ('down - sampling', (21, 24)), ('character - to - word representations', (26, 32)), ('limited supervised training corpora', (33, 37))]","[['funnel - shaped Int - Net', 'explores', 'deeper and wider architecture'], ['deeper and wider architecture', 'with no', 'down - sampling'], ['down - sampling', 'for learning', 'character - to - word representations'], ['character - to - word representations', 'from', 'limited supervised training corpora']]",[],[],"[['Model', 'has', 'funnel - shaped Int - Net']]",[],[],[],[],[],part-of-speech_tagging,1,44
model,"Lastly , we combine our IntNet model with LSTM - CRF , which captures both word shape and context information , and jointly decode tags for sequence labeling .","[('combine', (3, 4)), ('with', (7, 8)), ('captures', (13, 14)), ('jointly decode', (22, 24)), ('for', (25, 26))]","[('IntNet model', (5, 7)), ('LSTM - CRF', (8, 11)), ('word shape', (15, 17)), ('context information', (18, 20)), ('tags', (24, 25)), ('sequence labeling', (26, 28))]","[['IntNet model', 'with', 'LSTM - CRF'], ['LSTM - CRF', 'jointly decode', 'tags'], ['tags', 'for', 'sequence labeling'], ['LSTM - CRF', 'captures', 'word shape'], ['LSTM - CRF', 'captures', 'context information']]",[],"[['Model', 'combine', 'IntNet model']]",[],[],[],[],[],[],part-of-speech_tagging,1,45
hyperparameters,The size of the dimensions of character embeddings is 32 which are randomly initialized using a uniform distribution .,[],[],"[['size', 'of', 'dimensions'], ['dimensions', 'of', 'character embeddings'], ['character embeddings', 'are', 'randomly initialized'], ['randomly initialized', 'using', 'uniform distribution'], ['character embeddings', 'is', '32']]",[],[],"[['Hyperparameters', 'has', 'size']]",[],[],[],[],[],part-of-speech_tagging,1,177
hyperparameters,We adopt the same initialization method for randomly initialized word embeddings that are updated during training .,"[('adopt', (1, 2)), ('for', (6, 7)), ('updated during', (13, 15))]","[('same initialization method', (3, 6)), ('randomly initialized word embeddings', (7, 11)), ('training', (15, 16))]","[['same initialization method', 'for', 'randomly initialized word embeddings'], ['randomly initialized word embeddings', 'updated during', 'training']]",[],"[['Hyperparameters', 'adopt', 'same initialization method']]",[],[],[],[],[],[],part-of-speech_tagging,1,178
hyperparameters,"For IntNet , the filter size of the initial convolution is 32 and that of other convolutions is 16 .",[],[],"[['filter size', 'of', 'initial convolution'], ['initial convolution', 'is', '32'], ['filter size', 'of', 'other convolutions'], ['other convolutions', 'is', '16']]","[['IntNet', 'has', 'filter size']]","[['Hyperparameters', 'For', 'IntNet']]",[],[],[],[],[],[],part-of-speech_tagging,1,179
hyperparameters,"The number of convolutional layers are 5 and 9 for IntNet - 5 and IntNet - 9 , respectively , and we have adopted the same weight initialization as that of ResNet .","[('are', (5, 6)), ('for', (9, 10)), ('adopted', (23, 24)), ('as', (28, 29))]","[('number of convolutional layers', (1, 5)), ('5 and 9', (6, 9)), ('IntNet - 5 and IntNet - 9', (10, 17)), ('same weight initialization', (25, 28)), ('ResNet', (31, 32))]","[['number of convolutional layers', 'are', '5 and 9'], ['5 and 9', 'for', 'IntNet - 5 and IntNet - 9'], ['same weight initialization', 'as', 'ResNet']]",[],"[['Hyperparameters', 'adopted', 'same weight initialization']]","[['Hyperparameters', 'has', 'number of convolutional layers']]",[],[],[],[],[],part-of-speech_tagging,1,181
hyperparameters,"We use pre-trained word embeddings for initialization , GloVe 100 - dimension word embeddings for English , and fastText 300 dimension word embeddings for Spanish , Dutch , and German .",[],[],"[['fastText 300 dimension word embeddings', 'for', 'Spanish , Dutch , and German'], ['GloVe 100 - dimension word embeddings', 'for', 'English'], ['pre-trained word embeddings', 'for', 'initialization']]",[],"[['Hyperparameters', 'use', 'fastText 300 dimension word embeddings'], ['Hyperparameters', 'use', 'GloVe 100 - dimension word embeddings'], ['Hyperparameters', 'use', 'pre-trained word embeddings']]",[],[],[],[],[],[],part-of-speech_tagging,1,182
hyperparameters,The state size of the bi-directional LSTMs is set to 256 .,"[('of', (3, 4)), ('set to', (8, 10))]","[('state size', (1, 3)), ('bi-directional LSTMs', (5, 7)), ('256', (10, 11))]","[['state size', 'of', 'bi-directional LSTMs'], ['bi-directional LSTMs', 'set to', '256']]",[],[],"[['Hyperparameters', 'has', 'state size']]",[],[],[],[],[],part-of-speech_tagging,1,183
hyperparameters,We adopt standard BIOES tagging scheme for NER and Chunking .,"[('for', (6, 7))]","[('standard BIOES tagging scheme', (2, 6)), ('NER and Chunking', (7, 10))]","[['standard BIOES tagging scheme', 'for', 'NER and Chunking']]",[],[],"[['Hyperparameters', 'adopt', 'standard BIOES tagging scheme']]",[],[],[],[],[],part-of-speech_tagging,1,184
hyperparameters,We employ mini-batch stochastic gradient descent with momentum .,"[('employ', (1, 2)), ('with', (6, 7))]","[('mini-batch stochastic gradient descent', (2, 6)), ('momentum', (7, 8))]","[['mini-batch stochastic gradient descent', 'with', 'momentum']]",[],"[['Hyperparameters', 'employ', 'mini-batch stochastic gradient descent']]",[],[],[],[],[],[],part-of-speech_tagging,1,186
hyperparameters,"0.05 is the decay ratio , the value of gradient clipping is 5 .",[],[],"[['gradient clipping', 'is', '5'], ['decay ratio', 'is', '0.05']]",[],[],"[['Hyperparameters', 'has', 'gradient clipping'], ['Hyperparameters', 'has', 'decay ratio']]",[],[],[],[],[],part-of-speech_tagging,1,190
hyperparameters,"Dropout is applied on the input of IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .","[('applied on', (2, 4)), ('of', (6, 7)), ('ratio', (16, 17))]","[('Dropout', (0, 1)), ('input', (5, 6)), ('IntNet , LSTMs , and CRF', (7, 13)), ('0.5', (17, 18))]","[['Dropout', 'applied on', 'input'], ['input', 'of', 'IntNet , LSTMs , and CRF'], ['Dropout', 'ratio', '0.5']]",[],[],"[['Hyperparameters', 'has', 'Dropout']]",[],[],[],[],[],part-of-speech_tagging,1,191
baselines,"Firstly , we use LSTM - CRF with randomly initialized word embeddings as our initial baseline .","[('use', (3, 4)), ('with', (7, 8))]","[('LSTM - CRF', (4, 7)), ('randomly initialized word embeddings', (8, 12))]","[['LSTM - CRF', 'with', 'randomly initialized word embeddings']]",[],"[['Baselines', 'use', 'LSTM - CRF']]",[],[],[],[],[],[],part-of-speech_tagging,1,194
baselines,"We adopt two state - of - the - art methods in sequence labeling , denoted as char - LSTM and char - CNN .","[('adopt', (1, 2)), ('in', (11, 12)), ('denoted as', (15, 17))]","[('two state - of - the - art methods', (2, 11)), ('sequence labeling', (12, 14)), ('char - LSTM', (17, 20)), ('char - CNN', (21, 24))]","[['two state - of - the - art methods', 'in', 'sequence labeling'], ['two state - of - the - art methods', 'denoted as', 'char - LSTM'], ['two state - of - the - art methods', 'denoted as', 'char - CNN']]",[],"[['Baselines', 'adopt', 'two state - of - the - art methods']]",[],[],[],[],[],[],part-of-speech_tagging,1,195
baselines,"We add more layers to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .","[('add', (1, 2)), ('to', (4, 5)), ('refer to', (11, 13)), ('for', (28, 29))]","[('more layers', (2, 4)), ('char - CNN model', (6, 10)), ('char - CNN - 5 and char - CNN - 9', (15, 26)), ('5 and 9 convolutional layers', (29, 34))]","[['more layers', 'to', 'char - CNN model'], ['char - CNN model', 'refer to', 'char - CNN - 5 and char - CNN - 9'], ['char - CNN - 5 and char - CNN - 9', 'for', '5 and 9 convolutional layers']]",[],"[['Baselines', 'add', 'more layers']]",[],[],[],[],[],[],part-of-speech_tagging,1,196
baselines,"Furthermore , we add residual connections to the char - CNN - 9 and refer it as char - ResNet .","[('to', (6, 7)), ('refer it as', (14, 17))]","[('residual connections', (4, 6)), ('char - CNN - 9', (8, 13)), ('char - ResNet', (17, 20))]","[['residual connections', 'to', 'char - CNN - 9'], ['char - CNN - 9', 'refer it as', 'char - ResNet']]",[],[],"[['Baselines', 'add', 'residual connections']]",[],[],[],[],[],part-of-speech_tagging,1,197
baselines,"Also , we apply 3 dense blocks based on char - ResNet which we refer to as char - DenseNet , to compare the difference between residual connection and dense connection .","[('apply', (3, 4)), ('based on', (7, 9)), ('refer to as', (14, 17))]","[('3 dense blocks', (4, 7)), ('char - ResNet', (9, 12)), ('char - DenseNet', (17, 20))]","[['3 dense blocks', 'based on', 'char - ResNet'], ['char - ResNet', 'refer to as', 'char - DenseNet']]",[],"[['Baselines', 'apply', '3 dense blocks']]",[],[],[],[],[],[],part-of-speech_tagging,1,198
results,5 Results and Analysis 5.1 Character - to - word Models presents the performance of different character - to - word models on six benchmark datasets .,[],"[('Character - to - word Models', (5, 11))]",[],[],[],"[['Results', 'has', 'Character - to - word Models']]",[],[],[],[],"[['Character - to - word Models', 'has', 'F1 score']]",part-of-speech_tagging,1,200
results,"The result shows that for most of the datasets , the F1 score does not improve much when we directly add more layers .","[('does not', (13, 15)), ('when', (17, 18))]","[('F1 score', (11, 13)), ('improve much', (15, 17)), ('directly add', (19, 21)), ('more layers', (21, 23))]","[['F1 score', 'does not', 'improve much'], ['improve much', 'when', 'directly add']]","[['directly add', 'has', 'more layers']]",[],[],[],[],[],[],[],part-of-speech_tagging,1,204
results,We also observe some accuracy drop when we continuously increase the depth .,"[('observe', (2, 3)), ('when', (6, 7))]","[('some accuracy drop', (3, 6)), ('continuously increase', (8, 10)), ('depth', (11, 12))]","[['some accuracy drop', 'when', 'continuously increase']]","[['continuously increase', 'has', 'depth']]",[],[],[],"[['Character - to - word Models', 'observe', 'some accuracy drop']]",[],[],[],part-of-speech_tagging,1,205
results,"Furthermore , we add residual connections to char - CNN - 9 as char - ResNet - 9 , which confirms that residual connections can help train deep layers .",[],[],"[['residual connections', 'to', 'char - CNN - 9'], ['char - CNN - 9', 'as', 'char - ResNet - 9'], ['char - CNN - 9', 'confirms that', 'residual connections'], ['residual connections', 'help train', 'deep layers']]",[],[],[],[],"[['Character - to - word Models', 'add', 'residual connections']]",[],[],[],part-of-speech_tagging,1,207
results,"We further improve char - ResNet - 9 by changing residual connections into dense connection blocks as char - DenseNet - 9 , which shows that the dense connections are better than residual connections for learning word shape information .",[],[],"[['char - ResNet - 9', 'by changing', 'residual connections'], ['residual connections', 'into', 'dense connection blocks'], ['dense connection blocks', 'as', 'char - DenseNet - 9'], ['dense connection blocks', 'shows that', 'dense connections'], ['dense connections', 'are', 'better'], ['better', 'than', 'residual connections'], ['residual connections', 'for learning', 'word shape information']]",[],[],[],[],"[['Character - to - word Models', 'improve', 'char - ResNet - 9']]",[],[],[],part-of-speech_tagging,1,208
results,"Our proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9 generally improves the results across all datasets .","[('improves', (21, 22)), ('across', (24, 25))]","[('proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9', (1, 20)), ('results', (23, 24)), ('all datasets', (25, 27))]","[['proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9', 'improves', 'results'], ['results', 'across', 'all datasets']]",[],[],[],[],[],[],"[['Character - to - word Models', 'has', 'proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9']]",[],part-of-speech_tagging,1,209
results,"Our IntNet significantly outperforms other character embedding models , for example , the improvement is more than 2 % in terms of F 1 score for German and Dutch .","[('significantly outperforms', (2, 4))]","[('IntNet', (1, 2)), ('other character embedding models', (4, 8))]","[['IntNet', 'significantly outperforms', 'other character embedding models']]",[],[],[],[],[],[],"[['Character - to - word Models', 'has', 'IntNet']]",[],part-of-speech_tagging,1,210
results,"Also , we observe that char - IntNet - 5 is more effective for learning character - to - word representations than char - IntNet - 9 in most of the cases .","[('is', (10, 11)), ('for learning', (13, 15)), ('than', (21, 22)), ('in', (27, 28))]","[('char - IntNet - 5', (5, 10)), ('more effective', (11, 13)), ('character - to - word representations', (15, 21)), ('char - IntNet - 9', (22, 27)), ('most of the cases', (28, 32))]","[['char - IntNet - 5', 'is', 'more effective'], ['more effective', 'for learning', 'character - to - word representations'], ['character - to - word representations', 'in', 'most of the cases'], ['character - to - word representations', 'than', 'char - IntNet - 9']]",[],[],[],[],[],[],"[['Character - to - word Models', 'observe', 'char - IntNet - 5']]",[],part-of-speech_tagging,1,211
results,Table 2 presents our proposed model in comparison with state - of - the - art results .,"[('in comparison with', (6, 9))]","[('state - of - the - art results', (9, 17))]",[],[],"[['Results', 'in comparison with', 'state - of - the - art results']]",[],[],[],[],[],[],part-of-speech_tagging,1,214
results,These experiments show that our char - IntNet generally improves results across different models and datasets .,"[('show that', (2, 4)), ('improves', (9, 10)), ('across', (11, 12))]","[('our char - IntNet', (4, 8)), ('results', (10, 11)), ('different models and datasets', (12, 16))]","[['our char - IntNet', 'improves', 'results'], ['results', 'across', 'different models and datasets']]",[],[],[],[],"[['state - of - the - art results', 'show that', 'our char - IntNet']]",[],[],[],part-of-speech_tagging,1,220
results,"The improvement is more pronounced for non-English datasets , for example , IntNet improves the F - 1 score over the stateof - the - art results by more than 2 % for Dutch and Spanish .","[('for', (5, 6)), ('improves', (13, 14)), ('over', (19, 20)), ('by more than', (27, 30))]","[('IntNet', (12, 13)), ('F - 1 score', (15, 19)), ('stateof - the - art results', (21, 27)), ('2 %', (30, 32)), ('Dutch and Spanish', (33, 36))]","[['IntNet', 'improves', 'F - 1 score'], ['F - 1 score', 'over', 'stateof - the - art results'], ['F - 1 score', 'by more than', '2 %'], ['2 %', 'for', 'Dutch and Spanish']]",[],[],"[['Results', 'has', 'IntNet']]",[],[],[],[],[],part-of-speech_tagging,1,221
results,"It also shows that the results of LSTM - CRF are significantly improved after adding character - to - word models , which confirms that word shape information is very important for sequence labeling .","[('shows that', (2, 4)), ('of', (6, 7)), ('are', (10, 11)), ('after adding', (13, 15))]","[('results', (5, 6)), ('LSTM - CRF', (7, 10)), ('significantly improved', (11, 13)), ('character - to - word models', (15, 21))]","[['results', 'of', 'LSTM - CRF'], ['LSTM - CRF', 'are', 'significantly improved'], ['significantly improved', 'after adding', 'character - to - word models']]",[],"[['Results', 'shows that', 'results']]",[],[],[],[],[],[],part-of-speech_tagging,1,222
research-problem,TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS,[],"[('SEQUENCE TAGGING', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'SEQUENCE TAGGING']]",[],[],[],[],part-of-speech_tagging,2,2
code,1 Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],[],"[('https://github.com/kimiyoung/transfer', (5, 6))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/kimiyoung/transfer']]",[],[],[],[],part-of-speech_tagging,2,11
approach,"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .","[('present', (1, 2)), ('based on', (6, 8)), ('shares', (16, 17)), ('between', (27, 28))]","[('transfer learning approach', (3, 6)), ('deep hierarchical recurrent neural network', (9, 14)), ('hidden feature repre-sentation', (18, 21)), ('part of the model parameters', (22, 27)), ('source task and the target task', (29, 35))]","[['transfer learning approach', 'based on', 'deep hierarchical recurrent neural network'], ['transfer learning approach', 'between', 'source task and the target task'], ['source task and the target task', 'shares', 'hidden feature repre-sentation'], ['source task and the target task', 'shares', 'part of the model parameters']]",[],"[['Approach', 'present', 'transfer learning approach']]",[],[],[],[],[],[],part-of-speech_tagging,2,25
approach,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,"[('uses', (10, 11)), ('for', (15, 16))]","[('gradient - based methods', (11, 15)), ('efficient training', (16, 18))]","[['gradient - based methods', 'for', 'efficient training']]",[],"[['Approach', 'uses', 'gradient - based methods']]",[],[],[],[],[],[],part-of-speech_tagging,2,26
experiments,TRANSFER LEARNING PERFORMANCE,[],"[('TRANSFER LEARNING PERFORMANCE', (0, 3))]",[],[],[],"[['Experiments', 'has', 'TRANSFER LEARNING PERFORMANCE']]",[],[],[],[],"[['TRANSFER LEARNING PERFORMANCE', 'has', 'Hyperparameters']]",part-of-speech_tagging,2,132
experiments,"We fix the hyperparameters for all the results reported in this section : we set the character embedding dimension at 25 , the word embedding dimension at 50 for English and 64 for Spanish , the dimension of hidden states of the character - level GRUs at 80 , the dimension of hidden states of the word - level GRUs at 300 , and the initial learning rate at 0.01 .",[],[],"[['word embedding dimension', 'at', '50'], ['50', 'for', 'English'], ['word embedding dimension', 'at', '64'], ['64', 'for', 'Spanish'], ['initial learning rate', 'at', '0.01'], ['character embedding dimension', 'at', '25'], ['dimension', 'of', 'hidden states'], ['hidden states', 'of', 'character - level GRUs'], ['character - level GRUs', 'at', '80'], ['hidden states', 'of', 'word - level GRUs'], ['word - level GRUs', 'at', '300']]",[],[],[],[],"[['Hyperparameters', 'set', 'word embedding dimension'], ['Hyperparameters', 'set', 'initial learning rate'], ['Hyperparameters', 'set', 'character embedding dimension'], ['Hyperparameters', 'set', 'dimension']]",[],[],[],part-of-speech_tagging,2,134
experiments,We can see that our transfer learning approach consistently improved over the non-transfer results .,"[('see that', (2, 4)), ('over', (10, 11))]","[('transfer learning approach', (5, 8)), ('consistently improved', (8, 10)), ('non-transfer results', (12, 14))]","[['consistently improved', 'over', 'non-transfer results']]","[['transfer learning approach', 'has', 'consistently improved']]",[],[],[],"[['Results', 'see that', 'transfer learning approach']]",[],[],[],part-of-speech_tagging,2,142
experiments,We also observe that the improvement by transfer learning is more substantial when the labeling rate is lower .,[],[],"[['improvement', 'by', 'transfer learning'], ['transfer learning', 'is', 'more substantial'], ['more substantial', 'when', 'labeling rate'], ['labeling rate', 'is', 'lower']]",[],[],[],[],"[['Results', 'observe that', 'improvement']]",[],[],[],part-of-speech_tagging,2,143
experiments,"As shown in and 2 ( e ) , our transfer learning approach can improve the performance on Twitter POS tagging and NER for all labeling rates , and the improvements with 0.1 labels are more than 8 % for both datasets .",[],[],"[['improve the performance', 'on', 'Twitter POS tagging and NER'], ['Twitter POS tagging and NER', 'for', 'all labeling rates'], ['improvements', 'with', '0.1 labels'], ['0.1 labels', 'are', 'more than 8 %'], ['more than 8 %', 'for', 'both datasets']]","[['our transfer learning approach', 'has', 'improve the performance'], ['our transfer learning approach', 'has', 'improvements']]",[],[],[],[],[],"[['Results', 'has', 'our transfer learning approach']]",[],part-of-speech_tagging,2,146
experiments,Cross - application transfer also leads to substantial improvement under low - resource conditions .,"[('leads to', (5, 7)), ('under', (9, 10))]","[('Cross - application transfer', (0, 4)), ('substantial improvement', (7, 9)), ('low - resource conditions', (10, 14))]","[['Cross - application transfer', 'leads to', 'substantial improvement'], ['substantial improvement', 'under', 'low - resource conditions']]",[],[],[],[],[],[],"[['Results', 'has', 'Cross - application transfer']]",[],part-of-speech_tagging,2,147
experiments,COMPARISON WITH STATE - OF - THE - ART RESULTS,[],"[('COMPARISON WITH STATE - OF - THE - ART RESULTS', (0, 10))]",[],[],[],"[['Experiments', 'has', 'COMPARISON WITH STATE - OF - THE - ART RESULTS']]",[],[],[],[],"[['COMPARISON WITH STATE - OF - THE - ART RESULTS', 'has', 'Hyperparameters']]",part-of-speech_tagging,2,157
experiments,We use publicly available pretrained word embeddings as initialization .,"[('use', (1, 2)), ('as', (7, 8))]","[('publicly available pretrained word embeddings', (2, 7)), ('initialization', (8, 9))]","[['publicly available pretrained word embeddings', 'as', 'initialization']]",[],[],[],[],"[['Hyperparameters', 'use', 'publicly available pretrained word embeddings']]",[],[],[],part-of-speech_tagging,2,160
experiments,"On the English datasets , following previous works that are based on neural networks , we experiment with both the 50 - dimensional SENNA embeddings and the 100 - dimensional GloVe embeddings and use the development set to choose the embeddings for different tasks and settings .","[('On', (0, 1)), ('experiment with', (16, 18)), ('use', (33, 34)), ('to choose', (37, 39)), ('for', (41, 42))]","[('English datasets', (2, 4)), ('50 - dimensional SENNA embeddings', (20, 25)), ('100 - dimensional GloVe embeddings', (27, 32)), ('development set', (35, 37)), ('embeddings', (40, 41)), ('different tasks and settings', (42, 46))]","[['English datasets', 'experiment with', '50 - dimensional SENNA embeddings'], ['English datasets', 'experiment with', '100 - dimensional GloVe embeddings'], ['English datasets', 'use', 'development set'], ['development set', 'to choose', 'embeddings'], ['embeddings', 'for', 'different tasks and settings']]",[],[],[],[],"[['Hyperparameters', 'On', 'English datasets']]",[],[],[],part-of-speech_tagging,2,161
experiments,"For Spanish and Dutch , we use the 64 - dimensional Polyglot embeddings .","[('For', (0, 1)), ('use', (6, 7))]","[('Spanish and Dutch', (1, 4)), ('64 - dimensional Polyglot embeddings', (8, 13))]","[['Spanish and Dutch', 'use', '64 - dimensional Polyglot embeddings']]",[],[],[],[],"[['Hyperparameters', 'For', 'Spanish and Dutch']]",[],[],[],part-of-speech_tagging,2,162
experiments,We set the hidden state dimensions to be 300 for the word - level GRU .,"[('set', (1, 2)), ('to be', (6, 8)), ('for', (9, 10))]","[('hidden state dimensions', (3, 6)), ('300', (8, 9)), ('word - level GRU', (11, 15))]","[['hidden state dimensions', 'to be', '300'], ['300', 'for', 'word - level GRU']]",[],[],[],[],"[['Hyperparameters', 'set', 'hidden state dimensions']]",[],[],[],part-of-speech_tagging,2,163
experiments,The initial learning rate for AdaGrad is fixed at 0.01 .,"[('for', (4, 5)), ('fixed at', (7, 9))]","[('initial learning rate', (1, 4)), ('AdaGrad', (5, 6)), ('0.01', (9, 10))]","[['initial learning rate', 'for', 'AdaGrad'], ['AdaGrad', 'fixed at', '0.01']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],part-of-speech_tagging,2,164
experiments,"First , our transfer learning approach achieves new state - of - the - art results on all the considered benchmark datasets except PTB POS tagging , which indicates that transfer learning can still improve the performance even on datasets with relatively abundant labels .","[('achieves', (6, 7)), ('on', (16, 17))]","[('transfer learning approach', (3, 6)), ('new state - of - the - art results', (7, 16)), ('all the considered benchmark datasets', (17, 22))]","[['transfer learning approach', 'achieves', 'new state - of - the - art results'], ['new state - of - the - art results', 'on', 'all the considered benchmark datasets']]",[],[],[],[],[],[],"[['Results', 'has', 'transfer learning approach']]",[],part-of-speech_tagging,2,171
experiments,"Second , our base model ( w/o transfer ) performs competitively compared to the state - of - the - art systems , which means that the improvements shown in Section 4.2 are obtained over a strong baseline .","[('performs', (9, 10)), ('compared to', (11, 13))]","[('our base model ( w/o transfer )', (2, 9)), ('competitively', (10, 11)), ('state - of - the - art systems', (14, 22))]","[['our base model ( w/o transfer )', 'performs', 'competitively'], ['competitively', 'compared to', 'state - of - the - art systems']]",[],[],[],[],[],[],"[['Results', 'has', 'our base model ( w/o transfer )']]",[],part-of-speech_tagging,2,172
research-problem,End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,[],"[('End - to - end Sequence Labeling', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'End - to - end Sequence Labeling']]",[],[],[],[],part-of-speech_tagging,3,2
research-problem,State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,[],"[('sequence labeling', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'sequence labeling']]",[],[],[],[],part-of-speech_tagging,3,4
research-problem,"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community .",[],"[('Linguistic sequence labeling', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Linguistic sequence labeling']]",[],[],[],[],part-of-speech_tagging,3,10
model,"In this paper , we propose a neural network architecture for sequence labeling .","[('propose', (5, 6)), ('for', (10, 11))]","[('neural network architecture', (7, 10)), ('sequence labeling', (11, 13))]","[['neural network architecture', 'for', 'sequence labeling']]",[],"[['Model', 'propose', 'neural network architecture']]",[],[],[],[],[],[],part-of-speech_tagging,3,21
model,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .","[('is', (1, 2)), ('requiring no', (8, 10)), ('beyond', (21, 22)), ('on', (25, 26))]","[('endto - end model', (4, 8)), ('task - specific resources', (10, 14)), ('feature engineering', (15, 17)), ('data pre-processing', (19, 21)), ('pre-trained word embeddings', (22, 25)), ('unlabeled corpora', (26, 28))]","[['endto - end model', 'requiring no', 'task - specific resources'], ['endto - end model', 'requiring no', 'feature engineering'], ['endto - end model', 'requiring no', 'data pre-processing'], ['endto - end model', 'beyond', 'pre-trained word embeddings'], ['pre-trained word embeddings', 'on', 'unlabeled corpora']]",[],"[['Model', 'is', 'endto - end model']]",[],[],[],[],[],[],part-of-speech_tagging,3,22
model,"Thus , our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains .","[('easily applied to', (6, 9)), ('of', (12, 13)), ('on', (16, 17))]","[('wide range', (10, 12)), ('sequence labeling tasks', (13, 16)), ('different languages and domains', (17, 21))]","[['wide range', 'of', 'sequence labeling tasks'], ['sequence labeling tasks', 'on', 'different languages and domains']]",[],"[['Model', 'easily applied to', 'wide range']]",[],[],[],[],[],[],part-of-speech_tagging,3,23
model,We first use convolutional neural networks ( CNNs ) to encode character - level information of a word into its character - level representation .,"[('first use', (1, 3)), ('to encode', (9, 11)), ('of', (15, 16)), ('into', (18, 19))]","[('convolutional neural networks ( CNNs )', (3, 9)), ('character - level information', (11, 15)), ('a word', (16, 18)), ('its character - level representation', (19, 24))]","[['convolutional neural networks ( CNNs )', 'to encode', 'character - level information'], ['character - level information', 'of', 'a word'], ['a word', 'into', 'its character - level representation']]",[],"[['Model', 'first use', 'convolutional neural networks ( CNNs )']]",[],[],[],[],[],[],part-of-speech_tagging,3,24
model,Then we combine character - and word - level representations and feed them into bi-directional LSTM ( BLSTM ) to model context information of each word .,"[('combine', (2, 3)), ('feed them into', (11, 14)), ('to model', (19, 21)), ('of', (23, 24))]","[('character - and word - level representations', (3, 10)), ('bi-directional LSTM ( BLSTM )', (14, 19)), ('context information', (21, 23)), ('each word', (24, 26))]","[['character - and word - level representations', 'feed them into', 'bi-directional LSTM ( BLSTM )'], ['bi-directional LSTM ( BLSTM )', 'to model', 'context information'], ['context information', 'of', 'each word']]",[],"[['Model', 'combine', 'character - and word - level representations']]",[],[],[],[],[],[],part-of-speech_tagging,3,25
model,"On top of BLSTM , we use a sequential CRF to jointly decode labels for the whole sentence .","[('On top of', (0, 3)), ('use', (6, 7)), ('to jointly decode', (10, 13)), ('for', (14, 15))]","[('BLSTM', (3, 4)), ('sequential CRF', (8, 10)), ('labels', (13, 14)), ('whole sentence', (16, 18))]","[['BLSTM', 'use', 'sequential CRF'], ['sequential CRF', 'to jointly decode', 'labels'], ['labels', 'for', 'whole sentence']]",[],"[['Model', 'On top of', 'BLSTM']]",[],[],[],[],[],[],part-of-speech_tagging,3,26
hyperparameters,Parameter optimization is performed with minibatch stochastic gradient descent ( SGD ) with batch size 10 and momentum 0.9 .,"[('performed with', (3, 5)), ('with', (12, 13))]","[('Parameter optimization', (0, 2)), ('minibatch stochastic gradient descent ( SGD )', (5, 12)), ('batch size', (13, 15)), ('10', (15, 16)), ('momentum', (17, 18)), ('0.9', (18, 19))]","[['Parameter optimization', 'performed with', 'minibatch stochastic gradient descent ( SGD )'], ['minibatch stochastic gradient descent ( SGD )', 'with', 'batch size'], ['minibatch stochastic gradient descent ( SGD )', 'with', 'momentum']]","[['batch size', 'has', '10'], ['momentum', 'has', '0.9']]",[],"[['Hyperparameters', 'has', 'Parameter optimization']]",[],[],[],[],[],part-of-speech_tagging,3,97
hyperparameters,"We choose an initial learning rate of ? 0 ( ? 0 = 0.01 for POS tagging , and 0.015 for NER , see Section 3.3 . ) , and the learning rate is updated on each epoch of training as ? t = ? 0 / ( 1 + ?t ) , with decay rate ? =",[],[],"[['initial learning rate', 'updated on', 'each epoch'], ['each epoch', 'of', 'training'], ['initial learning rate', 'of', '0.01'], ['0.01', 'for', 'POS tagging'], ['initial learning rate', 'of', '0.015'], ['0.015', 'for', 'NER']]",[],"[['Hyperparameters', 'choose', 'initial learning rate']]",[],[],[],[],[],[],part-of-speech_tagging,3,98
hyperparameters,"To reduce the effects of "" gradient exploding "" , we use a gradient clipping of 5.0 .",[],[],"[['effects', 'of', 'gradient exploding'], ['gradient exploding', 'use', 'gradient clipping'], ['gradient clipping', 'of', '5.0']]",[],"[['Hyperparameters', 'reduce', 'effects']]",[],[],[],[],[],[],part-of-speech_tagging,3,100
hyperparameters,We use early stopping based on performance on validation sets .,"[('use', (1, 2)), ('based on', (4, 6)), ('on', (7, 8))]","[('early stopping', (2, 4)), ('performance', (6, 7)), ('validation sets', (8, 10))]","[['early stopping', 'based on', 'performance'], ['performance', 'on', 'validation sets']]",[],"[['Hyperparameters', 'use', 'early stopping']]",[],[],[],[],[],[],part-of-speech_tagging,3,103
hyperparameters,"The "" best "" parameters appear at around 50 epochs , according to our experiments .","[('appear at', (5, 7))]","[('"" best "" parameters', (1, 5)), ('around 50 epochs', (7, 10))]","[['"" best "" parameters', 'appear at', 'around 50 epochs']]",[],[],"[['Hyperparameters', 'has', '"" best "" parameters']]",[],[],[],[],[],part-of-speech_tagging,3,104
hyperparameters,"For each of the embeddings , we fine - tune initial embeddings , modifying them during gradient updates of the neural network model by back - propagating gradients .","[('For each of', (0, 3)), ('fine - tune', (7, 10)), ('modifying them during', (13, 16)), ('of', (18, 19)), ('by', (23, 24))]","[('embeddings', (4, 5)), ('initial embeddings', (10, 12)), ('gradient updates', (16, 18)), ('neural network model', (20, 23)), ('back - propagating gradients', (24, 28))]","[['embeddings', 'fine - tune', 'initial embeddings'], ['embeddings', 'modifying them during', 'gradient updates'], ['gradient updates', 'of', 'neural network model'], ['neural network model', 'by', 'back - propagating gradients']]",[],"[['Hyperparameters', 'For each of', 'embeddings']]",[],[],[],[],[],[],part-of-speech_tagging,3,106
hyperparameters,"To mitigate overfitting , we apply the dropout method ( Srivastava et al. , 2014 ) to regularize our model .","[('To mitigate', (0, 2)), ('apply', (5, 6)), ('to regularize', (16, 18))]","[('overfitting', (2, 3)), ('dropout method', (7, 9)), ('model', (19, 20))]","[['overfitting', 'apply', 'dropout method'], ['dropout method', 'to regularize', 'model']]",[],"[['Hyperparameters', 'To mitigate', 'overfitting']]",[],[],[],[],[],[],part-of-speech_tagging,3,109
hyperparameters,"As shown in and 3 , we apply dropout on character embeddings before inputting to CNN , and on both the input and output vectors of BLSTM .","[('apply', (7, 8)), ('on', (9, 10)), ('before inputting to', (12, 15)), ('of', (25, 26))]","[('dropout', (8, 9)), ('character embeddings', (10, 12)), ('CNN', (15, 16)), ('input and output vectors', (21, 25)), ('BLSTM', (26, 27))]","[['dropout', 'on', 'character embeddings'], ['character embeddings', 'before inputting to', 'CNN'], ['dropout', 'on', 'input and output vectors'], ['input and output vectors', 'of', 'BLSTM']]",[],"[['Hyperparameters', 'apply', 'dropout']]",[],[],[],[],[],[],part-of-speech_tagging,3,110
hyperparameters,We fix dropout rate at 0.5 for all dropout layers through all the experiments .,"[('fix', (1, 2)), ('at', (4, 5)), ('for', (6, 7))]","[('dropout rate', (2, 4)), ('0.5', (5, 6)), ('all dropout layers', (7, 10))]","[['dropout rate', 'at', '0.5'], ['0.5', 'for', 'all dropout layers']]",[],"[['Hyperparameters', 'fix', 'dropout rate']]",[],[],[],[],[],[],part-of-speech_tagging,3,111
baselines,"We compare the performance with three baseline systems - BRNN , the bi-direction RNN ; BLSTM , the bidirection LSTM , and BLSTM - CNNs , the combination of BLSTM with CNN to model characterlevel information .","[('to model', (32, 34))]","[('three baseline systems', (5, 8)), ('BRNN , the bi-direction RNN', (9, 14)), ('BLSTM , the bidirection LSTM', (15, 20)), ('BLSTM - CNNs , the combination of BLSTM with CNN', (22, 32)), ('characterlevel information', (34, 36))]","[['BLSTM - CNNs , the combination of BLSTM with CNN', 'to model', 'characterlevel information']]","[['three baseline systems', 'name', 'BRNN , the bi-direction RNN'], ['three baseline systems', 'name', 'BLSTM , the bidirection LSTM'], ['three baseline systems', 'name', 'BLSTM - CNNs , the combination of BLSTM with CNN']]",[],"[['Baselines', 'has', 'three baseline systems']]",[],[],[],[],[],part-of-speech_tagging,3,118
results,"Finally , by adding CRF layer for joint decoding we achieve significant improvements over BLSTM - CNN models for both POS tagging and NER on all metrics .",[],[],"[['CRF layer', 'achieve', 'significant improvements'], ['significant improvements', 'over', 'BLSTM - CNN models'], ['significant improvements', 'for', 'POS tagging and NER'], ['CRF layer', 'for', 'joint decoding']]",[],"[['Results', 'adding', 'CRF layer']]",[],[],[],[],[],[],part-of-speech_tagging,3,123
results,"Comparing with traditional statistical models , our system achieves state - of - the - art accuracy , obtaining 0.05 % improvement over the previously best reported results by .","[('Comparing with', (0, 2)), ('achieves', (8, 9)), ('obtaining', (18, 19)), ('over', (22, 23))]","[('traditional statistical models', (2, 5)), ('our system', (6, 8)), ('state - of - the - art accuracy', (9, 17)), ('0.05 % improvement', (19, 22)), ('previously best reported results', (24, 28))]","[['our system', 'achieves', 'state - of - the - art accuracy'], ['our system', 'obtaining', '0.05 % improvement'], ['0.05 % improvement', 'over', 'previously best reported results']]","[['traditional statistical models', 'has', 'our system']]","[['Results', 'Comparing with', 'traditional statistical models']]",[],[],[],[],[],[],part-of-speech_tagging,3,127
results,"Similar to the observations of POS tagging , our model achieves significant improvements over Senna and the other three neural models , namely the LSTM - CRF proposed by , LSTM - CNNs pro- :","[('achieves', (10, 11)), ('over', (13, 14))]","[('our model', (8, 10)), ('significant improvements', (11, 13)), ('Senna', (14, 15)), ('other three neural models', (17, 21))]","[['our model', 'achieves', 'significant improvements'], ['significant improvements', 'over', 'Senna'], ['significant improvements', 'over', 'other three neural models']]",[],[],"[['Results', 'has', 'our model']]",[],[],[],[],[],part-of-speech_tagging,3,133
research-problem,Hierarchically - Refined Label Attention Network for Sequence Labeling,[],"[('Sequence Labeling', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sequence Labeling']]",[],[],[],[],part-of-speech_tagging,4,2
research-problem,CRF has been used as a powerful model for statistical sequence labeling .,[],"[('statistical sequence labeling', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'statistical sequence labeling']]",[],[],[],[],part-of-speech_tagging,4,4
model,"To this question , we investigate a neural network model for output label sequences .","[('investigate', (5, 6)), ('for', (10, 11))]","[('neural network model', (7, 10)), ('output label sequences', (11, 14))]","[['neural network model', 'for', 'output label sequences']]",[],"[['Model', 'investigate', 'neural network model']]",[],[],[],[],[],[],part-of-speech_tagging,4,19
model,"In particular , we represent each possible label using an embedding vector , and aim to encode sequences of label distributions using a recurrent neural network .",[],[],"[['sequences', 'of', 'label distributions'], ['label distributions', 'using', 'recurrent neural network'], ['each possible label', 'using', 'embedding vector']]",[],"[['Model', 'aim to encode', 'sequences']]","[['Model', 'represent', 'each possible label']]",[],[],[],[],[],part-of-speech_tagging,4,20
model,This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,"[('represent', (6, 7)), ('without making', (13, 15))]","[('full - exponential search space', (8, 13)), ('Markov assumptions', (15, 17))]","[['full - exponential search space', 'without making', 'Markov assumptions']]",[],"[['Model', 'represent', 'full - exponential search space']]",[],[],[],[],[],[],part-of-speech_tagging,4,22
results,WSJ . shows the final POS tagging results on WSJ .,[],"[('WSJ', (0, 1))]",[],[],[],"[['Results', 'has', 'WSJ']]",[],[],[],[],"[['WSJ', 'has', 'BiLSTM - LAN']]",part-of-speech_tagging,4,179
results,"BiLSTM - LAN gives significant accuracy improvements over both BiLSTM - CRF and BiLSTM- softmax ( p < 0.01 ) , which is consistent with observations on development experiments .","[('gives', (3, 4)), ('over', (7, 8))]","[('BiLSTM - LAN', (0, 3)), ('significant accuracy improvements', (4, 7)), ('BiLSTM - CRF', (9, 12)), ('BiLSTM- softmax', (13, 15))]","[['BiLSTM - LAN', 'gives', 'significant accuracy improvements'], ['significant accuracy improvements', 'over', 'BiLSTM - CRF'], ['significant accuracy improvements', 'over', 'BiLSTM- softmax']]",[],[],[],[],[],[],[],[],part-of-speech_tagging,4,181
results,"Universal Dependencies ( UD ) v 2.2 . We design a multilingual experiment to compare BiLSTMsoftmax , BiLSTM - CRF ( strictly following 1 , which is the state - of - theart on multi-lingual POS tagging ) and BiLSTM - LAN .","[('on', (33, 34))]","[('Universal Dependencies ( UD ) v 2.2', (0, 7))]",[],[],[],"[['Results', 'has', 'Universal Dependencies ( UD ) v 2.2']]",[],[],[],[],"[['Universal Dependencies ( UD ) v 2.2', 'has', 'Our model']]",part-of-speech_tagging,4,186
results,Our model outperforms all the baselines on all the languages .,"[('outperforms', (2, 3))]","[('Our model', (0, 2)), ('all the baselines', (3, 6)), ('all the languages', (7, 10))]","[['Our model', 'outperforms', 'all the baselines']]","[['all the baselines', 'on', 'all the languages']]",[],[],[],[],[],[],[],part-of-speech_tagging,4,188
results,"The improvements are statistically significant for all the languages ( p < 0.01 ) , suggesting that BiLSTM - LAN is generally effective across languages .","[('are', (2, 3)), ('for', (5, 6)), ('suggesting that', (15, 17)), ('is', (20, 21)), ('across', (23, 24))]","[('improvements', (1, 2)), ('statistically significant', (3, 5)), ('all the languages ( p < 0.01 )', (6, 14)), ('BiLSTM - LAN', (17, 20)), ('generally effective', (21, 23)), ('languages', (24, 25))]","[['improvements', 'are', 'statistically significant'], ['statistically significant', 'for', 'all the languages ( p < 0.01 )'], ['all the languages ( p < 0.01 )', 'suggesting that', 'BiLSTM - LAN'], ['BiLSTM - LAN', 'is', 'generally effective'], ['generally effective', 'across', 'languages']]",[],[],[],[],[],[],"[['Universal Dependencies ( UD ) v 2.2', 'has', 'improvements']]",[],part-of-speech_tagging,4,189
results,"OntoNotes 5.0 . In NER , BiLSTM - CRF is widely used , because local dependencies between neighboring labels relatively more important that POS tagging and CCG supertagging .",[],"[('OntoNotes 5.0', (0, 2)), ('BiLSTM - CRF', (6, 9))]",[],[],[],"[['Results', 'has', 'OntoNotes 5.0']]",[],[],[],"[['significantly outperforms', 'has', 'BiLSTM - CRF']]","[['OntoNotes 5.0', 'has', 'BiLSTM - LAN']]",part-of-speech_tagging,4,190
results,BiLSTM - LAN also significantly outperforms BiLSTM - CRF by 1.17 F1-score ( p < 0.01 ) . CCGBank .,"[('by', (9, 10))]","[('BiLSTM - LAN', (0, 3)), ('significantly outperforms', (4, 6)), ('1.17 F1-score', (10, 12))]","[['significantly outperforms', 'by', '1.17 F1-score']]","[['BiLSTM - LAN', 'has', 'significantly outperforms']]",[],[],[],[],[],[],[],part-of-speech_tagging,4,191
results,"As shown in , BiLSTM - LAN significantly outperforms both BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 ) , showing the advantage of LAN . and explore BiRNN - softmax and BiLSTM - softmax , respectively .","[('both', (9, 10)), ('showing', (21, 22)), ('of', (24, 25))]","[('LAN', (6, 7)), ('BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )', (10, 20)), ('advantage', (23, 24))]","[['BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )', 'showing', 'advantage'], ['advantage', 'of', 'LAN']]",[],[],[],[],"[['significantly outperforms', 'both', 'BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )']]",[],[],[],part-of-speech_tagging,4,193
results,"Compared with these methods , BiLSTM - LAN obtains new state - of - theart results on CCGBank , matching the tri-training performance of , without training on external data .","[('obtains', (8, 9)), ('on', (16, 17))]","[('new state - of - theart results', (9, 16)), ('CCGBank', (17, 18))]","[['new state - of - theart results', 'on', 'CCGBank']]",[],[],[],[],"[['BiLSTM - LAN', 'obtains', 'new state - of - theart results']]",[],[],[],part-of-speech_tagging,4,198
research-problem,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,[],"[('Morphosyntactic Tagging', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Morphosyntactic Tagging']]",[],[],[],[],part-of-speech_tagging,5,2
model,We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,"[('learn', (7, 8)), ('through', (15, 16))]","[('context sensitive initial character and word representations', (8, 15)), ('two separate sentence - level recurrent models', (16, 23))]","[['context sensitive initial character and word representations', 'through', 'two separate sentence - level recurrent models']]",[],"[['Model', 'learn', 'context sensitive initial character and word representations']]",[],[],[],[],[],[],part-of-speech_tagging,5,24
model,These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,"[('combined via', (3, 5)), ('builds', (9, 10)), ('of', (13, 14)), ('used for', (19, 21))]","[('meta-BiLSTM model', (6, 8)), ('unified representation', (11, 13)), ('each word', (14, 16)), ('syntactic tagging', (21, 23))]","[['meta-BiLSTM model', 'builds', 'unified representation'], ['unified representation', 'used for', 'syntactic tagging'], ['unified representation', 'of', 'each word']]",[],[],[],[],"[['context sensitive initial character and word representations', 'combined via', 'meta-BiLSTM model']]",[],[],[],part-of-speech_tagging,5,25
hyperparameters,The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training .,"[('are', (3, 4)), ('with', (5, 6)), ('not', (13, 14)), ('during', (15, 16))]","[('word embeddings', (1, 3)), ('initialized', (4, 5)), ('zero values', (6, 8)), ('pre-trained embeddings', (10, 12)), ('updated', (14, 15)), ('training', (16, 17))]","[['word embeddings', 'are', 'initialized'], ['initialized', 'with', 'zero values'], ['pre-trained embeddings', 'not', 'updated'], ['updated', 'during', 'training']]",[],[],"[['Hyperparameters', 'has', 'word embeddings'], ['Hyperparameters', 'has', 'pre-trained embeddings']]",[],[],[],[],[],part-of-speech_tagging,5,119
hyperparameters,The dropout used on the embeddings is achieved by a RRIE is the relative reduction in error .,"[('used on', (2, 4)), ('is', (6, 7)), ('achieved by', (7, 9))]","[('dropout', (1, 2)), ('embeddings', (5, 6)), ('RRIE', (10, 11)), ('relative reduction in error', (13, 17))]","[['dropout', 'used on', 'embeddings'], ['embeddings', 'achieved by', 'RRIE'], ['RRIE', 'is', 'relative reduction in error']]",[],[],"[['Hyperparameters', 'has', 'dropout']]",[],[],[],[],[],part-of-speech_tagging,5,120
results,Part - of - Speech Tagging Results,[],"[('Part - of - Speech Tagging Results', (0, 7))]",[],[],[],"[['Results', 'has', 'Part - of - Speech Tagging Results']]",[],[],[],[],"[['Part - of - Speech Tagging Results', 'has', 'Our model']]",part-of-speech_tagging,5,135
results,Our model outperforms in 32 of the 54 treebanks with 13 ties .,"[('in', (3, 4)), ('of', (5, 6)), ('with', (9, 10))]","[('Our model', (0, 2)), ('outperforms', (2, 3)), ('32', (4, 5)), ('54 treebanks', (7, 9)), ('13 ties', (10, 12))]","[['outperforms', 'with', '13 ties'], ['outperforms', 'in', '32'], ['32', 'of', '54 treebanks']]","[['Our model', 'has', 'outperforms']]",[],[],[],[],[],[],[],part-of-speech_tagging,5,141
results,"Our model tends to produce better results , especially for morphologically rich languages ( e.g. Slavic","[('produce', (4, 5)), ('especially for', (8, 10))]","[('better results', (5, 7)), ('morphologically rich languages', (10, 13))]","[['better results', 'especially for', 'morphologically rich languages']]",[],[],[],[],"[['Our model', 'produce', 'better results']]",[],[],[],part-of-speech_tagging,5,143
results,Morphological Tagging Results,[],"[('Morphological Tagging Results', (0, 3))]",[],[],[],"[['Results', 'has', 'Morphological Tagging Results']]",[],[],[],[],"[['Morphological Tagging Results', 'has', 'Our models']]",part-of-speech_tagging,5,150
results,"Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .","[('produce', (4, 5)), ('than', (8, 9))]","[('Our models', (0, 2)), ('significantly better results', (5, 8)), ('winners of the CoNLL 2017 Shared Task', (10, 17))]","[['Our models', 'produce', 'significantly better results'], ['significantly better results', 'than', 'winners of the CoNLL 2017 Shared Task']]",[],[],[],[],[],[],[],[],part-of-speech_tagging,5,155
ablation-analysis,shows that separately optimized models are significantly more accurate on average than jointly optimized models .,"[('shows that', (0, 2)), ('are', (5, 6)), ('on', (9, 10)), ('than', (11, 12))]","[('separately optimized models', (2, 5)), ('significantly more accurate', (6, 9)), ('average', (10, 11)), ('jointly optimized models', (12, 15))]","[['separately optimized models', 'are', 'significantly more accurate'], ['significantly more accurate', 'than', 'jointly optimized models'], ['significantly more accurate', 'on', 'average']]",[],"[['Ablation analysis', 'shows that', 'separately optimized models']]",[],[],[],[],[],[],part-of-speech_tagging,5,167
ablation-analysis,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,[],[],"[['Separate optimization', 'leads to', 'better accuracy'], ['better accuracy', 'for', '30 out of 39 treebanks'], ['30 out of 39 treebanks', 'for', 'xpos tagging'], ['better accuracy', 'for', '34 out of 40 treebanks'], ['34 out of 40 treebanks', 'for', 'morphological features task']]",[],"[['Ablation analysis', 'for', 'Separate optimization']]",[],[],[],[],[],[],part-of-speech_tagging,5,168
ablation-analysis,"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .",[],[],"[['joint optimization', 'by', 'up to 2.1 percent absolute'], ['joint', 'never', 'out - performed'], ['out - performed', 'by', 'more than 0.5 % absolute']]","[['outperformed', 'has', 'joint optimization'], ['out - performed', 'has', 'separate']]",[],"[['Ablation analysis', 'has', 'joint']]",[],[],[],"[['Separate optimization', 'has', 'outperformed']]",[],part-of-speech_tagging,5,169
ablation-analysis,The examples show that the combined model has significantly higher accuracy compared with either the character and word models individually .,"[('compared with', (11, 13))]","[('combined model', (5, 7)), ('significantly higher accuracy', (8, 11)), ('character and word models', (15, 19))]","[['significantly higher accuracy', 'compared with', 'character and word models']]","[['combined model', 'has', 'significantly higher accuracy']]",[],"[['Ablation analysis', 'has', 'combined model']]",[],[],[],[],[],part-of-speech_tagging,5,179
ablation-analysis,"For all of the network sizes in the grid search , we still observed during training that the accuracy reach a high value and degrades with more iterations for the character and word model .","[('For', (0, 1)), ('in', (6, 7)), ('observed during', (13, 15)), ('that', (16, 17)), ('reach', (19, 20)), ('degrades with', (24, 26)), ('for', (28, 29))]","[('all of the network sizes', (1, 6)), ('grid search', (8, 10)), ('training', (15, 16)), ('accuracy', (18, 19)), ('high value', (21, 23)), ('more iterations', (26, 28)), ('character and word model', (30, 34))]","[['all of the network sizes', 'in', 'grid search'], ['all of the network sizes', 'observed during', 'training'], ['training', 'that', 'accuracy'], ['accuracy', 'degrades with', 'more iterations'], ['more iterations', 'for', 'character and word model'], ['accuracy', 'reach', 'high value']]",[],"[['Ablation analysis', 'For', 'all of the network sizes']]",[],[],[],[],[],[],part-of-speech_tagging,5,194
research-problem,A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing,[],"[('Joint POS Tagging and Graph - based Dependency Parsing', (6, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'Joint POS Tagging and Graph - based Dependency Parsing']]",[],[],[],[],part-of-speech_tagging,6,2
code,Our code is open - source and available together with pre-trained models at : https://github.com/ datquocnguyen/jPTDP .,[],"[('https://github.com/ datquocnguyen/jPTDP', (14, 16))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/ datquocnguyen/jPTDP']]",[],[],[],[],part-of-speech_tagging,6,7
model,"In this paper , we propose a novel neural architecture for joint POS tagging and graph - based dependency parsing .","[('propose', (5, 6)), ('for', (10, 11))]","[('novel neural architecture', (7, 10)), ('joint POS tagging and graph - based dependency parsing', (11, 20))]","[['novel neural architecture', 'for', 'joint POS tagging and graph - based dependency parsing']]",[],"[['Model', 'propose', 'novel neural architecture']]",[],[],[],[],[],[],part-of-speech_tagging,6,19
model,Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM .,"[('learns', (2, 3)), ('shared for', (6, 8)), ('using', (16, 17))]","[('latent feature representations', (3, 6)), ('POS tagging and dependency parsing tasks', (9, 15)), ('BiLSTMthe bidirectional LSTM', (17, 20))]","[['latent feature representations', 'shared for', 'POS tagging and dependency parsing tasks'], ['POS tagging and dependency parsing tasks', 'using', 'BiLSTMthe bidirectional LSTM']]",[],"[['Model', 'learns', 'latent feature representations']]",[],[],[],[],[],[],part-of-speech_tagging,6,20
experimental-setup,Our jPTDP is implemented using DYNET v 2.0 .,"[('implemented using', (3, 5))]","[('jPTDP', (1, 2)), ('DYNET v 2.0', (5, 8))]","[['jPTDP', 'implemented using', 'DYNET v 2.0']]",[],[],"[['Experimental setup', 'has', 'jPTDP']]",[],[],[],[],[],part-of-speech_tagging,6,75
experimental-setup,"We optimize the objective function using Adam ( Kingma and Ba , 2014 ) with default DYNET parameter settings and no mini-batches .","[('optimize', (1, 2)), ('using', (5, 6)), ('with', (14, 15))]","[('objective function', (3, 5)), ('Adam ( Kingma and Ba , 2014 )', (6, 14)), ('default DYNET parameter settings', (15, 19))]","[['objective function', 'using', 'Adam ( Kingma and Ba , 2014 )'], ['Adam ( Kingma and Ba , 2014 )', 'with', 'default DYNET parameter settings']]",[],"[['Experimental setup', 'optimize', 'objective function']]",[],[],[],[],[],[],part-of-speech_tagging,6,76
experimental-setup,"Following Kiperwasser and Goldberg ( 2016 b ) and , we apply a word dropout rate of 0.25 and Gaussian noise with ? = 0.2 .","[('apply', (11, 12)), ('of', (16, 17)), ('with', (21, 22))]","[('word dropout rate', (13, 16)), ('0.25', (17, 18)), ('Gaussian noise', (19, 21)), ('? = 0.2', (22, 25))]","[['Gaussian noise', 'with', '? = 0.2'], ['word dropout rate', 'of', '0.25']]",[],"[['Experimental setup', 'apply', 'Gaussian noise'], ['Experimental setup', 'apply', 'word dropout rate']]",[],[],[],[],[],[],part-of-speech_tagging,6,78
experimental-setup,"For training , we run for 30 epochs , and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch .","[('run for', (4, 6))]","[('30 epochs', (6, 8))]",[],[],"[['Experimental setup', 'run for', '30 epochs']]",[],[],[],[],[],[],part-of-speech_tagging,6,79
experimental-setup,We perform a minimal grid search of hyper - parameters on English .,"[('perform', (1, 2)), ('of', (6, 7)), ('on', (10, 11))]","[('minimal grid search', (3, 6)), ('hyper - parameters', (7, 10)), ('English', (11, 12))]","[['minimal grid search', 'of', 'hyper - parameters'], ['hyper - parameters', 'on', 'English']]",[],"[['Experimental setup', 'perform', 'minimal grid search']]",[],[],[],[],[],[],part-of-speech_tagging,6,80
experimental-setup,"compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work , using the same experimental setup .","[('compares', (0, 1)), ('of', (8, 9))]","[('POS tagging and dependency parsing results', (2, 8)), ('our model jPTDP', (9, 12))]","[['POS tagging and dependency parsing results', 'of', 'our model jPTDP']]",[],"[['Experimental setup', 'compares', 'POS tagging and dependency parsing results']]",[],[],[],[],[],[],part-of-speech_tagging,6,84
results,"In terms of dependency parsing , in most cases , our model jPTDP outperforms Stack - propagation .","[('In terms of', (0, 3)), ('outperforms', (13, 14))]","[('dependency parsing', (3, 5)), ('our model jPTDP', (10, 13)), ('Stack - propagation', (14, 17))]","[['our model jPTDP', 'outperforms', 'Stack - propagation']]","[['dependency parsing', 'has', 'our model jPTDP']]","[['Results', 'In terms of', 'dependency parsing']]",[],[],[],[],[],[],part-of-speech_tagging,6,90
results,It is somewhat unexpected that our model produces about 7 % absolute lower LAS score than Stack - propagation on Dutch ( nl ) .,"[('produces', (7, 8)), ('than', (15, 16)), ('on', (19, 20))]","[('about 7 % absolute lower LAS score', (8, 15)), ('Stack - propagation', (16, 19)), ('Dutch ( nl )', (20, 24))]","[['about 7 % absolute lower LAS score', 'than', 'Stack - propagation'], ['Stack - propagation', 'on', 'Dutch ( nl )']]",[],"[['Results', 'produces', 'about 7 % absolute lower LAS score']]",[],[],[],[],[],[],part-of-speech_tagging,6,91
results,"Without taking "" nl "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .","[('Without taking', (0, 2)), ('into', (5, 6)), ('over', (12, 13)), ('is', (16, 17)), ('than', (21, 22))]","[('nl', (3, 4)), ('account', (6, 7)), ('averaged LAS score', (9, 12)), ('all remaining languages', (13, 16)), ('1.1 % absolute higher', (17, 21)), (""Stack - propagation 's"", (22, 26))]","[['nl', 'into', 'account'], ['averaged LAS score', 'over', 'all remaining languages'], ['all remaining languages', 'is', '1.1 % absolute higher'], ['1.1 % absolute higher', 'than', ""Stack - propagation 's""]]","[['nl', 'has', 'averaged LAS score']]","[['Results', 'Without taking', 'nl']]",[],[],[],[],[],[],part-of-speech_tagging,6,94
results,"The last row in shows an absolute LAS improvement of 4.4 % on average when comparing our jPTDP with its simplified version of not using characterbased representations : specifically , morphologically rich languages get an averaged improvement of 9.3 % , vice versa 2.6 % for others .",[],[],"[['absolute LAS improvement', 'of', '4.4 %'], ['4.4 %', 'on', 'average'], ['morphologically rich languages', 'get', 'averaged improvement'], ['averaged improvement', 'of', '9.3 %']]",[],"[['Results', 'shows', 'absolute LAS improvement']]","[['Results', 'has', 'morphologically rich languages']]",[],[],[],[],[],part-of-speech_tagging,6,96
results,"So , our jPDTP is particularly good for morphologically rich languages , with 1.7 % higher averaged LAS than Stack - propagation over these languages .","[('good for', (6, 8)), ('with', (12, 13)), ('than', (18, 19))]","[('jPDTP', (3, 4)), ('morphologically rich languages', (8, 11)), ('1.7 % higher averaged LAS', (13, 18)), ('Stack - propagation', (19, 22))]","[['jPDTP', 'good for', 'morphologically rich languages'], ['morphologically rich languages', 'with', '1.7 % higher averaged LAS'], ['1.7 % higher averaged LAS', 'than', 'Stack - propagation']]",[],[],"[['Results', 'has', 'jPDTP']]",[],[],[],[],[],part-of-speech_tagging,6,98
research-problem,Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,[],"[('Multilingual Part - of - Speech Tagging', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multilingual Part - of - Speech Tagging']]",[],[],[],[],part-of-speech_tagging,7,2
research-problem,"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .",[],"[('POS tagging', (19, 21))]",[],[],[],[],"[['Contribution', 'has research problem', 'POS tagging']]",[],[],[],[],part-of-speech_tagging,7,5
model,"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .","[('introduce', (3, 4))]","[('novel model', (5, 7)), ('bi - LSTM trained with auxiliary loss', (9, 16))]",[],"[['novel model', 'has', 'bi - LSTM trained with auxiliary loss']]","[['Model', 'introduce', 'novel model']]",[],[],[],[],[],[],part-of-speech_tagging,7,22
model,The model jointly predicts the POS and the log frequency of the word .,"[('jointly predicts', (2, 4))]","[('POS and the log frequency of the word', (5, 13))]",[],[],"[['Model', 'jointly predicts', 'POS and the log frequency of the word']]",[],[],[],[],[],[],part-of-speech_tagging,7,23
hyperparameters,"epochs , default learning rate ( 0.1 ) , 128 dimensions for word embeddings , 100 for character and byte embeddings , 100 hidden states and Gaussian noise with ?= 0.2 .",[],[],"[['100', 'for', 'character and byte embeddings'], ['Gaussian noise', 'with', '0.2'], ['128 dimensions', 'for', 'word embeddings']]","[['100', 'has', 'hidden states'], ['default learning rate', 'has', '0.1']]",[],"[['Hyperparameters', 'has', '100'], ['Hyperparameters', 'has', 'Gaussian noise'], ['Hyperparameters', 'has', '128 dimensions'], ['Hyperparameters', 'has', 'default learning rate']]",[],[],[],[],[],part-of-speech_tagging,7,52
hyperparameters,"As training is stochastic in nature , we use a fixed seed throughout .","[('is', (2, 3)), ('use', (8, 9))]","[('training', (1, 2)), ('stochastic', (3, 4)), ('fixed seed', (10, 12))]","[['training', 'is', 'stochastic'], ['stochastic', 'use', 'fixed seed']]",[],[],"[['Hyperparameters', 'has', 'training']]",[],[],[],[],[],part-of-speech_tagging,7,53
hyperparameters,In that case we use offthe - shelf polyglot embeddings .,"[('use', (4, 5))]","[('offthe - shelf polyglot embeddings', (5, 10))]",[],[],"[['Hyperparameters', 'use', 'offthe - shelf polyglot embeddings']]",[],[],[],[],[],[],part-of-speech_tagging,7,55
code,The code is released at : https : //github.com/bplank/bilstm-aux,[],"[('https : //github.com/bplank/bilstm-aux', (6, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https : //github.com/bplank/bilstm-aux']]",[],[],[],[],part-of-speech_tagging,7,58
results,"In an initial investigation , we compared Tnt , HunPos and TreeTagger and found Tnt to be consistently better than Treetagger , Hunpos followed closely but crashed on some languages ( e.g. , Arabic ) .","[('compared', (6, 7)), ('found', (13, 14)), ('to be', (15, 17)), ('than', (19, 20))]","[('Tnt , HunPos and TreeTagger', (7, 12)), ('Tnt', (14, 15)), ('consistently better', (17, 19)), ('Treetagger', (20, 21))]","[['Tnt , HunPos and TreeTagger', 'found', 'Tnt'], ['Tnt', 'to be', 'consistently better'], ['consistently better', 'than', 'Treetagger']]",[],"[['Results', 'compared', 'Tnt , HunPos and TreeTagger']]",[],[],[],[],[],[],part-of-speech_tagging,7,79
results,"The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre-trained embeddings .","[('is', (7, 8)), ('on', (15, 16))]","[('combined word + character representation model', (1, 7)), ('best representation', (9, 11)), ('outperforming', (12, 13)), ('baseline', (14, 15)), ('all except one language ( Indonesian )', (16, 23))]","[['combined word + character representation model', 'is', 'best representation'], ['baseline', 'on', 'all except one language ( Indonesian )']]","[['best representation', 'has', 'outperforming'], ['outperforming', 'has', 'baseline']]",[],"[['Results', 'has', 'combined word + character representation model']]",[],[],[],[],[],part-of-speech_tagging,7,80
results,This model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .,"[('reaches', (7, 8)), ('on', (19, 20))]","[('biggest improvement', (9, 11)), ('more than + 2 % accuracy', (12, 18)), ('Hebrew and Slovene', (20, 23))]","[['biggest improvement', 'on', 'Hebrew and Slovene']]","[['biggest improvement', 'has', 'more than + 2 % accuracy']]",[],[],[],"[['combined word + character representation model', 'reaches', 'biggest improvement']]",[],[],[],part-of-speech_tagging,7,81
results,Initializing the word embeddings ( + POLYGLOT ) with off - the - shelf languagespecific embeddings further improves accuracy .,"[('Initializing', (0, 1)), ('with', (8, 9)), ('improves', (17, 18))]","[('word embeddings ( + POLYGLOT )', (2, 8)), ('off - the - shelf languagespecific embeddings', (9, 16)), ('accuracy', (18, 19))]","[['word embeddings ( + POLYGLOT )', 'with', 'off - the - shelf languagespecific embeddings'], ['off - the - shelf languagespecific embeddings', 'improves', 'accuracy']]",[],"[['Results', 'Initializing', 'word embeddings ( + POLYGLOT )']]",[],[],[],[],[],[],part-of-speech_tagging,7,82
results,The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,"[('is', (5, 6))]","[('over all best system', (1, 5)), ('multi-task bi - LSTM FREQBIN', (7, 12))]","[['over all best system', 'is', 'multi-task bi - LSTM FREQBIN']]",[],[],"[['Results', 'has', 'over all best system']]",[],[],[],[],[],part-of-speech_tagging,7,85
research-problem,Document Expansion by Query Prediction,[],"[('Document Expansion', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Document Expansion']]",[],[],[],[],passage_re-ranking,0,2
approach,"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .","[('explore', (5, 6)), ('based on', (9, 11))]","[('alternative approach', (7, 9)), ('enriching', (11, 12)), ('document representation', (13, 15))]","[['alternative approach', 'based on', 'enriching']]","[['enriching', 'has', 'document representation']]","[['Approach', 'explore', 'alternative approach']]",[],[],[],[],[],[],passage_re-ranking,0,24
approach,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .",[],[],"[['sequence - to - sequence model', 'given', 'document'], ['document', 'generates', 'possible questions'], ['possible questions', 'that', 'document'], ['document', 'might', 'answer'], ['sequence - to - sequence model', 'Focusing on', 'question answering']]",[],"[['Approach', 'train', 'sequence - to - sequence model']]",[],[],[],[],[],[],passage_re-ranking,0,25
baselines,BM25 : We use the Anserini open - source IR toolkit 3 to index the original ( non -expanded ) documents and BM25 to rank the passages .,"[('use', (3, 4)), ('to index', (12, 14)), ('to rank', (23, 25))]","[('BM25', (0, 1)), ('Anserini open - source IR toolkit', (5, 11)), ('original ( non -expanded ) documents', (15, 21)), ('passages', (26, 27))]","[['BM25', 'to rank', 'passages'], ['BM25', 'use', 'Anserini open - source IR toolkit'], ['Anserini open - source IR toolkit', 'to index', 'original ( non -expanded ) documents']]",[],[],"[['Baselines', 'has', 'BM25']]",[],[],[],[],[],passage_re-ranking,0,68
baselines,BM25 + Doc2query :,[],"[('BM25 + Doc2query', (0, 3))]",[],[],[],"[['Baselines', 'has', 'BM25 + Doc2query']]",[],[],[],[],[],passage_re-ranking,0,70
baselines,We first expand the documents using the proposed Doc2query method .,"[('expand', (2, 3)), ('using', (5, 6))]","[('documents', (4, 5)), ('proposed Doc2query method', (7, 10))]","[['documents', 'using', 'proposed Doc2query method']]",[],[],[],[],"[['BM25 + Doc2query', 'expand', 'documents']]",[],[],[],passage_re-ranking,0,71
baselines,We then index and rank the expanded documents exactly as in the BM25 method above .,"[('index and rank', (2, 5)), ('exactly as in', (8, 11))]","[('expanded documents', (6, 8)), ('BM25 method', (12, 14))]","[['expanded documents', 'exactly as in', 'BM25 method']]",[],[],[],[],"[['BM25 + Doc2query', 'index and rank', 'expanded documents']]",[],[],[],passage_re-ranking,0,72
baselines,RM3 :,[],"[('RM3', (0, 1))]",[],[],[],"[['Baselines', 'has', 'RM3']]",[],[],[],[],[],passage_re-ranking,0,76
baselines,"To compare document expansion with query expansion , we applied the RM3 query expansion technique .","[('compare', (1, 2)), ('with', (4, 5)), ('applied', (9, 10))]","[('document expansion', (2, 4)), ('query expansion', (5, 7)), ('RM3 query expansion technique', (11, 15))]","[['document expansion', 'with', 'query expansion'], ['document expansion', 'applied', 'RM3 query expansion technique']]",[],[],[],[],"[['RM3', 'compare', 'document expansion']]",[],[],[],passage_re-ranking,0,77
baselines,BM25 + BERT : We index and retrieve documents as in the BM25 condition and further re-rank the documents with BERT as described in .,[],[],"[['documents', 'with', 'BERT'], ['BM25 + BERT', 'further re-rank', 'documents'], ['documents', 'with', 'BERT'], ['BM25 + BERT', 'index and retrieve', 'documents'], ['documents', 'as in', 'BM25 condition']]",[],[],"[['Baselines', 'has', 'BM25 + BERT']]",[],[],[],[],[],passage_re-ranking,0,79
baselines,"BM25 + Doc2query + BERT : We expand , index , and retrieve documents as in the BM25 + Doc2query condition and further re-rank the documents with BERT .",[],[],"[['BM25 + Doc2query + BERT', 'expand , index , and retrieve', 'documents'], ['documents', 'as in', 'BM25 + Doc2query condition'], ['BM25 + Doc2query + BERT', 'further re-rank', 'documents']]",[],[],"[['Baselines', 'has', 'BM25 + Doc2query + BERT']]",[],[],[],[],[],passage_re-ranking,0,80
results,Document expansion with our method ( BM25 + Doc2query ) improves retrieval effectiveness by ? 15 % for both datasets .,"[('with', (2, 3)), ('improves', (10, 11)), ('by', (13, 14)), ('for', (17, 18))]","[('Document expansion', (0, 2)), ('our method ( BM25 + Doc2query )', (3, 10)), ('retrieval effectiveness', (11, 13)), ('15 %', (15, 17)), ('both datasets', (18, 20))]","[['Document expansion', 'with', 'our method ( BM25 + Doc2query )'], ['our method ( BM25 + Doc2query )', 'improves', 'retrieval effectiveness'], ['retrieval effectiveness', 'by', '15 %'], ['retrieval effectiveness', 'for', 'both datasets']]",[],[],"[['Results', 'has', 'Document expansion']]",[],[],[],[],[],passage_re-ranking,0,86
results,"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the state of the art .","[('combine', (2, 3)), ('with', (5, 6)), ('achieve', (24, 25)), ('on', (32, 33)), ('for', (36, 37)), ('near', (42, 43))]","[('document expansion', (3, 5)), ('state - of - the - art re-ranker ( BM25 + Doc2query + BERT )', (7, 22)), ('best - known results', (26, 30)), ('TREC CAR', (33, 35)), ('MS MARCO', (37, 39)), ('state of the art', (44, 48))]","[['document expansion', 'with', 'state - of - the - art re-ranker ( BM25 + Doc2query + BERT )'], ['state - of - the - art re-ranker ( BM25 + Doc2query + BERT )', 'achieve', 'best - known results'], ['best - known results', 'on', 'TREC CAR'], ['state - of - the - art re-ranker ( BM25 + Doc2query + BERT )', 'for', 'MS MARCO'], ['MS MARCO', 'near', 'state of the art']]",[],"[['Results', 'combine', 'document expansion']]",[],[],[],[],[],[],passage_re-ranking,0,87
results,"Our full re-ranking condition ( BM25 + Doc2query + BERT ) beats BM25 + BERT alone , which verifies that the contribution Input Document : July is the hottest month in Washington DC with an average temperature of 27C ( 80F ) and the coldest is January at 4C ( 38F ) with the most daily sunshine hours at 9 in July .","[('beats', (11, 12))]","[('Our full re-ranking condition ( BM25 + Doc2query + BERT )', (0, 11)), ('BM25 + BERT alone', (12, 16))]","[['Our full re-ranking condition ( BM25 + Doc2query + BERT )', 'beats', 'BM25 + BERT alone']]",[],[],"[['Results', 'has', 'Our full re-ranking condition ( BM25 + Doc2query + BERT )']]",[],[],[],[],[],passage_re-ranking,0,89
results,"We notice that the model tends to copy some words from the input document ( e.g. , Washington DC , River , chromosome ) , meaning that it can effectively perform term re-weighting ( i.e. , increasing the importance of key terms ) .","[('notice that', (1, 3)), ('copy', (7, 8)), ('from', (10, 11))]","[('model', (4, 5)), ('some words', (8, 10)), ('input document', (12, 14))]","[['model', 'copy', 'some words'], ['some words', 'from', 'input document']]",[],"[['Results', 'notice that', 'model']]",[],[],[],[],[],[],passage_re-ranking,0,99
results,"Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .","[('produces', (5, 6)), ('not present in', (7, 10)), ('characterized as', (24, 26)), ('by', (27, 28))]","[('words', (6, 7)), ('input document', (11, 13)), ('expansion', (26, 27)), ('synonyms and other related terms', (28, 33))]","[['words', 'not present in', 'input document'], ['words', 'characterized as', 'expansion'], ['expansion', 'by', 'synonyms and other related terms']]",[],[],[],[],"[['model', 'produces', 'words']]",[],[],[],passage_re-ranking,0,100
results,"If we expand MS MARCO documents using only new words and retrieve the development set queries with BM25 , we obtain an MRR@10 of 18.8 ( as opposed to 18.4 when indexing with original documents ) .","[('expand', (2, 3)), ('using', (6, 7)), ('retrieve', (11, 12)), ('with', (16, 17)), ('obtain', (20, 21)), ('of', (23, 24))]","[('MS MARCO documents', (3, 6)), ('only new words', (7, 10)), ('development set queries', (13, 16)), ('BM25', (17, 18)), ('MRR@10', (22, 23)), ('18.8', (24, 25))]","[['MS MARCO documents', 'obtain', 'MRR@10'], ['MRR@10', 'of', '18.8'], ['MS MARCO documents', 'using', 'only new words'], ['MS MARCO documents', 'retrieve', 'development set queries'], ['development set queries', 'with', 'BM25']]",[],"[['Results', 'expand', 'MS MARCO documents']]",[],[],[],[],[],[],passage_re-ranking,0,103
results,Expanding with copied words gives an MRR@10 of 19.7 .,"[('Expanding with', (0, 2)), ('gives', (4, 5)), ('of', (7, 8))]","[('copied words', (2, 4)), ('MRR@10', (6, 7)), ('19.7', (8, 9))]","[['copied words', 'gives', 'MRR@10'], ['MRR@10', 'of', '19.7']]",[],"[['Results', 'Expanding with', 'copied words']]",[],[],[],[],[],[],passage_re-ranking,0,104
results,"We achieve a higher MRR@10 of 21.5 when documents are expanded with both types of words , showing that they are complementary .","[('achieve', (1, 2)), ('of', (5, 6)), ('when', (7, 8)), ('expanded with', (10, 12))]","[('higher MRR@10', (3, 5)), ('21.5', (6, 7)), ('documents', (8, 9)), ('both types of words', (12, 16))]","[['higher MRR@10', 'of', '21.5'], ['higher MRR@10', 'when', 'documents'], ['documents', 'expanded with', 'both types of words']]",[],"[['Results', 'achieve', 'higher MRR@10']]",[],[],[],[],[],[],passage_re-ranking,0,105
results,We find that the Recall@1000 of the MS MARCO development set increased from 85.3 ( BM25 ) to 89.3 ( BM25 + Doc2query ) .,"[('find that', (1, 3)), ('of', (5, 6)), ('increased from', (11, 13)), ('to', (17, 18))]","[('Recall@1000', (4, 5)), ('MS MARCO development set', (7, 11)), ('85.3 ( BM25 )', (13, 17)), ('89.3 ( BM25 + Doc2query )', (18, 24))]","[['Recall@1000', 'of', 'MS MARCO development set'], ['MS MARCO development set', 'increased from', '85.3 ( BM25 )'], ['85.3 ( BM25 )', 'to', '89.3 ( BM25 + Doc2query )']]",[],"[['Results', 'find that', 'Recall@1000']]",[],[],[],[],[],[],passage_re-ranking,0,107
results,"As a contrastive condition , we find that query expansion with RM3 hurts in both datasets , whether applied to the unexpanded corpus ( BM25 + RM3 ) or the expanded version ( BM25 + Doc2query + RM3 ) .","[('with', (10, 11)), ('in', (13, 14))]","[('query expansion', (8, 10)), ('RM3', (11, 12)), ('hurts', (12, 13)), ('both datasets', (14, 16))]","[['query expansion', 'with', 'RM3'], ['hurts', 'in', 'both datasets']]","[['RM3', 'has', 'hurts']]",[],"[['Results', 'find that', 'query expansion']]",[],[],[],[],[],passage_re-ranking,0,109
results,"This result shows that document expansion can be more effective than query expansion , most likely because there are more signals to exploit as documents are much longer .","[('shows that', (2, 4)), ('more effective than', (8, 11))]","[('document expansion', (4, 6)), ('query expansion', (11, 13))]","[['document expansion', 'more effective than', 'query expansion']]",[],"[['Results', 'shows that', 'document expansion']]",[],[],[],[],[],[],passage_re-ranking,0,111
results,"Our method without a re-ranker ( BM25 + Doc2query ) adds a small latency increase over baseline BM25 ( 50 ms vs. 90 ms ) but is approximately seven times faster than a neural re-ranker that has a three points higher MRR@10 ( Single Duet v2 , which is presented as a baseline in MS MARCO by the organizers ) .","[('adds', (10, 11)), ('over', (15, 16)), ('is', (26, 27)), ('than', (31, 32)), ('that has', (35, 37))]","[('Our method without a re-ranker ( BM25 + Doc2query )', (0, 10)), ('small latency', (12, 14)), ('baseline BM25 ( 50 ms vs. 90 ms )', (16, 25)), ('seven times faster', (28, 31)), ('neural re-ranker', (33, 35)), ('three points higher MRR@10', (38, 42))]","[['Our method without a re-ranker ( BM25 + Doc2query )', 'is', 'seven times faster'], ['seven times faster', 'than', 'neural re-ranker'], ['neural re-ranker', 'that has', 'three points higher MRR@10'], ['Our method without a re-ranker ( BM25 + Doc2query )', 'adds', 'small latency'], ['small latency', 'over', 'baseline BM25 ( 50 ms vs. 90 ms )']]",[],[],"[['Results', 'has', 'Our method without a re-ranker ( BM25 + Doc2query )']]",[],[],[],[],[],passage_re-ranking,0,113
research-problem,PASSAGE RE - RANKING WITH BERT,[],"[('PASSAGE RE - RANKING', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'PASSAGE RE - RANKING']]",[],[],[],[],passage_re-ranking,1,2
research-problem,"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking .",[],"[('query - based passage re-ranking', (12, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'query - based passage re-ranking']]",[],[],[],[],passage_re-ranking,1,5
approach,"In this paper , we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state - of - the - art results on the MS MARCO passage re-ranking task .","[('re-purposed', (11, 12)), ('as', (13, 14))]","[('BERT', (12, 13)), ('passage re-ranker', (15, 17))]","[['BERT', 'as', 'passage re-ranker']]",[],"[['Approach', 're-purposed', 'BERT']]",[],[],[],[],[],[],passage_re-ranking,1,16
research-problem,PASSAGE RE - RANKING WITH BERT,[],[],[],[],[],[],[],[],[],[],[],passage_re-ranking,1,17
experiments,MS MARCO,[],"[('MS MARCO', (0, 2))]",[],[],[],"[['Experiments', 'has', 'MS MARCO']]",[],[],[],[],"[['MS MARCO', 'has', 'Hyperparameters']]",passage_re-ranking,1,35
experiments,"We fine - tune the model using TPUs 1 with a batch size of 32 ( 32 sequences * 512 tokens = 16,384 tokens / batch ) for 400 k iterations , which takes approximately 70 hours .","[('fine - tune', (1, 4)), ('using', (6, 7)), ('with', (9, 10)), ('of', (13, 14)), ('for', (27, 28)), ('takes', (33, 34))]","[('model', (5, 6)), ('TPUs', (7, 8)), ('batch size', (11, 13)), ('32', (14, 15)), ('400 k iterations', (28, 31)), ('approximately 70 hours', (34, 37))]","[['model', 'with', 'batch size'], ['batch size', 'of', '32'], ['model', 'using', 'TPUs'], ['model', 'for', '400 k iterations'], ['model', 'takes', 'approximately 70 hours']]",[],[],[],[],"[['Hyperparameters', 'fine - tune', 'model']]",[],[],[],passage_re-ranking,1,43
experiments,"We use ADAM ( Kingma & Ba , 2014 ) with the initial learning rate set to 3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",[],[],"[['ADAM ( Kingma & Ba , 2014 )', 'with', 'initial learning rate'], ['initial learning rate', 'set to', '3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999'], ['ADAM ( Kingma & Ba , 2014 )', 'with', 'learning rate warmup'], ['learning rate warmup', 'over', 'first 10,000 steps'], ['ADAM ( Kingma & Ba , 2014 )', 'with', 'L2 weight decay'], ['L2 weight decay', 'of', '0.01'], ['ADAM ( Kingma & Ba , 2014 )', 'with', 'linear decay'], ['linear decay', 'of', 'learning rate']]",[],[],[],[],"[['Hyperparameters', 'use', 'ADAM ( Kingma & Ba , 2014 )']]",[],[],[],passage_re-ranking,1,46
experiments,We use a dropout probability of 0.1 on all layers .,"[('of', (5, 6)), ('on', (7, 8))]","[('dropout probability', (3, 5)), ('0.1', (6, 7)), ('all layers', (8, 10))]","[['dropout probability', 'of', '0.1'], ['0.1', 'on', 'all layers']]",[],[],[],[],[],[],"[['Hyperparameters', 'use', 'dropout probability']]",[],passage_re-ranking,1,47
,TREC - CAR,[],[],[],[],[],[],[],[],[],[],[],passage_re-ranking,1,48
experiments,"For the fine - tuning data , we generate our query - passage pairs by retrieving the top ten passages from the entire TREC - CAR corpus using BM25 .","[('For', (0, 1)), ('generate', (8, 9)), ('by retrieving', (14, 16)), ('from', (20, 21)), ('using', (27, 28))]","[('fine - tuning', (2, 5)), ('data', (5, 6)), ('our query - passage pairs', (9, 14)), ('top ten passages', (17, 20)), ('entire TREC - CAR corpus', (22, 27)), ('BM25', (28, 29))]","[['data', 'generate', 'our query - passage pairs'], ['our query - passage pairs', 'by retrieving', 'top ten passages'], ['top ten passages', 'from', 'entire TREC - CAR corpus'], ['entire TREC - CAR corpus', 'using', 'BM25']]","[['fine - tuning', 'has', 'data']]",[],[],[],"[['TREC - CAR', 'For', 'fine - tuning']]",[],[],[],passage_re-ranking,1,63
experiments,"We train it for 400 k iterations , or 12.8 M examples ( 400 k iterations * 32 pairs / batch ) , which corresponds to only 40 % of the training set .","[('train', (1, 2))]","[('400 k iterations', (4, 7)), ('12.8 M examples', (9, 12))]",[],[],[],[],[],"[['TREC - CAR', 'train', '400 k iterations'], ['TREC - CAR', 'train', '12.8 M examples']]",[],[],[],passage_re-ranking,1,66
results,"Despite training on a fraction of the data available , the proposed BERT - based models surpass the previous state - of - the - art models by a large margin on both of the tasks .","[('Despite', (0, 1)), ('on', (2, 3)), ('surpass', (16, 17)), ('by', (27, 28))]","[('training', (1, 2)), ('fraction of the data available', (4, 9)), ('proposed BERT - based models', (11, 16)), ('previous state - of - the - art models', (18, 27)), ('large margin', (29, 31))]","[['proposed BERT - based models', 'surpass', 'previous state - of - the - art models'], ['previous state - of - the - art models', 'by', 'large margin'], ['training', 'on', 'fraction of the data available']]","[['training', 'has', 'proposed BERT - based models']]","[['Results', 'Despite', 'training']]",[],[],[],[],[],[],passage_re-ranking,1,70
research-problem,Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding,[],"[('Image - Phrase Grounding', (8, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Image - Phrase Grounding']]",[],[],[],[],phrase_grounding,0,2
research-problem,We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .,[],"[('phrase grounding', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'phrase grounding']]",[],[],[],[],phrase_grounding,0,4
model,"In this work , we propose to explicitly learn a non-linear mapping of the visual and textual modalities into a common space , and do so at different granularity for each domain .","[('explicitly learn', (7, 9)), ('of', (12, 13)), ('into', (18, 19)), ('at', (26, 27)), ('for', (29, 30))]","[('non-linear mapping', (10, 12)), ('visual and textual modalities', (14, 18)), ('common space', (20, 22)), ('different granularity', (27, 29)), ('each domain', (30, 32))]","[['non-linear mapping', 'of', 'visual and textual modalities'], ['visual and textual modalities', 'into', 'common space'], ['visual and textual modalities', 'at', 'different granularity'], ['different granularity', 'for', 'each domain']]",[],"[['Model', 'explicitly learn', 'non-linear mapping']]",[],[],[],[],[],[],phrase_grounding,0,24
model,"This common space mapping is trained with weak supervision and exploited at test - time with a multi - level multimodal attention mechanism , where a natural formalism for computing attention heatmaps at each level , attended features and pertinence scoring , enables us to solve the phrase grounding task elegantly and effectively .","[('trained with', (5, 7)), ('exploited at', (10, 12)), ('with', (15, 16)), ('where', (24, 25)), ('for computing', (28, 30)), ('at', (32, 33)), ('solve', (45, 46))]","[('common space mapping', (1, 4)), ('weak supervision', (7, 9)), ('test - time', (12, 15)), ('multi - level multimodal attention mechanism', (17, 23)), ('natural formalism', (26, 28)), ('attention heatmaps', (30, 32)), ('each level', (33, 35)), ('attended features', (36, 38)), ('pertinence scoring', (39, 41)), ('phrase grounding task', (47, 50)), ('elegantly and effectively', (50, 53))]","[['common space mapping', 'exploited at', 'test - time'], ['test - time', 'with', 'multi - level multimodal attention mechanism'], ['multi - level multimodal attention mechanism', 'where', 'natural formalism'], ['natural formalism', 'solve', 'phrase grounding task'], ['natural formalism', 'for computing', 'attention heatmaps'], ['attention heatmaps', 'at', 'each level'], ['natural formalism', 'for computing', 'attended features'], ['natural formalism', 'for computing', 'pertinence scoring'], ['common space mapping', 'trained with', 'weak supervision']]","[['phrase grounding task', 'has', 'elegantly and effectively']]",[],"[['Model', 'has', 'common space mapping']]",[],[],[],[],[],phrase_grounding,0,26
hyperparameters,"We use a batch size of B = 32 , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .","[('use', (1, 2)), ('of', (5, 6))]","[('batch size', (3, 5)), ('B = 32', (6, 9))]","[['batch size', 'of', 'B = 32']]",[],"[['Hyperparameters', 'use', 'batch size']]",[],[],[],[],[],[],phrase_grounding,0,168
hyperparameters,Image - caption pairs are sampled randomly with a uniform distribution .,"[('sampled', (5, 6)), ('with', (7, 8))]","[('Image - caption pairs', (0, 4)), ('randomly', (6, 7)), ('uniform distribution', (9, 11))]","[['Image - caption pairs', 'sampled', 'randomly'], ['randomly', 'with', 'uniform distribution']]",[],[],"[['Hyperparameters', 'has', 'Image - caption pairs']]",[],[],[],[],[],phrase_grounding,0,169
hyperparameters,We train the network for 20 epochs with the Adam optimizer with lr = 0.001 where the learning rate is divided by 2 once at the 10 - th epoch and again at the 15 - th epoch .,[],[],"[['network', 'with', 'Adam optimizer'], ['Adam optimizer', 'with', 'lr = 0.001'], ['lr = 0.001', 'where', 'learning rate'], ['learning rate', 'divided by', '2'], ['2', 'again at', '15 - th epoch'], ['2', 'once at', '10 - th epoch'], ['network', 'for', '20 epochs']]",[],"[['Hyperparameters', 'train', 'network']]",[],[],[],[],[],[],phrase_grounding,0,170
hyperparameters,We use D = 1024 for common space mapping dimension and ? = 0.25 for Leaky ReLU in the non-linear mappings .,[],[],"[['? = 0.25', 'for', 'Leaky ReLU'], ['Leaky ReLU', 'in', 'non-linear mappings'], ['D = 1024', 'for', 'common space mapping dimension']]",[],[],"[['Hyperparameters', 'use', '? = 0.25'], ['Hyperparameters', 'use', 'D = 1024']]",[],[],[],[],[],phrase_grounding,0,171
hyperparameters,We regularize weights of the mappings with l 2 regularization with reg value = 0.0005 .,[],[],"[['weights', 'with', 'l 2 regularization'], ['l 2 regularization', 'with', 'reg value = 0.0005'], ['weights', 'of', 'mappings']]",[],"[['Hyperparameters', 'regularize', 'weights']]",[],[],[],[],[],[],phrase_grounding,0,172
hyperparameters,"For VGG , we take outputs from { conv 4 1 , conv 4 3 , conv5 1 , conv5 3 } and map to semantic feature maps with dimension 18181024 , and for PNAS - Net we take outputs from { Cell 5 , Cell 7 , Cell 9 , Cell 11 } pointing game accuracy attention correctness Ours Ours Ours Ours Class",[],[],"[['VGG', 'take', 'outputs'], ['outputs', 'map to', 'semantic feature maps'], ['semantic feature maps', 'with dimension', '18181024'], ['outputs', 'from', '{ conv 4 1 , conv 4 3 , conv5 1 , conv5 3 }'], ['PNAS - Net', 'take', 'outputs'], ['outputs', 'from', '{ Cell 5 , Cell 7 , Cell 9 , Cell 11 }']]",[],"[['Hyperparameters', 'For', 'VGG'], ['Hyperparameters', 'for', 'PNAS - Net']]",[],[],[],[],[],[],phrase_grounding,0,173
hyperparameters,Both visual and textual networks weights are fixed during training and only common space mapping weights are trainable .,"[('are', (6, 7)), ('fixed during', (7, 9))]","[('Both visual and textual networks weights', (0, 6)), ('training', (9, 10)), ('common space mapping weights', (12, 16)), ('trainable', (17, 18))]","[['common space mapping weights', 'are', 'trainable'], ['Both visual and textual networks weights', 'fixed during', 'training']]",[],[],"[['Hyperparameters', 'has', 'common space mapping weights'], ['Hyperparameters', 'has', 'Both visual and textual networks weights']]",[],[],[],[],[],phrase_grounding,0,176
results,The results show that our method significantly outperforms all state - of - the - art methods in all conditions and all datasets .,"[('show that', (2, 4)), ('in', (17, 18))]","[('method', (5, 6)), ('significantly outperforms', (6, 8)), ('all state - of - the - art methods', (8, 17)), ('all conditions and all datasets', (18, 23))]","[['significantly outperforms', 'in', 'all conditions and all datasets']]","[['method', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all state - of - the - art methods']]","[['Results', 'show that', 'method']]",[],[],[],[],[],[],phrase_grounding,0,187
results,"For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .","[('on', (51, 52))]","[('model', (11, 12)), ('state - of - the - art', (43, 50)), ('Flickr30 k', (52, 54))]",[],"[['Flickr30 k', 'has', 'model']]","[['Results', 'on', 'Flickr30 k']]",[],[],[],"[['state - of - the - art', 'on', 'all categories'], ['state - of - the - art', 'on', 'both metrics']]","[['consistently outperforms', 'has', 'state - of - the - art']]",[],phrase_grounding,0,188
results,We observe that our method obtains a higher performance on almost all categories even when VGG16 is used as the visual backbone .,"[('observe that', (1, 3)), ('obtains', (5, 6)), ('on', (9, 10))]","[('our method', (3, 5)), ('higher performance', (7, 9)), ('almost all categories', (10, 13))]","[['our method', 'obtains', 'higher performance'], ['higher performance', 'on', 'almost all categories']]",[],[],[],[],"[['Flickr30 k', 'observe that', 'our method']]",[],[],[],phrase_grounding,0,189
results,The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .,[],[],[],"[['PNASNet', 'has', 'consistently outperforms']]",[],[],[],"[['model', 'based on', 'PNASNet']]",[],[],[],phrase_grounding,0,190
results,It shows that the 3rd level dominates the selection while the 4th level is also important for several categories such as scene and animals .,"[('dominates', (6, 7)), ('important for', (15, 17)), ('such as', (19, 21))]","[('3rd level', (4, 6)), ('selection', (8, 9)), ('4th level', (11, 13)), ('several categories', (17, 19)), ('scene and animals', (21, 24))]","[['3rd level', 'dominates', 'selection'], ['4th level', 'important for', 'several categories'], ['several categories', 'such as', 'scene and animals']]",[],[],"[['Results', 'show that', '3rd level'], ['Results', 'show that', '4th level']]",[],[],[],[],[],phrase_grounding,0,192
results,The 1st level is exploited mostly for the animals and people categories .,"[('exploited mostly for', (4, 7))]","[('1st level', (1, 3)), ('animals and people categories', (8, 12))]","[['1st level', 'exploited mostly for', 'animals and people categories']]",[],[],"[['Results', 'show that', '1st level']]",[],[],[],[],[],phrase_grounding,0,193
results,"The full sentence selection relies mostly on the 3rd level as well , while for some sentences the 4th model has been selected .","[('relies mostly on', (4, 7)), ('for', (14, 15)), ('been', (21, 22))]","[('full sentence selection', (1, 4)), ('3rd level', (8, 10)), ('some sentences', (15, 17)), ('4th model', (18, 20)), ('selected', (22, 23))]","[['4th model', 'been', 'selected'], ['full sentence selection', 'relies mostly on', '3rd level']]","[['some sentences', 'has', '4th model']]","[['Results', 'for', 'some sentences']]","[['Results', 'has', 'full sentence selection']]",[],[],[],[],[],phrase_grounding,0,194
ablation-analysis,"The results in rows 1 , 2 show that using level - attention mechanism based on multi-level feature maps significantly improves the performance over single visual - textual feature comparison .","[('using', (9, 10)), ('based on', (14, 16)), ('significantly improves', (19, 21)), ('over', (23, 24))]","[('level - attention mechanism', (10, 14)), ('multi-level feature maps', (16, 19)), ('performance', (22, 23)), ('single visual - textual feature comparison', (24, 30))]","[['level - attention mechanism', 'based on', 'multi-level feature maps'], ['level - attention mechanism', 'significantly improves', 'performance'], ['performance', 'over', 'single visual - textual feature comparison']]",[],"[['Ablation analysis', 'using', 'level - attention mechanism']]",[],[],[],[],[],[],phrase_grounding,0,202
ablation-analysis,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping with a linear one significantly degrades the performance .","[('see that', (12, 14)), ('in', (16, 17)), ('is', (19, 20)), ('replacing', (24, 25)), ('with', (27, 28)), ('significantly degrades', (31, 33))]","[('non-linear mapping', (14, 16)), ('model', (18, 19)), ('really important', (20, 22)), ('any mapping', (25, 27)), ('linear one', (29, 31)), ('performance', (34, 35))]","[['non-linear mapping', 'in', 'model'], ['non-linear mapping', 'is', 'really important'], ['any mapping', 'with', 'linear one'], ['linear one', 'significantly degrades', 'performance']]",[],"[['Ablation analysis', 'see that', 'non-linear mapping'], ['Ablation analysis', 'replacing', 'any mapping']]",[],[],[],[],[],[],phrase_grounding,0,204
ablation-analysis,"We can also see that non-linear mapping seems more important on the visual side , but best results are obtained with both text and visual non-linear mappings .","[('also see that', (2, 5)), ('seems', (7, 8)), ('on', (10, 11))]","[('non-linear mapping', (5, 7)), ('more important', (8, 10)), ('visual side', (12, 14))]","[['non-linear mapping', 'seems', 'more important'], ['more important', 'on', 'visual side']]",[],"[['Ablation analysis', 'also see that', 'non-linear mapping']]",[],[],[],[],[],[],phrase_grounding,0,205
ablation-analysis,"The results in rows 1 , 3 and 2 , 6 show the importance of using a strong contextualized text embedding as the performance drops significantly .","[('show', (11, 12)), ('of using', (14, 16)), ('as', (21, 22))]","[('importance', (13, 14)), ('strong contextualized text embedding', (17, 21)), ('performance', (23, 24)), ('drops significantly', (24, 26))]","[['importance', 'of using', 'strong contextualized text embedding'], ['strong contextualized text embedding', 'as', 'performance']]","[['performance', 'has', 'drops significantly']]","[['Ablation analysis', 'show', 'importance']]",[],[],[],[],[],[],phrase_grounding,0,208
ablation-analysis,"We also study the use of softmax on the heatmaps , comparing rows 2 , 8 , we see that applying softmax leads to a very negative effect on the performance .","[('on', (7, 8)), ('applying', (20, 21)), ('leads to', (22, 24))]","[('softmax', (6, 7)), ('very negative effect', (25, 28)), ('performance', (30, 31))]","[['softmax', 'leads to', 'very negative effect'], ['very negative effect', 'on', 'performance']]",[],"[['Ablation analysis', 'applying', 'softmax']]",[],[],[],[],[],[],phrase_grounding,0,209
research-problem,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,[],"[('Predicting Prosodic Prominence from Text', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Predicting Prosodic Prominence from Text']]",[],[],[],[],prosody_prediction,0,2
research-problem,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,[],"[('predicting prosodic prominence from written text', (14, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'predicting prosodic prominence from written text']]",[],[],[],[],prosody_prediction,0,4
dataset,"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .","[('for predicting', (11, 13)), ('from', (15, 16)), ('based on', (19, 21)), ('containing', (28, 29)), ('for', (34, 35)), ('of', (42, 43)), ('read by', (47, 49))]","[('prosodic prominence', (13, 15)), ('text', (16, 17)), ('recently published Libri TTS corpus', (22, 27)), ('automatically generated prosodic prominence labels', (29, 34)), ('over 260 hours', (35, 38)), ('2.8 million words', (39, 42)), ('English audio books', (43, 46)), ('1230 different speakers', (49, 52))]","[['prosodic prominence', 'based on', 'recently published Libri TTS corpus'], ['prosodic prominence', 'containing', 'automatically generated prosodic prominence labels'], ['automatically generated prosodic prominence labels', 'read by', '1230 different speakers'], ['automatically generated prosodic prominence labels', 'for', 'over 260 hours'], ['automatically generated prosodic prominence labels', 'for', '2.8 million words'], ['2.8 million words', 'of', 'English audio books'], ['prosodic prominence', 'from', 'text']]",[],"[['Dataset', 'for predicting', 'prosodic prominence']]",[],[],[],[],[],[],prosody_prediction,0,16
dataset,To our knowledge this will be the largest publicly available dataset with prosodic annotations .,"[('will be', (4, 6)), ('with', (11, 12))]","[('largest publicly available dataset', (7, 11)), ('prosodic annotations', (12, 14))]","[['largest publicly available dataset', 'with', 'prosodic annotations']]",[],"[['Dataset', 'will be', 'largest publicly available dataset']]",[],[],[],[],[],[],prosody_prediction,0,17
research-problem,Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,[],"[('Prosody prediction', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Prosody prediction']]",[],[],[],[],prosody_prediction,0,20
experimental-setup,We performed experiments with the following models :,[],"[('models', (6, 7))]",[],[],[],"[['Experimental setup', 'has', 'models']]",[],[],[],[],"[['models', 'name', 'BERT - base uncased'], ['models', 'name', '3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )'], ['models', 'name', 'Minitagger ( SVM ) ) + GloVe'], ['models', 'name', 'MarMoT ( CRF )'], ['models', 'name', 'Majority class per word']]",prosody_prediction,0,84
experimental-setup,"BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word",[],"[('BERT - base uncased', (0, 4)), ('3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )', (4, 17)), ('Minitagger ( SVM ) ) + GloVe', (24, 31)), ('MarMoT ( CRF )', (31, 35)), ('Majority class per word', (35, 39))]",[],[],[],[],[],[],[],[],[],prosody_prediction,0,85
experimental-setup,"We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library , 3 which we further fine - tune during training .","[('use', (1, 2)), ('of', (6, 7)), ('available in', (8, 10)), ('further', (18, 19)), ('during', (22, 23))]","[('Huggingface PyTorch implementation', (3, 6)), ('BERT', (7, 8)), ('pytorch transformers library', (11, 14)), ('fine - tune', (19, 22)), ('training', (23, 24))]","[['Huggingface PyTorch implementation', 'of', 'BERT'], ['BERT', 'available in', 'pytorch transformers library'], ['BERT', 'further', 'fine - tune'], ['fine - tune', 'during', 'training']]",[],"[['Experimental setup', 'use', 'Huggingface PyTorch implementation']]",[],[],[],[],[],[],prosody_prediction,0,88
experimental-setup,"We take the last hidden layer of BERT and train a single fully - connected classifier layer on top of it , mapping the representation of each word to the labels .",[],[],"[['representation', 'of', 'each word'], ['each word', 'to', 'labels'], ['last hidden layer', 'of', 'BERT'], ['last hidden layer', 'train', 'single fully - connected classifier layer'], ['single fully - connected classifier layer', 'mapping', 'representation'], ['representation', 'of', 'each word'], ['each word', 'to', 'labels'], ['single fully - connected classifier layer', 'on', 'top']]",[],"[['Experimental setup', 'take', 'last hidden layer']]",[],[],[],[],[],[],prosody_prediction,0,89
experimental-setup,For our experiments we use the smaller BERT - base model using the uncased alternative .,"[('using', (11, 12))]","[('smaller BERT - base model', (6, 11)), ('uncased alternative', (13, 15))]","[['smaller BERT - base model', 'using', 'uncased alternative']]",[],[],"[['Experimental setup', 'use', 'smaller BERT - base model']]",[],[],[],[],[],prosody_prediction,0,90
experimental-setup,We use a batch size of 32 and fine - tune the model for 2 epochs .,"[('of', (5, 6)), ('fine - tune', (8, 11)), ('for', (13, 14))]","[('batch size', (3, 5)), ('32', (6, 7)), ('model', (12, 13)), ('2 epochs', (14, 16))]","[['batch size', 'of', '32'], ['model', 'for', '2 epochs']]",[],"[['Experimental setup', 'fine - tune', 'model']]","[['Experimental setup', 'use', 'batch size']]",[],[],[],[],[],prosody_prediction,0,91
experimental-setup,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,"[('For', (0, 1)), ('use', (3, 4))]","[('BiLSTM', (1, 2)), ('pre-trained 300D Glo Ve 840B word embeddings', (4, 11))]","[['BiLSTM', 'use', 'pre-trained 300D Glo Ve 840B word embeddings']]",[],"[['Experimental setup', 'For', 'BiLSTM']]",[],[],[],[],[],[],prosody_prediction,0,92
experimental-setup,"As with BERT , we add one fullyconnected classifier layer on top of the BiLSTM , mapping the representation of each word to the labels .",[],[],"[['one fullyconnected classifier layer', 'mapping', 'representation'], ['one fullyconnected classifier layer', 'on top of', 'BiLSTM']]",[],"[['Experimental setup', 'add', 'one fullyconnected classifier layer']]",[],[],[],[],[],[],prosody_prediction,0,94
experimental-setup,We use a dropout of 0.2 between the layers of the BiLSTM .,"[('of', (4, 5)), ('between', (6, 7))]","[('dropout', (3, 4)), ('0.2', (5, 6)), ('layers of the BiLSTM', (8, 12))]","[['dropout', 'of', '0.2'], ['0.2', 'between', 'layers of the BiLSTM']]",[],[],"[['Experimental setup', 'use', 'dropout']]",[],[],[],[],[],prosody_prediction,0,95
experimental-setup,"For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D Glo Ve 840B word embeddings as features , with context - size 1 , i.e. including the previous and the next word in the context .","[('use', (4, 5)), ('using', (9, 10)), ('of', (12, 13)), ('as', (21, 22)), ('with', (24, 25))]","[('SVM', (2, 3)), ('Minitagger 4 implementation', (5, 8)), ('each dimension', (10, 12)), ('pre-trained 300D Glo Ve 840B word embeddings', (14, 21)), ('features', (22, 23)), ('context - size 1', (25, 29))]","[['SVM', 'use', 'Minitagger 4 implementation'], ['Minitagger 4 implementation', 'using', 'each dimension'], ['each dimension', 'of', 'pre-trained 300D Glo Ve 840B word embeddings'], ['pre-trained 300D Glo Ve 840B word embeddings', 'with', 'context - size 1'], ['pre-trained 300D Glo Ve 840B word embeddings', 'as', 'features']]",[],[],"[['Experimental setup', 'For', 'SVM']]",[],[],[],[],[],prosody_prediction,0,97
experimental-setup,For the conditional random field ( CRF ) model we use MarMot 5 by with the default configuration .,"[('use', (10, 11)), ('with', (14, 15))]","[('conditional random field ( CRF ) model', (2, 9)), ('MarMot', (11, 12)), ('default configuration', (16, 18))]","[['conditional random field ( CRF ) model', 'use', 'MarMot'], ['MarMot', 'with', 'default configuration']]",[],[],"[['Experimental setup', 'For', 'conditional random field ( CRF ) model']]",[],[],[],[],[],prosody_prediction,0,98
code,All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,[],"[('https://github.com/Helsinki - NLP / prosody', (19, 24))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/Helsinki - NLP / prosody']]",[],[],[],[],prosody_prediction,0,101
results,All models reach over 80 % in the 2 - way classification task while 3 - way classification accuracy stays below 70 % for all of them .,"[('reach over', (2, 4)), ('in', (6, 7)), ('stays below', (19, 21))]","[('All models', (0, 2)), ('80 %', (4, 6)), ('2 - way classification task', (8, 13)), ('3 - way classification accuracy', (14, 19)), ('70 %', (21, 23))]","[['3 - way classification accuracy', 'stays below', '70 %'], ['All models', 'reach over', '80 %'], ['80 %', 'in', '2 - way classification task']]","[['All models', 'has', '3 - way classification accuracy']]",[],"[['Results', 'has', 'All models']]",[],[],[],[],[],prosody_prediction,0,106
results,"The BERTbased model gets the highest accuracy of 83.2 % and 68.6 % in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/","[('gets', (3, 4)), ('of', (7, 8)), ('in', (13, 14))]","[('BERTbased model', (1, 3)), ('highest accuracy', (5, 7)), ('83.2 % and 68.6 %', (8, 13)), ('2 - way and 3 - way classification tasks', (15, 24))]","[['BERTbased model', 'gets', 'highest accuracy'], ['highest accuracy', 'of', '83.2 % and 68.6 %'], ['83.2 % and 68.6 %', 'in', '2 - way and 3 - way classification tasks']]",[],[],"[['Results', 'has', 'BERTbased model']]",[],[],[],[],[],prosody_prediction,0,107
results,The 3layer BiLSTM achieves 82.1 % in the 2 - way classification and 66.4 % in the 3 - way classification task .,[],[],"[['3layer BiLSTM', 'achieves', '82.1 %'], ['82.1 %', 'in', '2 - way classification'], ['3layer BiLSTM', 'achieves', '66.4 %'], ['66.4 %', 'in', '3 - way classification task']]",[],[],"[['Results', 'has', '3layer BiLSTM']]",[],[],[],[],[],prosody_prediction,0,111
results,"The traditional feature - based classifiers perform slightly below the neural network models , with the CRF obtaining 81.8 % and 66.4 % for the two classification tasks , respectively .","[('perform', (6, 7)), ('with', (14, 15)), ('obtaining', (17, 18)), ('for', (23, 24))]","[('traditional feature - based classifiers', (1, 6)), ('slightly below', (7, 9)), ('neural network models', (10, 13)), ('CRF', (16, 17)), ('81.8 % and 66.4 %', (18, 23)), ('two classification tasks', (25, 28))]","[['traditional feature - based classifiers', 'perform', 'slightly below'], ['neural network models', 'with', 'CRF'], ['CRF', 'obtaining', '81.8 % and 66.4 %'], ['81.8 % and 66.4 %', 'for', 'two classification tasks']]","[['slightly below', 'has', 'neural network models']]",[],"[['Results', 'has', 'traditional feature - based classifiers']]",[],[],[],[],[],prosody_prediction,0,112
results,The Minitagger SVM model 's test accuracies are slightly lower than the CRF 's with 80.8 % and 65.4 % test accuracies .,"[('are', (7, 8)), ('than', (10, 11)), ('with', (14, 15))]","[(""Minitagger SVM model 's test accuracies"", (1, 7)), ('slightly lower', (8, 10)), (""CRF 's"", (12, 14)), ('80.8 % and 65.4 % test accuracies', (15, 22))]","[[""Minitagger SVM model 's test accuracies"", 'are', 'slightly lower'], ['slightly lower', 'than', ""CRF 's""], [""CRF 's"", 'with', '80.8 % and 65.4 % test accuracies']]",[],[],"[['Results', 'has', ""Minitagger SVM model 's test accuracies""]]",[],[],[],[],[],prosody_prediction,0,113
results,Finally taking a simple majority class per word gives 80.2 % for the 2 - way classification task and 62.4 % for the 3 - way classification task .,[],[],"[['simple majority class per word', 'gives', '80.2 %'], ['80.2 %', 'for', '2 - way classification task'], ['simple majority class per word', 'gives', '62.4 %'], ['62.4 %', 'for', '3 - way classification task']]",[],"[['Results', 'taking', 'simple majority class per word']]",[],[],[],[],[],[],prosody_prediction,0,114
results,For most of the models the biggest improvement in performance is achieved when moving from 1 % of the training examples to 5 % .,"[('For', (0, 1)), ('in', (8, 9)), ('achieved when', (11, 13)), ('from', (14, 15)), ('of', (17, 18)), ('to', (21, 22))]","[('most of the models', (1, 5)), ('biggest improvement', (6, 8)), ('performance', (9, 10)), ('moving', (13, 14)), ('1 %', (15, 17)), ('training examples', (19, 21)), ('5 %', (22, 24))]","[['biggest improvement', 'in', 'performance'], ['biggest improvement', 'achieved when', 'moving'], ['moving', 'from', '1 %'], ['1 %', 'of', 'training examples'], ['1 %', 'to', '5 %']]","[['most of the models', 'has', 'biggest improvement']]","[['Results', 'For', 'most of the models']]",[],[],[],[],[],[],prosody_prediction,0,121
results,All models have reached close to their full predictive capacity with only 10 % of the training examples .,"[('reached close to', (3, 6)), ('with', (10, 11))]","[('full predictive capacity', (7, 10)), ('only 10 % of the training examples', (11, 18))]","[['full predictive capacity', 'with', 'only 10 % of the training examples']]",[],[],[],[],"[['All models', 'reached close to', 'full predictive capacity']]",[],[],[],prosody_prediction,0,122
results,"As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :","[('with', (26, 27))]","[('manually annotated test set from The Boston University radio news corpus', (28, 39))]",[],[],"[['Results', 'with', 'manually annotated test set from The Boston University radio news corpus']]",[],[],[],[],[],"[['manually annotated test set from The Boston University radio news corpus', 'has', 'good results']]",prosody_prediction,0,127
results,The good results 6 from this experiment provide further support for the quality of the new dataset .,"[('provide', (7, 8)), ('for', (10, 11))]","[('good results', (1, 3)), ('further support', (8, 10)), ('quality of the new dataset', (12, 17))]","[['good results', 'provide', 'further support'], ['further support', 'for', 'quality of the new dataset']]",[],[],[],[],[],[],[],[],prosody_prediction,0,131
results,Notice also that the difference between BERT and BiLSTM is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,"[('between', (5, 6))]","[('difference', (4, 5)), ('BERT and BiLSTM', (6, 9)), ('much bigger', (10, 12))]","[['difference', 'between', 'BERT and BiLSTM']]","[['difference', 'has', 'much bigger']]",[],[],[],[],[],"[['manually annotated test set from The Boston University radio news corpus', 'has', 'difference']]",[],prosody_prediction,0,132
research-problem,Identifying Well - formed Natural Language Questions,[],"[('Identifying Well - formed Natural Language Questions', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Identifying Well - formed Natural Language Questions']]",[],[],[],[],query_wellformedness,0,2
model,"Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .","[('to predict', (9, 11)), ('is', (15, 16))]","[('given query', (13, 15)), ('well - formed natural language question', (17, 23))]","[['given query', 'is', 'well - formed natural language question']]",[],"[['Model', 'to predict', 'given query']]",[],[],[],[],[],[],query_wellformedness,0,19
dataset,"We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .","[('construct and publicly release', (1, 5)), ('annotated with', (10, 12)), ('of being', (14, 16))]","[('dataset of 25,100 queries', (6, 10)), ('probability', (13, 14)), ('well - formed natural language question', (17, 23))]","[['dataset of 25,100 queries', 'annotated with', 'probability'], ['probability', 'of being', 'well - formed natural language question']]",[],"[['Dataset', 'construct and publicly release', 'dataset of 25,100 queries']]",[],[],[],[],[],[],query_wellformedness,0,20
model,We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .,"[('train', (2, 3)), ('uses', (11, 12)), ('extracted from', (17, 19)), ('on', (21, 22))]","[('feed - forward neural network classifier', (4, 10)), ('lexical and syntactic features', (13, 17)), ('query', (20, 21)), ('data', (23, 24))]","[['feed - forward neural network classifier', 'uses', 'lexical and syntactic features'], ['lexical and syntactic features', 'extracted from', 'query'], ['query', 'on', 'data']]",[],"[['Model', 'train', 'feed - forward neural network classifier']]",[],[],[],[],[],[],query_wellformedness,0,21
dataset,Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .,"[('available for download at', (3, 7))]","[('http://goo.gl/language/ query-wellformedness', (7, 9))]",[],[],"[['Dataset', 'available for download at', 'http://goo.gl/language/ query-wellformedness']]",[],[],[],[],[],[],query_wellformedness,0,24
results,"The best performance obtained is 70.7 % while using word - 1 , 2 - grams and POS - 1 , 2 , 3 - grams as features .","[('obtained', (3, 4)), ('while using', (7, 9))]","[('best performance', (1, 3)), ('70.7 %', (5, 7)), ('word - 1 , 2 - grams', (9, 16)), ('POS - 1 , 2 , 3 - grams', (17, 26))]","[['best performance', 'obtained', '70.7 %'], ['70.7 %', 'while using', 'word - 1 , 2 - grams'], ['70.7 %', 'while using', 'POS - 1 , 2 , 3 - grams']]",[],[],"[['Results', 'has', 'best performance']]",[],[],[],[],[],query_wellformedness,0,58
results,Using POS n-grams gave a strong boost of 5.2 points over word unigrams and bigrams .,"[('Using', (0, 1)), ('gave', (3, 4)), ('of', (7, 8)), ('over', (10, 11))]","[('POS n-grams', (1, 3)), ('strong boost', (5, 7)), ('5.2 points', (8, 10)), ('word unigrams and bigrams', (11, 15))]","[['POS n-grams', 'gave', 'strong boost'], ['strong boost', 'of', '5.2 points'], ['5.2 points', 'over', 'word unigrams and bigrams']]",[],"[['Results', 'Using', 'POS n-grams']]",[],[],[],[],[],[],query_wellformedness,0,59
results,"Although character - 3 , 4 grams gave improvement over word unigrams and bigrams , the performance did not sustain when combined with POS tags .","[('gave improvement over', (7, 10)), ('not', (18, 19)), ('when', (20, 21)), ('with', (22, 23))]","[('character - 3 , 4 grams', (1, 7)), ('word unigrams and bigrams', (10, 14)), ('performance', (16, 17)), ('sustain', (19, 20)), ('combined', (21, 22)), ('POS tags', (23, 25))]","[['character - 3 , 4 grams', 'gave improvement over', 'word unigrams and bigrams'], ['character - 3 , 4 grams', 'when', 'combined'], ['combined', 'with', 'POS tags'], ['performance', 'not', 'sustain']]","[['combined', 'has', 'performance']]",[],"[['Results', 'has', 'character - 3 , 4 grams']]",[],[],[],[],[],query_wellformedness,0,60
baselines,The majority class baseline is 61.5 % which corresponds to all queries being classified non-wellformed .,"[('is', (4, 5)), ('corresponds to', (8, 10))]","[('majority class baseline', (1, 4)), ('61.5 %', (5, 7)), ('all queries being classified non-wellformed', (10, 15))]","[['majority class baseline', 'corresponds to', 'all queries being classified non-wellformed'], ['majority class baseline', 'is', '61.5 %']]",[],[],"[['Baselines', 'has', 'majority class baseline']]",[],[],[],[],[],query_wellformedness,0,71
baselines,The question word baseline that classifies any query starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,"[('classifies', (5, 6)), ('starting with', (8, 10))]","[('question word baseline', (1, 4)), ('any query', (6, 8)), ('question word', (11, 13))]","[['question word baseline', 'classifies', 'any query'], ['any query', 'starting with', 'question word']]",[],[],"[['Baselines', 'has', 'question word baseline']]",[],[],[],[],[],query_wellformedness,0,72
research-problem,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,[],"[('natural language understanding', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language understanding']]",[],[],[],[],question-answering,7,4
model,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,"[('propose', (4, 5)), ('called', (13, 14)), ('for', (20, 21))]","[('a novel class of memory augmented neural networks', (5, 13)), ('Neural Semantic Encoders ( NSE )', (14, 20)), ('natural language understanding', (21, 24))]","[['a novel class of memory augmented neural networks', 'called', 'Neural Semantic Encoders ( NSE )'], ['Neural Semantic Encoders ( NSE )', 'for', 'natural language understanding']]",[],"[['Model', 'propose', 'a novel class of memory augmented neural networks']]",[],[],[],[],[],[],question-answering,7,18
model,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,"[('to access', (11, 13)), ('during', (16, 17)), ('efficiently delivering', (22, 24))]","[('NSE', (0, 1)), ('variable sized encoding memory', (3, 7)), ('entire input sequence', (13, 16)), ('reading process', (18, 20)), ('long - term dependencies overtime', (24, 29))]","[['variable sized encoding memory', 'efficiently delivering', 'long - term dependencies overtime'], ['variable sized encoding memory', 'to access', 'entire input sequence'], ['entire input sequence', 'during', 'reading process']]","[['NSE', 'has', 'variable sized encoding memory']]",[],"[['Model', 'name', 'NSE']]",[],[],[],[],[],question-answering,7,20
model,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .","[('evolves', (3, 4)), ('maintains', (6, 7)), ('of', (9, 10)), ('through', (13, 14))]","[('encoding memory', (1, 3)), ('overtime', (4, 5)), ('memory', (8, 9)), ('input sequence', (11, 13)), ('read , compose and write operations', (14, 20))]","[['encoding memory', 'maintains', 'memory'], ['memory', 'of', 'input sequence'], ['input sequence', 'through', 'read , compose and write operations'], ['encoding memory', 'evolves', 'overtime']]",[],[],"[['Model', 'has', 'encoding memory']]",[],[],[],[],[],question-answering,7,21
model,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,"[('sequentially processes', (1, 3)), ('supports', (6, 7))]","[('the input', (3, 5)), ('word compositionality', (7, 9))]",[],[],[],[],[],"[['NSE', 'supports', 'word compositionality'], ['NSE', 'sequentially processes', 'the input']]",[],[],[],question-answering,7,22
model,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,"[('read from and write to', (2, 7)), ('access', (18, 19)), ('supporting', (24, 25))]","[('set of relevant encoding memories simultaneously', (8, 14)), ('shared encoding memory', (20, 23)), ('knowledge and representation sharing', (25, 29))]","[['shared encoding memory', 'supporting', 'knowledge and representation sharing']]",[],[],[],[],"[['NSE', 'access', 'shared encoding memory'], ['NSE', 'read from and write to', 'set of relevant encoding memories simultaneously']]",[],[],[],question-answering,7,23
research-problem,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",[],"[('NLU', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'NLU']]",[],[],[],[],question-answering,7,24
experimental-setup,The models are trained using Adam with hyperparameters selected on development set .,"[('trained using', (3, 5))]","[('Adam', (5, 6))]",[],[],"[['Experimental setup', 'trained using', 'Adam']]",[],[],[],[],[],[],question-answering,7,127
experimental-setup,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,"[('chose', (1, 2)), ('for', (7, 8)), ('on', (12, 13))]","[('two one - layer LSTM', (2, 7)), ('read / write modules', (8, 12)), ('tasks other than QA', (14, 18))]","[['two one - layer LSTM', 'for', 'read / write modules'], ['read / write modules', 'on', 'tasks other than QA']]",[],"[['Experimental setup', 'chose', 'two one - layer LSTM']]",[],[],[],[],[],[],question-answering,7,128
experimental-setup,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,"[('obtained for', (16, 18))]","[('pre-trained 300 - D Glove 840B vectors', (1, 8)), ('100 - D Glove 6B vectors', (9, 15)), ('word embeddings', (19, 21))]",[],"[['word embeddings', 'has', 'pre-trained 300 - D Glove 840B vectors'], ['word embeddings', 'has', '100 - D Glove 6B vectors']]","[['Experimental setup', 'obtained for', 'word embeddings']]",[],[],[],[],[],[],question-answering,7,129
experimental-setup,We crop or pad the input sequence to a fixed length .,"[('crop or pad', (1, 4)), ('to', (7, 8))]","[('input sequence', (5, 7)), ('fixed length', (9, 11))]","[['input sequence', 'to', 'fixed length']]",[],"[['Experimental setup', 'crop or pad', 'input sequence']]",[],[],[],[],[],[],question-answering,7,132
experimental-setup,The models were regularized by using dropouts and an l 2 weight decay .,"[('regularized by', (3, 5))]","[('dropouts', (6, 7)), ('l 2 weight decay', (9, 13))]",[],[],"[['Experimental setup', 'regularized by', 'dropouts'], ['Experimental setup', 'regularized by', 'l 2 weight decay']]",[],[],[],[],[],[],question-answering,7,134
tasks,Natural Language Inference,"[('Natural Language Inference', (0, 3))]",[],[],[],[],[],[],[],[],[],"[['Tasks', 'Natural Language Inference', 'Hyperparameters']]",question-answering,7,135
tasks,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .","[('hidden layer', (7, 9)), ('with', (9, 10))]","[('1024 units', (10, 12)), ('ReLU activation', (13, 15)), ('sof tmax layer', (17, 20))]","[['1024 units', 'with', 'ReLU activation'], ['1024 units', 'with', 'sof tmax layer']]",[],[],[],[],"[['Hyperparameters', 'hidden layer', '1024 units']]",[],[],[],question-answering,7,142
tasks,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .","[('batch size', (3, 5)), ('initial learning rate', (9, 12)), ('l 2 regularizer strength', (17, 21)), ('train', (28, 29))]","[('128', (6, 7)), ('3e - 4', (13, 16)), ('3 e - 5', (22, 26)), ('40 epochs', (32, 34))]",[],[],[],[],[],"[['Hyperparameters', 'initial learning rate', '3e - 4'], ['Hyperparameters', 'l 2 regularizer strength', '3 e - 5'], ['Hyperparameters', 'batch size', '128'], ['Hyperparameters', 'train', '40 epochs'], ['Hyperparameters', 'initial learning rate', '3e - 4'], ['Hyperparameters', 'l 2 regularizer strength', '3 e - 5'], ['Hyperparameters', 'batch size', '128'], ['Hyperparameters', 'train', '40 epochs'], ['Hyperparameters', 'initial learning rate', '3e - 4'], ['Hyperparameters', 'l 2 regularizer strength', '3 e - 5'], ['Hyperparameters', 'initial learning rate', '3e - 4']]",[],[],[],question-answering,7,143
tasks,Our MMA - NSE attention model is similar to the LSTM attention model .,[],"[('MMA - NSE attention model', (1, 6))]",[],[],[],[],[],[],[],"[['Results', 'has', 'MMA - NSE attention model'], ['Results', 'has', 'MMA - NSE attention model']]",[],question-answering,7,158
tasks,This model obtained 85.4 % accuracy score .,"[('obtained', (2, 3))]","[('85.4 % accuracy score', (3, 7))]",[],[],[],[],[],"[['MMA - NSE attention model', 'obtained', '85.4 % accuracy score']]",[],[],[],question-answering,7,160
tasks,Answer Sentence Selection,"[('Answer Sentence Selection', (0, 3))]",[],[],[],[],[],[],[],[],[],"[['Tasks', 'Answer Sentence Selection', 'Hyperparameters']]",question-answering,7,162
tasks,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .","[('batch size', (3, 5)), ('initial learning rate', (9, 12)), ('train the model for', (19, 23))]","[('4', (6, 7)), ('1 e - 5', (13, 17)), ('10 epochs', (23, 25))]",[],[],[],[],[],"[['Hyperparameters', 'initial learning rate', '1 e - 5'], ['Hyperparameters', 'batch size', '4'], ['Hyperparameters', 'train the model for', '10 epochs']]",[],[],[],question-answering,7,173
tasks,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,"[('used', (1, 2)), ('afterword', (5, 6)), ('no', (8, 9))]","[('40 % dropouts', (2, 5)), ('embeddings', (6, 7)), ('l 2 weight decay', (9, 13))]","[['40 % dropouts', 'no', 'l 2 weight decay'], ['40 % dropouts', 'afterword', 'embeddings']]",[],[],[],[],"[['Hyperparameters', 'used', '40 % dropouts']]",[],[],[],question-answering,7,174
tasks,The word embeddings are pre-trained 300 - D Glove 840B vectors .,"[('are', (3, 4))]","[('word embeddings', (1, 3)), ('pre-trained 300 - D Glove 840B vectors', (4, 11))]","[['word embeddings', 'are', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'word embeddings']]","[['word embeddings', 'has', 'linear mapping layer']]",question-answering,7,175
tasks,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .","[('transforms', (8, 9)), ('to', (15, 16))]","[('linear mapping layer', (5, 8)), ('300 - D word embeddings', (10, 15)), ('512- D LSTM inputs', (17, 21))]","[['linear mapping layer', 'transforms', '300 - D word embeddings'], ['300 - D word embeddings', 'to', '512- D LSTM inputs']]",[],[],[],[],[],[],[],[],question-answering,7,176
tasks,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,[],[],"[['MMA - NSE attention model', 'exceeds', 'NASM'], ['NASM', 'by approximately', '1 %'], ['1 %', 'on', 'MAP'], ['NASM', 'by approximately', '0.8 %'], ['0.8 %', 'on', 'MRR']]",[],[],[],[],[],[],[],[],question-answering,7,181
tasks,Sentence Classification,"[('Sentence Classification', (0, 2))]",[],[],[],[],[],[],[],[],[],"[['Tasks', 'Sentence Classification', 'Hyperparameters']]",question-answering,7,186
tasks,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,"[('first layer of the MLP', (1, 6)), ('for', (14, 15))]","[('ReLU activation', (7, 9)), ('1024 or 300 units', (10, 14)), ('binary or fine - grained setting', (15, 21))]","[['1024 or 300 units', 'for', 'binary or fine - grained setting']]",[],[],[],[],"[['Hyperparameters', 'first layer of the MLP', 'ReLU activation'], ['Hyperparameters', 'first layer of the MLP', '1024 or 300 units']]",[],[],[],question-answering,7,191
tasks,The second layer is a sof tmax layer .,"[('second layer', (1, 3))]","[('sof tmax layer', (5, 8))]",[],[],[],[],[],"[['Hyperparameters', 'second layer', 'sof tmax layer']]",[],[],[],question-answering,7,192
tasks,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,"[('read / write modules', (1, 5)), ('with', (11, 12)), ('word embeddings', (17, 19))]","[('two one - layer LSTM', (6, 11)), ('300 hidden units', (12, 15)), ('pre-trained 300 - D Glove 840B vectors', (21, 28))]","[['two one - layer LSTM', 'with', '300 hidden units']]",[],[],[],[],"[['Hyperparameters', 'read / write modules', 'two one - layer LSTM'], ['Hyperparameters', 'word embeddings', 'pre-trained 300 - D Glove 840B vectors']]",[],[],[],question-answering,7,193
tasks,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",[],[],[],[],[],[],[],"[['Hyperparameters', 'batch size', '64'], ['Hyperparameters', 'train', '25 epochs']]",[],[],[],question-answering,7,194
tasks,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,"[('outperformed', (2, 3)), ('set', (6, 7)), ('on', (16, 17))]","[('Our model', (0, 2)), ('DMN', (4, 5)), ('state - of - the - art results', (8, 16)), ('both subtasks', (17, 19))]","[['Our model', 'outperformed', 'DMN'], ['Our model', 'set', 'state - of - the - art results'], ['state - of - the - art results', 'on', 'both subtasks']]",[],[],[],[],[],[],"[['Results', 'has', 'Our model']]",[],question-answering,7,199
tasks,Document Sentiment Analysis,"[('Document Sentiment Analysis', (0, 3))]",[],[],[],[],[],[],[],[],[],"[['Tasks', 'Document Sentiment Analysis', 'Hyperparameters']]",question-answering,7,200
tasks,We stack a NSE or LSTM on the top of another NSE for document modeling .,"[('stack', (1, 2)), ('on the top of', (6, 10)), ('for', (12, 13))]","[('NSE or LSTM', (3, 6)), ('another NSE', (10, 12)), ('document modeling', (13, 15))]","[['NSE or LSTM', 'on the top of', 'another NSE'], ['another NSE', 'for', 'document modeling']]",[],[],[],[],"[['Hyperparameters', 'stack', 'NSE or LSTM']]",[],[],[],question-answering,7,204
tasks,The whole network is trained jointly by backpropagating the cross entropy loss .,"[('trained jointly by backpropagating', (4, 8))]","[('The whole network', (0, 3)), ('cross entropy loss', (9, 12))]","[['The whole network', 'trained jointly by backpropagating', 'cross entropy loss']]",[],[],[],[],[],[],"[['Hyperparameters', 'has', 'The whole network']]",[],question-answering,7,207
tasks,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,"[('used', (1, 2)), ('with', (6, 7)), ('for', (10, 11))]","[('one - layer LSTM', (2, 6)), ('100 hidden units', (7, 10)), ('read / write modules', (12, 16)), ('pre-trained 100 - D Glove 6B vectors', (18, 25))]","[['one - layer LSTM', 'with', '100 hidden units'], ['100 hidden units', 'for', 'read / write modules']]",[],[],[],[],"[['Hyperparameters', 'used', 'one - layer LSTM'], ['Hyperparameters', 'used', 'pre-trained 100 - D Glove 6B vectors']]",[],[],[],question-answering,7,208
tasks,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",[],[],[],[],[],[],[],"[['Hyperparameters', 'trained', '50 epochs'], ['Hyperparameters', 'l 2 regularizer strength', '1 e - 5'], ['Hyperparameters', 'batch size', '32']]",[],[],[],question-answering,7,209
tasks,Machine Translation,"[('Machine Translation', (0, 2))]",[],[],[],[],[],[],[],[],[],"[['Tasks', 'Machine Translation', 'Hyperparameters']]",question-answering,7,221
tasks,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,"[('trained to minimize', (3, 6)), ('regularized by', (14, 16))]","[('word - level cross entropy loss', (6, 12)), ('20 % input dropouts', (16, 20)), ('30 % output dropouts', (22, 26))]",[],[],[],[],[],"[['Hyperparameters', 'regularized by', '20 % input dropouts'], ['Hyperparameters', 'regularized by', '30 % output dropouts'], ['Hyperparameters', 'trained to minimize', 'word - level cross entropy loss']]",[],[],[],question-answering,7,241
tasks,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",[],[],"[['3e - 4', 'for', 'other models'], ['1e - 3', 'for', 'LSTM - LSTM']]",[],[],[],[],"[['Hyperparameters', 'initial learning rate', '1e - 3']]",[],[],[],question-answering,7,242
research-problem,Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,[],"[('Knowledge Base Question Answering', (8, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Knowledge Base Question Answering']]",[],[],[],[],question_answering,0,2
research-problem,Knowledge base question answering ( QA ) is an important natural language processing problem .,[],"[('Knowledge base question answering ( QA )', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Knowledge base question answering ( QA )']]",[],[],[],[],question_answering,0,11
research-problem,QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .,[],"[('QA', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'QA']]",[],[],[],[],question_answering,0,15
research-problem,"In this paper , we describe a semantic parsing approach to the problem of KB QA .","[('to', (10, 11)), ('of', (13, 14))]","[('semantic parsing', (7, 9)), ('problem', (12, 13)), ('KB QA', (14, 16))]","[['semantic parsing', 'to', 'problem'], ['problem', 'of', 'KB QA']]",[],[],"[['Approach', 'has', 'semantic parsing']]","[['Contribution', 'has research problem', 'KB QA']]",[],[],[],[],question_answering,0,19
approach,"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .","[('for', (3, 4)), ('construct', (9, 10))]","[('each input question', (4, 7)), ('explicit structural semantic parse ( semantic graph )', (11, 19))]","[['each input question', 'construct', 'explicit structural semantic parse ( semantic graph )']]",[],"[['Approach', 'for', 'each input question']]",[],[],[],[],[],[],question_answering,0,20
approach,Semantic parses can be deterministically converted to a query to extract the answers from the KB .,"[('deterministically converted to', (4, 7)), ('to extract', (9, 11)), ('from', (13, 14))]","[('Semantic parses', (0, 2)), ('query', (8, 9)), ('answers', (12, 13)), ('KB', (15, 16))]","[['Semantic parses', 'deterministically converted to', 'query'], ['query', 'to extract', 'answers'], ['answers', 'from', 'KB']]",[],[],"[['Approach', 'has', 'Semantic parses']]",[],[],[],[],[],question_answering,0,21
approach,"In particular , we adapt Gated Graph Neural Networks ( GGNNs ) , described in , to process and score semantic parses .","[('adapt', (4, 5)), ('to process and score', (16, 20))]","[('Gated Graph Neural Networks ( GGNNs )', (5, 12)), ('semantic parses', (20, 22))]","[['Gated Graph Neural Networks ( GGNNs )', 'to process and score', 'semantic parses']]",[],"[['Approach', 'adapt', 'Gated Graph Neural Networks ( GGNNs )']]",[],[],[],[],[],[],question_answering,0,34
code,https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering.,[],[],[],[],[],[],[],[],[],[],[],question_answering,0,48
baselines,3 . Pooled Edges model - We use the DCNN to encode the question and the label of each edge in the semantic graph .,"[('use', (7, 8)), ('to encode', (10, 12)), ('in', (20, 21))]","[('Pooled Edges model', (2, 5)), ('DCNN', (9, 10)), ('question and the label of each edge', (13, 20)), ('semantic graph', (22, 24))]","[['Pooled Edges model', 'use', 'DCNN'], ['DCNN', 'to encode', 'question and the label of each edge'], ['question and the label of each edge', 'in', 'semantic graph']]",[],[],"[['Baselines', 'has', 'Pooled Edges model']]",[],[],[],[],[],question_answering,0,171
baselines,Graph Neural Network ( GNN ) -,[],"[('Graph Neural Network ( GNN )', (0, 6))]",[],[],[],"[['Baselines', 'has', 'Graph Neural Network ( GNN )']]",[],[],[],[],[],question_answering,0,174
baselines,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .","[('of', (4, 5)), ('include', (13, 14)), ('that does not use', (17, 21)), ('directly computes', (25, 27)), ('as', (30, 31))]","[('model variant', (15, 17)), ('gating mechanism', (22, 24)), ('hidden state', (28, 30)), ('combination', (32, 33)), ('activations ( Eq 1 ) and the previous state', (35, 44))]","[['model variant', 'directly computes', 'hidden state'], ['hidden state', 'as', 'combination'], ['combination', 'of', 'activations ( Eq 1 ) and the previous state'], ['model variant', 'that does not use', 'gating mechanism']]",[],[],[],[],"[['Graph Neural Network ( GNN )', 'include', 'model variant']]",[],[],[],question_answering,0,175
baselines,"Gated Graph Neural Network ( GGNN ) - We use the GGNN to process semantic parses , as described in Section 3.2 .","[('to process', (12, 14))]","[('Gated Graph Neural Network ( GGNN )', (0, 7)), ('semantic parses', (14, 16))]","[['Gated Graph Neural Network ( GGNN )', 'to process', 'semantic parses']]",[],[],"[['Baselines', 'has', 'Gated Graph Neural Network ( GGNN )']]",[],[],[],[],[],question_answering,0,176
results,We compare the results on the WebQSP - WD data set in .,"[('on', (4, 5))]","[('WebQSP - WD data set', (6, 11))]",[],[],"[['Results', 'on', 'WebQSP - WD data set']]",[],[],[],[],[],"[['WebQSP - WD data set', 'has', 'graph models']]",question_answering,0,219
results,"As can be seen , the graph models outperform all other models across precision , recall and F-score , with GGNN showing the best over all result .","[('outperform', (8, 9)), ('across', (12, 13)), ('showing', (21, 22))]","[('graph models', (6, 8)), ('all other models', (9, 12)), ('precision , recall and F-score', (13, 18)), ('GGNN', (20, 21)), ('best over all result', (23, 27))]","[['graph models', 'outperform', 'all other models'], ['all other models', 'across', 'precision , recall and F-score'], ['GGNN', 'showing', 'best over all result']]","[['graph models', 'has', 'GGNN']]",[],[],[],[],[],[],[],question_answering,0,220
results,"The STAGG architecture delivers the worst results in our experiments , the main reason being supposedly that the model had to rely on manually defined features that are less flexible .","[('delivers', (3, 4))]","[('STAGG architecture', (1, 3)), ('worst results', (5, 7))]","[['STAGG architecture', 'delivers', 'worst results']]",[],[],"[['Results', 'has', 'STAGG architecture']]",[],[],[],[],[],question_answering,0,223
results,The Single Edge model outperforms the more complex Pooled Edges model by a noticeable margin .,"[('outperforms', (4, 5)), ('by', (11, 12))]","[('Single Edge model', (1, 4)), ('more complex Pooled Edges model', (6, 11)), ('noticeable margin', (13, 15))]","[['Single Edge model', 'outperforms', 'more complex Pooled Edges model'], ['more complex Pooled Edges model', 'by', 'noticeable margin']]",[],[],"[['Results', 'has', 'Single Edge model']]",[],[],[],[],[],question_answering,0,224
results,The Single Edge baseline prefers simple graphs that consist of a single edge which is a good strategy to achieve higher recall values .,"[('prefers', (4, 5)), ('that consist of', (7, 10)), ('is', (14, 15)), ('to achieve', (18, 20))]","[('Single Edge baseline', (1, 4)), ('simple graphs', (5, 7)), ('single edge', (11, 13)), ('good strategy', (16, 18)), ('higher recall values', (20, 23))]","[['Single Edge baseline', 'prefers', 'simple graphs'], ['simple graphs', 'is', 'good strategy'], ['good strategy', 'to achieve', 'higher recall values'], ['simple graphs', 'that consist of', 'single edge']]",[],[],"[['Results', 'has', 'Single Edge baseline']]",[],[],[],[],[],question_answering,0,225
results,"In , we see that for the STAGG and Single Edge baselines the performance on more complex questions drops compared to the results on simpler questions .",[],[],"[['performance', 'compared to', 'results'], ['results', 'on', 'simpler questions'], ['performance', 'on', 'more complex questions']]","[['STAGG and Single Edge baselines', 'has', 'performance'], ['performance', 'has', 'drops']]","[['Results', 'for', 'STAGG and Single Edge baselines']]",[],[],[],[],[],[],question_answering,0,228
results,"The Pooled Edges model maintains a better performance across questions of different complexity , which shows the benefits of encoding all graph edges .","[('maintains', (4, 5)), ('across', (8, 9)), ('shows', (15, 16)), ('of encoding', (18, 20))]","[('Pooled Edges model', (1, 4)), ('better performance', (6, 8)), ('questions of different complexity', (9, 13)), ('benefits', (17, 18)), ('all graph edges', (20, 23))]","[['Pooled Edges model', 'maintains', 'better performance'], ['better performance', 'across', 'questions of different complexity'], ['better performance', 'shows', 'benefits'], ['benefits', 'of encoding', 'all graph edges']]",[],[],"[['Results', 'has', 'Pooled Edges model']]",[],[],[],[],[],question_answering,0,229
results,"We see that the GGNN model offers the best results both on simple and complex questions , as it effectively encodes the structure of semantic graphs .","[('see that', (1, 3)), ('offers', (6, 7)), ('on', (11, 12))]","[('GGNN model', (4, 6)), ('best results', (8, 10)), ('simple and complex questions', (12, 16))]","[['GGNN model', 'offers', 'best results'], ['best results', 'on', 'simple and complex questions']]",[],"[['Results', 'see that', 'GGNN model']]",[],[],[],[],[],[],question_answering,0,233
research-problem,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,[],"[('MACHINE COMPREHENSION', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'MACHINE COMPREHENSION']]",[],[],[],[],question_answering,1,2
research-problem,"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .",[],"[('Machine comprehension ( MC )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Machine comprehension ( MC )']]",[],[],[],[],question_answering,1,4
research-problem,"Recently , attention mechanisms have been successfully extended to MC .",[],"[('MC', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'MC']]",[],[],[],[],question_answering,1,5
research-problem,The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,[],"[('question answering ( QA )', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'question answering ( QA )']]",[],[],[],[],question_answering,1,10
model,"In this paper , we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity ) .","[('introduce', (5, 6)), ('for modeling', (20, 22)), ('of', (24, 25)), ('at', (28, 29))]","[('Bi- Directional Attention Flow ( BIDAF ) network', (7, 15)), ('hierarchical multi-stage architecture', (17, 20)), ('representations', (23, 24)), ('context paragraph', (26, 28)), ('different levels of granularity', (29, 33))]","[['hierarchical multi-stage architecture', 'for modeling', 'representations'], ['representations', 'at', 'different levels of granularity'], ['representations', 'of', 'context paragraph']]","[['Bi- Directional Attention Flow ( BIDAF ) network', 'has', 'hierarchical multi-stage architecture']]","[['Model', 'introduce', 'Bi- Directional Attention Flow ( BIDAF ) network']]",[],[],[],[],[],[],question_answering,1,17
model,"BIDAF includes character - level , word - level , and contextual embeddings , and uses bi-directional attention flow to obtain a query - aware context representation .","[('includes', (1, 2)), ('uses', (15, 16)), ('to obtain', (19, 21))]","[('BIDAF', (0, 1)), ('character - level , word - level , and contextual embeddings', (2, 13)), ('bi-directional attention flow', (16, 19)), ('query - aware context representation', (22, 27))]","[['BIDAF', 'includes', 'character - level , word - level , and contextual embeddings'], ['BIDAF', 'uses', 'bi-directional attention flow'], ['bi-directional attention flow', 'to obtain', 'query - aware context representation']]",[],[],"[['Model', 'has', 'BIDAF']]",[],[],[],[],[],question_answering,1,18
model,"Instead , the attention is computed for every time step , and the attended vector at each time step , along with the representations from previous layers , is allowed to flow through to the subsequent modeling layer .","[('computed for', (5, 7)), ('at', (15, 16)), ('allowed to', (29, 31)), ('to', (33, 34))]","[('attention', (3, 4)), ('every time step', (7, 10)), ('attended vector', (13, 15)), ('each time step', (16, 19)), ('flow through', (31, 33)), ('subsequent modeling layer', (35, 38))]","[['attended vector', 'allowed to', 'flow through'], ['flow through', 'to', 'subsequent modeling layer'], ['attended vector', 'at', 'each time step'], ['attention', 'computed for', 'every time step']]","[['attention', 'has', 'attended vector']]",[],"[['Model', 'has', 'attention']]",[],[],[],[],[],question_answering,1,21
model,"Second , we use a memory - less attention mechanism .","[('use', (3, 4))]","[('memory - less attention mechanism', (5, 10))]",[],[],"[['Model', 'use', 'memory - less attention mechanism']]",[],[],[],[],[],"[['memory - less attention mechanism', 'allows', 'attention']]",question_answering,1,23
model,"It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .",[],[],"[['attention layer', 'to focus on', 'learning'], ['attention', 'between', 'query and the context'], ['modeling layer', 'to focus on', 'learning'], ['interaction', 'within', 'query - aware context representation']]","[['learning', 'has', 'attention'], ['learning', 'has', 'interaction']]",[],[],[],"[['memory - less attention mechanism', 'forces', 'attention layer'], ['memory - less attention mechanism', 'enables', 'modeling layer']]",[],[],[],question_answering,1,26
model,It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .,[],[],"[['each time step', 'to be', 'unaffected'], ['unaffected', 'from', 'incorrect attendances'], ['incorrect attendances', 'at', 'previous time steps']]",[],[],[],[],"[['attention', 'at', 'each time step']]",[],[],[],question_answering,1,27
model,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .","[('in', (6, 7)), ('which provide', (22, 24)), ('to', (26, 27))]","[('attention mechanisms', (4, 6)), ('both directions', (7, 9)), ('query - to - context', (10, 15)), ('context - to - query', (16, 21)), ('complimentary information', (24, 26)), ('each other', (27, 29))]","[['attention mechanisms', 'in', 'both directions'], ['both directions', 'which provide', 'complimentary information'], ['complimentary information', 'to', 'each other']]","[['both directions', 'name', 'query - to - context'], ['both directions', 'name', 'context - to - query']]",[],"[['Model', 'use', 'attention mechanisms']]",[],[],[],[],[],question_answering,1,29
experimental-setup,Each paragraph and question are tokenized by a regular - expression - based word tokenizer ( PTB Tokenizer ) and fed into the model .,"[('tokenized by', (5, 7)), ('fed into', (20, 22))]","[('Each paragraph and question', (0, 4)), ('regular - expression - based word tokenizer ( PTB Tokenizer )', (8, 19)), ('model', (23, 24))]","[['Each paragraph and question', 'tokenized by', 'regular - expression - based word tokenizer ( PTB Tokenizer )'], ['Each paragraph and question', 'fed into', 'model']]",[],[],"[['Experimental setup', 'has', 'Each paragraph and question']]",[],[],[],[],[],question_answering,1,174
experimental-setup,"We use 100 1D filters for CNN char embedding , each with a width of 5 .","[('use', (1, 2)), ('for', (5, 6)), ('with', (11, 12))]","[('100 1D filters', (2, 5)), ('CNN char embedding', (6, 9)), ('width of 5', (13, 16))]","[['100 1D filters', 'with', 'width of 5'], ['100 1D filters', 'for', 'CNN char embedding']]",[],"[['Experimental setup', 'use', '100 1D filters']]",[],[],[],[],[],[],question_answering,1,175
experimental-setup,The hidden state size ( d ) of the model is 100 .,"[('of', (7, 8)), ('is', (10, 11))]","[('hidden state size ( d )', (1, 7)), ('model', (9, 10)), ('100', (11, 12))]","[['hidden state size ( d )', 'of', 'model'], ['model', 'is', '100']]",[],[],"[['Experimental setup', 'has', 'hidden state size ( d )']]",[],[],[],[],[],question_answering,1,176
experimental-setup,The model has about 2.6 million parameters .,"[('has about', (2, 4))]","[('The model', (0, 2)), ('2.6 million parameters', (4, 7))]","[['The model', 'has about', '2.6 million parameters']]",[],[],"[['Experimental setup', 'has', 'The model']]",[],[],[],[],[],question_answering,1,177
experimental-setup,"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .",[],[],"[['AdaDelta ( Zeiler , 2012 ) optimizer', 'with', 'minibatch size'], ['minibatch size', 'of', '60'], ['AdaDelta ( Zeiler , 2012 ) optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.5'], ['initial learning rate', 'for', '12 epochs']]",[],[],"[['Experimental setup', 'use', 'AdaDelta ( Zeiler , 2012 ) optimizer']]",[],[],[],[],[],question_answering,1,178
experimental-setup,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .","[('of', (4, 5)), ('used for', (7, 9)), ('before', (20, 21)), ('for', (23, 24))]","[('dropout ) rate', (1, 4)), ('0.2', (5, 6)), ('CNN', (10, 11)), ('all LSTM layers', (12, 15)), ('linear transformation', (18, 20)), ('softmax', (22, 23)), ('answers', (25, 26))]","[['dropout ) rate', 'of', '0.2'], ['0.2', 'used for', 'CNN'], ['0.2', 'used for', 'all LSTM layers'], ['0.2', 'used for', 'linear transformation'], ['linear transformation', 'before', 'softmax'], ['softmax', 'for', 'answers']]",[],[],"[['Experimental setup', 'has', 'dropout ) rate']]",[],[],[],[],[],question_answering,1,179
experimental-setup,"During training , the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999 .",[],[],"[['moving averages', 'of', 'all weights'], ['all weights', 'of', 'model'], ['all weights', 'maintained with', 'exponential decay rate'], ['exponential decay rate', 'of', '0.999']]","[['training', 'has', 'moving averages']]","[['Experimental setup', 'During', 'training']]",[],[],[],[],[],[],question_answering,1,180
experimental-setup,"At test time , the moving averages instead of the raw weights are used .","[('At', (0, 1)), ('instead of', (7, 9)), ('are used', (12, 14))]","[('test time', (1, 3)), ('moving averages', (5, 7)), ('raw weights', (10, 12))]","[['test time', 'are used', 'moving averages'], ['moving averages', 'instead of', 'raw weights']]",[],"[['Experimental setup', 'At', 'test time']]",[],[],[],[],[],[],question_answering,1,181
experimental-setup,The training process takes roughly 20 hours on a single Titan X GPU .,"[('takes', (3, 4)), ('on', (7, 8))]","[('training process', (1, 3)), ('roughly 20 hours', (4, 7)), ('single Titan X GPU', (9, 13))]","[['training process', 'takes', 'roughly 20 hours'], ['roughly 20 hours', 'on', 'single Titan X GPU']]",[],[],"[['Experimental setup', 'has', 'training process']]",[],[],[],[],[],question_answering,1,182
results,"BIDAF ( ensemble ) achieves an EM score of 73.3 and an F 1 score of 81.1 , outperforming all previous approaches .",[],[],"[['BIDAF ( ensemble )', 'achieves', 'F 1 score'], ['F 1 score', 'of', '81.1'], ['BIDAF ( ensemble )', 'achieves', 'EM score'], ['EM score', 'of', '73.3']]",[],[],"[['Results', 'has', 'BIDAF ( ensemble )']]",[],[],[],[],[],question_answering,1,187
ablation-analysis,Both char - level and word - level embeddings contribute towards the model 's performance .,"[('contribute towards', (9, 11))]","[('char - level and word - level embeddings', (1, 9)), (""model 's performance"", (12, 15))]","[['char - level and word - level embeddings', 'contribute towards', ""model 's performance""]]",[],[],"[['Ablation analysis', 'has', 'char - level and word - level embeddings']]",[],[],[],[],[],question_answering,1,190
ablation-analysis,C2Q attention proves to be critical with a drop of more than 10 points on both metrics .,"[('proves to be', (2, 5)), ('with', (6, 7)), ('of', (9, 10))]","[('C2Q attention', (0, 2)), ('critical', (5, 6)), ('drop', (8, 9)), ('more than 10 points', (10, 14))]","[['C2Q attention', 'proves to be', 'critical'], ['critical', 'with', 'drop'], ['drop', 'of', 'more than 10 points']]",[],[],"[['Ablation analysis', 'has', 'C2Q attention']]",[],[],[],[],[],question_answering,1,195
ablation-analysis,"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .","[('outperforms', (11, 12)), ('by', (16, 17))]","[('proposed static attention', (8, 11)), ('dynamically computed attention', (13, 16)), ('more than 3 points', (17, 21))]","[['proposed static attention', 'outperforms', 'dynamically computed attention'], ['dynamically computed attention', 'by', 'more than 3 points']]",[],[],"[['Ablation analysis', 'has', 'proposed static attention']]",[],[],[],[],[],question_answering,1,199
ablation-analysis,"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .","[('At', (0, 1)), ('such as', (8, 10)), ('not well aligned to', (16, 20)), ('in', (22, 23))]","[('word embedding layer', (2, 5)), ('query words', (6, 8)), ('When , Where and Who', (10, 15)), ('possible answers', (20, 22)), ('context', (24, 25))]","[['query words', 'not well aligned to', 'possible answers'], ['possible answers', 'in', 'context'], ['query words', 'such as', 'When , Where and Who']]","[['word embedding layer', 'has', 'query words']]","[['Ablation analysis', 'At', 'word embedding layer']]",[],[],[],[],[],[],question_answering,1,207
research-problem,Focal Visual - Text Attention for Visual Question Answering,[],"[('Visual Question Answering', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Visual Question Answering']]",[],[],[],[],question_answering,2,2
research-problem,"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image .",[],"[('Visual question answering ( VQA )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Visual question answering ( VQA )']]",[],[],[],[],question_answering,2,13
research-problem,"Extending from VQA on a single image , this paper considers the following problem :",[],"[('VQA', (2, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'VQA']]",[],[],[],[],question_answering,2,15
model,"To address these two challenges , we propose a focal visual - text attention ( FVTA ) model for sequential data","[('propose', (7, 8)), ('for', (18, 19))]","[('focal visual - text attention ( FVTA ) model', (9, 18)), ('sequential data', (19, 21))]","[['focal visual - text attention ( FVTA ) model', 'for', 'sequential data']]",[],"[['Model', 'propose', 'focal visual - text attention ( FVTA ) model']]",[],[],[],[],[],[],question_answering,2,32
model,"Inspired by this process , FVTA first learns to localize relevant information within a few , small , temporally consecutive regions over the input sequences , and learns to infer an answer based on the cross-modal statistics pooled from these regions .","[('learns to', (7, 9)), ('within', (12, 13)), ('over', (21, 22)), ('based on', (32, 34)), ('pooled from', (37, 39))]","[('FVTA', (5, 6)), ('localize', (9, 10)), ('relevant information', (10, 12)), ('few , small , temporally consecutive regions', (14, 21)), ('input sequences', (23, 25)), ('infer', (29, 30)), ('answer', (31, 32)), ('cross-modal statistics', (35, 37)), ('these regions', (39, 41))]","[['FVTA', 'learns to', 'localize'], ['relevant information', 'within', 'few , small , temporally consecutive regions'], ['few , small , temporally consecutive regions', 'over', 'input sequences'], ['FVTA', 'learns to', 'infer'], ['answer', 'based on', 'cross-modal statistics'], ['cross-modal statistics', 'pooled from', 'these regions']]","[['localize', 'has', 'relevant information'], ['infer', 'has', 'answer']]",[],"[['Model', 'has', 'FVTA']]",[],[],[],[],[],question_answering,2,37
model,FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :,"[('proposes', (1, 2)), ('to compute', (5, 7)), ('jointly models', (11, 13)), ('in', (16, 17))]","[('novel kernel', (3, 5)), ('attention tensor', (8, 10)), ('latent information', (14, 16)), ('three sources', (17, 19))]","[['novel kernel', 'to compute', 'attention tensor'], ['attention tensor', 'jointly models', 'latent information'], ['latent information', 'in', 'three sources']]",[],[],[],[],"[['FVTA', 'proposes', 'novel kernel']]",[],[],"[['three sources', 'name', 'answer - signaling words'], ['three sources', 'name', 'temporal correlation'], ['three sources', 'name', 'cross-modal interaction']]",question_answering,2,38
model,"1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .","[('in', (6, 7)), ('within', (14, 15)), ('between', (23, 24))]","[('answer - signaling words', (2, 6)), ('question', (8, 9)), ('temporal correlation', (12, 14)), ('sequence', (16, 17)), ('cross-modal interaction', (21, 23)), ('text and image', (25, 28))]","[['answer - signaling words', 'in', 'question'], ['temporal correlation', 'within', 'sequence'], ['cross-modal interaction', 'between', 'text and image']]",[],[],[],[],[],[],[],[],question_answering,2,39
model,"FVTA attention allows for collective reasoning by the attention kernel learned over a few , small , consecutive sub-sequences of text and image .","[('allows for', (2, 4)), ('by', (6, 7)), ('learned over', (10, 12)), ('of', (19, 20))]","[('FVTA attention', (0, 2)), ('collective reasoning', (4, 6)), ('attention kernel', (8, 10)), ('few , small , consecutive sub-sequences', (13, 19)), ('text and image', (20, 23))]","[['FVTA attention', 'allows for', 'collective reasoning'], ['collective reasoning', 'by', 'attention kernel'], ['attention kernel', 'learned over', 'few , small , consecutive sub-sequences'], ['few , small , consecutive sub-sequences', 'of', 'text and image']]",[],[],"[['Model', 'has', 'FVTA attention']]",[],[],[],[],[],question_answering,2,40
model,We propose a novel attention kernel for VQA on visual - text data .,"[('for', (6, 7)), ('on', (8, 9))]","[('novel attention kernel', (3, 6)), ('VQA', (7, 8)), ('visual - text data', (9, 13))]","[['novel attention kernel', 'for', 'VQA'], ['VQA', 'on', 'visual - text data']]",[],[],"[['Model', 'propose', 'novel attention kernel']]",[],[],[],[],[],question_answering,2,44
experiments,Memex QA provides 4 answer choices and only one correct answer for each question .,[],"[('Memex QA', (0, 2)), ('answer', (4, 5))]",[],[],[],"[['Experiments', 'has', 'Memex QA']]",[],[],[],[],"[['Memex QA', 'has', 'Baselines']]",question_answering,2,183
experiments,"We implement the following methods as baselines : Logistic Regression predicts the answer with concatenated image , question and metadata features as reported in .","[('predicts', (10, 11)), ('with', (13, 14))]","[('Logistic Regression', (8, 10)), ('concatenated image , question and metadata features', (14, 21))]",[],[],[],[],[],"[['answer', 'with', 'concatenated image , question and metadata features']]","[['Logistic Regression', 'predicts', 'answer']]","[['Baselines', 'has', 'Logistic Regression']]",[],question_answering,2,191
experiments,"Embedding + LSTM utilizes word embeddings and character embeddings , along with the same visual embeddings used in FVTA .","[('utilizes', (3, 4)), ('along with', (10, 12)), ('used in', (16, 18))]","[('Embedding + LSTM', (0, 3)), ('word embeddings and character embeddings', (4, 9)), ('same visual embeddings', (13, 16)), ('FVTA', (18, 19))]","[['Embedding + LSTM', 'utilizes', 'word embeddings and character embeddings'], ['word embeddings and character embeddings', 'along with', 'same visual embeddings'], ['same visual embeddings', 'used in', 'FVTA']]",[],[],[],[],[],[],"[['Baselines', 'has', 'Embedding + LSTM']]",[],question_answering,2,192
experiments,Embedding + LSTM + Concat concatenates the last LSTM output from different modalities to produce the final output .,"[('concatenates', (5, 6)), ('from', (10, 11)), ('to produce', (13, 15))]","[('Embedding + LSTM + Concat', (0, 5)), ('last LSTM output', (7, 10)), ('different modalities', (11, 13)), ('final output', (16, 18))]","[['Embedding + LSTM + Concat', 'concatenates', 'last LSTM output'], ['last LSTM output', 'from', 'different modalities'], ['different modalities', 'to produce', 'final output']]",[],[],[],[],[],[],"[['Baselines', 'has', 'Embedding + LSTM + Concat']]",[],question_answering,2,194
experiments,Classic Soft Attention uses classic one dimensional question - to - context attention to summarize context for question answering .,"[('uses', (3, 4)), ('to summarize', (13, 15)), ('for', (16, 17))]","[('Classic Soft Attention', (0, 3)), ('classic one dimensional question - to - context attention', (4, 13)), ('context', (15, 16)), ('question answering', (17, 19))]","[['Classic Soft Attention', 'uses', 'classic one dimensional question - to - context attention'], ['classic one dimensional question - to - context attention', 'to summarize', 'context'], ['context', 'for', 'question answering']]",[],[],[],[],[],[],"[['Baselines', 'has', 'Classic Soft Attention']]",[],question_answering,2,196
experiments,"DMN + is the improved dynamic memory networks , which is one of the representative architectures that achieve good performance on the VQA Task .","[('is', (2, 3)), ('which is one of', (9, 13)), ('that achieve', (16, 18)), ('on', (20, 21))]","[('DMN +', (0, 2)), ('improved dynamic memory networks', (4, 8)), ('representative architectures', (14, 16)), ('good performance', (18, 20)), ('VQA Task', (22, 24))]","[['DMN +', 'is', 'improved dynamic memory networks'], ['improved dynamic memory networks', 'which is one of', 'representative architectures'], ['representative architectures', 'that achieve', 'good performance'], ['good performance', 'on', 'VQA Task']]",[],[],[],[],[],[],"[['Baselines', 'has', 'DMN +']]",[],question_answering,2,198
experiments,TGIF Temporal Attention is a recently proposed spatial - temporal reasoning network on sequential animated image QA .,"[('is', (3, 4)), ('on', (12, 13))]","[('TGIF Temporal Attention', (0, 3)), ('spatial - temporal reasoning network', (7, 12)), ('sequential animated image QA', (13, 17))]","[['TGIF Temporal Attention', 'is', 'spatial - temporal reasoning network'], ['spatial - temporal reasoning network', 'on', 'sequential animated image QA']]",[],[],[],[],[],[],"[['Baselines', 'has', 'TGIF Temporal Attention']]",[],question_answering,2,201
experiments,We encode GPS locations using words .,"[('encode', (1, 2)), ('using', (4, 5))]","[('GPS locations', (2, 4)), ('words', (5, 6))]","[['GPS locations', 'using', 'words']]",[],[],[],[],"[['Experimental setup', 'encode', 'GPS locations']]",[],[],[],question_answering,2,211
experiments,"All questions , textual context and answers are tokenized using the Stanford word tokenizer .","[('are', (7, 8)), ('using', (9, 10))]","[('questions , textual context and answers', (1, 7)), ('tokenized', (8, 9)), ('Stanford word tokenizer', (11, 14))]","[['questions , textual context and answers', 'are', 'tokenized'], ['tokenized', 'using', 'Stanford word tokenizer']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'questions , textual context and answers']]",[],question_answering,2,213
experiments,"We use pre-trained Glo Ve word embeddings , which is fixed during training .","[('use', (1, 2)), ('fixed during', (10, 12))]","[('pre-trained Glo Ve word embeddings', (2, 7)), ('training', (12, 13))]","[['pre-trained Glo Ve word embeddings', 'fixed during', 'training']]",[],[],[],[],"[['Experimental setup', 'use', 'pre-trained Glo Ve word embeddings']]",[],[],[],question_answering,2,214
experiments,"For image / video embedding , we extract fixed - size features using the pre-trained CNN model , Inception - ResNet , by concatenating the pool5 layer and classification layer 's output before softmax .","[('For', (0, 1)), ('extract', (7, 8)), ('using', (12, 13)), ('by concatenating', (22, 24)), ('before', (32, 33))]","[('image / video embedding', (1, 5)), ('fixed - size features', (8, 12)), ('pre-trained CNN model', (14, 17)), ('Inception - ResNet', (18, 21)), (""pool5 layer and classification layer 's output"", (25, 32)), ('softmax', (33, 34))]","[['image / video embedding', 'extract', 'fixed - size features'], ['fixed - size features', 'using', 'pre-trained CNN model'], ['pre-trained CNN model', 'by concatenating', ""pool5 layer and classification layer 's output""], [""pool5 layer and classification layer 's output"", 'before', 'softmax']]","[['pre-trained CNN model', 'name', 'Inception - ResNet']]",[],[],[],"[['Experimental setup', 'For', 'image / video embedding']]",[],[],[],question_answering,2,215
experiments,We then use a linear transformation to compress the image feature into 100 dimensional .,"[('to compress', (6, 8)), ('into', (11, 12))]","[('linear transformation', (4, 6)), ('image feature', (9, 11)), ('100 dimensional', (12, 14))]","[['linear transformation', 'to compress', 'image feature'], ['image feature', 'into', '100 dimensional']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'linear transformation']]",[],question_answering,2,216
experiments,Then a bi-directional LSTM is used for each modality to obtain contextual representations .,"[('used for', (5, 7)), ('to obtain', (9, 11))]","[('bi-directional LSTM', (2, 4)), ('each modality', (7, 9)), ('contextual representations', (11, 13))]","[['bi-directional LSTM', 'used for', 'each modality'], ['each modality', 'to obtain', 'contextual representations']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'bi-directional LSTM']]",[],question_answering,2,217
experiments,"Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?",[],[],"[['output', 'of', 'both directions'], ['both directions', 'of', 'LSTM'], ['LSTM', 'Given', 'hidden state size'], ['hidden state size', 'of', 'd'], ['d', 'set to', '50']]",[],[],[],[],"[['Experimental setup', 'concatenate', 'output'], ['all media documents', 'get', 'question matrix Q']]",[],[],"[['all media documents', 'get', 'context tensor H'], ['question matrix Q', 'has', 'R 2 d M']]",question_answering,2,218
experiments,R 2 d M and context tensor H ?,[],"[('R 2 d M', (0, 4)), ('context tensor H', (5, 8))]",[],[],[],[],[],[],[],[],"[['context tensor H', 'has', 'R 2dV KN 6']]",question_answering,2,219
experiments,R 2dV KN 6 for all media documents .,"[('for', (4, 5))]","[('R 2dV KN 6', (0, 4)), ('all media documents', (5, 8))]",[],[],[],[],[],"[['LSTM', 'for', 'all media documents']]",[],[],[],question_answering,2,220
experiments,We reshape the context tensor into H ? R 2 d T 6 .,"[('reshape', (1, 2)), ('into', (5, 6))]","[('context tensor', (3, 5)), ('H ? R 2 d T 6', (6, 13))]","[['context tensor', 'into', 'H ? R 2 d T 6']]",[],[],[],[],"[['Experimental setup', 'reshape', 'context tensor']]",[],[],[],question_answering,2,221
experiments,"To select the best hyperparmeters , we randomly select 20 % of the official training set as the validation set .","[('To select', (0, 2)), ('randomly select', (7, 9)), ('of', (11, 12)), ('as', (16, 17))]","[('best hyperparmeters', (3, 5)), ('20 %', (9, 11)), ('official training set', (13, 16)), ('validation set', (18, 20))]","[['best hyperparmeters', 'randomly select', '20 %'], ['20 %', 'of', 'official training set'], ['official training set', 'as', 'validation set']]",[],[],[],[],"[['Experimental setup', 'To select', 'best hyperparmeters']]",[],[],[],question_answering,2,222
experiments,We use the AdaDelta optimizer and an initial learning rate of 0.5 to train for 200 epochs with a dropout rate of 0.3 ..,[],[],"[['initial learning rate', 'of', '0.5'], ['initial learning rate', 'of', '0.5'], ['0.5', 'to train for', '200 epochs'], ['200 epochs', 'with', 'dropout rate'], ['dropout rate', 'of', '0.3']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'AdaDelta optimizer'], ['Experimental setup', 'use', 'AdaDelta optimizer'], ['Experimental setup', 'use', 'initial learning rate']]",[],question_answering,2,223
experiments,FVTA outperforms other attention models on finding the relevant photos for the question .,"[('outperforms', (1, 2)), ('on finding', (5, 7)), ('for', (10, 11))]","[('FVTA', (0, 1)), ('other attention models', (2, 5)), ('relevant photos', (8, 10)), ('question', (12, 13))]","[['FVTA', 'outperforms', 'other attention models'], ['other attention models', 'on finding', 'relevant photos'], ['relevant photos', 'for', 'question']]",[],[],[],[],[],[],"[['Results', 'has', 'FVTA'], ['Results', 'has', 'FVTA']]",[],question_answering,2,234
experiments,"To evaluate the FVTA attention mechanism , we first replace our kernel tensor with simple cosine similarity function .","[('evaluate', (1, 2)), ('first replace', (8, 10)), ('with', (13, 14))]","[('FVTA attention mechanism', (3, 6)), ('our kernel tensor', (10, 13)), ('simple cosine similarity function', (14, 18))]","[['FVTA attention mechanism', 'first replace', 'our kernel tensor'], ['our kernel tensor', 'with', 'simple cosine similarity function']]",[],[],[],[],"[['Ablation analysis', 'evaluate', 'FVTA attention mechanism']]",[],[],"[['our kernel tensor', 'has', 'Results']]",question_answering,2,242
experiments,Results show that standard cosine similarity is inferior to our similarity function .,"[('show that', (1, 3)), ('inferior to', (7, 9))]","[('standard cosine similarity', (3, 6)), ('our similarity function', (9, 12))]","[['standard cosine similarity', 'inferior to', 'our similarity function']]",[],[],[],[],"[['Results', 'show that', 'standard cosine similarity']]",[],[],[],question_answering,2,243
experiments,"For ablating intra-sequence dependency , we use the representations from the last timestep of each context document .","[('For ablating', (0, 2)), ('use', (6, 7)), ('from', (9, 10)), ('of', (13, 14))]","[('intra-sequence dependency', (2, 4)), ('representations', (8, 9)), ('last timestep', (11, 13)), ('each context document', (14, 17))]","[['intra-sequence dependency', 'use', 'representations'], ['representations', 'from', 'last timestep'], ['last timestep', 'of', 'each context document']]",[],[],[],[],"[['Ablation analysis', 'For ablating', 'intra-sequence dependency']]",[],[],[],question_answering,2,244
experiments,"For ablating cross sequence interaction , we average all attended context representation from different modalities to get the final context vector .","[('average', (7, 8)), ('from', (12, 13)), ('to get', (15, 17))]","[('cross sequence interaction', (2, 5)), ('all attended context representation', (8, 12)), ('different modalities', (13, 15)), ('final context vector', (18, 21))]","[['cross sequence interaction', 'average', 'all attended context representation'], ['all attended context representation', 'from', 'different modalities'], ['different modalities', 'to get', 'final context vector']]",[],[],[],[],[],[],"[['Ablation analysis', 'For ablating', 'cross sequence interaction']]",[],question_answering,2,245
experiments,"Both aspects of correlation of the FVTA attention tensor contribute towards the model 's performance , while intra-sequence dependency shows more importance in this experiment .","[('of', (4, 5)), ('contribute towards', (9, 11)), ('while', (16, 17)), ('shows', (19, 20))]","[('Both aspects of correlation', (0, 4)), ('FVTA attention tensor', (6, 9)), (""model 's performance"", (12, 15)), ('intra-sequence dependency', (17, 19)), ('more importance', (20, 22))]","[['Both aspects of correlation', 'of', 'FVTA attention tensor'], ['Both aspects of correlation', 'while', 'intra-sequence dependency'], ['intra-sequence dependency', 'shows', 'more importance'], ['Both aspects of correlation', 'contribute towards', ""model 's performance""]]",[],[],[],[],[],[],"[['Ablation analysis', 'has', 'Both aspects of correlation']]",[],question_answering,2,246
experiments,We compare the effectiveness of context - aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation .,[],[],"[['effectiveness', 'of', 'context - aware question attention'], ['context - aware question attention', 'by removing', 'question attention'], ['context - aware question attention', 'use', 'last timestep'], ['last timestep', 'of', 'LSTM output'], ['LSTM output', 'as', 'question representation'], ['LSTM output', 'from', 'question']]","[['context - aware question attention', 'shows', 'question attention']]",[],[],[],"[['Ablation analysis', 'compare', 'effectiveness']]",[],[],[],question_answering,2,247
experiments,It shows the question attention provides slight improvement .,"[('shows', (1, 2)), ('provides', (5, 6))]","[('question attention', (3, 5)), ('slight improvement', (6, 8))]","[['question attention', 'provides', 'slight improvement']]",[],[],[],[],[],[],[],[],question_answering,2,248
experiments,"Finally , we train FVTA without photos to see the contribution of visual information .","[('train', (3, 4)), ('to see', (7, 9)), ('of', (11, 12))]","[('FVTA without photos', (4, 7)), ('contribution', (10, 11)), ('visual information', (12, 14))]","[['FVTA without photos', 'to see', 'contribution'], ['contribution', 'of', 'visual information']]",[],[],[],[],"[['Ablation analysis', 'train', 'FVTA without photos']]",[],[],"[['FVTA without photos', 'has', 'result']]",question_answering,2,249
experiments,"The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset , which is not uncommon in VQA dataset and in Visual7W .","[('is', (2, 3))]","[('result', (1, 2)), ('quite good', (3, 5))]","[['result', 'is', 'quite good']]",[],[],[],[],[],[],[],[],question_answering,2,250
experiments,"In the MovieQA dataset , each QA is given a set of N movie clips of the same movie , and each clip comes with subtitles .","[('of', (11, 12)), ('with', (24, 25))]","[('MovieQA dataset', (2, 4))]",[],[],[],"[['Experiments', 'has', 'MovieQA dataset']]",[],[],[],[],"[['MovieQA dataset', 'has', 'Experimental setup']]",question_answering,2,257
experiments,We implement FVTA network for Movie QA task with modality number of 2 ( video & text ) .,"[('implement', (1, 2)), ('for', (4, 5))]","[('FVTA network', (2, 4)), ('Movie QA task', (5, 8)), ('modality number', (9, 11)), ('2 ( video & text )', (12, 18))]","[['FVTA network', 'for', 'Movie QA task']]","[['FVTA network', 'with', 'modality number'], ['modality number', 'of', '2 ( video & text )']]",[],[],[],"[['Experimental setup', 'implement', 'FVTA network']]",[],[],[],question_answering,2,258
experiments,"We set the maximum number of movie clips per question to N = 20 , the maximum number of frames to consider to F = 10 , the maximum number of subtitle sentences in a clip to K = 100 and the maximum words to V = 10 .",[],[],"[['maximum number of frames to consider', 'to', 'F = 10'], ['maximum number of subtitle sentences in a clip', 'to', 'K = 100'], ['maximum words', 'to', 'V = 10'], ['maximum number of movie clips per question', 'to', 'N = 20']]",[],[],[],[],"[['Experimental setup', 'set', 'maximum number of frames to consider'], ['Experimental setup', 'set', 'maximum number of subtitle sentences in a clip'], ['Experimental setup', 'set', 'maximum words'], ['Experimental setup', 'set', 'maximum number of movie clips per question']]",[],[],[],question_answering,2,259
experiments,We use the AdaDelta optimizer with a minibatch of 16 and an initial learning rate of 0.5 to trained for 300 epochs .,[],[],"[['AdaDelta optimizer', 'with', 'initial learning rate'], ['initial learning rate', 'trained for', '300 epochs'], ['AdaDelta optimizer', 'with', 'minibatch'], ['minibatch', 'of', '16']]",[],[],[],[],[],[],[],[],question_answering,2,261
experiments,FVTA model outperforms all baseline methods and achieves comparable performance to the state - of - the - art result 2 on the MovieQA test server .,"[('outperforms', (2, 3)), ('achieves', (7, 8)), ('to', (10, 11)), ('on', (21, 22))]","[('FVTA model', (0, 2)), ('all baseline methods', (3, 6)), ('comparable performance', (8, 10)), ('state - of - the - art result', (12, 20)), ('MovieQA test server', (23, 26))]","[['FVTA model', 'achieves', 'comparable performance'], ['comparable performance', 'to', 'state - of - the - art result'], ['state - of - the - art result', 'on', 'MovieQA test server'], ['FVTA model', 'outperforms', 'all baseline methods']]",[],[],[],[],[],[],"[['Results', 'has', 'FVTA model']]",[],question_answering,2,264
experiments,Our accuracy is 0.410 ( vs 0.387 by RWMN ) on the validation set and 0.373 ( vs 0.363 ) on the test set .,[],[],"[['accuracy', 'is', '0.373'], ['0.373', 'vs', '0.363'], ['0.373', 'on', 'test set'], ['accuracy', 'is', '0.410'], ['0.410', 'vs', '0.387'], ['0.387', 'by', 'RWMN'], ['0.410', 'on', 'validation set']]",[],[],[],[],[],[],"[['Results', 'has', 'accuracy']]",[],question_answering,2,266
experiments,"Benefiting from such modeling ability , FVTA consistently outperforms the classical attention models including soft attention , MCB and TGIF .","[('consistently outperforms', (7, 9)), ('including', (13, 14))]","[('FVTA', (6, 7)), ('classical attention models', (10, 13)), ('soft attention', (14, 16)), ('MCB', (17, 18)), ('TGIF', (19, 20))]","[['FVTA', 'consistently outperforms', 'classical attention models'], ['classical attention models', 'including', 'soft attention'], ['classical attention models', 'including', 'MCB'], ['classical attention models', 'including', 'TGIF']]",[],[],[],[],[],[],[],[],question_answering,2,267
research-problem,Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,[],"[('Reading Comprehension', (10, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],question_answering,3,2
research-problem,This paper presents a new compositional encoder for reading comprehension ( RC ) .,[],"[('reading comprehension ( RC )', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'reading comprehension ( RC )']]",[],[],[],[],question_answering,3,5
research-problem,"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block .",[],"[('RC', (5, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],question_answering,3,9
model,"To this end , we propose a new compositional encoder that can either be used in place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","[('propose', (5, 6)), ('be used', (13, 15)), ('of', (17, 18)), ('serve as', (22, 24)), ('complementary to', (29, 31))]","[('new compositional encoder', (7, 10)), ('in place', (15, 17)), ('standard RNN encoders', (18, 21)), ('new module', (25, 27)), ('existing neural architectures', (31, 34))]","[['new compositional encoder', 'serve as', 'new module'], ['new module', 'complementary to', 'existing neural architectures'], ['new compositional encoder', 'be used', 'in place'], ['in place', 'of', 'standard RNN encoders']]",[],"[['Model', 'propose', 'new compositional encoder']]",[],[],[],[],[],[],question_answering,3,22
model,Our proposed encoder leverages dilated compositions to model relationships across multiple granularities .,"[('leverages', (3, 4)), ('to model', (6, 8)), ('across', (9, 10))]","[('Our proposed encoder', (0, 3)), ('dilated compositions', (4, 6)), ('relationships', (8, 9)), ('multiple granularities', (10, 12))]","[['Our proposed encoder', 'leverages', 'dilated compositions'], ['dilated compositions', 'to model', 'relationships'], ['relationships', 'across', 'multiple granularities']]",[],[],"[['Model', 'has', 'Our proposed encoder']]",[],[],[],[],[],question_answering,3,23
model,"That is , for a given word in the target sequence , our encoder exploits both long - term ( far ) and short - term ( near ) information to decide how much information to retain for it .","[('for', (3, 4)), ('in', (7, 8)), ('exploits', (14, 15)), ('to decide', (30, 32)), ('to', (35, 36))]","[('given word', (5, 7)), ('target sequence', (9, 11)), ('our encoder', (12, 14)), ('long - term ( far ) and short - term ( near ) information', (16, 30)), ('information', (34, 35)), ('retain', (36, 37))]","[['given word', 'in', 'target sequence'], ['our encoder', 'exploits', 'long - term ( far ) and short - term ( near ) information'], ['long - term ( far ) and short - term ( near ) information', 'to decide', 'information'], ['information', 'to', 'retain']]","[['given word', 'has', 'our encoder']]","[['Model', 'for', 'given word']]",[],[],[],[],[],[],question_answering,3,24
model,"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .",[],[],"[['output', 'of', 'dilated composition mechanism'], ['dilated composition mechanism', 'acts as', 'gating functions'], ['dilated composition mechanism', 'used to learn', 'compositional representations'], ['compositional representations', 'of', 'input sequence']]",[],[],"[['Model', 'has', 'output']]",[],[],[],[],[],question_answering,3,26
baselines,RACE,[],"[('RACE', (0, 1))]",[],[],[],"[['Baselines', 'has', 'RACE']]",[],[],[],[],"[['RACE', 'has', 'key competitors']]",question_answering,3,178
baselines,"The key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","[('are', (3, 4))]","[('key competitors', (1, 3)), ('Stanford Attention Reader ( Stanford AR )', (5, 12)), ('Gated Attention Reader ( GA )', (13, 19)), ('Dynamic Fusion Networks ( DFN )', (21, 27))]","[['key competitors', 'are', 'Stanford Attention Reader ( Stanford AR )'], ['key competitors', 'are', 'Gated Attention Reader ( GA )'], ['key competitors', 'are', 'Dynamic Fusion Networks ( DFN )']]",[],[],[],[],[],[],[],[],question_answering,3,179
baselines,SearchQA,[],"[('SearchQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'SearchQA']]",[],[],[],[],"[['SearchQA', 'has', 'main competitor baseline']]",question_answering,3,185
baselines,The main competitor baseline is the AMANDA model proposed by .,"[('is', (4, 5))]","[('main competitor baseline', (1, 4)), ('AMANDA model', (6, 8))]","[['main competitor baseline', 'is', 'AMANDA model']]",[],[],[],[],[],[],[],[],question_answering,3,186
baselines,NarrativeQA,[],"[('NarrativeQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'NarrativeQA']]",[],[],[],[],"[['NarrativeQA', 'has', 'baselines']]",question_answering,3,190
baselines,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","[('are', (11, 12))]","[('baselines', (10, 11)), ('context - less sequence to sequence ( seq2seq ) model', (13, 23)), ('ASR', (24, 25)), ('BiDAF', (26, 27))]","[['baselines', 'are', 'context - less sequence to sequence ( seq2seq ) model'], ['baselines', 'are', 'ASR'], ['baselines', 'are', 'BiDAF']]",[],[],[],[],[],[],[],[],question_answering,3,192
experimental-setup,We implement all models in TensorFlow .,"[('implement', (1, 2)), ('in', (4, 5))]","[('all models', (2, 4)), ('TensorFlow', (5, 6))]","[['all models', 'in', 'TensorFlow']]",[],"[['Experimental setup', 'implement', 'all models']]",[],[],[],[],[],[],question_answering,3,211
experimental-setup,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"[('initialized with', (3, 5)), ('not fine - tuned during', (11, 16))]","[('Word embeddings', (0, 2)), ('300d Glo Ve vectors', (5, 9)), ('training', (16, 17))]","[['Word embeddings', 'initialized with', '300d Glo Ve vectors'], ['Word embeddings', 'not fine - tuned during', 'training']]",[],[],"[['Experimental setup', 'has', 'Word embeddings']]",[],[],[],[],[],question_answering,3,212
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","[('tuned amongst', (3, 5)), ('on', (12, 13)), ('including', (15, 16))]","[('Dropout rate', (0, 2)), ('{ 0.1 , 0.2 , 0.3 }', (5, 12)), ('all layers', (13, 15)), ('embedding layer', (17, 19))]","[['Dropout rate', 'tuned amongst', '{ 0.1 , 0.2 , 0.3 }'], ['{ 0.1 , 0.2 , 0.3 }', 'on', 'all layers'], ['all layers', 'including', 'embedding layer']]",[],[],"[['Experimental setup', 'has', 'Dropout rate']]",[],[],[],[],[],question_answering,3,213
experimental-setup,"We adopt the Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / Narrative QA respectively .","[('adopt', (1, 2)), ('with', (12, 13)), ('of', (16, 17)), ('for', (19, 20))]","[('Adam optimizer', (3, 5)), ('learning rate', (14, 16)), ('0.0003/ 0.001/0.001', (17, 19)), ('RACE / SearchQA / Narrative QA', (20, 26))]","[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.0003/ 0.001/0.001'], ['0.0003/ 0.001/0.001', 'for', 'RACE / SearchQA / Narrative QA']]",[],"[['Experimental setup', 'adopt', 'Adam optimizer']]",[],[],[],[],[],[],question_answering,3,216
experimental-setup,The batch size is set to 64/256/32 accordingly .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('64/256/32', (6, 7))]","[['batch size', 'set to', '64/256/32']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],question_answering,3,217
experimental-setup,The maximum sequence lengths are 500/200/1100 respectively .,"[('are', (4, 5))]","[('maximum sequence lengths', (1, 4)), ('500/200/1100', (5, 6))]","[['maximum sequence lengths', 'are', '500/200/1100']]",[],[],"[['Experimental setup', 'has', 'maximum sequence lengths']]",[],[],[],[],[],question_answering,3,218
experimental-setup,"For Narrative QA , we use the Rouge - L score to find the best approximate answer relative to the human written answer for training the span model .","[('For', (0, 1)), ('use', (5, 6)), ('to find', (11, 13)), ('relative to', (17, 19)), ('for training', (23, 25))]","[('Narrative QA', (1, 3)), ('Rouge - L score', (7, 11)), ('best approximate answer', (14, 17)), ('human written answer', (20, 23)), ('span model', (26, 28))]","[['Narrative QA', 'use', 'Rouge - L score'], ['Rouge - L score', 'to find', 'best approximate answer'], ['best approximate answer', 'relative to', 'human written answer'], ['human written answer', 'for training', 'span model']]",[],"[['Experimental setup', 'For', 'Narrative QA']]",[],[],[],[],[],[],question_answering,3,219
experimental-setup,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"[('based on', (9, 11))]","[('runtime benchmarks', (6, 8)), ('TitanXP GPU', (12, 14))]","[['runtime benchmarks', 'based on', 'TitanXP GPU']]",[],[],"[['Experimental setup', 'has', 'runtime benchmarks']]",[],[],[],[],[],question_answering,3,220
results,reports our results on the RACE benchmark dataset .,"[('on', (3, 4))]","[('RACE benchmark dataset', (5, 8))]",[],[],"[['Results', 'on', 'RACE benchmark dataset']]",[],[],[],[],[],"[['RACE benchmark dataset', 'has', 'Our proposed DCU model']]",question_answering,3,221
results,Our proposed DCU model achieves the best result for both single models and ensemble models .,"[('achieves', (4, 5)), ('for', (8, 9))]","[('Our proposed DCU model', (0, 4)), ('best result', (6, 8)), ('single models', (10, 12)), ('ensemble models', (13, 15))]","[['Our proposed DCU model', 'achieves', 'best result'], ['best result', 'for', 'single models'], ['best result', 'for', 'ensemble models']]",[],[],[],[],[],[],[],[],question_answering,3,222
results,We outperform highly complex models such as DFN .,"[('outperform', (1, 2)), ('such as', (5, 7))]","[('highly complex models', (2, 5)), ('DFN', (7, 8))]","[['highly complex models', 'such as', 'DFN']]",[],[],[],[],"[['RACE benchmark dataset', 'outperform', 'highly complex models']]",[],[],[],question_answering,3,223
results,We also pull ahead of other recent baselines such as ElimiNet and GA by at least 5 % .,"[('pull ahead of', (2, 5)), ('such as', (8, 10)), ('by', (13, 14))]","[('other recent baselines', (5, 8)), ('ElimiNet', (10, 11)), ('GA', (12, 13)), ('at least 5 %', (14, 18))]","[['other recent baselines', 'by', 'at least 5 %'], ['other recent baselines', 'such as', 'ElimiNet'], ['other recent baselines', 'such as', 'GA']]",[],[],[],[],"[['RACE benchmark dataset', 'pull ahead of', 'other recent baselines']]",[],[],[],question_answering,3,224
results,The best single model score from RACE - H and RACE - M alternates between Sim - DCU and DCU .,"[('from', (5, 6)), ('alternates between', (13, 15))]","[('best single model score', (1, 5)), ('RACE - H', (6, 9)), ('RACE - M', (10, 13)), ('Sim - DCU', (15, 18)), ('DCU', (19, 20))]","[['best single model score', 'from', 'RACE - H'], ['best single model score', 'from', 'RACE - M'], ['best single model score', 'alternates between', 'Sim - DCU'], ['best single model score', 'alternates between', 'DCU']]",[],[],[],[],[],[],"[['RACE benchmark dataset', 'has', 'best single model score']]",[],question_answering,3,225
results,Table 2 reports our results on the Search QA dataset .,[],"[('Search QA dataset', (7, 10))]",[],[],[],"[['Results', 'on', 'Search QA dataset']]",[],[],[],[],[],question_answering,3,243
results,We achieve the same accuracy as AMANDA without using any LSTM or GRU encoder .,"[('achieve', (1, 2)), ('as', (5, 6)), ('without using', (7, 9))]","[('same accuracy', (3, 5)), ('AMANDA', (6, 7)), ('LSTM or GRU encoder', (10, 14))]","[['same accuracy', 'as', 'AMANDA'], ['same accuracy', 'without using', 'LSTM or GRU encoder']]",[],[],[],[],"[['Search QA dataset', 'achieve', 'same accuracy']]",[],[],[],question_answering,3,245
results,"Finally , the hybrid combination , DCU - LSTM significantly outperforms AMANDA by 3 % .","[('significantly outperforms', (9, 11)), ('by', (12, 13))]","[('hybrid combination , DCU - LSTM', (3, 9)), ('AMANDA', (11, 12)), ('3 %', (13, 15))]","[['hybrid combination , DCU - LSTM', 'significantly outperforms', 'AMANDA'], ['AMANDA', 'by', '3 %']]",[],[],[],[],[],[],"[['Search QA dataset', 'has', 'hybrid combination , DCU - LSTM']]",[],question_answering,3,248
results,"Contrary to MCQ - based datasets , we found that reports our results on the NarrativeQA benchmark .",[],"[('NarrativeQA benchmark', (15, 17))]",[],[],[],"[['Results', 'on', 'NarrativeQA benchmark']]",[],[],[],[],[],question_answering,3,249
results,"First , we observe that 300d DCU can achieve comparable performance with BiDAF .","[('observe that', (3, 5)), ('achieve', (8, 9)), ('with', (11, 12))]","[('300d DCU', (5, 7)), ('comparable performance', (9, 11)), ('BiDAF', (12, 13))]","[['300d DCU', 'achieve', 'comparable performance'], ['comparable performance', 'with', 'BiDAF']]",[],[],[],[],"[['NarrativeQA benchmark', 'observe that', '300d DCU']]",[],[],[],question_answering,3,250
results,"Finally , DCU - LSTM significantly outperforms all models in terms of ROUGE - L , including BiDAF on this dataset .","[('significantly outperforms', (5, 7)), ('in terms of', (9, 12)), ('including', (16, 17))]","[('DCU - LSTM', (2, 5)), ('all models', (7, 9)), ('ROUGE - L', (12, 15)), ('BiDAF', (17, 18))]","[['DCU - LSTM', 'significantly outperforms', 'all models'], ['all models', 'including', 'BiDAF'], ['all models', 'in terms of', 'ROUGE - L']]",[],[],[],[],[],[],"[['NarrativeQA benchmark', 'has', 'DCU - LSTM']]",[],question_answering,3,256
results,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that DCU encoders are also effective as a complementary neural building block .","[('over', (2, 3)), ('ranges from', (7, 9)), ('across', (14, 15))]","[('Performance improvement', (0, 2)), ('vanilla BiLSTM model', (4, 7)), ('1 % ? 3 %', (9, 14)), ('all metrics', (15, 17))]","[['Performance improvement', 'over', 'vanilla BiLSTM model'], ['vanilla BiLSTM model', 'ranges from', '1 % ? 3 %'], ['1 % ? 3 %', 'across', 'all metrics']]",[],[],[],[],[],[],"[['NarrativeQA benchmark', 'has', 'Performance improvement']]",[],question_answering,3,257
research-problem,Densely Connected Attention Propagation for Reading Comprehension,[],"[('Reading Comprehension', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Reading Comprehension']]",[],[],[],[],question_answering,4,2
research-problem,"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .",[],"[('reading comprehension ( RC )', (17, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'reading comprehension ( RC )']]",[],[],[],[],question_answering,4,4
research-problem,We conduct extensive experiments on four challenging RC benchmarks .,[],"[('RC', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'RC']]",[],[],[],[],question_answering,4,9
model,"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .","[('is', (4, 5))]","[('network', (3, 4)), ('densely connected', (5, 7))]","[['network', 'is', 'densely connected']]",[],[],"[['Model', 'has', 'network']]",[],[],[],[],[],question_answering,4,31
model,"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .","[('propose', (5, 6)), ('as', (13, 14)), ('to connect', (18, 20)), ('at', (22, 23))]","[('efficient Bidirectional Attention Connectors ( BAC )', (6, 13)), ('base building block', (15, 18)), ('two sequences', (20, 22)), ('arbitrary layers', (23, 25))]","[['efficient Bidirectional Attention Connectors ( BAC )', 'as', 'base building block'], ['base building block', 'to connect', 'two sequences'], ['two sequences', 'at', 'arbitrary layers']]",[],"[['Model', 'propose', 'efficient Bidirectional Attention Connectors ( BAC )']]",[],[],[],[],[],[],question_answering,4,37
model,"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .",[],[],"[['attention outputs', 'be', 'small'], ['small', 'to', 'propagate']]",[],"[['Model', 'compress', 'attention outputs']]",[],[],[],[],[],[],question_answering,4,38
model,"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .","[('collectively passed into', (4, 7)), ('effectively connect', (11, 13)), ('to', (15, 16))]","[('propagated features', (1, 3)), ('prediction layers', (7, 9)), ('shallow layers', (13, 15)), ('deeper layers', (16, 18))]","[['propagated features', 'collectively passed into', 'prediction layers'], ['propagated features', 'effectively connect', 'shallow layers'], ['shallow layers', 'to', 'deeper layers']]",[],[],"[['Model', 'has', 'propagated features']]",[],[],[],[],[],question_answering,4,39
model,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .","[('for', (15, 16))]","[('DECAPROP ( Densely Connected Attention Propagation )', (4, 11)), ('novel architecture', (13, 15)), ('reading comprehension', (16, 18))]","[['novel architecture', 'for', 'reading comprehension']]","[['DECAPROP ( Densely Connected Attention Propagation )', 'is', 'novel architecture']]",[],"[['Model', 'propose', 'DECAPROP ( Densely Connected Attention Propagation )']]",[],[],[],[],[],question_answering,4,41
baselines,NewsQA,[],"[('NewsQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'NewsQA']]",[],[],[],[],"[['NewsQA', 'has', 'key competitors']]",question_answering,4,152
baselines,"On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .","[('are', (7, 8))]","[('key competitors', (5, 7)), ('BiDAF', (8, 9)), ('Match - LSTM', (10, 13)), ('FastQA / Fast QA - Ext', (14, 20)), ('R2-BiLSTM', (21, 22)), ('AMANDA', (23, 24))]","[['key competitors', 'are', 'BiDAF'], ['key competitors', 'are', 'Match - LSTM'], ['key competitors', 'are', 'FastQA / Fast QA - Ext'], ['key competitors', 'are', 'R2-BiLSTM'], ['key competitors', 'are', 'AMANDA'], ['key competitors', 'are', 'BiDAF']]",[],[],[],[],[],[],[],[],question_answering,4,156
baselines,Quasar -T,[],"[('Quasar -T', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Quasar -T']]",[],[],[],[],"[['Quasar -T', 'has', 'key competitors']]",question_answering,4,157
baselines,The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .,[],[],"[['key competitors', 'are', 'Reinforced Ranker - Reader ( R 3 )']]",[],[],[],[],[],[],[],[],question_answering,4,159
baselines,SearchQA,[],"[('SearchQA', (0, 1))]",[],[],[],"[['Baselines', 'has', 'SearchQA']]",[],[],[],[],"[['SearchQA', 'has', 'competitor baselines']]",question_answering,4,161
baselines,"The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .","[('are', (6, 7))]","[('competitor baselines', (1, 3)), ('Attention Sum Reader ( ASR )', (7, 13)), ('Focused Hierarchical RNNs ( FH - RNN )', (14, 22)), ('AMANDA', (23, 24)), ('BiDAF', (25, 26)), ('AQA', (27, 28)), ('Reinforced Ranker - Reader ( R 3 )', (30, 38))]","[['competitor baselines', 'are', 'Attention Sum Reader ( ASR )'], ['competitor baselines', 'are', 'Focused Hierarchical RNNs ( FH - RNN )'], ['competitor baselines', 'are', 'AMANDA'], ['competitor baselines', 'are', 'BiDAF'], ['competitor baselines', 'are', 'AQA'], ['competitor baselines', 'are', 'Reinforced Ranker - Reader ( R 3 )']]",[],[],[],[],[],[],[],[],question_answering,4,165
baselines,Narrative QA ] is a recent QA dataset that involves comprehension over stories .,[],"[('Narrative QA', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Narrative QA']]",[],[],[],[],[],question_answering,4,166
baselines,"We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .","[('compare with', (1, 3)), ('namely', (10, 11))]","[('baselines', (4, 5)), ('Seq2Seq', (11, 12)), ('Attention Sum Reader', (13, 16)), ('BiDAF', (17, 18))]","[['baselines', 'namely', 'Seq2Seq'], ['baselines', 'namely', 'Attention Sum Reader'], ['baselines', 'namely', 'BiDAF']]",[],[],[],[],"[['Narrative QA', 'compare with', 'baselines']]",[],[],[],question_answering,4,168
baselines,We also compare with the recent BiAttention + MRU model .,[],"[('recent BiAttention + MRU model', (5, 10))]",[],[],[],[],[],[],[],"[['Narrative QA', 'compare with', 'recent BiAttention + MRU model']]",[],question_answering,4,169
experimental-setup,Our model is implemented in Tensorflow .,"[('implemented in', (3, 5))]","[('model', (1, 2)), ('Tensorflow', (5, 6))]","[['model', 'implemented in', 'Tensorflow']]",[],[],"[['Experimental setup', 'has', 'model']]",[],[],[],[],[],question_answering,4,178
experimental-setup,"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .","[('capped at', (4, 6)), ('for', (7, 8))]","[('sequence lengths', (1, 3)), ('800/700/1500/1100', (6, 7)), ('News QA , Search QA , Quasar - T and Narrative QA', (8, 20))]","[['sequence lengths', 'capped at', '800/700/1500/1100'], ['800/700/1500/1100', 'for', 'News QA , Search QA , Quasar - T and Narrative QA']]",[],[],"[['Experimental setup', 'has', 'sequence lengths']]",[],[],[],[],[],question_answering,4,179
experimental-setup,"We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .",[],[],"[['Adam', 'with', '? = 0.001'], ['? = 0.001', 'for', 'Search QA'], ['? = 0.001', 'for', 'Quasar - T'], ['? = 0.001', 'for', 'Narrative QA'], ['Adadelta', 'with', '? = 0.5'], ['? = 0.5', 'for', 'News QA']]",[],"[['Experimental setup', 'use', 'Adam'], ['Experimental setup', 'use', 'Adadelta']]",[],[],[],[],[],[],question_answering,4,180
experimental-setup,"The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .","[('choice of', (1, 3)), ('tuned between', (7, 9)), ('tuned amongst', (18, 20))]","[('RNN encoder', (4, 6)), ('GRU and LSTM cells', (9, 13)), ('hidden size', (15, 17)), ('{ 32 , 50 , 64 , 75 }', (20, 29))]","[['hidden size', 'tuned amongst', '{ 32 , 50 , 64 , 75 }'], ['RNN encoder', 'tuned between', 'GRU and LSTM cells']]",[],"[['Experimental setup', 'choice of', 'hidden size'], ['Experimental setup', 'choice of', 'RNN encoder']]",[],[],[],[],[],[],question_answering,4,181
experimental-setup,We use the CUDNN implementation of the RNN encoder .,"[('of', (5, 6))]","[('CUDNN implementation', (3, 5)), ('RNN encoder', (7, 9))]","[['CUDNN implementation', 'of', 'RNN encoder']]",[],[],"[['Experimental setup', 'use', 'CUDNN implementation']]",[],[],[],[],[],question_answering,4,182
experimental-setup,"Batch size is tuned amongst { 16 , 32 , 64 } .","[('tuned amongst', (3, 5))]","[('Batch size', (0, 2)), ('{ 16 , 32 , 64 }', (5, 12))]","[['Batch size', 'tuned amongst', '{ 16 , 32 , 64 }']]",[],[],"[['Experimental setup', 'has', 'Batch size']]",[],[],[],[],[],question_answering,4,183
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .","[('tuned amongst', (3, 5)), ('applied to', (13, 15))]","[('Dropout rate', (0, 2)), ('{ 0.1 , 0.2 , 0.3 }', (5, 12)), ('all RNN and fully - connected layers', (15, 22))]","[['Dropout rate', 'tuned amongst', '{ 0.1 , 0.2 , 0.3 }'], ['Dropout rate', 'applied to', 'all RNN and fully - connected layers']]",[],[],"[['Experimental setup', 'has', 'Dropout rate']]",[],[],[],[],[],question_answering,4,184
experimental-setup,We apply variational dropout in - between RNN layers .,"[('apply', (1, 2)), ('in - between', (4, 7))]","[('variational dropout', (2, 4)), ('RNN layers', (7, 9))]","[['variational dropout', 'in - between', 'RNN layers']]",[],"[['Experimental setup', 'apply', 'variational dropout']]",[],[],[],[],[],[],question_answering,4,185
experimental-setup,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,"[('initialize', (1, 2)), ('with', (5, 6)), ('fixed during', (12, 14))]","[('word embeddings', (3, 5)), ('300D Glo Ve embeddings', (6, 10)), ('training', (14, 15))]","[['word embeddings', 'with', '300D Glo Ve embeddings'], ['300D Glo Ve embeddings', 'fixed during', 'training']]",[],"[['Experimental setup', 'initialize', 'word embeddings']]",[],[],[],[],[],[],question_answering,4,186
experimental-setup,The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,[],[],"[['size', 'of', 'character embeddings'], ['character embeddings', 'set to', '8'], ['size', 'of', 'character RNN'], ['character RNN', 'set to', 'word - level RNN encoders']]",[],[],"[['Experimental setup', 'has', 'size']]",[],[],[],[],[],question_answering,4,187
experimental-setup,The maximum characters per word is set to 16 .,"[('set to', (6, 8))]","[('maximum characters per word', (1, 5)), ('16', (8, 9))]","[['maximum characters per word', 'set to', '16']]",[],[],"[['Experimental setup', 'has', 'maximum characters per word']]",[],[],[],[],[],question_answering,4,188
experimental-setup,The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .,[],[],"[['number of factors', 'in', 'factorization kernel'], ['factorization kernel', 'set to', '64'], ['number of layers', 'in', 'DECAENC'], ['DECAENC', 'set to', '3']]",[],[],"[['Experimental setup', 'has', 'number of factors'], ['Experimental setup', 'has', 'number of layers']]",[],[],[],[],[],question_answering,4,189
experimental-setup,We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .,[],[],"[['learning rate decay factor', 'of', '2'], ['patience', 'of', '3 epochs']]",[],[],"[['Experimental setup', 'use', 'learning rate decay factor'], ['Experimental setup', 'use', 'patience']]",[],[],[],[],[],question_answering,4,190
results,"Overall , our results are optimistic and promising , with results indicating that DECAPROP achieves state - of - the - art performance 6 on all four datasets . 66.2 75.9 DCN + CoVE 71.3 79.9 R- NET 72.3 80.6 R - NET","[('achieves', (14, 15)), ('on', (24, 25))]","[('DECAPROP', (13, 14)), ('state - of - the - art performance', (15, 23)), ('all four datasets', (25, 28))]","[['DECAPROP', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'all four datasets']]",[],[],"[['Results', 'has', 'DECAPROP']]",[],[],[],[],[],question_answering,4,192
results,"On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .","[('outperforms', (5, 6)), ('i.e.', (16, 17)), ('by', (22, 23))]","[('existing state - of - the - art', (7, 15)), ('recent AMANDA model', (19, 22)), ('+ 4.7 % EM', (24, 28)), ('+ 2.6 % F1', (29, 33))]","[['existing state - of - the - art', 'by', '+ 4.7 % EM'], ['existing state - of - the - art', 'by', '+ 2.6 % F1'], ['existing state - of - the - art', 'i.e.', 'recent AMANDA model']]",[],[],[],[],"[['DECAPROP', 'outperforms', 'existing state - of - the - art']]",[],[],[],question_answering,4,194
results,"Moreover , our proposed model also outperforms well - established baselines such as Match - LSTM ( + 18 % EM / + 16.3 % F1 ) and BiDAF ( + 16 % EM / + 14 % F1 ) .","[('outperforms', (6, 7)), ('such as', (11, 13))]","[('our proposed model', (2, 5)), ('well - established baselines', (7, 11)), ('Match - LSTM ( + 18 % EM / + 16.3 % F1 )', (13, 27)), ('BiDAF ( + 16 % EM / + 14 % F1 )', (28, 40))]","[['our proposed model', 'outperforms', 'well - established baselines'], ['well - established baselines', 'such as', 'Match - LSTM ( + 18 % EM / + 16.3 % F1 )'], ['well - established baselines', 'such as', 'BiDAF ( + 16 % EM / + 14 % F1 )']]",[],[],"[['Results', 'has', 'our proposed model']]",[],[],[],[],[],question_answering,4,196
results,reports the results on Quasar - T .,"[('on', (3, 4))]","[('Quasar - T', (4, 7))]",[],[],"[['Results', 'on', 'Quasar - T']]",[],[],[],[],[],"[['Quasar - T', 'has', 'Our model']]",question_answering,4,197
results,"Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .","[('achieves', (2, 3)), ('outperforming', (15, 16)), ('by', (31, 32)), ('of', (35, 36))]","[('Our model', (0, 2)), ('state - of - the - art performance', (3, 11)), ('state - of - the - art R 3 ( Reinforced Ranker Reader )', (17, 31)), ('considerable margin', (33, 35)), ('+ 4.4 % EM / + 6 % F1', (36, 45))]","[['Our model', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'outperforming', 'state - of - the - art R 3 ( Reinforced Ranker Reader )'], ['state - of - the - art R 3 ( Reinforced Ranker Reader )', 'by', 'considerable margin'], ['considerable margin', 'of', '+ 4.4 % EM / + 6 % F1']]",[],[],[],[],[],[],[],[],question_answering,4,198
results,"On the original setting , our model outperforms AMANDA by + 15.4 % EM and + 14.2 % in terms of F1 score .","[('outperforms', (7, 8)), ('by', (9, 10))]","[('our model', (5, 7)), ('AMANDA', (8, 9)), ('+ 15.4 % EM and + 14.2 % in terms of F1 score', (10, 23))]","[['our model', 'outperforms', 'AMANDA'], ['AMANDA', 'by', '+ 15.4 % EM and + 14.2 % in terms of F1 score']]",[],[],[],[],[],[],"[['Quasar - T', 'has', 'our model']]",[],question_answering,4,202
results,"On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .",[],"[('AQA', (10, 11)), ('+ 18.1 % EM / + 18 % F1', (12, 21)), ('Reinforced Reader Ranker', (23, 26)), ('+ 7.8 % EM / + 8.3 % F1', (27, 36))]",[],"[['AQA', 'has', '+ 18.1 % EM / + 18 % F1'], ['Reinforced Reader Ranker', 'has', '+ 7.8 % EM / + 8.3 % F1']]",[],[],[],[],[],"[['our model', 'outperforms', 'AQA'], ['our model', 'outperforms', 'Reinforced Reader Ranker']]",[],question_answering,4,203
results,SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .,[],"[('our model', (6, 8)), ('popular SQuAD benchmark', (14, 17))]",[],"[['popular SQuAD benchmark', 'has', 'our model']]",[],"[['Results', 'on', 'popular SQuAD benchmark']]",[],[],[],[],[],question_answering,4,209
results,"While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .","[('does not achieve', (3, 6)), ('can outperform', (17, 19))]","[('state - of - the - art performance', (6, 14)), ('base R - NET', (20, 24))]",[],[],[],[],[],"[['our model', 'can outperform', 'base R - NET'], ['our model', 'does not achieve', 'state - of - the - art performance']]",[],[],[],question_answering,4,210
ablation-analysis,We conduct an ablation study on the New s QA development set .,"[('on', (5, 6))]","[('New s QA development set', (7, 12))]",[],[],"[['Ablation analysis', 'on', 'New s QA development set']]",[],[],[],[],[],[],question_answering,4,213
ablation-analysis,"Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .","[('in', (2, 3)), ('observe', (28, 29)), ('between', (34, 35))]","[('significant gap', (30, 32)), ('performance', (33, 34)), ('DECAPROP and R - NET', (35, 40))]","[['significant gap', 'in', 'performance'], ['performance', 'between', 'DECAPROP and R - NET']]",[],"[['Ablation analysis', 'observe', 'significant gap']]",[],[],[],[],[],[],question_answering,4,221
ablation-analysis,"Overall , the key insight is that all model components are crucial to DECAPROP .","[('is', (5, 6)), ('are crucial to', (10, 13))]","[('key insight', (3, 5)), ('all model components', (7, 10)), ('DECAPROP', (13, 14))]","[['key insight', 'is', 'all model components'], ['all model components', 'are crucial to', 'DECAPROP']]",[],[],"[['Ablation analysis', 'has', 'key insight']]",[],[],[],[],[],question_answering,4,223
ablation-analysis,"Notably , the DECAENC seems to contribute the most to the over all performance .","[('seems to contribute', (4, 7)), ('to', (9, 10))]","[('DECAENC', (3, 4)), ('most', (8, 9)), ('over all performance', (11, 14))]","[['DECAENC', 'seems to contribute', 'most'], ['most', 'to', 'over all performance']]",[],[],"[['Ablation analysis', 'has', 'DECAENC']]",[],[],[],[],[],question_answering,4,224
ablation-analysis,We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .,"[('of', (5, 6)), ('over', (7, 8)), ('is', (11, 12))]","[('superiority', (4, 5)), ('DECAPROP', (6, 7)), ('R - NET', (8, 11)), ('consistent and relatively stable', (12, 16))]","[['superiority', 'of', 'DECAPROP'], ['DECAPROP', 'over', 'R - NET'], ['DECAPROP', 'is', 'consistent and relatively stable']]",[],[],"[['Ablation analysis', 'observe', 'superiority']]",[],[],[],[],[],question_answering,4,226
research-problem,EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING,[],"[('OPEN - DOMAIN QUESTION ANSWERING', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'OPEN - DOMAIN QUESTION ANSWERING']]",[],[],[],[],question_answering,5,2
research-problem,Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .,[],"[('Open-domain question answering ( QA )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Open-domain question answering ( QA )']]",[],[],[],[],question_answering,5,16
research-problem,Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .,[],"[('open - domain QA', (3, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'open - domain QA']]",[],[],[],[],question_answering,5,18
model,"In this paper , we propose a method to improve open - domain QA by explicitly aggregating evidence from across multiple passages .","[('propose', (5, 6)), ('to improve', (8, 10)), ('by explicitly aggregating', (14, 17)), ('from across', (18, 20))]","[('method', (7, 8)), ('open - domain QA', (10, 14)), ('evidence', (17, 18)), ('multiple passages', (20, 22))]","[['method', 'by explicitly aggregating', 'evidence'], ['evidence', 'from across', 'multiple passages'], ['method', 'to improve', 'open - domain QA']]",[],"[['Model', 'propose', 'method']]",[],[],[],[],[],[],question_answering,5,23
model,We formulate the above evidence aggregation as an answer re-ranking problem .,"[('formulate', (1, 2)), ('as', (6, 7))]","[('evidence aggregation', (4, 6)), ('answer re-ranking problem', (8, 11))]","[['evidence aggregation', 'as', 'answer re-ranking problem']]",[],"[['Model', 'formulate', 'evidence aggregation']]",[],[],[],[],[],[],question_answering,5,35
model,"Here we apply the idea of re-ranking ; for each answer candidate , we efficiently incorporate global information from multiple pieces of textual evidence without significantly increasing the complexity of the prediction of the RC model .",[],[],"[['each answer candidate', 'efficiently incorporate', 'global information'], ['global information', 'from', 'multiple pieces'], ['multiple pieces', 'of', 'textual evidence'], ['global information', 'without', 'significantly increasing'], ['complexity', 'of', 'prediction'], ['prediction', 'of', 'RC model']]","[['significantly increasing', 'has', 'complexity']]","[['Model', 'for', 'each answer candidate']]",[],[],[],[],[],[],question_answering,5,37
model,The re-rankers are :,"[('are', (2, 3))]","[('re-rankers', (1, 2))]",[],[],[],"[['Model', 'has', 're-rankers']]",[],[],"[['re-rankers', 'are', 'strength - based re-ranker']]",[],[],question_answering,5,39
model,"A strength - based re-ranker , which ranks the answer candidates according to how often their evidence occurs in different passages .","[('ranks', (7, 8)), ('according to how often', (11, 15)), ('occurs in', (17, 19))]","[('strength - based re-ranker', (1, 5)), ('answer candidates', (9, 11)), ('evidence', (16, 17)), ('different passages', (19, 21))]","[['strength - based re-ranker', 'ranks', 'answer candidates'], ['answer candidates', 'according to how often', 'evidence'], ['evidence', 'occurs in', 'different passages']]",[],[],[],[],[],[],[],[],question_answering,5,40
model,"A coverage - based re-ranker , which aims to rank an answer candidate higher if the union of all its contexts in different passages could cover more aspects included in the question .","[('aims to rank', (7, 10)), ('if', (14, 15)), ('of', (17, 18)), ('in', (21, 22)), ('could cover', (24, 26)), ('included in', (28, 30))]","[('coverage - based re-ranker', (1, 5)), ('answer candidate', (11, 13)), ('higher', (13, 14)), ('union', (16, 17)), ('all its contexts', (18, 21)), ('different passages', (22, 24)), ('more aspects', (26, 28)), ('question', (31, 32))]","[['coverage - based re-ranker', 'aims to rank', 'answer candidate'], ['higher', 'if', 'union'], ['union', 'of', 'all its contexts'], ['all its contexts', 'in', 'different passages'], ['all its contexts', 'could cover', 'more aspects'], ['more aspects', 'included in', 'question']]","[['answer candidate', 'has', 'higher']]",[],[],[],[],[],"[['re-rankers', 'are', 'coverage - based re-ranker']]",[],question_answering,5,43
baselines,"Our baseline models 9 include the following : GA , a reading comprehension model with gated - attention ; BiDAF ) , a RC model with bidirectional attention flow ; AQA ) , a reinforced system learning to aggregate the answers generated by the re-written questions ; R 3 ) , a reinforced model making use of a ranker for selecting passages to train the RC model .",[],[],"[['RC model', 'with', 'bidirectional attention flow'], ['reinforced system learning', 'to aggregate', 'answers'], ['answers', 'generated by', 're-written questions'], ['reinforced model', 'making use of', 'ranker'], ['ranker', 'for selecting', 'passages'], ['passages', 'to train', 'RC model'], ['reading comprehension model', 'with', 'gated - attention']]","[['BiDAF', 'has', 'RC model'], ['AQA', 'has', 'reinforced system learning'], ['R 3', 'has', 'reinforced model'], ['GA', 'has', 'reading comprehension model']]","[['Baselines', 'include', 'BiDAF'], ['Baselines', 'include', 'AQA'], ['Baselines', 'include', 'R 3'], ['Baselines', 'include', 'GA']]",[],[],[],[],[],[],question_answering,5,160
hyperparameters,"We first use a pre-trained R 3 model , which gets the state - of - the - art performance on the three public datasets we consider , to generate the top 50 candidate spans for the training , development and test datasets , and we use them for further ranking .","[('use', (2, 3)), ('to generate', (28, 30)), ('for', (35, 36)), ('use them for', (46, 49))]","[('pre-trained R 3 model', (4, 8)), ('top 50 candidate spans', (31, 35)), ('training , development and test datasets', (37, 43)), ('further ranking', (49, 51))]","[['pre-trained R 3 model', 'to generate', 'top 50 candidate spans'], ['top 50 candidate spans', 'use them for', 'further ranking'], ['top 50 candidate spans', 'for', 'training , development and test datasets']]",[],"[['Hyperparameters', 'use', 'pre-trained R 3 model']]",[],[],[],[],[],[],question_answering,5,167
hyperparameters,"For the coverage - based re-ranker , we use Adam to optimize the model .","[('For', (0, 1)), ('use', (8, 9)), ('to optimize', (10, 12))]","[('coverage - based re-ranker', (2, 6)), ('Adam', (9, 10)), ('model', (13, 14))]","[['coverage - based re-ranker', 'use', 'Adam'], ['Adam', 'to optimize', 'model']]",[],"[['Hyperparameters', 'For', 'coverage - based re-ranker']]",[],[],[],[],[],[],question_answering,5,169
hyperparameters,We set all the words beyond Glove as zero vectors .,"[('set', (1, 2)), ('beyond', (5, 6)), ('as', (7, 8))]","[('all the words', (2, 5)), ('Glove', (6, 7)), ('zero vectors', (8, 10))]","[['all the words', 'as', 'zero vectors'], ['all the words', 'beyond', 'Glove']]",[],"[['Hyperparameters', 'set', 'all the words']]",[],[],[],[],[],[],question_answering,5,171
hyperparameters,"We set l to 300 , batch size to 30 , learning rate to 0.002 .",[],[],"[['learning rate', 'to', '0.002'], ['batch size', 'to', '30'], ['l', 'to', '300']]",[],[],"[['Hyperparameters', 'set', 'learning rate'], ['Hyperparameters', 'set', 'batch size'], ['Hyperparameters', 'set', 'l']]",[],[],[],[],[],question_answering,5,172
hyperparameters,"We tune the dropout probability from 0 to 0.5 and the number of candidate answers for re-ranking ( K ) in [ 3 , 5 , 10 ] 11 .","[('tune', (1, 2)), ('from', (5, 6)), ('for', (15, 16)), ('in', (20, 21))]","[('dropout probability', (3, 5)), ('0 to 0.5', (6, 9)), ('number of candidate answers', (11, 15)), ('re-ranking ( K )', (16, 20)), ('[ 3 , 5 , 10 ]', (21, 28))]","[['dropout probability', 'from', '0 to 0.5'], ['number of candidate answers', 'for', 're-ranking ( K )'], ['re-ranking ( K )', 'in', '[ 3 , 5 , 10 ]']]",[],"[['Hyperparameters', 'tune', 'dropout probability'], ['Hyperparameters', 'tune', 'number of candidate answers']]",[],[],[],[],[],[],question_answering,5,173
results,"The results showed that R 3 achieved F1 56.0 , EM 50.9 on Wiki domain and F1 68.5 , EM 63.0 on Web domain , which is competitive to the state - of - the - arts .",[],[],"[['R 3', 'competitive to', 'state - of - the - arts'], ['R 3', 'achieved', 'F1 68.5 , EM 63.0'], ['F1 68.5 , EM 63.0', 'on', 'Web domain'], ['R 3', 'achieved', 'F1 56.0 , EM 50.9'], ['F1 56.0 , EM 50.9', 'on', 'Wiki domain']]",[],"[['Results', 'showed that', 'R 3']]",[],[],[],[],[],[],question_answering,5,181
code,Our code will be released under https://github.com/shuohangwang/mprc.,[],[],[],[],[],[],[],[],[],[],[],question_answering,5,184
results,"From the results , we can clearly see that the full re-ranker , the combination of different re-rankers , significantly outperforms the previous best performance by a large margin , especially on Quasar - T and Search QA .","[('see that', (7, 9)), ('combination of', (14, 16)), ('significantly outperforms', (19, 21)), ('by', (25, 26)), ('especially on', (30, 32))]","[('full re-ranker', (10, 12)), ('different re-rankers', (16, 18)), ('previous best performance', (22, 25)), ('large margin', (27, 29)), ('Quasar - T', (32, 35)), ('Search QA', (36, 38))]","[['full re-ranker', 'combination of', 'different re-rankers'], ['full re-ranker', 'significantly outperforms', 'previous best performance'], ['previous best performance', 'especially on', 'Quasar - T'], ['previous best performance', 'especially on', 'Search QA'], ['previous best performance', 'by', 'large margin']]",[],"[['Results', 'see that', 'full re-ranker']]",[],[],[],[],[],[],question_answering,5,190
results,"Moreover , our model is much better than the human performance on the Search QA dataset .","[('is', (4, 5)), ('than', (7, 8)), ('on', (11, 12))]","[('our model', (2, 4)), ('much better', (5, 7)), ('human performance', (9, 11)), ('Search QA dataset', (13, 16))]","[['our model', 'is', 'much better'], ['much better', 'than', 'human performance'], ['human performance', 'on', 'Search QA dataset']]",[],[],"[['Results', 'has', 'our model']]",[],[],[],[],[],question_answering,5,191
results,"In addition , we see that our coverage - based re-ranker achieves consistently good performance on the three datasets , even though its performance is marginally lower than the strength - based re-ranker on the Search QA dataset .","[('achieves', (11, 12)), ('on', (15, 16))]","[('our coverage - based re-ranker', (6, 11)), ('consistently good performance', (12, 15)), ('three datasets', (17, 19))]","[['our coverage - based re-ranker', 'achieves', 'consistently good performance'], ['consistently good performance', 'on', 'three datasets']]",[],[],"[['Results', 'see that', 'our coverage - based re-ranker']]",[],[],[],[],[],question_answering,5,192
research-problem,Neural Question Generation from Text : A Preliminary Study,[],"[('Neural Question Generation from Text', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Question Generation from Text']]",[],[],[],[],question_generation,0,2
research-problem,Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,[],"[('Automatic question generation', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic question generation']]",[],[],[],[],question_generation,0,4
research-problem,"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .",[],"[('Automatic question generation from natural language text', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic question generation from natural language text']]",[],[],[],[],question_generation,0,10
research-problem,"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .",[],"[('question generation', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'question generation']]",[],[],[],[],question_generation,0,11
model,"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .","[('denoted as', (19, 21)), ('to generate', (30, 32)), ('without', (37, 38))]","[('Neural Question Generation ( NQG ) framework', (22, 29)), ('natural language questions from text', (32, 37)), ('pre-defined rules', (38, 40))]","[['Neural Question Generation ( NQG ) framework', 'to generate', 'natural language questions from text'], ['natural language questions from text', 'without', 'pre-defined rules']]",[],"[['Model', 'denoted as', 'Neural Question Generation ( NQG ) framework']]",[],[],[],[],[],[],question_generation,0,16
model,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,"[('extends', (5, 6)), ('by enriching', (13, 15)), ('with', (17, 18)), ('to generate', (22, 24))]","[('Neural Question Generation framework', (1, 5)), ('sequence - to - sequence models', (7, 13)), ('encoder', (16, 17)), ('answer', (18, 19)), ('lexical features', (20, 22)), ('answer focused questions', (24, 27))]","[['Neural Question Generation framework', 'extends', 'sequence - to - sequence models'], ['sequence - to - sequence models', 'by enriching', 'encoder'], ['encoder', 'with', 'answer'], ['encoder', 'with', 'lexical features'], ['encoder', 'to generate', 'answer focused questions']]",[],[],"[['Model', 'has', 'Neural Question Generation framework']]",[],[],[],[],[],question_generation,0,17
model,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .","[('reads', (4, 5))]","[('encoder', (3, 4)), ('input sentence', (8, 10)), ('answer position indicator', (14, 17)), ('lexical features', (18, 20))]","[['encoder', 'reads', 'input sentence'], ['encoder', 'reads', 'answer position indicator'], ['encoder', 'reads', 'lexical features']]",[],[],"[['Model', 'has', 'encoder']]",[],[],[],[],[],question_generation,0,18
model,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .","[('denotes', (4, 5)), ('in', (8, 9))]","[('answer position feature', (1, 4)), ('answer span', (6, 8)), ('input sentence', (10, 12))]","[['answer position feature', 'denotes', 'answer span'], ['answer span', 'in', 'input sentence']]",[],[],"[['Model', 'has', 'answer position feature']]",[],[],[],[],[],question_generation,0,19
model,The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,"[('include', (3, 4)), ('to help produce', (19, 22))]","[('lexical features', (1, 3)), ('part - of - speech ( POS )', (4, 12)), ('named entity ( NER ) tags', (13, 19)), ('better sentence encoding', (22, 25))]","[['lexical features', 'to help produce', 'better sentence encoding'], ['better sentence encoding', 'include', 'part - of - speech ( POS )'], ['better sentence encoding', 'include', 'named entity ( NER ) tags']]",[],[],"[['Model', 'has', 'lexical features']]",[],[],[],[],[],question_generation,0,20
model,"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .","[('with', (4, 5)), ('generates', (7, 8)), ('of', (12, 13))]","[('decoder', (3, 4)), ('attention mechanism', (5, 7)), ('answer specific question', (9, 12)), ('sentence', (14, 15))]","[['decoder', 'with', 'attention mechanism'], ['decoder', 'generates', 'answer specific question'], ['answer specific question', 'of', 'sentence']]",[],[],"[['Model', 'has', 'decoder']]",[],[],[],[],[],question_generation,0,21
baselines,PCFG - Trans,[],"[('PCFG - Trans', (0, 3))]",[],[],[],"[['Baselines', 'has', 'PCFG - Trans']]",[],[],[],[],"[['PCFG - Trans', 'has', 'rule - based system']]",question_generation,0,71
baselines,The rule - based system 1 modified on the code released by .,[],"[('rule - based system', (1, 5))]",[],[],[],[],[],[],[],[],[],question_generation,0,72
baselines,s 2 s+ att,[],"[('s 2 s+ att', (0, 4))]",[],[],[],"[['Baselines', 'has', 's 2 s+ att']]",[],[],[],[],[],question_generation,0,74
baselines,We implement a seq2seq with attention as the baseline method .,"[('implement', (1, 2))]","[('seq2seq with attention', (3, 6))]",[],[],[],[],[],"[['s 2 s+ att', 'implement', 'seq2seq with attention']]",[],[],[],question_generation,0,75
baselines,NQG,[],"[('NQG', (0, 1))]",[],[],[],"[['Baselines', 'has', 'NQG']]",[],[],[],[],[],question_generation,0,76
baselines,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,"[('extend', (1, 2)), ('with', (6, 7)), ('to build', (12, 14))]","[('s 2s+ att', (3, 6)), ('feature - rich encoder', (8, 12)), ('NQG system', (15, 17))]","[['s 2s+ att', 'with', 'feature - rich encoder'], ['feature - rich encoder', 'to build', 'NQG system']]",[],[],[],[],"[['NQG', 'extend', 's 2s+ att']]",[],[],[],question_generation,0,77
baselines,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .","[('incorporate', (7, 8)), ('deal with', (11, 13))]","[('NQG +', (0, 2)), ('copy mechanism', (8, 10)), ('rare words problem', (13, 16))]","[['NQG +', 'incorporate', 'copy mechanism'], ['copy mechanism', 'deal with', 'rare words problem']]",[],[],"[['Baselines', 'has', 'NQG +']]",[],[],[],[],[],question_generation,0,78
baselines,"NQG + Pretrain Based on NQG + , we initialize the word embedding matrix with pre-trained GloVe vectors .","[('Based on', (3, 5)), ('initialize', (9, 10)), ('with', (14, 15))]","[('NQG + Pretrain', (0, 3)), ('NQG +', (5, 7)), ('word embedding matrix', (11, 14)), ('pre-trained GloVe vectors', (15, 18))]","[['NQG + Pretrain', 'Based on', 'NQG +'], ['NQG +', 'initialize', 'word embedding matrix'], ['word embedding matrix', 'with', 'pre-trained GloVe vectors']]",[],[],"[['Baselines', 'has', 'NQG + Pretrain']]",[],[],[],[],[],question_generation,0,79
baselines,"NQG + STshare Based on NQG + , we make the encoder and decoder share the same embedding matrix .","[('Based on', (3, 5)), ('make', (9, 10)), ('share', (14, 15))]","[('NQG + STshare', (0, 3)), ('NQG +', (5, 7)), ('encoder and decoder', (11, 14)), ('same embedding matrix', (16, 19))]","[['NQG + STshare', 'Based on', 'NQG +'], ['NQG + STshare', 'make', 'encoder and decoder'], ['encoder and decoder', 'share', 'same embedding matrix']]",[],[],"[['Baselines', 'has', 'NQG + STshare']]",[],[],[],[],[],question_generation,0,80
baselines,NQG ++,[],"[('NQG ++', (0, 2))]",[],[],[],"[['Baselines', 'has', 'NQG ++']]",[],[],[],[],[],question_generation,0,81
baselines,"Based on NQG + , we use both pre-train word embedding and STshare methods , to further improve the performance .","[('Based on', (0, 2)), ('use', (6, 7)), ('to further improve', (15, 18))]","[('NQG +', (2, 4)), ('pre-train word embedding and STshare methods', (8, 14)), ('performance', (19, 20))]","[['NQG +', 'use', 'pre-train word embedding and STshare methods'], ['pre-train word embedding and STshare methods', 'to further improve', 'performance']]",[],[],[],[],"[['NQG ++', 'Based on', 'NQG +']]",[],[],[],question_generation,0,82
results,Our NQG framework outperforms the PCFG - Trans and s 2s + att baselines by a large margin .,"[('outperforms', (3, 4)), ('by', (14, 15))]","[('NQG framework', (1, 3)), ('PCFG - Trans and s 2s + att baselines', (5, 14)), ('large margin', (16, 18))]","[['NQG framework', 'outperforms', 'PCFG - Trans and s 2s + att baselines'], ['PCFG - Trans and s 2s + att baselines', 'by', 'large margin']]",[],[],"[['Results', 'has', 'NQG framework']]",[],[],[],[],[],question_generation,0,89
results,"With the help of copy mechanism , NQG + has a 2.05 BLEU improvement since it solves the rare words problem .","[('With the help of', (0, 4))]","[('copy mechanism', (4, 6)), ('NQG +', (7, 9)), ('2.05 BLEU improvement', (11, 14))]",[],"[['copy mechanism', 'has', 'NQG +'], ['NQG +', 'has', '2.05 BLEU improvement']]","[['Results', 'With the help of', 'copy mechanism']]",[],[],[],[],[],[],question_generation,0,91
results,"The extended version , NQG ++ , has 1.11 BLEU score gain over NQG + , which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation .","[('over', (12, 13))]","[('NQG ++', (4, 6)), ('1.11 BLEU score gain', (8, 12)), ('NQG +', (13, 15))]","[['1.11 BLEU score gain', 'over', 'NQG +']]","[['NQG ++', 'has', '1.11 BLEU score gain']]",[],"[['Results', 'has', 'NQG ++']]",[],[],[],[],[],question_generation,0,92
ablation-analysis,"The answer position indicator , as expected , plays a crucial role in answer focused question generation as shown in the NQG ?","[('plays', (8, 9)), ('in', (12, 13))]","[('answer position indicator', (1, 4)), ('crucial role', (10, 12)), ('answer focused question generation', (13, 17))]","[['answer position indicator', 'plays', 'crucial role'], ['crucial role', 'in', 'answer focused question generation']]",[],[],"[['Ablation analysis', 'has', 'answer position indicator']]",[],[],[],[],[],question_generation,0,104
ablation-analysis,"NER , show that word case , POS and NER tag features contributes to question generation .","[('show that', (2, 4)), ('contributes to', (12, 14))]","[('word case , POS and NER tag features', (4, 12)), ('question generation', (14, 16))]","[['word case , POS and NER tag features', 'contributes to', 'question generation']]",[],"[['Ablation analysis', 'show that', 'word case , POS and NER tag features']]",[],[],[],[],[],[],question_generation,0,109
research-problem,Multimodal Differential Network for Visual Question Generation,[],"[('Visual Question Generation', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Visual Question Generation']]",[],[],[],[],question_generation,1,2
research-problem,Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,[],"[('Generating natural questions from an image', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generating natural questions from an image']]",[],[],[],[],question_generation,1,4
research-problem,Here the au-thors have proposed the challenging task of generating natural questions for an image .,[],"[('generating natural questions for an image', (9, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'generating natural questions for an image']]",[],[],[],[],question_generation,1,18
approach,"To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .","[('use', (6, 7)), ('by considering', (10, 12)), ('between', (19, 20))]","[('context', (8, 9)), ('exemplars', (12, 13)), ('difference', (18, 19)), ('relevant and irrelevant exemplars', (20, 24))]","[['context', 'by considering', 'exemplars'], ['difference', 'between', 'relevant and irrelevant exemplars']]",[],"[['Approach', 'use', 'context'], ['Approach', 'use', 'difference']]",[],[],[],[],[],[],question_generation,1,24
approach,"We consider different contexts in the form of Location , Caption , and Part of Speech tags .","[('consider', (1, 2)), ('in the form of', (4, 8))]","[('different contexts', (2, 4)), ('Location , Caption , and Part of Speech tags', (8, 17))]","[['different contexts', 'in the form of', 'Location , Caption , and Part of Speech tags']]",[],"[['Approach', 'consider', 'different contexts']]",[],[],[],[],[],[],question_generation,1,25
approach,Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .,"[('uses', (3, 4)), ('obtained through', (7, 9)), ('to obtain', (13, 15))]","[('differential context', (5, 7)), ('supporting and contrasting exemplars', (9, 13)), ('differentiable embedding', (16, 18))]","[['differential context', 'obtained through', 'supporting and contrasting exemplars'], ['supporting and contrasting exemplars', 'to obtain', 'differentiable embedding']]",[],"[['Approach', 'uses', 'differential context']]",[],[],[],[],[],[],question_generation,1,26
approach,This embedding is used by a question decoder to decode the appropriate question .,"[('used by', (3, 5)), ('to decode', (8, 10))]","[('question decoder', (6, 8)), ('appropriate question', (11, 13))]","[['question decoder', 'to decode', 'appropriate question']]",[],[],[],[],"[['differentiable embedding', 'used by', 'question decoder']]",[],[],[],question_generation,1,27
approach,"To summarize , we propose a multimodal differential network to solve the task of visual question generation .","[('propose', (4, 5)), ('to solve', (9, 11)), ('of', (13, 14))]","[('multimodal differential network', (6, 9)), ('task', (12, 13)), ('visual question generation', (14, 17))]","[['multimodal differential network', 'to solve', 'task'], ['task', 'of', 'visual question generation']]",[],"[['Approach', 'propose', 'multimodal differential network']]",[],[],[],[],[],[],question_generation,1,36
research-problem,Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic,[],"[('Semantic Question Similarity in Arabic', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Question Similarity in Arabic']]",[],[],[],[],question_similarity,0,2
research-problem,"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 .",[],"[('semantic text question similarity', (12, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic text question similarity']]",[],[],[],[],question_similarity,0,4
research-problem,Semantic Text Similarity ( STS ) problems are both real - life and challenging .,[],"[('Semantic Text Similarity ( STS )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Text Similarity ( STS )']]",[],[],[],[],question_similarity,0,10
research-problem,"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not .",[],"[('STS', (9, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'STS']]",[],[],[],[],question_similarity,0,11
research-problem,A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,[],"[('Semantic Question Similarity ( SQS ) for the Arabic language', (21, 31))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Question Similarity ( SQS ) for the Arabic language']]",[],[],[],[],question_similarity,0,14
research-problem,"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not .",[],"[('SQS', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'SQS']]",[],[],[],[],question_similarity,0,15
model,We then build a neural network model with four components .,"[('build', (2, 3)), ('with', (7, 8))]","[('neural network model', (4, 7)), ('four components', (8, 10))]","[['neural network model', 'with', 'four components']]",[],"[['Model', 'build', 'neural network model']]",[],[],[],[],[],[],question_similarity,0,19
model,The model uses ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs .,"[('uses', (2, 3)), ('as', (16, 17)), ('builds', (20, 21)), ('to predict', (27, 29))]","[('ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings', (3, 16)), ('input', (18, 19)), ('sequence representation vectors', (21, 24)), ('relation between the question pairs', (30, 35))]","[['sequence representation vectors', 'to predict', 'relation between the question pairs'], ['ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings', 'as', 'input']]",[],"[['Model', 'builds', 'sequence representation vectors'], ['Model', 'uses', 'ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings']]",[],[],[],[],[],[],question_similarity,0,20
experimental-setup,All experiments discussed in this work have been done on the Google Colab 7 environment using Tesla T4 GPU accelerator with the following hyperparameters :,"[('done on', (8, 10)), ('using', (15, 16))]","[('Google Colab 7 environment', (11, 15)), ('Tesla T4 GPU accelerator', (16, 20))]","[['Google Colab 7 environment', 'using', 'Tesla T4 GPU accelerator']]",[],[],[],[],"[['Experimental Setup', 'done on', 'Google Colab 7 environment']]",[],[],[],question_similarity,0,85
results,"The tables show that while GRU cells are the most efficient , the ON - LSTM cells ( with chunk size 8 ) are the most effective ( in terms of all considered measures ) .",[],[],"[['GRU cells', 'are', 'most efficient'], ['ON - LSTM cells', 'with', 'chunk size 8'], ['ON - LSTM cells', 'are', 'most effective'], ['most effective', 'in terms of', 'all considered measures']]",[],[],"[['Results', 'has', 'GRU cells'], ['Results', 'has', 'ON - LSTM cells']]",[],[],[],[],[],question_similarity,0,100
results,Effect of Data Augmentation,[],"[('Effect of Data Augmentation', (0, 4))]",[],[],[],"[['Results', 'has', 'Effect of Data Augmentation']]",[],[],[],[],[],question_similarity,0,101
results,The tables show that each augmentation step affects the model 's efficiency negatively .,"[('show', (2, 3)), ('affects', (7, 8))]","[('each augmentation step', (4, 7)), (""model 's efficiency"", (9, 12)), ('negatively', (12, 13))]","[['each augmentation step', 'affects', ""model 's efficiency""]]","[[""model 's efficiency"", 'has', 'negatively']]",[],[],[],"[['Effect of Data Augmentation', 'show', 'each augmentation step']]",[],[],[],question_similarity,0,106
results,"On the other hand , not each increment step has a positive effect on the model 's effectiveness .","[('not', (5, 6)), ('on', (13, 14))]","[('each increment step', (6, 9)), ('positive effect', (11, 13)), (""model 's effectiveness"", (15, 18))]","[['positive effect', 'on', ""model 's effectiveness""]]","[['each increment step', 'has', 'positive effect']]",[],[],[],"[['Effect of Data Augmentation', 'not', 'each increment step']]",[],[],[],question_similarity,0,108
results,"For example , using pre-trained FastText embeddings as an input to our model yields worse F1score on both public and private leaderboards with 94.254 and 93.118 , respectively , compared with the ELMo contextual embeddings model .","[('using', (3, 4)), ('as', (7, 8)), ('to', (10, 11)), ('yields', (13, 14)), ('on', (16, 17)), ('with', (22, 23)), ('compared with', (29, 31))]","[('pre-trained FastText embeddings', (4, 7)), ('input', (9, 10)), ('our model', (11, 13)), ('worse F1score', (14, 16)), ('public and private leaderboards', (18, 22)), ('94.254 and 93.118', (23, 26)), ('ELMo contextual embeddings model', (32, 36))]","[['pre-trained FastText embeddings', 'as', 'input'], ['input', 'to', 'our model'], ['our model', 'yields', 'worse F1score'], ['our model', 'compared with', 'ELMo contextual embeddings model'], ['our model', 'on', 'public and private leaderboards'], ['public and private leaderboards', 'with', '94.254 and 93.118']]",[],"[['Results', 'using', 'pre-trained FastText embeddings']]",[],[],[],[],[],[],question_similarity,0,114
results,"However , the sequence weighted attention gives better results by about 1 point of the F1-score .","[('gives', (6, 7)), ('by', (9, 10)), ('of', (13, 14))]","[('sequence weighted attention', (3, 6)), ('better results', (7, 9)), ('about 1 point', (10, 13)), ('F1-score', (15, 16))]","[['sequence weighted attention', 'gives', 'better results'], ['better results', 'by', 'about 1 point'], ['about 1 point', 'of', 'F1-score']]",[],[],"[['Results', 'has', 'sequence weighted attention']]",[],[],[],[],[],question_similarity,0,116
results,"Moreover , an attempt to overcome the weakness of the Arabic ELMo model is done by translating the data to English using Google Translate 8 and treating the problem as an English SQS problem instead , but the results are much worse with 88.868 and 87.504 F1 - scores on public and private leaderboards , respectively .","[('to', (4, 5)), ('overcome', (5, 6)), ('of', (8, 9)), ('by translating', (15, 17)), ('using', (21, 22)), ('treating', (26, 27)), ('as', (29, 30)), ('results are', (38, 40)), ('with', (42, 43)), ('on', (49, 50))]","[('weakness', (7, 8)), ('Arabic ELMo model', (10, 13)), ('data', (18, 19)), ('English', (20, 21)), ('Google Translate', (22, 24)), ('problem', (28, 29)), ('English SQS problem', (31, 34)), ('much worse', (40, 42)), ('88.868 and 87.504 F1 - scores', (43, 49)), ('public and private leaderboards', (50, 54))]","[['weakness', 'of', 'Arabic ELMo model'], ['Arabic ELMo model', 'by translating', 'data'], ['data', 'to', 'English'], ['English', 'using', 'Google Translate'], ['Arabic ELMo model', 'results are', 'much worse'], ['much worse', 'with', '88.868 and 87.504 F1 - scores'], ['88.868 and 87.504 F1 - scores', 'on', 'public and private leaderboards'], ['Arabic ELMo model', 'treating', 'problem'], ['problem', 'as', 'English SQS problem']]",[],"[['Results', 'overcome', 'weakness']]",[],[],[],[],[],[],question_similarity,0,117
research-problem,SCIBERT : A Pretrained Language Model for Scientific Text,[],"[('Pretrained Language Model', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Pretrained Language Model']]",[],[],[],[],relation-classification,9,2
research-problem,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,[],"[('Obtaining large - scale annotated data for NLP tasks in the scientific domain', (0, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Obtaining large - scale annotated data for NLP tasks in the scientific domain']]",[],[],[],[],relation-classification,9,4
research-problem,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",[],"[('pretrained language model based on BERT ( Devlin et al. , 2019 )', (5, 18)), ('address the lack of high - quality , large - scale labeled scientific data', (19, 33))]",[],[],[],[],"[['Contribution', 'has research problem', 'pretrained language model based on BERT ( Devlin et al. , 2019 )'], ['Contribution', 'has research problem', 'address the lack of high - quality , large - scale labeled scientific data']]",[],[],[],[],relation-classification,9,5
code,The code and pretrained models are available at https://github.com/allenai/scibert/.,[],[],[],[],[],[],[],[],[],[],[],relation-classification,9,9
research-problem,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",[],"[('annotated data is difficult and expensive to collect due to the expertise required for quality annotation', (22, 38))]",[],[],[],[],"[['Contribution', 'has research problem', 'annotated data is difficult and expensive to collect due to the expertise required for quality annotation']]",[],[],[],[],relation-classification,9,13
approach,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,"[('follows', (3, 4)), ('pretrained on', (12, 14))]","[('SCIB - ERT', (0, 3)), ('same architecture as BERT', (5, 9)), ('scientific text', (14, 16))]","[['SCIB - ERT', 'follows', 'same architecture as BERT'], ['same architecture as BERT', 'pretrained on', 'scientific text']]",[],[],"[['Approach', 'name', 'SCIB - ERT']]",[],[],[],[],[],relation-classification,9,27
approach,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .","[('construct', (1, 2)), ('on', (8, 9)), ('using', (12, 13))]","[('SCIVOCAB', (2, 3)), ('a new WordPiece vocabulary', (4, 8)), ('our scientific corpus', (9, 12)), ('Sen - tencePiece 1 library', (14, 19))]","[['a new WordPiece vocabulary', 'on', 'our scientific corpus'], ['our scientific corpus', 'using', 'Sen - tencePiece 1 library']]","[['SCIVOCAB', 'has', 'a new WordPiece vocabulary']]","[['Approach', 'construct', 'SCIVOCAB']]",[],[],[],[],[],[],relation-classification,9,31
approach,Corpus,[],"[('Corpus', (0, 1))]",[],[],[],"[['Approach', 'has', 'Corpus']]",[],[],[],[],[],relation-classification,9,34
approach,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,"[('train', (1, 2)), ('on', (3, 4)), ('of', (7, 8)), ('from', (11, 12))]","[('SCIBERT', (2, 3)), ('random sample', (5, 7)), ('1.14 M papers', (8, 11)), ('Semantic Scholar', (12, 14))]","[['SCIBERT', 'on', 'random sample'], ['random sample', 'of', '1.14 M papers'], ['1.14 M papers', 'from', 'Semantic Scholar']]",[],[],[],[],"[['Corpus', 'train', 'SCIBERT']]",[],[],[],relation-classification,9,35
approach,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,[],[],"[['82 %', 'from', 'broad biomedical domain'], ['18 % papers', 'from', 'computer science domain']]",[],[],[],[],"[['Corpus', 'consists', '82 %'], ['Corpus', 'consists', '18 % papers']]",[],[],[],relation-classification,9,36
tasks,Named Entity Recognition ( NER ),[],"[('Named Entity Recognition', (0, 3)), ('NER', (4, 5))]",[],"[['Named Entity Recognition', 'name', 'NER']]",[],"[['Tasks', 'has', 'Named Entity Recognition']]",[],[],[],[],[],relation-classification,9,44
tasks,2 . PICO Extraction ( PICO ),[],"[('PICO Extraction', (2, 4)), ('PICO', (5, 6))]",[],"[['PICO Extraction', 'name', 'PICO']]",[],"[['Tasks', 'has', 'PICO Extraction']]",[],[],[],[],[],relation-classification,9,45
tasks,3 . Text Classification ( CLS ),[],"[('Text Classification', (2, 4)), ('CLS', (5, 6))]",[],"[['Text Classification', 'name', 'CLS']]",[],"[['Tasks', 'has', 'Text Classification']]",[],[],[],[],[],relation-classification,9,46
tasks,4 . Relation Classification ( REL ),[],"[('Relation Classification', (2, 4)), ('REL', (5, 6))]",[],"[['Relation Classification', 'name', 'REL']]",[],"[['Tasks', 'has', 'Relation Classification']]",[],[],[],[],[],relation-classification,9,47
,5 . Dependency Parsing ( DEP ) ,[],[],[],[],[],[],[],[],[],[],[],relation-classification,9,48
results,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),"[('observe', (1, 2)), ('outperforms', (4, 5)), ('on', (8, 9))]","[('SCIBERT', (3, 4)), ('BERT - Base', (5, 8)), ('scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without )', (9, 23))]","[['SCIBERT', 'outperforms', 'BERT - Base'], ['SCIBERT', 'outperforms', 'BERT - Base'], ['SCIBERT', 'outperforms', 'BERT - Base'], ['SCIBERT', 'outperforms', 'BERT - Base'], ['BERT - Base', 'on', 'scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without )']]",[],"[['Results', 'observe', 'SCIBERT']]",[],[],[],[],[],[],relation-classification,9,103
results,Biomedical Domain,[],"[('Biomedical Domain', (0, 2))]",[],[],[],"[['Results', 'has', 'Biomedical Domain']]",[],[],[],[],[],relation-classification,9,106
results,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,[],[],"[['BERT - Base', 'on', 'biomedical tasks'], ['biomedical tasks', 'with finetuning', '+ 1.92 F1'], ['biomedical tasks', 'without', '+ 3.59 F1']]",[],[],[],[],"[['Biomedical Domain', 'observe', 'SCIBERT']]",[],[],[],relation-classification,9,107
results,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .","[('achieves', (6, 7)), ('on', (10, 11))]","[('SCIB - ERT', (3, 6)), ('new SOTA results', (7, 10)), ('BC5 CDR and ChemProt', (11, 15)), ('EBM - NLP', (17, 20))]","[['SCIB - ERT', 'achieves', 'new SOTA results'], ['new SOTA results', 'on', 'BC5 CDR and ChemProt'], ['new SOTA results', 'on', 'EBM - NLP']]",[],[],[],[],[],[],"[['Biomedical Domain', 'has', 'SCIB - ERT']]",[],relation-classification,9,108
results,Computer Science Domain,[],"[('Computer Science Domain', (0, 3))]",[],[],[],"[['Results', 'has', 'Computer Science Domain']]",[],[],[],[],[],relation-classification,9,118
results,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,[],[],"[['BERT - Base', 'on', 'computer science tasks'], ['computer science tasks', 'with finetuning', '+ 3.55 F1'], ['computer science tasks', 'without', '+ 1.13 F1']]",[],[],[],[],"[['Computer Science Domain', 'observe', 'SCIBERT']]",[],[],[],relation-classification,9,119
results,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .","[('achieves', (4, 5)), ('on', (8, 9))]","[('SCIBERT', (3, 4)), ('new SOTA results', (5, 8)), ('ACL - ARC', (9, 12)), ('NER part of SciERC', (15, 19))]","[['SCIBERT', 'achieves', 'new SOTA results'], ['new SOTA results', 'on', 'ACL - ARC'], ['new SOTA results', 'on', 'NER part of SciERC']]",[],[],[],[],[],[],"[['Computer Science Domain', 'has', 'SCIBERT']]",[],relation-classification,9,120
results,Multiple Domains,[],"[('Multiple Domains', (0, 2))]",[],[],[],"[['Results', 'has', 'Multiple Domains']]",[],[],[],[],[],relation-classification,9,122
results,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,[],[],"[['BERT - Base', 'on', 'multidomain tasks'], ['multidomain tasks', 'with finetuning', '+ 0.49 F1'], ['multidomain tasks', 'without', '+ 0.93 F1']]",[],[],[],[],"[['Multiple Domains', 'observe', 'SCIBERT']]",[],[],[],relation-classification,9,123
results,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .","[('outperforms', (4, 5)), ('on', (7, 8))]","[('SCIBERT', (3, 4)), ('SOTA', (6, 7)), ('Sci - Cite', (8, 11))]","[['SCIBERT', 'outperforms', 'SOTA'], ['SOTA', 'on', 'Sci - Cite']]",[],[],[],[],[],[],"[['Multiple Domains', 'has', 'SCIBERT']]",[],relation-classification,9,124
research-problem,Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees,[],"[('Joint Extraction of Entity Mentions and Relations', (6, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Joint Extraction of Entity Mentions and Relations']]",[],[],[],[],relation_extraction,0,2
research-problem,Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .,[],"[('Extraction of entities and their relations from text', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Extraction of entities and their relations from text']]",[],[],[],[],relation_extraction,0,10
research-problem,Several methods have been proposed for entity mention and relation extraction at the sentencelevel .,[],"[('entity mention and relation extraction at the sentencelevel', (6, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'entity mention and relation extraction at the sentencelevel']]",[],[],[],[],relation_extraction,0,12
model,"In this paper , we propose a novel RNN - based model for the joint extraction of entity mentions and relations .","[('propose', (5, 6)), ('for', (12, 13))]","[('novel RNN - based model', (7, 12)), ('joint extraction of entity mentions and relations', (14, 21))]","[['novel RNN - based model', 'for', 'joint extraction of entity mentions and relations']]",[],"[['Model', 'propose', 'novel RNN - based model']]",[],[],[],[],[],[],relation_extraction,0,24
model,"Unlike other models , our model does not depend on any dependency tree information .","[('does not', (6, 8)), ('on', (9, 10))]","[('depend', (8, 9)), ('any dependency tree information', (10, 14))]","[['depend', 'on', 'any dependency tree information']]",[],"[['Model', 'does not', 'depend']]",[],[],[],[],[],[],relation_extraction,0,25
model,Our RNN - based model is a multi - layer bidirectional LSTM over a sequence .,"[('is', (5, 6)), ('over', (12, 13))]","[('Our RNN - based model', (0, 5)), ('multi - layer bidirectional LSTM', (7, 12)), ('sequence', (14, 15))]","[['Our RNN - based model', 'is', 'multi - layer bidirectional LSTM'], ['multi - layer bidirectional LSTM', 'over', 'sequence']]",[],[],"[['Model', 'has', 'Our RNN - based model']]",[],[],[],[],[],relation_extraction,0,26
model,We encode the output sequence from left - to - right .,"[('encode', (1, 2)), ('from', (5, 6))]","[('output sequence', (3, 5)), ('left - to - right', (6, 11))]","[['output sequence', 'from', 'left - to - right']]",[],"[['Model', 'encode', 'output sequence']]",[],[],[],[],[],[],relation_extraction,0,27
model,"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .","[('At', (0, 1)), ('use', (6, 7)), ('on', (12, 13)), ('to identify', (19, 21)), ('in', (23, 24)), ('with', (27, 28))]","[('each time step', (1, 4)), ('attention - like model', (8, 12)), ('previously decoded time steps', (14, 18)), ('tokens', (22, 23)), ('specified relation', (25, 27)), ('current token', (29, 31))]","[['each time step', 'use', 'attention - like model'], ['attention - like model', 'to identify', 'tokens'], ['tokens', 'in', 'specified relation'], ['specified relation', 'with', 'current token'], ['attention - like model', 'on', 'previously decoded time steps']]",[],"[['Model', 'At', 'each time step']]",[],[],[],[],[],[],relation_extraction,0,28
model,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,"[('add', (2, 3)), ('to', (6, 7)), ('to encode', (9, 11)), ('from', (14, 15))]","[('additional layer', (4, 6)), ('our network', (7, 9)), ('output sequence', (12, 14)), ('right - to - left', (15, 20))]","[['additional layer', 'to encode', 'output sequence'], ['output sequence', 'from', 'right - to - left'], ['additional layer', 'to', 'our network']]",[],"[['Model', 'add', 'additional layer']]",[],[],[],[],[],[],relation_extraction,0,29
baselines,The model proposed by is a feature - based structured perceptron model with efficient beam - search .,"[('with', (12, 13))]","[('feature - based structured perceptron model', (6, 12)), ('efficient beam - search', (13, 17))]","[['feature - based structured perceptron model', 'with', 'efficient beam - search']]",[],[],"[['Baselines', 'has', 'feature - based structured perceptron model']]",[],[],[],[],[],relation_extraction,0,170
baselines,They employ a segment - based decoder instead of token - based decoding .,"[('employ', (1, 2)), ('instead of', (7, 9))]","[('segment - based decoder', (3, 7)), ('token - based decoding', (9, 13))]","[['segment - based decoder', 'instead of', 'token - based decoding']]",[],[],[],[],"[['feature - based structured perceptron model', 'employ', 'segment - based decoder']]",[],[],[],relation_extraction,0,171
baselines,"( SPTree ) recently proposed a LSTM - based model with a sequence layer for entity identification , and a tree - based dependency layer which identifies relations between pairs of candidate entities using the shortest dependency path between them .","[('proposed', (4, 5)), ('with', (10, 11)), ('for', (14, 15)), ('which identifies', (25, 27))]","[('SPTree', (1, 2)), ('LSTM - based model', (6, 10)), ('sequence layer', (12, 14)), ('entity identification', (15, 17)), ('tree - based dependency layer', (20, 25)), ('relations between pairs of candidate entities', (27, 33))]","[['SPTree', 'proposed', 'LSTM - based model'], ['LSTM - based model', 'with', 'sequence layer'], ['sequence layer', 'for', 'entity identification'], ['LSTM - based model', 'with', 'tree - based dependency layer'], ['tree - based dependency layer', 'which identifies', 'relations between pairs of candidate entities']]",[],[],"[['Baselines', 'has', 'SPTree']]",[],[],[],[],[],relation_extraction,0,173
baselines,We also employed our previous approach for extraction of opinion entities and relations to this task .,"[('employed', (2, 3)), ('for', (6, 7)), ('of', (8, 9)), ('to', (13, 14))]","[('our previous approach', (3, 6)), ('extraction', (7, 8)), ('opinion entities and relations', (9, 13)), ('this task', (14, 16))]","[['our previous approach', 'for', 'extraction'], ['extraction', 'of', 'opinion entities and relations'], ['opinion entities and relations', 'to', 'this task']]",[],"[['Baselines', 'employed', 'our previous approach']]",[],[],[],[],[],[],relation_extraction,0,174
hyperparameters,We train our model using Adadelta with gradient clipping .,"[('train', (1, 2)), ('using', (4, 5)), ('with', (6, 7))]","[('our model', (2, 4)), ('Adadelta', (5, 6)), ('gradient clipping', (7, 9))]","[['our model', 'using', 'Adadelta'], ['Adadelta', 'with', 'gradient clipping']]",[],"[['Hyperparameters', 'train', 'our model']]",[],[],[],[],[],[],relation_extraction,0,180
hyperparameters,We regularize our network using dropout with the drop - out rate tuned using development set .,"[('regularize', (1, 2)), ('using', (4, 5)), ('with', (6, 7)), ('tuned using', (12, 14))]","[('our network', (2, 4)), ('dropout', (5, 6)), ('drop - out rate', (8, 12)), ('development set', (14, 16))]","[['our network', 'using', 'dropout'], ['dropout', 'with', 'drop - out rate'], ['drop - out rate', 'tuned using', 'development set']]",[],"[['Hyperparameters', 'regularize', 'our network']]",[],[],[],[],[],[],relation_extraction,0,181
hyperparameters,We have 3 hidden layers in our network and the dimensionality of the hidden units is 100 .,"[('have', (1, 2)), ('in', (5, 6)), ('of', (11, 12)), ('is', (15, 16))]","[('3 hidden layers', (2, 5)), ('our network', (6, 8)), ('dimensionality', (10, 11)), ('hidden units', (13, 15)), ('100', (16, 17))]","[['dimensionality', 'of', 'hidden units'], ['hidden units', 'is', '100'], ['3 hidden layers', 'in', 'our network']]",[],"[['Hyperparameters', 'have', 'dimensionality'], ['Hyperparameters', 'have', '3 hidden layers']]",[],[],[],[],[],[],relation_extraction,0,186
hyperparameters,All the weights in the network are initialized from small random uniform noise .,"[('in', (3, 4)), ('initialized from', (7, 9))]","[('weights', (2, 3)), ('network', (5, 6)), ('random uniform noise', (10, 13))]","[['weights', 'in', 'network'], ['network', 'initialized from', 'random uniform noise']]",[],[],"[['Hyperparameters', 'has', 'weights']]",[],[],[],[],[],relation_extraction,0,187
hyperparameters,We tune our hyperparameters based on ACE05 development set and use them for training on ACE04 dataset .,"[('tune', (1, 2)), ('based on', (4, 6)), ('use them for', (10, 13)), ('on', (14, 15))]","[('hyperparameters', (3, 4)), ('ACE05 development set', (6, 9)), ('training', (13, 14)), ('ACE04 dataset', (15, 17))]","[['hyperparameters', 'use them for', 'training'], ['training', 'on', 'ACE04 dataset'], ['hyperparameters', 'based on', 'ACE05 development set']]",[],"[['Hyperparameters', 'tune', 'hyperparameters']]",[],[],[],[],[],[],relation_extraction,0,188
results,Multiple Relations,[],[],[],[],[],[],[],[],[],[],[],relation_extraction,0,196
results,"We find that modifying our objective to include multiple relations improves the recall of our system on relations , leading to slight improvement on the over all performance on relations .",[],[],"[['modifying our objective', 'to include', 'multiple relations'], ['multiple relations', 'improves', 'recall'], ['recall', 'leading to', 'slight improvement'], ['slight improvement', 'on', 'over all performance on relations'], ['recall', 'of', 'our system'], ['recall', 'on', 'relations']]",[],[],[],[],"[['Multiple Relations', 'find that', 'modifying our objective']]",[],[],[],relation_extraction,0,197
results,"By adding bidirectional encoding to our system , we find that we can significantly improve the performance of our system compared to left - to - right encoding .",[],[],"[['bidirectional encoding', 'find', 'significantly improve'], ['performance', 'of', 'our system'], ['performance', 'compared to', 'left - to - right encoding'], ['bidirectional encoding', 'to', 'our system']]","[['significantly improve', 'has', 'performance']]","[['Results', 'adding', 'bidirectional encoding']]",[],[],[],[],[],[],relation_extraction,0,201
results,It also improves precision compared to left - toright decoding combined with multiple relations objective .,"[('improves', (2, 3)), ('compared to', (4, 6)), ('combined with', (10, 12))]","[('precision', (3, 4)), ('left - toright decoding', (6, 10)), ('multiple relations objective', (12, 15))]","[['precision', 'compared to', 'left - toright decoding'], ['left - toright decoding', 'combined with', 'multiple relations objective']]",[],"[['Results', 'improves', 'precision']]",[],[],[],[],[],[],relation_extraction,0,202
results,We find that for some relations it is easier to detect them with respect to one of the entities in the entity pair .,"[('find that for', (1, 4)), ('is', (7, 8)), ('to detect', (9, 11)), ('in', (19, 20))]","[('some relations', (4, 6)), ('easier', (8, 9)), ('with respect to', (12, 15)), ('one of the entities', (15, 19)), ('entity pair', (21, 23))]","[['some relations', 'is', 'easier'], ['easier', 'to detect', 'with respect to'], ['one of the entities', 'in', 'entity pair']]","[['with respect to', 'has', 'one of the entities']]","[['Results', 'find that for', 'some relations']]",[],[],[],[],[],[],relation_extraction,0,203
results,PHYS relation is easier identified with respect to GPE entity than PER entity .,"[('is', (2, 3)), ('with respect to', (5, 8)), ('than', (10, 11))]","[('PHYS relation', (0, 2)), ('easier identified', (3, 5)), ('GPE entity', (8, 10)), ('PER entity', (11, 13))]","[['PHYS relation', 'is', 'easier identified'], ['easier identified', 'with respect to', 'GPE entity'], ['GPE entity', 'than', 'PER entity']]",[],[],"[['Results', 'has', 'PHYS relation']]",[],[],[],[],[],relation_extraction,0,204
research-problem,Simple BERT Models for Relation Extraction and Semantic Role Labeling,[],"[('Relation Extraction', (4, 6)), ('Semantic Role Labeling', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction'], ['Contribution', 'has research problem', 'Semantic Role Labeling']]",[],[],[],[],relation_extraction,1,2
research-problem,Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .,[],"[('semantic role labeling ( SRL )', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic role labeling ( SRL )']]",[],[],[],[],relation_extraction,1,10
research-problem,"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc .",[],"[('SRL', (1, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],relation_extraction,1,14
model,We show that simple neural architectures built on top of BERT yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,"[('show', (1, 2)), ('built on top of', (6, 10)), ('yields', (11, 12)), ('on', (20, 21))]","[('simple neural architectures', (3, 6)), ('BERT', (10, 11)), ('state - of - the - art performance', (12, 20)), ('variety of benchmark datasets', (22, 26))]","[['simple neural architectures', 'built on top of', 'BERT'], ['simple neural architectures', 'yields', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'variety of benchmark datasets']]",[],"[['Model', 'show', 'simple neural architectures']]",[],[],[],[],[],[],relation_extraction,1,24
hyperparameters,We conduct experiments on two SRL tasks : and the predicate indicator embedding size is 10 .,"[('is', (14, 15))]","[('predicate indicator embedding size', (10, 14)), ('10', (15, 16))]","[['predicate indicator embedding size', 'is', '10']]",[],[],"[['Hyperparameters', 'has', 'predicate indicator embedding size']]",[],[],[],[],[],relation_extraction,1,64
hyperparameters,The learning rate is 5 10 ?5 . BERT base - cased and large - cased models are used in our experiments .,"[('used in', (18, 20))]","[('learning rate', (1, 3)), ('5 10 ?5', (4, 7)), ('BERT base - cased and large - cased models', (8, 17)), ('our experiments', (20, 22))]","[['BERT base - cased and large - cased models', 'used in', 'our experiments']]","[['learning rate', 'has', '5 10 ?5']]",[],"[['Hyperparameters', 'has', 'BERT base - cased and large - cased models'], ['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],relation_extraction,1,65
hyperparameters,The position embeddings are randomly initialized and fine - tuned during the training process .,"[('are', (3, 4)), ('during', (10, 11))]","[('position embeddings', (1, 3)), ('randomly initialized and fine - tuned', (4, 10)), ('training process', (12, 14))]","[['position embeddings', 'are', 'randomly initialized and fine - tuned'], ['randomly initialized and fine - tuned', 'during', 'training process']]",[],[],"[['Hyperparameters', 'has', 'position embeddings']]",[],[],[],[],[],relation_extraction,1,66
results,We see that the BERT - LSTM - large model achieves the state - of - the - art F 1 score among single models and outperforms the ensemble model on the CoNLL 2005 in - domain and out - of - domain tests .,"[('see', (1, 2)), ('achieves', (10, 11)), ('among', (22, 23)), ('outperforms', (26, 27)), ('on', (30, 31))]","[('BERT - LSTM - large model', (4, 10)), ('state - of - the - art F 1 score', (12, 22)), ('single models', (23, 25)), ('ensemble model', (28, 30)), ('CoNLL 2005', (32, 34)), ('in - domain and out - of - domain tests', (34, 44))]","[['BERT - LSTM - large model', 'achieves', 'state - of - the - art F 1 score'], ['state - of - the - art F 1 score', 'among', 'single models'], ['BERT - LSTM - large model', 'outperforms', 'ensemble model'], ['ensemble model', 'on', 'CoNLL 2005']]","[['ensemble model', 'has', 'in - domain and out - of - domain tests']]","[['Results', 'see', 'BERT - LSTM - large model']]",[],[],[],[],[],[],relation_extraction,1,79
results,"However , it falls short on the CoNLL 2012 benchmark because the model of obtains very high precision .","[('falls short on', (3, 6))]","[('CoNLL 2012 benchmark', (7, 10))]",[],[],"[['Results', 'falls short on', 'CoNLL 2012 benchmark']]",[],[],[],[],[],[],relation_extraction,1,80
research-problem,Span - Level Model for Relation Extraction,[],"[('Relation Extraction', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction']]",[],[],[],[],relation_extraction,10,2
research-problem,"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions .",[],"[('Relation Extraction ( RE )', (4, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction ( RE )']]",[],[],[],[],relation_extraction,10,13
research-problem,"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction .",[],"[('RE', (4, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'RE']]",[],[],[],[],relation_extraction,10,21
model,We propose a simple bi - LSTM based model which generates span representations for each possible span .,"[('propose', (1, 2)), ('which generates', (9, 11)), ('for', (13, 14))]","[('simple bi - LSTM based model', (3, 9)), ('span representations', (11, 13)), ('each possible span', (14, 17))]","[['simple bi - LSTM based model', 'which generates', 'span representations'], ['span representations', 'for', 'each possible span']]",[],"[['Model', 'propose', 'simple bi - LSTM based model']]",[],[],[],[],[],[],relation_extraction,10,36
model,The span representations are used to perform entity mention detection on all spans in parallel .,"[('to perform', (5, 7)), ('on', (10, 11)), ('in', (13, 14))]","[('span representations', (1, 3)), ('entity mention detection', (7, 10)), ('all spans', (11, 13)), ('parallel', (14, 15))]","[['span representations', 'to perform', 'entity mention detection'], ['entity mention detection', 'on', 'all spans'], ['all spans', 'in', 'parallel']]",[],[],"[['Model', 'has', 'span representations']]",[],[],[],[],[],relation_extraction,10,37
model,The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,"[('on', (11, 12)), ('of', (14, 15))]","[('relation extraction', (9, 11)), ('all pairs', (12, 14)), ('detected entity mentions', (15, 18))]","[['relation extraction', 'on', 'all pairs'], ['all pairs', 'of', 'detected entity mentions']]",[],[],[],[],[],[],"[['span representations', 'to perform', 'relation extraction']]",[],relation_extraction,10,38
hyperparameters,"The learned character embeddings are of size 8 . 1 - dimensional convolutions of window size 3 , 4,5 are applied per-token with 50 filters of each window size .","[('of size', (5, 7)), ('of window size', (13, 16))]","[('learned character embeddings', (1, 4)), ('8', (7, 8)), ('1 - dimensional convolutions', (9, 13)), ('3', (16, 17))]","[['learned character embeddings', 'of size', '8'], ['1 - dimensional convolutions', 'of window size', '3']]",[],[],"[['Hyperparameters', 'has', 'learned character embeddings'], ['Hyperparameters', 'has', '1 - dimensional convolutions']]",[],[],[],[],[],relation_extraction,10,143
hyperparameters,Our stacked bi - LSTMs ( Section 3.1 ) has 3 layers with 200 - dimensional hidden states and highway connections .,"[('with', (12, 13))]","[('stacked bi - LSTMs', (1, 5)), ('3 layers', (10, 12)), ('200 - dimensional hidden states', (13, 18)), ('highway connections', (19, 21))]","[['3 layers', 'with', '200 - dimensional hidden states'], ['3 layers', 'with', 'highway connections']]","[['stacked bi - LSTMs', 'has', '3 layers']]",[],"[['Hyperparameters', 'has', 'stacked bi - LSTMs']]",[],[],[],[],[],relation_extraction,10,146
hyperparameters,"All Multi Layer Perceptrons ( MLP ) has two hidden layers with 500 dimensions , each followed by ReLU activation .","[('with', (11, 12)), ('followed by', (16, 18))]","[('All Multi Layer Perceptrons ( MLP )', (0, 7)), ('two hidden layers', (8, 11)), ('500 dimensions', (12, 14)), ('ReLU activation', (18, 20))]","[['two hidden layers', 'with', '500 dimensions'], ['two hidden layers', 'followed by', 'ReLU activation']]","[['All Multi Layer Perceptrons ( MLP )', 'has', 'two hidden layers']]",[],"[['Hyperparameters', 'has', 'All Multi Layer Perceptrons ( MLP )']]",[],[],[],[],[],relation_extraction,10,147
hyperparameters,We only consider spans that are entirely within a sentence and limit spans to a max length of L = 10 .,[],[],"[['spans', 'to', 'max length'], ['max length', 'of', 'L = 10'], ['spans', 'entirely within', 'sentence']]",[],"[['Hyperparameters', 'limit', 'spans'], ['Hyperparameters', 'consider', 'spans']]",[],[],[],[],[],[],relation_extraction,10,152
hyperparameters,"Regularization Dropout is applied with dropout rate 0.2 to all hidden layers of all MLPs and feature encodings , with dropout rate 0.5 to all word and character embeddings and with dropout rate 0.4 to all LSTM layer outputs .",[],[],"[['Regularization Dropout', 'applied with', 'dropout rate 0.2'], ['dropout rate 0.2', 'to', 'all hidden layers'], ['all hidden layers', 'of', 'all MLPs and feature encodings'], ['Regularization Dropout', 'applied with', 'dropout rate 0.4'], ['dropout rate 0.4', 'to', 'all LSTM layer outputs'], ['Regularization Dropout', 'applied with', 'dropout rate 0.5'], ['dropout rate 0.5', 'to', 'all word and character embeddings']]",[],[],"[['Hyperparameters', 'has', 'Regularization Dropout']]",[],[],[],[],[],relation_extraction,10,155
hyperparameters,"Learning Learning is done with Adam ( Kingma and Ba , 2015 ) with default parameters .","[('done with', (3, 5)), ('with', (13, 14))]","[('Learning', (0, 1)), ('Adam ( Kingma and Ba , 2015 )', (5, 13)), ('default parameters', (14, 16))]","[['Learning', 'done with', 'Adam ( Kingma and Ba , 2015 )'], ['Adam ( Kingma and Ba , 2015 )', 'with', 'default parameters']]",[],[],"[['Hyperparameters', 'has', 'Learning']]",[],[],[],[],[],relation_extraction,10,156
hyperparameters,The learning rate is annealed by 1 % every 100 iterations .,"[('annealed by', (4, 6)), ('every', (8, 9))]","[('learning rate', (1, 3)), ('1 %', (6, 8)), ('100 iterations', (9, 11))]","[['learning rate', 'annealed by', '1 %'], ['1 %', 'every', '100 iterations']]",[],[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],relation_extraction,10,157
hyperparameters,Minibatch Size is 1 .,"[('is', (2, 3))]","[('Minibatch Size', (0, 2)), ('1', (3, 4))]","[['Minibatch Size', 'is', '1']]",[],[],"[['Hyperparameters', 'has', 'Minibatch Size']]",[],[],[],[],[],relation_extraction,10,158
hyperparameters,Early Stopping of 20 evaluations on the dev set is used .,"[('of', (2, 3)), ('on', (5, 6))]","[('Early Stopping', (0, 2)), ('20 evaluations', (3, 5)), ('dev set', (7, 9))]","[['Early Stopping', 'of', '20 evaluations'], ['20 evaluations', 'on', 'dev set']]",[],[],"[['Hyperparameters', 'has', 'Early Stopping']]",[],[],[],[],[],relation_extraction,10,159
results,"Our proposed model achieves a new SOTA on RE with a F 1 of 62. 83 , more than 2.3 F 1 above the previous SOTA .","[('achieves', (3, 4)), ('on', (7, 8)), ('with', (9, 10)), ('of', (13, 14))]","[('Our proposed model', (0, 3)), ('new SOTA', (5, 7)), ('RE', (8, 9)), ('F 1', (11, 13)), ('62. 83', (14, 16))]","[['Our proposed model', 'achieves', 'new SOTA'], ['new SOTA', 'with', 'F 1'], ['F 1', 'of', '62. 83'], ['new SOTA', 'on', 'RE']]",[],[],"[['Results', 'has', 'Our proposed model']]",[],[],[],[],[],relation_extraction,10,181
results,Our proposed model also beats a multitask model which uses signals from additional tasks by more than 1.5 F 1 points .,"[('also beats', (3, 5)), ('which uses', (8, 10)), ('from', (11, 12)), ('by more than', (14, 17))]","[('multitask model', (6, 8)), ('signals', (10, 11)), ('additional tasks', (12, 14)), ('1.5 F 1 points', (17, 21))]","[['multitask model', 'by more than', '1.5 F 1 points'], ['multitask model', 'which uses', 'signals'], ['signals', 'from', 'additional tasks']]",[],[],[],[],"[['Our proposed model', 'also beats', 'multitask model']]",[],[],[],relation_extraction,10,182
results,"For both tasks , our model 's Precision is close to and Recall is significantly higher than previous works .","[('For', (0, 1)), ('close to', (9, 11)), ('significantly higher', (14, 16)), ('than', (16, 17))]","[('both tasks', (1, 3)), (""our model 's Precision"", (4, 8)), ('Recall', (12, 13)), ('previous works', (17, 19))]","[['both tasks', 'than', 'previous works'], ['previous works', 'close to', ""our model 's Precision""], ['previous works', 'significantly higher', 'Recall']]",[],"[['Results', 'For', 'both tasks']]",[],[],[],[],[],[],relation_extraction,10,183
results,The Recall gains for RE ( 4.3 absolute points ) are much higher than for EMD ( 0.6 absolute points ) .,"[('for', (3, 4)), ('much higher than for', (11, 15))]","[('Recall gains', (1, 3)), ('RE', (4, 5)), ('4.3 absolute points', (6, 9)), ('EMD', (15, 16)), ('0.6 absolute points', (17, 20))]","[['Recall gains', 'much higher than for', 'EMD'], ['Recall gains', 'for', 'RE']]","[['EMD', 'has', '0.6 absolute points'], ['RE', 'has', '4.3 absolute points']]",[],"[['Results', 'has', 'Recall gains']]",[],[],[],[],[],relation_extraction,10,184
results,"Thus , our large gains in RE Recall ( and F 1 ) showcase the effectiveness of our simple modeling of ordered span pairs for relation extraction ( Section 3.3 ) .","[('in', (5, 6)), ('showcase', (13, 14)), ('of', (16, 17)), ('for', (24, 25))]","[('our large gains', (2, 5)), ('RE Recall ( and F 1 )', (6, 13)), ('effectiveness', (15, 16)), ('our simple modeling of ordered span pairs', (17, 24)), ('relation extraction', (25, 27))]","[['our large gains', 'in', 'RE Recall ( and F 1 )'], ['RE Recall ( and F 1 )', 'showcase', 'effectiveness'], ['effectiveness', 'of', 'our simple modeling of ordered span pairs'], ['our simple modeling of ordered span pairs', 'for', 'relation extraction']]",[],[],"[['Results', 'has', 'our large gains']]",[],[],[],[],[],relation_extraction,10,187
research-problem,RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,[],"[('Distantly - Supervised Neural Relation Extraction', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Distantly - Supervised Neural Relation Extraction']]",[],[],[],[],relation_extraction,11,2
research-problem,Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .,[],"[('Distantly - supervised Relation Extraction ( RE )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Distantly - supervised Relation Extraction ( RE )']]",[],[],[],[],relation_extraction,11,4
research-problem,RE models usually ignore such readily available side information .,[],"[('RE', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'RE']]",[],[],[],[],relation_extraction,11,6
research-problem,Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .,[],"[('Relation Extraction ( RE )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction ( RE )']]",[],[],[],[],relation_extraction,11,15
model,"In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .","[('propose', (5, 6)), ('utilizes', (16, 17)), ('from', (19, 20)), ('through', (21, 22))]","[('RESIDE', (6, 7)), ('novel distant supervised relation extraction method', (9, 15)), ('additional supervision', (17, 19)), ('KB', (20, 21)), ('neural network based architecture', (23, 27))]","[['novel distant supervised relation extraction method', 'utilizes', 'additional supervision'], ['additional supervision', 'through', 'neural network based architecture'], ['additional supervision', 'from', 'KB']]","[['RESIDE', 'has', 'novel distant supervised relation extraction method']]","[['Model', 'propose', 'RESIDE']]",[],[],[],[],[],[],relation_extraction,11,39
model,"RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .","[('makes', (1, 2)), ('of', (4, 5)), ('from', (11, 12)), ('to impose', (14, 16)), ('while predicting', (18, 20))]","[('RESIDE', (0, 1)), ('principled use', (2, 4)), ('entity type and relation alias information', (5, 11)), ('KBs', (12, 13)), ('soft constraints', (16, 18)), ('relation', (21, 22))]","[['RESIDE', 'makes', 'principled use'], ['principled use', 'of', 'entity type and relation alias information'], ['entity type and relation alias information', 'to impose', 'soft constraints'], ['soft constraints', 'while predicting', 'relation'], ['entity type and relation alias information', 'from', 'KBs']]",[],[],"[['Model', 'has', 'RESIDE']]",[],[],[],[],[],relation_extraction,11,40
model,"It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .","[('uses', (1, 2)), ('obtained from', (5, 7)), ('along with', (14, 16)), ('to improve', (20, 22))]","[('encoded syntactic information', (2, 5)), ('Graph Convolution Networks ( GCN )', (7, 13)), ('embedded side information', (16, 19)), ('neural relation extraction', (22, 25))]","[['encoded syntactic information', 'obtained from', 'Graph Convolution Networks ( GCN )'], ['encoded syntactic information', 'to improve', 'neural relation extraction'], ['encoded syntactic information', 'along with', 'embedded side information']]",[],"[['Model', 'uses', 'encoded syntactic information']]",[],[],[],[],[],[],relation_extraction,11,41
code,RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .,[],"[('http://github.com / malllabiisc / RESIDE', (13, 18))]",[],[],[],[],"[['Contribution', 'Code', 'http://github.com / malllabiisc / RESIDE']]",[],[],[],[],relation_extraction,11,45
baselines,Mintz : Multi-class logistic regression model proposed by for distant supervision paradigm .,"[('for', (8, 9))]","[('Mintz', (0, 1)), ('Multi-class logistic regression model', (2, 6)), ('distant supervision paradigm', (9, 12))]","[['Multi-class logistic regression model', 'for', 'distant supervision paradigm']]","[['Mintz', 'has', 'Multi-class logistic regression model']]",[],"[['Baselines', 'has', 'Mintz']]",[],[],[],[],[],relation_extraction,11,198
baselines,MultiR : Probabilistic graphical model for multi instance learning by MIMLRE :,"[('for', (5, 6))]","[('MultiR', (0, 1)), ('Probabilistic graphical model', (2, 5)), ('multi instance learning', (6, 9)), ('MIMLRE', (10, 11))]","[['Probabilistic graphical model', 'for', 'multi instance learning']]","[['MultiR', 'has', 'Probabilistic graphical model']]",[],"[['Baselines', 'has', 'MIMLRE'], ['Baselines', 'has', 'MultiR']]",[],[],[],[],"[['MIMLRE', 'has', 'graphical model']]",relation_extraction,11,199
baselines,A graphical model which jointly models multiple instances and multiple labels .,"[('jointly models', (4, 6))]","[('graphical model', (1, 3)), ('multiple instances and multiple labels', (6, 11))]","[['graphical model', 'jointly models', 'multiple instances and multiple labels']]",[],[],[],[],[],[],[],[],relation_extraction,11,200
baselines,More details in . PCNN : A CNN based relation extraction model by which uses piecewise max - pooling for sentence representation .,"[('uses', (14, 15)), ('for', (19, 20))]","[('PCNN', (4, 5)), ('CNN based relation extraction model', (7, 12)), ('piecewise max - pooling', (15, 19)), ('sentence representation', (20, 22))]","[['CNN based relation extraction model', 'uses', 'piecewise max - pooling'], ['piecewise max - pooling', 'for', 'sentence representation']]","[['PCNN', 'has', 'CNN based relation extraction model']]",[],"[['Baselines', 'has', 'PCNN']]",[],[],[],[],[],relation_extraction,11,201
baselines,PCNN + ATT : A piecewise max - pooling over CNN based model which is used by to get sentence representation followed by attention over sentences .,"[('over', (9, 10)), ('to get', (17, 19)), ('followed by', (21, 23))]","[('PCNN + ATT', (0, 3)), ('piecewise max - pooling', (5, 9)), ('CNN based model', (10, 13)), ('sentence representation', (19, 21)), ('attention over sentences', (23, 26))]","[['piecewise max - pooling', 'over', 'CNN based model'], ['CNN based model', 'to get', 'sentence representation'], ['sentence representation', 'followed by', 'attention over sentences']]","[['PCNN + ATT', 'has', 'piecewise max - pooling']]",[],"[['Baselines', 'has', 'PCNN + ATT']]",[],[],[],[],[],relation_extraction,11,202
baselines,BGWA : Bi - GRU based relation extraction model with word and sentence level attention ) .,"[('with', (9, 10))]","[('BGWA', (0, 1)), ('Bi - GRU based relation extraction model', (2, 9)), ('word and sentence level attention', (10, 15))]","[['Bi - GRU based relation extraction model', 'with', 'word and sentence level attention']]","[['BGWA', 'has', 'Bi - GRU based relation extraction model']]",[],"[['Baselines', 'has', 'BGWA']]",[],[],[],[],[],relation_extraction,11,203
results,"Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .","[('find that', (3, 5)), ('achieves', (6, 7)), ('over', (9, 10)), ('on', (14, 15))]","[('RESIDE', (5, 6)), ('higher precision', (7, 9)), ('entire recall range', (11, 14)), ('both the datasets', (15, 18))]","[['RESIDE', 'achieves', 'higher precision'], ['higher precision', 'over', 'entire recall range'], ['entire recall range', 'on', 'both the datasets']]",[],"[['Results', 'find that', 'RESIDE']]",[],[],[],[],[],[],relation_extraction,11,219
results,All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .,"[('could not perform', (4, 7))]","[('non-neural baselines', (2, 4)), ('well', (7, 8))]","[['non-neural baselines', 'could not perform', 'well']]",[],[],"[['Results', 'has', 'non-neural baselines']]",[],[],[],[],[],relation_extraction,11,220
results,RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,"[('outperforms', (1, 2))]","[('RESIDE', (0, 1)), ('PCNN + ATT and BGWA', (2, 7))]","[['RESIDE', 'outperforms', 'PCNN + ATT and BGWA']]",[],[],"[['Results', 'has', 'RESIDE']]",[],[],[],[],[],relation_extraction,11,221
results,The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .,"[('of', (3, 4)), ('over', (9, 10)), ('shows that', (11, 13)), ('helps in', (14, 16))]","[('higher performance', (1, 3)), ('BGWA and PCNN + ATT', (4, 9)), ('PCNN', (10, 11)), ('attention', (13, 14)), ('distant supervised RE', (16, 19))]","[['higher performance', 'of', 'BGWA and PCNN + ATT'], ['BGWA and PCNN + ATT', 'over', 'PCNN'], ['higher performance', 'shows that', 'attention'], ['attention', 'helps in', 'distant supervised RE']]",[],[],"[['Results', 'has', 'higher performance']]",[],[],[],[],[],relation_extraction,11,222
ablation-analysis,The results validate that GCNs are effective at encoding syntactic information .,"[('validate that', (2, 4)), ('effective at encoding', (6, 9))]","[('GCNs', (4, 5)), ('syntactic information', (9, 11))]","[['GCNs', 'effective at encoding', 'syntactic information']]",[],"[['Ablation analysis', 'validate that', 'GCNs']]",[],[],[],[],[],[],relation_extraction,11,230
ablation-analysis,"Further , the improvement from side information shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to improved relation extraction .","[('inducing', (29, 30)), ('leads to', (32, 34))]","[('side information', (5, 7)), ('improved relation extraction', (34, 37))]","[['side information', 'leads to', 'improved relation extraction']]",[],"[['Ablation analysis', 'inducing', 'side information']]",[],[],[],[],[],[],relation_extraction,11,231
ablation-analysis,We find that the model performs best when aliases are provided by the KB itself .,"[('performs', (5, 6)), ('when', (7, 8)), ('provided by', (10, 12))]","[('model', (4, 5)), ('best', (6, 7)), ('aliases', (8, 9)), ('KB', (13, 14))]","[['model', 'performs', 'best'], ['best', 'when', 'aliases'], ['aliases', 'provided by', 'KB']]",[],[],"[['Ablation analysis', 'has', 'model']]",[],[],[],[],[],relation_extraction,11,240
ablation-analysis,"Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .","[('gives', (6, 7)), ('when', (10, 11)), ('is', (18, 19))]","[('RESIDE', (5, 6)), ('competitive performance', (7, 9)), ('very limited amount of relation alias information', (11, 18)), ('available', (19, 20))]","[['RESIDE', 'gives', 'competitive performance'], ['competitive performance', 'when', 'very limited amount of relation alias information'], ['very limited amount of relation alias information', 'is', 'available']]",[],[],"[['Ablation analysis', 'has', 'RESIDE']]",[],[],[],[],[],relation_extraction,11,241
ablation-analysis,We observe that performance improves further with the availability of more alias information .,"[('observe', (1, 2)), ('improves further with', (4, 7))]","[('performance', (3, 4)), ('availability of more alias information', (8, 13))]","[['performance', 'improves further with', 'availability of more alias information']]",[],"[['Ablation analysis', 'observe', 'performance']]",[],[],[],[],[],[],relation_extraction,11,242
research-problem,Attention Guided Graph Convolutional Networks for Relation Extraction,[],"[('Relation Extraction', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction']]",[],[],[],[],relation_extraction,12,2
model,"In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .","[('propose', (5, 6)), ('operate directly on', (18, 21))]","[('novel Attention Guided Graph Convolutional Networks ( AGGCNs )', (7, 16)), ('full tree', (22, 24))]","[['novel Attention Guided Graph Convolutional Networks ( AGGCNs )', 'operate directly on', 'full tree']]",[],"[['Model', 'propose', 'novel Attention Guided Graph Convolutional Networks ( AGGCNs )']]",[],[],[],[],[],[],relation_extraction,12,32
model,"Intuitively , we develop a "" soft pruning "" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .","[('develop', (3, 4)), ('that transforms', (10, 12)), ('into', (16, 17))]","[('"" soft pruning "" strategy', (5, 10)), ('original dependency tree', (13, 16)), ('fully connected edgeweighted graph', (18, 22))]","[['"" soft pruning "" strategy', 'that transforms', 'original dependency tree'], ['original dependency tree', 'into', 'fully connected edgeweighted graph']]",[],"[['Model', 'develop', '"" soft pruning "" strategy']]",[],[],[],[],[],[],relation_extraction,12,33
model,"These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .","[('viewed as', (4, 6)), ('between', (10, 11)), ('can be learned in', (14, 18)), ('by using', (25, 27))]","[('weights', (1, 2)), ('strength of relatedness', (7, 10)), ('nodes', (11, 12)), ('end - to - end fashion', (19, 25)), ('self - attention mechanism', (27, 31))]","[['weights', 'viewed as', 'strength of relatedness'], ['strength of relatedness', 'can be learned in', 'end - to - end fashion'], ['end - to - end fashion', 'by using', 'self - attention mechanism'], ['strength of relatedness', 'between', 'nodes']]",[],[],"[['Model', 'has', 'weights']]",[],[],[],[],[],relation_extraction,12,34
model,we next introduce dense connections ) to the GCN model following .,"[('introduce', (2, 3)), ('to', (6, 7))]","[('dense connections', (3, 5)), ('GCN model', (8, 10))]","[['dense connections', 'to', 'GCN model']]",[],"[['Model', 'introduce', 'dense connections']]",[],[],[],[],[],[],relation_extraction,12,41
model,"For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .","[('For', (0, 1)), ('to capture', (10, 12)), ('that is', (14, 16))]","[('GCNs', (1, 2)), ('L layers', (3, 5)), ('neighborhood information', (12, 14)), ('L hops away', (16, 19))]","[['L layers', 'to capture', 'neighborhood information'], ['neighborhood information', 'that is', 'L hops away']]","[['GCNs', 'has', 'L layers']]","[['Model', 'For', 'GCNs']]",[],[],[],[],[],[],relation_extraction,12,42
model,"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .","[('With the help of', (0, 4)), ('able to', (9, 11)), ('with', (15, 16))]","[('dense connections', (4, 6)), ('train', (11, 12)), ('AGGCN model', (13, 15)), ('large depth', (17, 19))]","[['dense connections', 'able to', 'train'], ['AGGCN model', 'with', 'large depth']]","[['train', 'has', 'AGGCN model']]","[['Model', 'With the help of', 'dense connections']]",[],[],[],[],[],[],relation_extraction,12,45
code,Our code is available at https://github.com/Cartus / AGGCN_TACRED,[],"[('https://github.com/Cartus / AGGCN_TACRED', (5, 8))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/Cartus / AGGCN_TACRED']]",[],[],[],[],relation_extraction,12,49
hyperparameters,"We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.",[],[],"[['number of heads N', 'for', 'attention guided layer'], ['attention guided layer', 'from', '{ 1 , 2 , 3 , 4 }'], ['number of sub - layers L', 'in', 'each densely connected layer'], ['block number M', 'from', '{ 1 , 2 , 3 }']]",[],"[['Hyperparameters', 'choose', 'number of heads N'], ['Hyperparameters', 'choose', 'number of sub - layers L'], ['Hyperparameters', 'choose', 'block number M']]",[],[],[],"[['each densely connected layer', 'from', '{ 2 , 3 , 4 }']]",[],[],relation_extraction,12,174
hyperparameters,Glo Ve vectors are used as the initialization for word embeddings .,"[('used as', (4, 6)), ('for', (8, 9))]","[('Glo Ve vectors', (0, 3)), ('initialization', (7, 8)), ('word embeddings', (9, 11))]","[['Glo Ve vectors', 'used as', 'initialization'], ['initialization', 'for', 'word embeddings']]",[],[],"[['Hyperparameters', 'has', 'Glo Ve vectors']]",[],[],[],[],[],relation_extraction,12,176
baselines,"For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :","[('For', (0, 1))]","[('cross - sentence n- ary relation extraction task', (1, 9))]",[],[],"[['Baselines', 'For', 'cross - sentence n- ary relation extraction task']]",[],[],[],[],[],"[['cross - sentence n- ary relation extraction task', 'has', 'feature - based classifier'], ['cross - sentence n- ary relation extraction task', 'has', 'Graph - structured LSTM methods']]",relation_extraction,12,182
baselines,"1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .","[('based on', (7, 9)), ('between', (12, 13)), ('including', (25, 26))]","[('feature - based classifier', (3, 7)), ('shortest dependency paths', (9, 12)), ('all entity pairs', (13, 16)), ('Graph - structured LSTM methods', (19, 24)), ('Graph LSTM', (26, 28)), ('bidirectional DAG LSTM ( Bidir DAG LSTM )', (29, 37)), ('Graph State LSTM ( GS GLSTM )', (38, 45))]","[['feature - based classifier', 'based on', 'shortest dependency paths'], ['shortest dependency paths', 'between', 'all entity pairs'], ['Graph - structured LSTM methods', 'including', 'Graph LSTM'], ['Graph - structured LSTM methods', 'including', 'bidirectional DAG LSTM ( Bidir DAG LSTM )'], ['Graph - structured LSTM methods', 'including', 'Graph State LSTM ( GS GLSTM )']]",[],[],[],[],[],[],[],[],relation_extraction,12,183
baselines,"These methods extend LSTM to encode graphs constructed from input sentences with dependency edges , 3 ) Graph convolutional networks ( GCN ) with pruned trees , 6 https://nlp.stanford.edu/projects/","[('with', (11, 12))]","[('Graph convolutional networks ( GCN )', (17, 23)), ('pruned trees', (24, 26))]","[['Graph convolutional networks ( GCN )', 'with', 'pruned trees']]",[],[],[],[],[],[],"[['cross - sentence n- ary relation extraction task', 'has', 'Graph convolutional networks ( GCN )']]",[],relation_extraction,12,184
results,"For ternary relation extraction ( first two columns in ) , our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence ( Single ) and on all instances ( Cross ) , respectively , which outperform all the baselines .","[('For', (0, 1)), ('achieves', (14, 15)), ('of', (16, 17)), ('on', (20, 21)), ('within', (22, 23)), ('outperform', (39, 40))]","[('ternary relation extraction', (1, 4)), ('our AGGCN model', (11, 14)), ('accuracies', (15, 16)), ('87.1 and 87.0', (17, 20)), ('instances', (21, 22)), ('single sentence ( Single )', (23, 28)), ('all instances ( Cross )', (30, 35)), ('all the baselines', (40, 43))]","[['our AGGCN model', 'achieves', 'accuracies'], ['accuracies', 'of', '87.1 and 87.0'], ['87.1 and 87.0', 'outperform', 'all the baselines'], ['87.1 and 87.0', 'on', 'instances'], ['instances', 'within', 'single sentence ( Single )'], ['87.1 and 87.0', 'on', 'all instances ( Cross )']]","[['ternary relation extraction', 'has', 'our AGGCN model']]","[['Results', 'For', 'ternary relation extraction']]",[],[],[],[],[],[],relation_extraction,12,202
results,"More specifically , our AG - GCN model surpasses the state - of - the - art Graphstructured LSTM model ( GS GLSTM ) by 6.8 and 3.8 points for the Single and Cross settings , respectively .","[('surpasses', (8, 9)), ('by', (24, 25)), ('for', (29, 30))]","[('AG - GCN model', (4, 8)), ('state - of - the - art Graphstructured LSTM model ( GS GLSTM )', (10, 24)), ('6.8 and 3.8 points', (25, 29)), ('Single and Cross settings', (31, 35))]","[['AG - GCN model', 'surpasses', 'state - of - the - art Graphstructured LSTM model ( GS GLSTM )'], ['state - of - the - art Graphstructured LSTM model ( GS GLSTM )', 'by', '6.8 and 3.8 points'], ['state - of - the - art Graphstructured LSTM model ( GS GLSTM )', 'for', 'Single and Cross settings']]",[],[],"[['Results', 'has', 'AG - GCN model']]",[],[],[],[],[],relation_extraction,12,203
results,"Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree ( K=1 ) .","[('Compared to', (0, 2)), ('obtains', (7, 8)), ('than', (13, 14)), ('with', (18, 19))]","[('GCN models', (2, 4)), ('our model', (5, 7)), ('1.3 and 1.2 points higher', (8, 13)), ('best performing model', (15, 18)), ('pruned tree ( K=1 )', (19, 24))]","[['our model', 'obtains', '1.3 and 1.2 points higher'], ['1.3 and 1.2 points higher', 'than', 'best performing model'], ['best performing model', 'with', 'pruned tree ( K=1 )']]","[['GCN models', 'has', 'our model']]","[['Results', 'Compared to', 'GCN models']]",[],[],[],[],[],[],relation_extraction,12,204
results,"For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and GCN as well .","[('consistently outperforms', (13, 15))]","[('binary relation extraction', (1, 4)), ('AGGCN', (12, 13)), ('GS GLSTM', (15, 17)), ('GCN', (18, 19))]","[['AGGCN', 'consistently outperforms', 'GS GLSTM'], ['AGGCN', 'consistently outperforms', 'GCN']]","[['binary relation extraction', 'has', 'AGGCN']]",[],"[['Results', 'For', 'binary relation extraction']]",[],[],[],[],[],relation_extraction,12,205
results,"AGGCN also performs better than GCNs , although its performance can be boosted via pruned trees .","[('performs', (2, 3)), ('than', (4, 5))]","[('AGGCN', (0, 1)), ('better', (3, 4)), ('GCNs', (5, 6))]","[['AGGCN', 'performs', 'better'], ['better', 'than', 'GCNs']]",[],[],"[['Results', 'has', 'AGGCN']]",[],[],[],[],[],relation_extraction,12,207
results,"However , our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations , respectively .","[('obtains', (6, 7)), ('higher than', (11, 13)), ('for', (17, 18))]","[('our AGGCN model', (2, 5)), ('8.0 and 5.7 points', (7, 11)), ('GS GLSTM model', (14, 17)), ('ternary and binary relations', (18, 22))]","[['our AGGCN model', 'obtains', '8.0 and 5.7 points'], ['8.0 and 5.7 points', 'higher than', 'GS GLSTM model'], ['GS GLSTM model', 'for', 'ternary and binary relations']]",[],[],"[['Results', 'has', 'our AGGCN model']]",[],[],[],[],[],relation_extraction,12,215
results,"We also notice that our AGGCN achieves a better test accuracy than all GCN models , which further demonstrates its ability to learn better representations from full trees .","[('achieves', (6, 7)), ('than', (11, 12))]","[('our AGGCN', (4, 6)), ('better test accuracy', (8, 11)), ('all GCN models', (12, 15))]","[['our AGGCN', 'achieves', 'better test accuracy'], ['better test accuracy', 'than', 'all GCN models']]",[],[],"[['Results', 'has', 'our AGGCN']]",[],[],[],[],[],relation_extraction,12,216
results,"Our C - AGGCN model achieves an F1 score of 68.2 , which outperforms the state - ofart C - GCN model by 1.8 points .","[('achieves', (5, 6)), ('of', (9, 10)), ('outperforms', (13, 14)), ('by', (22, 23))]","[('C - AGGCN model', (1, 5)), ('F1 score', (7, 9)), ('68.2', (10, 11)), ('state - ofart C - GCN model', (15, 22)), ('1.8 points', (23, 25))]","[['C - AGGCN model', 'achieves', 'F1 score'], ['F1 score', 'of', '68.2'], ['68.2', 'outperforms', 'state - ofart C - GCN model'], ['state - ofart C - GCN model', 'by', '1.8 points']]",[],[],"[['Results', 'has', 'C - AGGCN model']]",[],[],[],[],[],relation_extraction,12,227
results,"We also notice that AGGCN and C - AGGCN achieve better precision and recall scores than GCN and C - GCN , respectively .","[('notice', (2, 3)), ('achieve', (9, 10)), ('than', (15, 16))]","[('AGGCN and C - AGGCN', (4, 9)), ('better precision and recall scores', (10, 15)), ('GCN and C - GCN', (16, 21))]","[['AGGCN and C - AGGCN', 'achieve', 'better precision and recall scores'], ['better precision and recall scores', 'than', 'GCN and C - GCN']]",[],"[['Results', 'notice', 'AGGCN and C - AGGCN']]",[],[],[],[],[],[],relation_extraction,12,228
results,The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .,"[('between', (3, 4)), ('empirically show', (13, 15)), ('better at distinguishing', (20, 23)), ('for learning', (27, 29))]","[('performance gap', (1, 3)), ('GCNs with pruned trees and AGGCNs with full trees', (4, 13)), ('AGGCN model', (17, 19)), ('relevant from irrelevant information', (23, 27)), ('better graph representation', (30, 33))]","[['performance gap', 'between', 'GCNs with pruned trees and AGGCNs with full trees'], ['GCNs with pruned trees and AGGCNs with full trees', 'empirically show', 'AGGCN model'], ['AGGCN model', 'better at distinguishing', 'relevant from irrelevant information'], ['relevant from irrelevant information', 'for learning', 'better graph representation']]",[],[],"[['Results', 'has', 'performance gap']]",[],[],[],[],[],relation_extraction,12,229
results,We also evaluate our model on the SemEval dataset under the same settings as .,"[('on', (5, 6))]","[('SemEval dataset', (7, 9))]",[],[],"[['Results', 'on', 'SemEval dataset']]",[],[],[],[],[],"[['SemEval dataset', 'has', 'Our C - AGGCN model ( 85.7 )']]",relation_extraction,12,230
results,"Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .","[('consistently outperforms', (8, 10)), ('showing', (19, 20))]","[('Our C - AGGCN model ( 85.7 )', (0, 8)), ('C - GCN model ( 84.8 )', (11, 18)), ('good generalizability', (21, 23))]","[['Our C - AGGCN model ( 85.7 )', 'consistently outperforms', 'C - GCN model ( 84.8 )'], ['Our C - AGGCN model ( 85.7 )', 'showing', 'good generalizability']]",[],[],[],[],[],[],[],[],relation_extraction,12,233
ablation-analysis,We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .,"[('observe that', (2, 4)), ('improves', (13, 14)), ('of', (16, 17))]","[('adding', (4, 5)), ('either attention guided layers or densely connected layers', (5, 13)), ('performance', (15, 16)), ('model', (18, 19))]","[['either attention guided layers or densely connected layers', 'improves', 'performance'], ['performance', 'of', 'model']]","[['adding', 'has', 'either attention guided layers or densely connected layers']]","[['Ablation analysis', 'observe that', 'adding']]",[],[],[],[],[],[],relation_extraction,12,238
ablation-analysis,We also notice that the feed - forward layer is effective in our model .,"[('notice that', (2, 4)), ('effective in', (10, 12))]","[('feed - forward layer', (5, 9)), ('our model', (12, 14))]","[['feed - forward layer', 'effective in', 'our model']]",[],"[['Ablation analysis', 'notice that', 'feed - forward layer']]",[],[],[],[],[],[],relation_extraction,12,240
ablation-analysis,"Without the feed - forward layer , the result drops to an F1 score of 67.8 .","[('Without', (0, 1)), ('drops to', (9, 11))]","[('feed - forward layer', (2, 6)), ('result', (8, 9)), ('F1 score of 67.8', (12, 16))]","[['result', 'drops to', 'F1 score of 67.8']]","[['feed - forward layer', 'has', 'result']]","[['Ablation analysis', 'Without', 'feed - forward layer']]",[],[],[],[],[],[],relation_extraction,12,241
ablation-analysis,We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .,"[('with', (10, 11)), ('outperform', (18, 19))]","[('all the C - AGGCN models', (4, 10)), ('varied values of K', (11, 15)), ('state - of - the - art C - GCN model', (20, 31))]","[['all the C - AGGCN models', 'with', 'varied values of K'], ['varied values of K', 'outperform', 'state - of - the - art C - GCN model']]",[],[],"[['Ablation analysis', 'observe that', 'all the C - AGGCN models']]",[],[],[],[],[],relation_extraction,12,244
ablation-analysis,"In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .","[('of', (8, 9)), ('outperforms', (15, 16))]","[('performance', (7, 8)), ('C - AGGCN with full trees', (9, 15)), ('all C - AGGCNs with pruned trees', (16, 23))]","[['performance', 'of', 'C - AGGCN with full trees'], ['C - AGGCN with full trees', 'outperforms', 'all C - AGGCNs with pruned trees']]",[],[],"[['Ablation analysis', 'notice that', 'performance']]",[],[],[],[],[],relation_extraction,12,247
ablation-analysis,"In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .","[('outperforms', (9, 10)), ('against', (20, 21))]","[('C - AGGCN with full trees', (3, 9)), ('C - AGGCN with pruned trees and C - GCN', (10, 20)), ('various sentence lengths', (21, 24))]","[['C - AGGCN with full trees', 'outperforms', 'C - AGGCN with pruned trees and C - GCN'], ['C - AGGCN with pruned trees and C - GCN', 'against', 'various sentence lengths']]",[],[],"[['Ablation analysis', 'has', 'C - AGGCN with full trees']]",[],[],[],[],[],relation_extraction,12,252
ablation-analysis,"Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .","[('achieved by', (4, 6)), ('when', (13, 14))]","[('improvement', (3, 4)), ('C - AGGCN with pruned trees', (6, 12)), ('decays', (12, 13)), ('sentence length', (15, 17)), ('increases', (17, 18))]","[['improvement', 'achieved by', 'C - AGGCN with pruned trees'], ['decays', 'when', 'sentence length']]","[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]",[],"[['Ablation analysis', 'has', 'improvement']]",[],[],[],[],[],relation_extraction,12,254
ablation-analysis,This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .,"[('suggests that', (1, 3)), ('benefit more from', (7, 10))]","[('C - AGGCN', (3, 6)), ('larger graphs ( full tree )', (10, 16))]","[['C - AGGCN', 'benefit more from', 'larger graphs ( full tree )']]",[],"[['Ablation analysis', 'suggests that', 'C - AGGCN']]",[],[],[],[],[],[],relation_extraction,12,257
ablation-analysis,C - AGGCN consistently outperforms C - GCN under the same amount of training data .,"[('consistently outperforms', (3, 5)), ('under', (8, 9))]","[('C - AGGCN', (0, 3)), ('C - GCN', (5, 8)), ('same amount of training data', (10, 15))]","[['C - AGGCN', 'consistently outperforms', 'C - GCN'], ['C - GCN', 'under', 'same amount of training data']]",[],[],"[['Ablation analysis', 'has', 'C - AGGCN']]",[],[],[],[],[],relation_extraction,12,261
ablation-analysis,"When the size of training data increases , we can observe that the performance gap becomes more obvious .","[('When', (0, 1)), ('of', (3, 4)), ('observe that', (10, 12)), ('becomes', (15, 16))]","[('size', (2, 3)), ('training data', (4, 6)), ('increases', (6, 7)), ('performance gap', (13, 15)), ('more obvious', (16, 18))]","[['size', 'of', 'training data'], ['training data', 'observe that', 'performance gap'], ['performance gap', 'becomes', 'more obvious']]","[['training data', 'has', 'increases']]","[['Ablation analysis', 'When', 'size']]",[],[],[],[],[],[],relation_extraction,12,262
ablation-analysis,"Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .",[],[],"[['80 %', 'of', 'training data'], ['C - AGGCN model', 'able to achieve', 'F 1 score'], ['F 1 score', 'of', '66.5'], ['66.5', 'higher than', 'C - GCN'], ['C - GCN', 'trained on', 'whole dataset']]","[['training data', 'has', 'C - AGGCN model']]","[['Ablation analysis', 'using', '80 %']]",[],[],[],[],[],[],relation_extraction,12,263
research-problem,Matching the Blanks : Distributional Similarity for Relation Learning,[],"[('Relation Learning', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Learning']]",[],[],[],[],relation_extraction,13,2
research-problem,Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,[],"[('identify and extract relations between entities', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'identify and extract relations between entities']]",[],[],[],[],relation_extraction,13,11
research-problem,Typically efforts in relation extraction fall into one of three groups .,[],"[('relation extraction', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'relation extraction']]",[],[],[],[],relation_extraction,13,12
model,"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .","[('study the ability of', (3, 7)), ('to encode', (12, 14)), ('between', (15, 16)), ('identify', (21, 22)), ('that outperforms', (26, 28)), ('in', (30, 31))]","[('Transformer neural network architecture', (8, 12)), ('relations', (14, 15)), ('entity pairs', (16, 18)), ('representation', (25, 26)), ('previous work', (28, 30)), ('supervised relation extraction', (31, 34))]","[['Transformer neural network architecture', 'identify', 'representation'], ['representation', 'that outperforms', 'previous work'], ['previous work', 'in', 'supervised relation extraction'], ['Transformer neural network architecture', 'to encode', 'relations'], ['relations', 'between', 'entity pairs']]",[],"[['Model', 'study the ability of', 'Transformer neural network architecture']]",[],[],[],[],[],[],relation_extraction,13,19
model,"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .","[('present', (3, 4)), ('of', (6, 7)), ('without', (11, 12)), ('from', (14, 15)), ('by matching', (21, 23))]","[('method', (5, 6)), ('training', (7, 8)), ('relation representation', (9, 11)), ('any supervision', (12, 14)), ('knowledge graph or human annotators', (16, 21)), ('blanks', (24, 25))]","[['method', 'of', 'training'], ['training', 'by matching', 'blanks'], ['relation representation', 'without', 'any supervision'], ['any supervision', 'from', 'knowledge graph or human annotators']]","[['training', 'has', 'relation representation']]","[['Model', 'present', 'method']]",[],[],[],[],[],[],relation_extraction,13,20
results,shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .,"[('outperform', (13, 14)), ('on', (21, 22)), ('when they have not seen', (25, 30))]","[('task agnostic BERT EM and BERT EM + MTB models', (3, 13)), ('previous published state of the art', (15, 21)), ('FewRel task', (22, 24)), ('any FewRel training data', (30, 34))]","[['task agnostic BERT EM and BERT EM + MTB models', 'outperform', 'previous published state of the art'], ['previous published state of the art', 'on', 'FewRel task'], ['FewRel task', 'when they have not seen', 'any FewRel training data']]",[],[],"[['Results', 'has', 'task agnostic BERT EM and BERT EM + MTB models']]",[],[],[],[],[],relation_extraction,13,187
results,"For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .",[],[],"[['BERT EM + MTB', 'increase', 'very significant'], ['12.7 %', 'on', '10 - way - 1 - shot task'], ['8.8 %', 'on', '5 - way - 1 - shot task']]","[['very significant', 'has', '12.7 %'], ['very significant', 'has', '8.8 %']]","[['Results', 'For', 'BERT EM + MTB']]",[],[],[],[],[],[],relation_extraction,13,188
results,"BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .","[('significantly outperforms', (5, 7))]","[('BERT EM', (0, 2))]",[],[],[],[],[],"[['BERT EM + MTB', 'significantly outperforms', 'BERT EM']]",[],[],[],relation_extraction,13,189
results,"When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .","[('given', (1, 2)), ('to', (3, 4)), ('approaches', (12, 13))]","[('access', (2, 3)), ('all of the training data', (4, 9)), ('BERT EM', (10, 12)), (""BERT EM + MTB 's performance"", (13, 19))]","[['access', 'to', 'all of the training data'], ['BERT EM', 'approaches', ""BERT EM + MTB 's performance""]]","[['all of the training data', 'has', 'BERT EM']]","[['Results', 'given', 'access']]",[],[],[],[],[],[],relation_extraction,13,192
results,The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,"[('show that', (3, 5)), ('could be used to', (7, 11)), ('in implementing', (14, 16))]","[('MTB training', (5, 7)), ('significantly reduce effort', (11, 14)), ('exemplar based relation extraction system', (17, 22))]","[['MTB training', 'could be used to', 'significantly reduce effort'], ['significantly reduce effort', 'in implementing', 'exemplar based relation extraction system']]",[],"[['Results', 'show that', 'MTB training']]",[],[],[],[],[],[],relation_extraction,13,195
results,The additional MTB based training further increases F 1 scores for all tasks .,"[('further increases', (5, 7)), ('for', (10, 11))]","[('additional MTB based training', (1, 5)), ('F 1 scores', (7, 10)), ('all tasks', (11, 13))]","[['additional MTB based training', 'further increases', 'F 1 scores'], ['F 1 scores', 'for', 'all tasks']]",[],[],"[['Results', 'has', 'additional MTB based training']]",[],[],[],[],[],relation_extraction,13,199
results,"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .","[('see that', (5, 7)), ('even more effective for', (11, 15))]","[('MTB based training', (7, 10)), ('low - resource cases', (15, 19))]","[['MTB based training', 'even more effective for', 'low - resource cases']]",[],"[['Results', 'see that', 'MTB based training']]",[],[],[],[],[],[],relation_extraction,13,202
results,"This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .","[('training by', (6, 8)), ('can', (11, 12)), ('amount of', (15, 17)), ('to create', (20, 22)), ('populate', (26, 27))]","[('matching the blanks', (8, 11)), ('significantly reduce', (12, 14)), ('human input', (17, 19)), ('relation extractors', (22, 24)), ('knowledge base', (28, 30))]","[['matching the blanks', 'can', 'significantly reduce'], ['significantly reduce', 'amount of', 'human input'], ['human input', 'to create', 'relation extractors'], ['human input', 'populate', 'knowledge base']]",[],"[['Results', 'training by', 'matching the blanks']]",[],[],[],[],[],[],relation_extraction,13,203
research-problem,Enriching Pre-trained Language Model with Entity Information for Relation Classification,[],"[('Relation Classification', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Classification']]",[],[],[],[],relation_extraction,2,2
model,"In this paper , we apply the pretrained BERT model for relation classification .","[('apply', (5, 6)), ('for', (10, 11))]","[('pretrained BERT model', (7, 10)), ('relation classification', (11, 13))]","[['pretrained BERT model', 'for', 'relation classification']]",[],"[['Model', 'apply', 'pretrained BERT model']]",[],[],[],[],[],[],relation_extraction,2,25
model,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .","[('insert', (1, 2)), ('before and after', (4, 7)), ('before feeding', (10, 12)), ('to', (14, 15)), ('for', (16, 17)), ('to identify', (23, 25)), ('of', (27, 28)), ('transfer', (33, 34)), ('into', (36, 37))]","[('special tokens', (2, 4)), ('target entities', (8, 10)), ('text', (13, 14)), ('BERT', (15, 16)), ('fine - tuning', (17, 20)), ('locations', (26, 27)), ('two target entities', (29, 32)), ('information', (35, 36)), ('BERT model', (38, 40))]","[['special tokens', 'before feeding', 'text'], ['text', 'to', 'BERT'], ['BERT', 'for', 'fine - tuning'], ['special tokens', 'before and after', 'target entities'], ['target entities', 'to identify', 'locations'], ['locations', 'transfer', 'information'], ['information', 'into', 'BERT model'], ['locations', 'of', 'two target entities']]",[],"[['Model', 'insert', 'special tokens']]",[],[],[],[],[],[],relation_extraction,2,26
model,We then locate the positions of the two target entities in the output embedding from BERT model .,"[('locate', (2, 3)), ('of', (5, 6)), ('in', (10, 11)), ('from', (14, 15))]","[('positions', (4, 5)), ('two target entities', (7, 10)), ('output embedding', (12, 14)), ('BERT model', (15, 17))]","[['positions', 'in', 'output embedding'], ['output embedding', 'from', 'BERT model'], ['positions', 'of', 'two target entities']]",[],"[['Model', 'locate', 'positions']]",[],[],[],[],[],[],relation_extraction,2,27
model,We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,"[('use', (1, 2)), ('of', (12, 13)), ('in the setting of', (17, 21)), ('as the input to', (23, 27)), ('for', (33, 34))]","[('embeddings', (3, 4)), ('sentence encoding', (8, 10)), ('embedding', (11, 12)), ('special first token', (14, 17)), ('BERT', (21, 22)), ('multi - layer neural network', (28, 33)), ('classification', (34, 35))]","[['multi - layer neural network', 'for', 'classification'], ['classification', 'use', 'embeddings'], ['classification', 'use', 'sentence encoding'], ['embedding', 'of', 'special first token'], ['special first token', 'in the setting of', 'BERT']]","[['sentence encoding', 'has', 'embedding']]","[['Model', 'as the input to', 'multi - layer neural network']]",[],[],[],[],[],[],relation_extraction,2,28
hyperparameters,We add dropout before each add - on layer .,"[('add', (1, 2)), ('before', (3, 4))]","[('dropout', (2, 3)), ('each add - on layer', (4, 9))]","[['dropout', 'before', 'each add - on layer']]",[],"[['Hyperparameters', 'add', 'dropout']]",[],[],[],[],[],[],relation_extraction,2,95
hyperparameters,"For the pre-trained BERT model , we use the uncased basic model .","[('For', (0, 1)), ('use', (7, 8))]","[('pre-trained BERT model', (2, 5)), ('uncased basic model', (9, 12))]","[['pre-trained BERT model', 'use', 'uncased basic model']]",[],"[['Hyperparameters', 'For', 'pre-trained BERT model']]",[],[],[],[],[],[],relation_extraction,2,96
baselines,"We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .",[],"[('SVM', (26, 27)), ('RNN', (28, 29)), ('MVRNN', (30, 31)), ('CNN + Softmax', (32, 35)), ('FCM', (36, 37)), ('CR - CNN', (38, 41)), ('Attention - CNN', (42, 45)), ('Entity Attention Bi-LSTM', (46, 49))]",[],[],[],"[['Baselines', 'has', 'SVM'], ['Baselines', 'has', 'RNN'], ['Baselines', 'has', 'MVRNN'], ['Baselines', 'has', 'CNN + Softmax'], ['Baselines', 'has', 'FCM'], ['Baselines', 'has', 'CR - CNN'], ['Baselines', 'has', 'Attention - CNN'], ['Baselines', 'has', 'Entity Attention Bi-LSTM']]",[],[],[],[],[],relation_extraction,2,100
results,We can see that R - BERT significantly beats all the baseline methods .,"[('see that', (2, 4)), ('significantly beats', (7, 9))]","[('R - BERT', (4, 7)), ('baseline methods', (11, 13))]","[['R - BERT', 'significantly beats', 'baseline methods']]",[],"[['Results', 'see that', 'R - BERT']]",[],[],[],[],[],[],relation_extraction,2,103
results,"The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .","[('of', (4, 5)), ('is', (8, 9)), ('much better than', (14, 17))]","[('MACRO F1 value', (1, 4)), ('R - BERT', (5, 8)), ('89. 25', (9, 11)), ('previous best solution', (18, 21))]","[['MACRO F1 value', 'of', 'R - BERT'], ['R - BERT', 'is', '89. 25'], ['89. 25', 'much better than', 'previous best solution']]",[],[],"[['Results', 'has', 'MACRO F1 value']]",[],[],[],[],[],relation_extraction,2,104
ablation-analysis,We observe that the three methods all perform worse than R - BERT .,"[('observe', (1, 2)), ('perform', (7, 8)), ('than', (9, 10))]","[('three methods', (4, 6)), ('worse', (8, 9)), ('R - BERT', (10, 13))]","[['three methods', 'perform', 'worse'], ['worse', 'than', 'R - BERT']]",[],"[['Ablation analysis', 'observe', 'three methods']]",[],[],[],[],[],[],relation_extraction,2,124
ablation-analysis,"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .","[('performs', (13, 14)), ('with', (16, 17)), ('worse than', (22, 24))]","[('BERT - NO - SEP - NO - ENT', (4, 13)), ('worst', (14, 15)), ('F1 8.16 absolute points', (18, 22)), ('R - BERT', (24, 27))]","[['BERT - NO - SEP - NO - ENT', 'performs', 'worst'], ['worst', 'with', 'F1 8.16 absolute points'], ['F1 8.16 absolute points', 'worse than', 'R - BERT']]",[],[],"[['Ablation analysis', 'has', 'BERT - NO - SEP - NO - ENT']]",[],[],[],[],[],relation_extraction,2,125
ablation-analysis,This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,"[('demonstrates', (3, 4)), ('make', (15, 16))]","[('special separate tokens and the hidden entity vectors', (7, 15)), ('important contributions', (16, 18))]","[['special separate tokens and the hidden entity vectors', 'make', 'important contributions']]",[],"[['Ablation analysis', 'demonstrates', 'special separate tokens and the hidden entity vectors']]",[],[],[],[],[],[],relation_extraction,2,126
ablation-analysis,BERT without special separate tokens can not locate the target entities and lose this key information .,"[('without', (1, 2)), ('can not locate', (5, 8))]","[('BERT', (0, 1)), ('special separate tokens', (2, 5)), ('target entities', (9, 11))]","[['BERT', 'can not locate', 'target entities'], ['BERT', 'without', 'special separate tokens']]",[],[],"[['Ablation analysis', 'has', 'BERT']]",[],[],[],[],[],relation_extraction,2,128
ablation-analysis,"On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .","[('incorporating', (5, 6)), ('of', (8, 9)), ('further enriches', (13, 15)), ('to make', (19, 21))]","[('output', (7, 8)), ('target entity vectors', (10, 13)), ('information', (16, 17)), ('more accurate prediction', (21, 24))]","[['output', 'of', 'target entity vectors'], ['target entity vectors', 'to make', 'more accurate prediction'], ['target entity vectors', 'further enriches', 'information']]",[],"[['Ablation analysis', 'incorporating', 'output']]",[],[],[],[],[],[],relation_extraction,2,130
research-problem,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,[],"[('Extracting Multiple - Relations', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Extracting Multiple - Relations']]",[],[],[],[],relation_extraction,3,2
research-problem,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,[],"[('extracting multiple entity - relations', (10, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'extracting multiple entity - relations']]",[],[],[],[],relation_extraction,3,4
research-problem,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",[],"[('multiple entityrelations extraction', (10, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'multiple entityrelations extraction']]",[],[],[],[],relation_extraction,3,5
research-problem,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,[],"[('Relation extraction ( RE )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation extraction ( RE )']]",[],[],[],[],relation_extraction,3,10
research-problem,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,[],"[('RE', (5, 6)), ('multiplerelations extraction ( MRE )', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'RE'], ['Contribution', 'has research problem', 'multiplerelations extraction ( MRE )']]",[],[],[],[],relation_extraction,3,12
research-problem,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",[],"[('MRE', (23, 24))]",[],[],[],[],"[['Contribution', 'has research problem', 'MRE']]",[],[],[],[],relation_extraction,3,13
model,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .","[('presents', (2, 3)), ('resolve', (7, 8)), ('of', (14, 15)), ('for', (17, 18)), ('by encoding', (19, 21))]","[('solution', (4, 5)), ('inefficient multiple - passes issue', (9, 14)), ('existing solutions', (15, 17)), ('MRE', (18, 19)), ('input only once', (22, 25))]","[['solution', 'resolve', 'inefficient multiple - passes issue'], ['inefficient multiple - passes issue', 'of', 'existing solutions'], ['existing solutions', 'for', 'MRE'], ['MRE', 'by encoding', 'input only once']]",[],"[['Model', 'presents', 'solution']]",[],[],[],[],[],[],relation_extraction,3,16
model,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .","[('built on top of', (6, 10))]","[('proposed solution', (3, 5)), ('existing transformer - based , pretrained general - purposed language encoders', (11, 22))]","[['proposed solution', 'built on top of', 'existing transformer - based , pretrained general - purposed language encoders']]",[],[],"[['Model', 'has', 'proposed solution']]",[],[],[],[],[],relation_extraction,3,17
model,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .","[('use', (4, 5)), ('as', (13, 14))]","[('Bidirectional Encoder Representations from Transformers ( BERT )', (5, 13)), ('transformer - based encoder', (15, 19))]","[['Bidirectional Encoder Representations from Transformers ( BERT )', 'as', 'transformer - based encoder']]",[],"[['Model', 'use', 'Bidirectional Encoder Representations from Transformers ( BERT )']]",[],[],[],[],[],[],relation_extraction,3,18
model,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,"[('introduce', (15, 16)), ('for predicting', (20, 22)), ('for', (24, 25)), ('make', (34, 35)), ('aware of', (38, 40)), ('of', (42, 43)), ('in', (45, 46))]","[('structured prediction layer', (17, 20)), ('multiple relations', (22, 24)), ('different entity pairs', (25, 28)), ('selfattention layers', (36, 38)), ('positions', (41, 42)), ('all en-tities', (43, 45)), ('input paragraph', (47, 49))]","[['structured prediction layer', 'for predicting', 'multiple relations'], ['multiple relations', 'for', 'different entity pairs'], ['selfattention layers', 'aware of', 'positions'], ['positions', 'of', 'all en-tities'], ['all en-tities', 'in', 'input paragraph']]",[],"[['Model', 'introduce', 'structured prediction layer'], ['Model', 'make', 'selfattention layers']]",[],[],[],[],[],[],relation_extraction,3,19
baselines,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .",[],"[('BERT SP', (0, 2)), ('BERT with structured prediction only', (3, 8))]",[],"[['BERT SP', 'has', 'BERT with structured prediction only']]",[],"[['Baselines', 'has', 'BERT SP']]",[],[],[],[],[],relation_extraction,3,77
baselines,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .",[],"[('Entity - Aware BERT SP', (0, 5)), ('our full model', (6, 9))]",[],"[['Entity - Aware BERT SP', 'is', 'our full model']]",[],"[['Baselines', 'has', 'Entity - Aware BERT SP']]",[],[],[],[],[],relation_extraction,3,78
baselines,BERT SP with position embedding on the final attention layer .,[],"[('BERT SP with position embedding on the final attention layer', (0, 10))]",[],[],[],"[['Baselines', 'has', 'BERT SP with position embedding on the final attention layer']]",[],[],[],[],"[['BERT SP with position embedding on the final attention layer', 'has', 'more straightforward way']]",relation_extraction,3,79
baselines,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,[],[],"[['more straightforward way', 'to achieve', 'MRE'], ['MRE', 'using', 'position embeddings'], ['MRE', 'in', 'one - pass']]",[],[],[],[],[],[],[],[],relation_extraction,3,80
baselines,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )","[('replaces', (10, 11)), ('adds', (17, 18)), ('of', (19, 20))]","[('BERT SP with entity indicators on input layer', (0, 8)), ('our structured attention layer', (11, 15)), ('indicators', (18, 19)), ('entities ( transformed to embeddings )', (20, 26))]","[['BERT SP with entity indicators on input layer', 'replaces', 'our structured attention layer'], ['BERT SP with entity indicators on input layer', 'adds', 'indicators'], ['indicators', 'of', 'entities ( transformed to embeddings )']]",[],[],"[['Baselines', 'has', 'BERT SP with entity indicators on input layer']]",[],[],[],[],[],relation_extraction,3,83
results,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,"[('is that', (3, 5)), ('achieves', (8, 9)), ('compared to', (12, 14))]","[('first observation', (1, 3)), ('our model architecture', (5, 8)), ('much better results', (9, 12)), ('previous state - of - the - art methods', (15, 24))]","[['first observation', 'is that', 'our model architecture'], ['our model architecture', 'achieves', 'much better results'], ['much better results', 'compared to', 'previous state - of - the - art methods']]",[],[],"[['Results', 'has', 'first observation']]",[],[],[],[],[],relation_extraction,3,86
results,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .",[],[],"[['our method', 'not designed for', 'domain adaptation'], ['our method', 'still outperforms', 'methods'], ['methods', 'with', 'domain adaptation']]",[],"[['Results', 'Note', 'our method']]",[],[],[],[],[],[],relation_extraction,3,87
results,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .","[('Among', (0, 1)), ('finetuning', (8, 9)), ('does not give', (16, 19))]","[('all the BERT - based approaches', (1, 7)), ('off - the - shelf BERT', (10, 16)), ('satisfying result', (20, 22))]","[['all the BERT - based approaches', 'finetuning', 'off - the - shelf BERT'], ['off - the - shelf BERT', 'does not give', 'satisfying result']]",[],"[['Results', 'Among', 'all the BERT - based approaches']]",[],[],[],[],[],[],relation_extraction,3,89
results,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .","[('successfully adapt', (11, 13)), ('to', (16, 17)), ('achieves', (22, 23))]","[('BERT SP', (7, 9)), ('pre-trained BERT', (14, 16)), ('MRE task', (18, 20)), ('comparable performance', (23, 25))]","[['BERT SP', 'successfully adapt', 'pre-trained BERT'], ['pre-trained BERT', 'to', 'MRE task'], ['BERT SP', 'achieves', 'comparable performance']]",[],[],"[['Results', 'has', 'BERT SP']]",[],[],[],[],[],relation_extraction,3,90
results,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .","[('works for', (1, 3)), ('lags behind using', (12, 15)), ('of', (17, 18))]","[('singlerelation per pass setting', (4, 8)), ('performance', (11, 12)), ('only indicators', (15, 17)), ('two target entities', (19, 22))]","[['performance', 'lags behind using', 'only indicators'], ['only indicators', 'of', 'two target entities']]","[['singlerelation per pass setting', 'has', 'performance']]","[['Results', 'works for', 'singlerelation per pass setting']]",[],[],[],[],[],[],relation_extraction,3,92
results,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .",[],[],"[['Our full model', 'with', 'structured fine - tuning'], ['structured fine - tuning', 'of', 'attention layers'], ['Our full model', 'brings', 'further improvement'], ['further improvement', 'in', 'MRE one - pass setting'], ['further improvement', 'of', 'about 5.5 %'], ['further improvement', 'achieves', 'new state - of - the - art performance'], ['new state - of - the - art performance', 'compared to', 'methods with domain adaptation']]",[],[],"[['Results', 'has', 'Our full model']]",[],[],[],[],[],relation_extraction,3,94
results,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .","[('For', (0, 1)), ('with', (3, 4)), ('on', (6, 7))]","[('BERT SP', (1, 3)), ('entity indicators', (4, 6)), ('inputs', (7, 8))]","[['BERT SP', 'with', 'entity indicators'], ['entity indicators', 'on', 'inputs']]",[],"[['Results', 'For', 'BERT SP']]",[],[],[],[],[],[],relation_extraction,3,99
results,A 2 % gap is observed as expected .,"[('observed', (5, 6))]","[('2 % gap', (1, 4))]",[],[],[],[],[],"[['BERT SP', 'observed', '2 % gap']]",[],[],[],relation_extraction,3,100
results,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .","[('on', (6, 7)), ('train', (13, 14)), ('in', (16, 17)), ('test with', (23, 25)), ('so', (29, 30)), ('are', (32, 33))]","[('BERT SP with position embeddings', (1, 6)), ('final attention layer', (8, 11)), ('model', (15, 16)), ('single - relation setting', (18, 22)), ('two different settings', (25, 28)), ('results', (31, 32)), ('same', (34, 35))]","[['BERT SP with position embeddings', 'test with', 'two different settings'], ['two different settings', 'so', 'results'], ['results', 'are', 'same'], ['BERT SP with position embeddings', 'on', 'final attention layer'], ['BERT SP with position embeddings', 'train', 'model'], ['model', 'in', 'single - relation setting']]",[],[],"[['Results', 'For', 'BERT SP with position embeddings']]",[],[],[],[],[],relation_extraction,3,103
results,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .","[('gives', (6, 7)), ('to', (9, 10)), ('in', (15, 16)), ('with', (20, 21))]","[('Our Entity - Aware BERT SP', (0, 6)), ('comparable results', (7, 9)), ('top - ranked system', (11, 15)), ('shared task', (17, 19)), ('slightly lower Macro - F1', (21, 26)), ('slightly higher Micro - F1', (37, 42))]","[['Our Entity - Aware BERT SP', 'gives', 'comparable results'], ['comparable results', 'with', 'slightly lower Macro - F1'], ['comparable results', 'with', 'slightly higher Micro - F1'], ['comparable results', 'to', 'top - ranked system'], ['top - ranked system', 'in', 'shared task']]",[],[],"[['Results', 'has', 'Our Entity - Aware BERT SP']]",[],[],[],[],[],relation_extraction,3,117
results,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .",[],[],"[['multiple relations', 'in', 'one - pass'], ['multiple relations', 'have', 'further 0.8 % improvement'], ['further 0.8 % improvement', 'on', 'Micro - F1'], ['multiple relations', 'have', '0.9 % drop'], ['0.9 % drop', 'on', 'Macro - F1']]",[],"[['Results', 'When predicting', 'multiple relations']]",[],[],[],[],[],[],relation_extraction,3,118
results,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .","[('compared to', (5, 7)), ('which makes use of', (12, 16)), ('pretrained on', (21, 23)), ('demonstrate', (30, 31)), ('as', (33, 34))]","[('top singlemodel result', (8, 11)), ('additional word and entity embeddings', (16, 21)), ('in - domain data', (23, 27)), ('our methods', (28, 30)), ('clear advantage', (31, 33)), ('single model', (35, 37))]","[['top singlemodel result', 'which makes use of', 'additional word and entity embeddings'], ['additional word and entity embeddings', 'pretrained on', 'in - domain data'], ['our methods', 'demonstrate', 'clear advantage'], ['clear advantage', 'as', 'single model']]","[['additional word and entity embeddings', 'has', 'our methods']]","[['Results', 'compared to', 'top singlemodel result']]",[],[],[],[],[],[],relation_extraction,3,120
research-problem,"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .",[],"[('populate knowledge bases with facts', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'populate knowledge bases with facts']]",[],[],[],[],relation_extraction,4,5
research-problem,A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .,[],"[('populate a knowledge base with relational facts', (14, 21))]",[],[],[],[],"[['Contribution', 'has research problem', 'populate a knowledge base with relational facts']]",[],[],[],[],relation_extraction,4,12
research-problem,"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .",[],"[('relation extraction', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'relation extraction']]",[],[],[],[],relation_extraction,4,21
model,"We propose a new , effective neural network sequence model for relation classification .","[('propose', (1, 2)), ('for', (10, 11))]","[('new , effective neural network sequence model', (3, 10)), ('relation classification', (11, 13))]","[['new , effective neural network sequence model', 'for', 'relation classification']]",[],"[['Model', 'propose', 'new , effective neural network sequence model']]",[],[],[],[],[],[],relation_extraction,4,30
model,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,"[('better customized for', (3, 6)), ('augmented by', (15, 17)), ('relative to', (23, 25)), ('of', (29, 30))]","[('architecture', (1, 2)), ('slot filling task', (7, 10)), ('word representations', (12, 14)), ('extra distributed representations of word position', (17, 23)), ('subject and object', (26, 29)), ('putative relation', (31, 33))]","[['architecture', 'better customized for', 'slot filling task'], ['word representations', 'augmented by', 'extra distributed representations of word position'], ['extra distributed representations of word position', 'relative to', 'subject and object'], ['subject and object', 'of', 'putative relation']]","[['architecture', 'has', 'word representations']]",[],"[['Model', 'has', 'architecture']]",[],[],[],[],[],relation_extraction,4,31
model,This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .,"[('means that', (1, 3)), ('effectively exploit', (8, 10))]","[('neural attention model', (4, 7)), ('combination of semantic similarity - based attention and positionbased attention', (11, 21))]","[['neural attention model', 'effectively exploit', 'combination of semantic similarity - based attention and positionbased attention']]",[],"[['Model', 'means that', 'neural attention model']]",[],[],[],[],[],[],relation_extraction,4,32
dataset,"Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .","[('markedly improve', (3, 5)), ('by using', (11, 13)), ('to produce', (17, 19)), ('suitable for', (25, 27)), ('between', (30, 31)), ('used in', (38, 40))]","[('availability of supervised training data', (6, 11)), ('Mechanical Turk crowd annotation', (13, 17)), ('large supervised training dataset', (20, 24)), ('common relations', (28, 30)), ('people , organizations and locations', (31, 36)), ('TAC KBP evaluations', (41, 44))]","[['availability of supervised training data', 'by using', 'Mechanical Turk crowd annotation'], ['Mechanical Turk crowd annotation', 'to produce', 'large supervised training dataset'], ['large supervised training dataset', 'suitable for', 'common relations'], ['common relations', 'used in', 'TAC KBP evaluations'], ['common relations', 'between', 'people , organizations and locations']]",[],"[['Dataset', 'markedly improve', 'availability of supervised training data']]",[],[],[],[],[],[],relation_extraction,4,33
dataset,"We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .","[('make it available through', (15, 19)), ('to respect', (28, 30)), ('on', (31, 32))]","[('TAC Relation Extraction Dataset ( TACRED )', (5, 12)), ('Linguistic Data Consortium ( LDC )', (20, 26)), ('copyrights', (30, 31)), ('underlying text', (33, 35))]","[['TAC Relation Extraction Dataset ( TACRED )', 'make it available through', 'Linguistic Data Consortium ( LDC )'], ['Linguistic Data Consortium ( LDC )', 'to respect', 'copyrights'], ['copyrights', 'on', 'underlying text']]",[],[],"[['Dataset', 'name', 'TAC Relation Extraction Dataset ( TACRED )']]",[],[],[],[],[],relation_extraction,4,34
hyperparameters,We map words that occur less than 2 times in the training set to a special < UNK > token .,"[('map', (1, 2)), ('occur less than', (4, 7)), ('in', (9, 10)), ('to', (13, 14))]","[('words', (2, 3)), ('2 times', (7, 9)), ('training set', (11, 13)), ('special < UNK > token', (15, 20))]","[['words', 'occur less than', '2 times'], ['2 times', 'in', 'training set'], ['words', 'to', 'special < UNK > token']]",[],"[['Hyperparameters', 'map', 'words']]",[],[],[],[],[],[],relation_extraction,4,120
hyperparameters,We use the pre-trained GloVe vectors to initialize word embeddings .,"[('use', (1, 2)), ('to initialize', (6, 8))]","[('pre-trained GloVe vectors', (3, 6)), ('word embeddings', (8, 10))]","[['pre-trained GloVe vectors', 'to initialize', 'word embeddings']]",[],"[['Hyperparameters', 'use', 'pre-trained GloVe vectors']]",[],[],[],[],[],[],relation_extraction,4,121
hyperparameters,"For all the LSTM layers , we find that 2 - layer stacked LSTMs generally work better than one - layer LSTMs .","[('For', (0, 1)), ('find that', (7, 9)), ('work better than', (15, 18))]","[('all the LSTM layers', (1, 5)), ('2 - layer stacked LSTMs', (9, 14)), ('one - layer LSTMs', (18, 22))]","[['all the LSTM layers', 'find that', '2 - layer stacked LSTMs'], ['2 - layer stacked LSTMs', 'work better than', 'one - layer LSTMs']]",[],"[['Hyperparameters', 'For', 'all the LSTM layers']]",[],[],[],[],[],[],relation_extraction,4,122
hyperparameters,We minimize cross - entropy loss over all 42 relations using AdaGrad .,"[('minimize', (1, 2)), ('over', (6, 7)), ('using', (10, 11))]","[('cross - entropy loss', (2, 6)), ('all 42 relations', (7, 10)), ('AdaGrad', (11, 12))]","[['cross - entropy loss', 'over', 'all 42 relations'], ['cross - entropy loss', 'using', 'AdaGrad']]",[],"[['Hyperparameters', 'minimize', 'cross - entropy loss']]",[],[],[],[],[],[],relation_extraction,4,123
hyperparameters,We apply Dropout with p = 0.5 to CNNs and LSTMs .,"[('apply', (1, 2)), ('with', (3, 4)), ('to', (7, 8))]","[('Dropout', (2, 3)), ('p = 0.5', (4, 7)), ('CNNs and LSTMs', (8, 11))]","[['Dropout', 'with', 'p = 0.5'], ['p = 0.5', 'to', 'CNNs and LSTMs']]",[],"[['Hyperparameters', 'apply', 'Dropout']]",[],[],[],[],[],[],relation_extraction,4,124
hyperparameters,During training we also find a word dropout strategy to be very effective : we randomly set a token to be < UNK > with a probability p.,[],[],"[['training', 'find', 'word dropout strategy'], ['word dropout strategy', 'randomly set', 'token'], ['token', 'to be', '< UNK >'], ['word dropout strategy', 'to be', 'very effective']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],"[['< UNK >', 'with', 'probability p']]",[],[],relation_extraction,4,125
hyperparameters,We set p to be 0.06 for the SDP - LSTM model and 0.04 for all other models .,[],[],"[['p', 'to be', '0.06'], ['0.06', 'for', 'SDP - LSTM model'], ['p', 'to be', '0.04'], ['0.04', 'for', 'all other models']]",[],"[['Hyperparameters', 'set', 'p']]",[],[],[],[],[],[],relation_extraction,4,126
results,"We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .","[('observe', (1, 2)), ('achieve', (6, 7)), ('than', (11, 12)), ('demonstrates', (20, 21)), ('of', (23, 24)), ('for', (26, 27))]","[('all neural models', (3, 6)), ('higher F 1 scores', (7, 11)), ('logistic regression and patterns systems', (13, 18)), ('effectiveness', (22, 23)), ('neural models', (24, 26)), ('relation extraction', (27, 29))]","[['all neural models', 'achieve', 'higher F 1 scores'], ['higher F 1 scores', 'demonstrates', 'effectiveness'], ['effectiveness', 'of', 'neural models'], ['effectiveness', 'for', 'relation extraction'], ['higher F 1 scores', 'than', 'logistic regression and patterns systems']]",[],"[['Results', 'observe', 'all neural models']]",[],[],[],[],[],[],relation_extraction,4,141
results,"Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .","[('help increase', (3, 5)), ('by around', (8, 10)), ('over', (12, 13))]","[('positional embeddings', (1, 3)), ('F 1', (6, 8)), ('2 %', (10, 12)), ('plain CNN model', (14, 17))]","[['positional embeddings', 'help increase', 'F 1'], ['F 1', 'by around', '2 %'], ['2 %', 'over', 'plain CNN model']]",[],[],"[['Results', 'has', 'positional embeddings']]",[],[],[],[],[],relation_extraction,4,142
results,"Lastly , our proposed position - aware mechanism is very effective and achieves an F 1 score of 65.4 % , with an absolute increase of 3.9 % over the best baseline neural model ( LSTM ) and 7.9 % over the baseline logistic regression system .",[],[],"[['proposed position - aware mechanism', 'achieves', 'F 1 score'], ['F 1 score', 'of', '65.4 %'], ['65.4 %', 'with', 'absolute increase'], ['absolute increase', 'of', '3.9 %'], ['3.9 %', 'over', 'best baseline neural model ( LSTM )'], ['absolute increase', 'of', '7.9 %'], ['7.9 %', 'over', 'baseline logistic regression system'], ['proposed position - aware mechanism', 'is', 'very effective']]",[],[],"[['Results', 'has', 'proposed position - aware mechanism']]",[],[],[],[],[],relation_extraction,4,143
results,We also run an ensemble of our position - aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F 1 score up by 1.6 % .,"[('run', (2, 3)), ('of', (5, 6)), ('takes', (13, 14)), ('from', (16, 17)), ('with', (19, 20)), ('further pushes', (24, 26)), ('by', (31, 32))]","[('ensemble', (4, 5)), ('position - aware attention model', (7, 12)), ('majority votes', (14, 16)), ('5 runs', (17, 19)), ('random initializations', (20, 22)), ('F 1 score', (27, 30)), ('1.6 %', (32, 34))]","[['ensemble', 'of', 'position - aware attention model'], ['position - aware attention model', 'further pushes', 'F 1 score'], ['F 1 score', 'by', '1.6 %'], ['position - aware attention model', 'takes', 'majority votes'], ['majority votes', 'from', '5 runs'], ['5 runs', 'with', 'random initializations']]",[],"[['Results', 'run', 'ensemble']]",[],[],[],[],[],[],relation_extraction,4,144
results,CNN - based models tend to have higher precision ; RNN - based models have better recall .,"[('tend to have', (4, 7)), ('have', (14, 15))]","[('CNN - based models', (0, 4)), ('higher precision', (7, 9)), ('RNN - based models', (10, 14)), ('better recall', (15, 17))]","[['CNN - based models', 'tend to have', 'higher precision'], ['RNN - based models', 'have', 'better recall']]",[],[],"[['Results', 'has', 'CNN - based models'], ['Results', 'has', 'RNN - based models']]",[],[],[],[],[],relation_extraction,4,146
results,"Evaluating relation extraction systems on slot filling is particularly challenging in that : ( 1 ) Endto - end cold start slot filling scores conflate the performance of all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ) .","[('conflate', (24, 25)), ('of', (27, 28))]","[('Endto - end cold start', (16, 21)), ('slot filling scores', (21, 24)), ('performance', (26, 27)), ('all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor )', (28, 45))]","[['slot filling scores', 'conflate', 'performance'], ['performance', 'of', 'all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor )']]","[['Endto - end cold start', 'has', 'slot filling scores']]",[],"[['Results', 'has', 'Endto - end cold start']]",[],[],[],[],[],relation_extraction,4,156
results,( 2 ) Errors in hop - 0 predictions can easily propagate to hop - 1 predictions .,"[('in', (4, 5)), ('easily propagate to', (10, 13))]","[('Errors', (3, 4)), ('hop - 0 predictions', (5, 9)), ('hop - 1 predictions', (13, 17))]","[['Errors', 'in', 'hop - 0 predictions'], ['hop - 0 predictions', 'easily propagate to', 'hop - 1 predictions']]",[],[],"[['Results', 'has', 'Errors']]",[],[],[],[],[],relation_extraction,4,157
results,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -","[('find', (1, 2)), ('on', (14, 15)), ('used in', (26, 28)), ('combining it with', (34, 37))]","[('training', (9, 10)), ('our logistic regression model', (10, 14)), ('TACRED', (15, 16)), ('2 million bootstrapped examples', (22, 26)), ('2015 Stanford system', (29, 32)), ('patterns', (37, 38))]","[['our logistic regression model', 'on', 'TACRED'], ['our logistic regression model', 'on', '2 million bootstrapped examples'], ['2 million bootstrapped examples', 'used in', '2015 Stanford system'], ['2015 Stanford system', 'combining it with', 'patterns']]","[['training', 'has', 'our logistic regression model']]","[['Results', 'find', 'training']]",[],[],[],[],[],[],relation_extraction,4,163
ablation-analysis,presents the results of an ablation test of our position - aware attention model on the development set of TACRED .,"[('presents', (0, 1)), ('of', (7, 8)), ('on', (14, 15))]","[('results of an ablation test', (2, 7)), ('our position - aware attention model', (8, 14)), ('development set of TACRED', (16, 20))]","[['results of an ablation test', 'of', 'our position - aware attention model'], ['our position - aware attention model', 'on', 'development set of TACRED']]",[],"[['Ablation analysis', 'presents', 'results of an ablation test']]",[],[],[],[],[],[],relation_extraction,4,166
ablation-analysis,"The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.","[('contributes', (4, 5)), ('where', (11, 12))]","[('about 1.5 % F 1', (5, 10)), ('position - aware term', (13, 17))]","[['about 1.5 % F 1', 'where', 'position - aware term']]",[],[],[],[],"[['our position - aware attention model', 'contributes', 'about 1.5 % F 1']]","[['position - aware term', 'contributes', 'about 1 % F 1 score']]",[],[],relation_extraction,4,167
ablation-analysis,( 3 ) alone contributes about 1 % F 1 score .,[],[],[],[],[],[],[],[],[],[],[],relation_extraction,4,168
ablation-analysis,"shows how the slot filling evaluation scores change as we change the amount of negative ( i.e. , no relation ) training data provided to our proposed model .","[('shows how', (0, 2)), ('as', (8, 9)), ('provided to', (23, 25))]","[('slot filling evaluation scores', (3, 7)), ('change', (7, 8)), ('amount of negative ( i.e. , no relation ) training data', (12, 23)), ('our proposed model', (25, 28))]","[['change', 'as', 'amount of negative ( i.e. , no relation ) training data'], ['amount of negative ( i.e. , no relation ) training data', 'provided to', 'our proposed model']]","[['slot filling evaluation scores', 'has', 'change']]","[['Ablation analysis', 'shows how', 'slot filling evaluation scores']]",[],[],[],[],[],[],relation_extraction,4,170
ablation-analysis,"We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .","[('At', (7, 8)), ('provide more', (17, 19)), ('while', (22, 23)), ('stays', (24, 25))]","[('hop - 0 level', (8, 12)), ('precision', (13, 14)), ('increases', (14, 15)), ('negative examples', (19, 21)), ('recall', (23, 24)), ('almost unchanged', (25, 27))]","[['hop - 0 level', 'provide more', 'negative examples'], ['hop - 0 level', 'while', 'recall'], ['recall', 'stays', 'almost unchanged']]","[['hop - 0 level', 'has', 'precision'], ['precision', 'has', 'increases']]","[['Ablation analysis', 'At', 'hop - 0 level']]",[],[],[],[],[],"[['hop - 0 level', 'has', 'F 1 score']]",relation_extraction,4,171
ablation-analysis,F 1 score keeps increasing .,"[('keeps', (3, 4))]","[('F 1 score', (0, 3)), ('increasing', (4, 5))]","[['F 1 score', 'keeps', 'increasing']]",[],[],[],[],[],[],[],[],relation_extraction,4,172
ablation-analysis,"( 2 ) At hop - all level , F 1 score increases by Performance by sentence length .","[('increases by', (12, 14)), ('by', (15, 16))]","[('hop - all level', (4, 8)), ('F 1 score', (9, 12)), ('Performance', (14, 15)), ('sentence length', (16, 18))]","[['F 1 score', 'increases by', 'Performance'], ['Performance', 'by', 'sentence length']]","[['hop - all level', 'has', 'F 1 score']]",[],"[['Ablation analysis', 'At', 'hop - all level']]",[],[],[],[],[],relation_extraction,4,173
ablation-analysis,We find that : ( 1 ) Performance of all models degrades substantially as the sentences get longer .,"[('find that', (1, 3)), ('of', (8, 9)), ('degrades', (11, 12)), ('as', (13, 14)), ('get', (16, 17))]","[('Performance', (7, 8)), ('all models', (9, 11)), ('substantially', (12, 13)), ('sentences', (15, 16)), ('longer', (17, 18))]","[['Performance', 'of', 'all models'], ['Performance', 'degrades', 'substantially'], ['substantially', 'as', 'sentences'], ['sentences', 'get', 'longer']]",[],"[['Ablation analysis', 'find that', 'Performance']]",[],[],[],[],[],[],relation_extraction,4,175
ablation-analysis,"When compared with the CNN - PE model , our position - aware attention model achieves improved F 1 scores on 30 out of the 41 slot types , with the top 5 slot types being org : members , per: country of death , org : shareholders , per:children and per:religion .","[('compared with', (1, 3)), ('achieves', (15, 16)), ('on', (20, 21)), ('with', (29, 30)), ('being', (35, 36))]","[('CNN - PE model', (4, 8)), ('position - aware attention model', (10, 15)), ('improved F 1 scores', (16, 20)), ('30 out of the 41 slot types', (21, 28)), ('top 5 slot types', (31, 35)), ('org : members', (36, 39)), ('per: country of death', (40, 44)), ('org : shareholders', (45, 48)), ('per:children', (49, 50)), ('per:religion', (51, 52))]","[['position - aware attention model', 'achieves', 'improved F 1 scores'], ['improved F 1 scores', 'with', 'top 5 slot types'], ['top 5 slot types', 'being', 'org : members'], ['top 5 slot types', 'being', 'per: country of death'], ['top 5 slot types', 'being', 'org : shareholders'], ['top 5 slot types', 'being', 'per:children'], ['top 5 slot types', 'being', 'per:religion'], ['improved F 1 scores', 'on', '30 out of the 41 slot types']]","[['CNN - PE model', 'has', 'position - aware attention model']]","[['Ablation analysis', 'compared with', 'CNN - PE model']]",[],[],[],[],[],[],relation_extraction,4,178
ablation-analysis,"When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .","[('with', (2, 3)), ('achieves', (10, 11)), ('on', (15, 16)), ('being', (30, 31))]","[('SDP - LSTM model', (3, 7)), ('our model', (8, 10)), ('improved F 1 scores', (11, 15)), ('26 out of the 41 slot types', (16, 23)), ('top 5 slot', (26, 29)), ('org : political / religious affiliation', (31, 37)), ('per: country of death', (38, 42)), ('org : alternate names', (43, 47)), ('per:religion', (48, 49)), ('per: alternate names', (50, 53))]","[['our model', 'achieves', 'improved F 1 scores'], ['improved F 1 scores', 'with', 'top 5 slot'], ['top 5 slot', 'being', 'org : political / religious affiliation'], ['top 5 slot', 'being', 'per: country of death'], ['top 5 slot', 'being', 'org : alternate names'], ['top 5 slot', 'being', 'per:religion'], ['top 5 slot', 'being', 'per: alternate names'], ['improved F 1 scores', 'on', '26 out of the 41 slot types']]","[['SDP - LSTM model', 'has', 'our model']]",[],"[['Ablation analysis', 'compared with', 'SDP - LSTM model']]",[],[],[],[],[],relation_extraction,4,179
ablation-analysis,We observe that slot types with relatively sparse training examples tend to be improved by using the position - aware attention model .,"[('observe that', (1, 3)), ('with', (5, 6)), ('improved by', (13, 15))]","[('slot types', (3, 5)), ('relatively sparse training examples', (6, 10)), ('position - aware attention model', (17, 22))]","[['slot types', 'with', 'relatively sparse training examples'], ['slot types', 'improved by', 'position - aware attention model']]",[],"[['Ablation analysis', 'observe that', 'slot types']]",[],[],[],[],[],[],relation_extraction,4,180
ablation-analysis,"We find that the model learns to pay more attention to words that are informative for the relation ( e.g. , "" graduated from "" , "" niece "" and "" chairman "" ) , though it still makes mistakes ( e.g. , "" refused to name the three "" ) .","[('find', (1, 2)), ('to pay', (6, 8)), ('to', (10, 11)), ('informative for', (14, 16))]","[('model', (4, 5)), ('more attention', (8, 10)), ('words', (11, 12)), ('relation', (17, 18))]","[['model', 'to pay', 'more attention'], ['more attention', 'to', 'words'], ['words', 'informative for', 'relation']]",[],"[['Ablation analysis', 'find', 'model']]",[],[],[],[],[],[],relation_extraction,4,183
ablation-analysis,"We also observe that the model tends to put a lot of weight onto object entities , as the object NER signatures are very informative to the classification of relations .","[('observe', (2, 3)), ('tends to put', (6, 9)), ('onto', (13, 14))]","[('model', (5, 6)), ('lot of weight', (10, 13)), ('object entities', (14, 16))]","[['model', 'tends to put', 'lot of weight'], ['lot of weight', 'onto', 'object entities']]",[],"[['Ablation analysis', 'observe', 'model']]",[],[],[],[],[],[],relation_extraction,4,184
research-problem,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,[],"[('Relation Extraction', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Extraction']]",[],[],[],[],relation_extraction,5,2
model,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .","[('propose', (5, 6)), ('tailored for', (17, 19))]","[('novel extension of the graph convolutional network', (7, 14)), ('relation extraction', (19, 21))]","[['novel extension of the graph convolutional network', 'tailored for', 'relation extraction']]",[],"[['Model', 'propose', 'novel extension of the graph convolutional network']]",[],[],[],[],[],[],relation_extraction,5,28
model,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .","[('encodes', (2, 3)), ('over', (6, 7)), ('with', (10, 11)), ('extracts', (17, 18)), ('to make', (22, 24))]","[('dependency structure', (4, 6)), ('input sentence', (8, 10)), ('efficient graph convolution operations', (11, 15)), ('entity - centric representations', (18, 22)), ('robust relation predictions', (24, 27))]","[['dependency structure', 'over', 'input sentence'], ['input sentence', 'with', 'efficient graph convolution operations'], ['dependency structure', 'extracts', 'entity - centric representations'], ['entity - centric representations', 'to make', 'robust relation predictions']]",[],"[['Model', 'encodes', 'dependency structure']]",[],[],[],[],[],[],relation_extraction,5,29
model,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .","[('apply', (2, 3)), ('to remove', (10, 12)), ('from', (14, 15)), ('maximally keeping', (18, 20))]","[('novel path - centric pruning technique', (4, 10)), ('irrelevant information', (12, 14)), ('the tree', (15, 17)), ('relevant content', (20, 22))]","[['novel path - centric pruning technique', 'to remove', 'irrelevant information'], ['irrelevant information', 'from', 'the tree'], ['irrelevant information', 'maximally keeping', 'relevant content']]",[],"[['Model', 'apply', 'novel path - centric pruning technique']]",[],[],[],[],[],[],relation_extraction,5,30
baselines,Dependency - based models .,[],"[('Dependency - based models', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Dependency - based models']]",[],[],[],[],"[['Dependency - based models', 'has', 'A logistic regression ( LR ) classifier']]",relation_extraction,5,124
baselines,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,"[('combines', (11, 12)), ('with', (14, 15))]","[('A logistic regression ( LR ) classifier', (3, 10)), ('dependencybased features', (12, 14)), ('other lexical features', (15, 18))]","[['A logistic regression ( LR ) classifier', 'combines', 'dependencybased features'], ['dependencybased features', 'with', 'other lexical features']]",[],[],[],[],[],[],[],[],relation_extraction,5,126
baselines,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","[('applies', (14, 15)), ('on', (19, 20)), ('between', (23, 24)), ('in', (29, 30))]","[('Shortest Dependency Path LSTM ( SDP - LSTM )', (3, 12)), ('neural sequence model', (16, 19)), ('shortest path', (21, 23)), ('subject and object entities', (25, 29)), ('dependency tree', (31, 33))]","[['Shortest Dependency Path LSTM ( SDP - LSTM )', 'applies', 'neural sequence model'], ['neural sequence model', 'on', 'shortest path'], ['shortest path', 'between', 'subject and object entities'], ['subject and object entities', 'in', 'dependency tree']]",[],[],[],[],[],[],"[['Dependency - based models', 'has', 'Shortest Dependency Path LSTM ( SDP - LSTM )']]",[],relation_extraction,5,127
baselines,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .","[('is', (5, 6)), ('generalizes', (10, 11)), ('to', (13, 14))]","[('Tree - LSTM', (0, 3)), ('recursive model', (7, 9)), ('LSTM', (12, 13)), ('arbitrary tree structures', (14, 17))]","[['Tree - LSTM', 'is', 'recursive model'], ['recursive model', 'generalizes', 'LSTM'], ['LSTM', 'to', 'arbitrary tree structures']]",[],[],[],[],[],[],"[['Dependency - based models', 'has', 'Tree - LSTM']]",[],relation_extraction,5,128
baselines,Neural sequence model .,[],"[('Neural sequence model', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Neural sequence model']]",[],[],[],[],"[['Neural sequence model', 'has', 'competitive sequence model']]",relation_extraction,5,132
baselines,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .","[('employs', (8, 9)), ('over', (15, 16))]","[('competitive sequence model', (4, 7)), ('position - aware attention mechanism', (10, 15)), ('LSTM outputs ( PA - LSTM )', (16, 23))]","[['competitive sequence model', 'employs', 'position - aware attention mechanism'], ['position - aware attention mechanism', 'over', 'LSTM outputs ( PA - LSTM )']]",[],[],[],[],[],[],[],[],relation_extraction,5,133
results,Results on the TACRED Dataset,"[('on', (1, 2))]","[('TACRED Dataset', (3, 5))]",[],[],"[['Results', 'on', 'TACRED Dataset']]",[],[],[],[],[],[],relation_extraction,5,149
results,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,"[('observe', (1, 2)), ('outperforms', (17, 18)), ('by', (23, 24))]","[('GCN model', (4, 6)), ('all dependency - based models', (18, 23)), ('at least 1.6 F 1', (24, 29))]","[['GCN model', 'outperforms', 'all dependency - based models'], ['all dependency - based models', 'by', 'at least 1.6 F 1']]",[],[],[],[],"[['TACRED Dataset', 'observe', 'GCN model']]",[],[],[],relation_extraction,5,151
results,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .","[('using', (1, 2)), ('outperforms', (12, 13)), ('by', (19, 20)), ('achieves', (25, 26))]","[('contextualized word representations', (2, 5)), ('C - GCN model', (7, 11)), ('strong PA - LSTM model', (14, 19)), ('1.3 F 1', (20, 23)), ('new state of the art', (27, 32))]","[['C - GCN model', 'outperforms', 'strong PA - LSTM model'], ['strong PA - LSTM model', 'by', '1.3 F 1'], ['strong PA - LSTM model', 'achieves', 'new state of the art']]","[['contextualized word representations', 'has', 'C - GCN model']]",[],[],[],"[['TACRED Dataset', 'using', 'contextualized word representations']]",[],[],[],relation_extraction,5,152
results,"In addition , we find our model improves upon other dependencybased models in both precision and recall .","[('find', (4, 5)), ('improves upon', (7, 9)), ('in', (12, 13))]","[('our model', (5, 7)), ('other dependencybased models', (9, 12)), ('both precision and recall', (13, 17))]","[['our model', 'improves upon', 'other dependencybased models'], ['other dependencybased models', 'in', 'both precision and recall']]",[],[],[],[],"[['TACRED Dataset', 'find', 'our model']]",[],[],[],relation_extraction,5,153
results,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .","[('Comparing', (0, 1)), ('with', (6, 7)), ('find that', (12, 14)), ('mainly comes from', (16, 19))]","[('C - GCN model', (2, 6)), ('GCN model', (8, 10)), ('gain', (15, 16)), ('improved recall', (19, 21))]","[['C - GCN model', 'with', 'GCN model'], ['GCN model', 'find that', 'gain'], ['gain', 'mainly comes from', 'improved recall']]",[],[],[],[],"[['TACRED Dataset', 'Comparing', 'C - GCN model']]",[],[],[],relation_extraction,5,154
results,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .","[('have', (14, 15)), ('when compared to', (17, 20))]","[('our GCN models', (11, 14)), ('complementary strengths', (15, 17)), ('PA - LSTM', (21, 24))]","[['our GCN models', 'have', 'complementary strengths'], ['complementary strengths', 'when compared to', 'PA - LSTM']]",[],[],[],[],[],[],"[['TACRED Dataset', 'find', 'our GCN models']]",[],relation_extraction,5,156
results,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .","[('between', (3, 4)), ('achieves', (11, 12)), ('of', (16, 17)), ('outperforming', (19, 20)), ('by', (23, 24))]","[('simple interpolation', (1, 3)), ('GCN and a PA - LSTM', (5, 11)), ('F 1 score', (13, 16)), ('67.1', (17, 18)), ('each model alone', (20, 23)), ('at least 2.0 F 1', (24, 29))]","[['simple interpolation', 'between', 'GCN and a PA - LSTM'], ['GCN and a PA - LSTM', 'achieves', 'F 1 score'], ['F 1 score', 'of', '67.1'], ['GCN and a PA - LSTM', 'outperforming', 'each model alone'], ['each model alone', 'by', 'at least 2.0 F 1']]",[],[],[],[],[],[],"[['TACRED Dataset', 'has', 'simple interpolation']]",[],relation_extraction,5,161
results,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,"[('between', (2, 3)), ('further improves', (12, 14)), ('to', (16, 17))]","[('interpolation', (1, 2)), ('C - GCN and a PA - LSTM', (4, 12)), ('result', (15, 16)), ('68.2', (17, 18))]","[['interpolation', 'between', 'C - GCN and a PA - LSTM'], ['C - GCN and a PA - LSTM', 'further improves', 'result'], ['result', 'to', '68.2']]",[],[],[],[],[],[],"[['TACRED Dataset', 'has', 'interpolation']]",[],relation_extraction,5,162
results,Results on the SemEval Dataset,[],"[('SemEval Dataset', (3, 5))]",[],[],[],"[['Results', 'on', 'SemEval Dataset']]",[],[],[],[],[],relation_extraction,5,163
results,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .","[('find that under', (1, 4)), ('outperforms', (15, 16))]","[('conventional with- entity evaluation', (5, 9)), ('our C - GCN model', (10, 15)), ('all existing dependency - based neural models', (16, 23))]","[['our C - GCN model', 'outperforms', 'all existing dependency - based neural models']]","[['conventional with- entity evaluation', 'has', 'our C - GCN model']]",[],[],[],"[['SemEval Dataset', 'find that under', 'conventional with- entity evaluation']]",[],[],[],relation_extraction,5,165
results,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .","[('by properly incorporating', (2, 5)), ('outperforms', (12, 13))]","[('off - path information', (5, 9)), ('our model', (10, 12)), ('previous shortest dependency path - based model ( SDP - LSTM )', (14, 26))]","[['our model', 'outperforms', 'previous shortest dependency path - based model ( SDP - LSTM )']]","[['off - path information', 'has', 'our model']]",[],[],[],"[['SemEval Dataset', 'by properly incorporating', 'off - path information']]",[],[],[],relation_extraction,5,166
results,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .","[('Under', (0, 1)), ('outperforms', (13, 14)), ('by', (17, 18))]","[('mask - entity evaluation', (2, 6)), ('our C - GCN model', (7, 12)), ('PA - LSTM', (14, 17)), ('substantial margin', (19, 21))]","[['our C - GCN model', 'outperforms', 'PA - LSTM'], ['PA - LSTM', 'by', 'substantial margin']]","[['mask - entity evaluation', 'has', 'our C - GCN model']]",[],[],[],"[['SemEval Dataset', 'Under', 'mask - entity evaluation']]",[],[],[],relation_extraction,5,167
results,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .","[('show', (1, 2)), ('of', (4, 5)), ('compare', (11, 12)), ('when', (21, 22))]","[('effectiveness', (3, 4)), ('path - centric pruning', (5, 9)), ('two GCN models and the Tree - LSTM', (13, 21))]","[['effectiveness', 'of', 'path - centric pruning'], ['path - centric pruning', 'compare', 'two GCN models and the Tree - LSTM']]",[],"[['Results', 'show', 'effectiveness']]",[],[],[],[],[],[],relation_extraction,5,170
results,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .","[('performance', (5, 6)), ('outperforming', (16, 17))]","[('peaks', (10, 11)), ('K = 1', (12, 15)), ('respective dependency path - based counterpart ( K = 0 )', (18, 29))]","[['peaks', 'outperforming', 'respective dependency path - based counterpart ( K = 0 )']]","[['peaks', 'when', 'K = 1']]",[],[],[],"[['two GCN models and the Tree - LSTM', 'performance', 'peaks']]",[],[],[],relation_extraction,5,172
results,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .","[('find that', (1, 3)), ('are', (6, 7)), ('when', (9, 10)), ('is', (14, 15))]","[('all three models', (3, 6)), ('less effective', (7, 9)), ('entire dependency tree', (11, 14)), ('present', (15, 16))]","[['all three models', 'are', 'less effective'], ['less effective', 'when', 'entire dependency tree'], ['entire dependency tree', 'is', 'present']]",[],"[['Results', 'find that', 'all three models']]",[],[],[],[],[],[],relation_extraction,5,176
results,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .","[('contextualizing', (5, 6)), ('makes it', (8, 10)), ('to', (12, 13)), ('in', (14, 15))]","[('GCN', (7, 8)), ('less sensitive', (10, 12)), ('changes', (13, 14)), ('tree structures', (16, 18))]","[['GCN', 'makes it', 'less sensitive'], ['less sensitive', 'to', 'changes'], ['changes', 'in', 'tree structures']]",[],"[['Results', 'contextualizing', 'GCN']]",[],[],[],[],[],[],relation_extraction,5,177
ablation-analysis,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,"[('contribute', (10, 11))]","[('entity representations and feedforward layers', (5, 10)), ('1.0 F 1', (11, 14))]","[['entity representations and feedforward layers', 'contribute', '1.0 F 1']]",[],[],"[['Ablation analysis', 'has', 'entity representations and feedforward layers']]",[],[],[],[],[],relation_extraction,5,181
ablation-analysis,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","[('remove', (5, 6)), ('drops by', (19, 21))]","[('dependency structure', (7, 9)), ('score', (18, 19)), ('3.2 F 1', (21, 24))]","[['score', 'drops by', '3.2 F 1']]","[['dependency structure', 'has', 'score']]","[['Ablation analysis', 'remove', 'dependency structure']]",[],[],[],[],[],[],relation_extraction,5,182
ablation-analysis,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .","[('drops by', (5, 7)), ('when we remove', (8, 11))]","[('F 1', (3, 5)), ('10.3', (7, 8)), ('feedforward layers', (12, 14)), ('LSTM component', (16, 18)), ('dependency structure', (20, 22))]","[['F 1', 'drops by', '10.3'], ['10.3', 'when we remove', 'feedforward layers'], ['10.3', 'when we remove', 'LSTM component'], ['10.3', 'when we remove', 'dependency structure']]",[],[],"[['Ablation analysis', 'has', 'F 1']]",[],[],[],[],[],relation_extraction,5,183
ablation-analysis,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .","[('Removing', (3, 4)), ('hurts', (16, 17)), ('by another', (19, 21))]","[('pruning', (5, 6)), ('result', (18, 19)), ('9.7 F 1', (21, 24))]","[['pruning', 'hurts', 'result'], ['result', 'by another', '9.7 F 1']]",[],"[['Ablation analysis', 'Removing', 'pruning']]",[],[],[],[],[],[],relation_extraction,5,184
research-problem,Context - Aware Representations for Knowledge Base Relation Extraction,[],"[('Knowledge Base Relation Extraction', (5, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Knowledge Base Relation Extraction']]",[],[],[],[],relation_extraction,6,2
research-problem,We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,[],"[('sentence - level relation extraction', (4, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentence - level relation extraction']]",[],[],[],[],relation_extraction,6,4
research-problem,The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .,[],"[('relation extraction', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'relation extraction']]",[],[],[],[],relation_extraction,6,12
research-problem,"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .",[],"[('sentential relation extraction', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentential relation extraction']]",[],[],[],[],relation_extraction,6,13
model,We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .,"[('present', (1, 2)), ('considers', (6, 7)), ('in', (9, 10)), ('as', (12, 13)), ('for predicting', (15, 17)), ('of', (19, 20))]","[('novel architecture', (3, 5)), ('other relations', (7, 9)), ('sentence', (11, 12)), ('context', (14, 15)), ('label', (18, 19)), ('target relation', (21, 23))]","[['novel architecture', 'considers', 'other relations'], ['other relations', 'as', 'context'], ['other relations', 'in', 'sentence'], ['other relations', 'for predicting', 'label'], ['label', 'of', 'target relation']]",[],"[['Model', 'present', 'novel architecture']]",[],[],[],[],[],[],relation_extraction,6,20
model,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,"[('uses', (2, 3)), ('to jointly learn', (8, 11)), ('for', (12, 13)), ('in', (15, 16))]","[('architecture', (1, 2)), ('LSTM - based encoder', (4, 8)), ('representations', (11, 12)), ('all relations', (13, 15)), ('single sentence', (17, 19))]","[['architecture', 'uses', 'LSTM - based encoder'], ['LSTM - based encoder', 'to jointly learn', 'representations'], ['representations', 'for', 'all relations'], ['all relations', 'in', 'single sentence']]",[],[],"[['Model', 'has', 'architecture']]",[],[],[],[],[],relation_extraction,6,22
model,The representation of the target relation and representations of the context relations are combined to make the final prediction .,"[('representation of', (1, 3)), ('representations of', (7, 9)), ('are', (12, 13)), ('to make', (14, 16))]","[('target relation', (4, 6)), ('context relations', (10, 12)), ('combined', (13, 14)), ('final prediction', (17, 19))]","[['combined', 'representations of', 'context relations'], ['combined', 'to make', 'final prediction'], ['combined', 'representation of', 'target relation']]",[],"[['Model', 'are', 'combined']]",[],[],[],[],[],[],relation_extraction,6,23
hyperparameters,All models were trained using the Adam optimizer with categorical crossentropy as the loss function .,"[('trained using', (3, 5)), ('with', (8, 9)), ('as', (11, 12))]","[('Adam optimizer', (6, 8)), ('categorical crossentropy', (9, 11)), ('loss function', (13, 15))]","[['Adam optimizer', 'with', 'categorical crossentropy'], ['categorical crossentropy', 'as', 'loss function']]",[],"[['Hyperparameters', 'trained using', 'Adam optimizer']]",[],[],[],[],[],[],relation_extraction,6,99
hyperparameters,We use an early stopping criterion on the validation data to determine the number of training epochs .,"[('use', (1, 2)), ('on', (6, 7)), ('to determine', (10, 12))]","[('early stopping criterion', (3, 6)), ('validation data', (8, 10)), ('number of training epochs', (13, 17))]","[['early stopping criterion', 'on', 'validation data'], ['validation data', 'to determine', 'number of training epochs']]",[],"[['Hyperparameters', 'use', 'early stopping criterion']]",[],[],[],[],[],[],relation_extraction,6,100
hyperparameters,"The learning rate is fixed to 0.01 and the rest of the optimization parameters are set as recommended in : ? 1 = 0.9 , ? 2 = 0.999 , ? = 1e ? 08 . The training is performed in batches of 128 instances .","[('fixed to', (4, 6)), ('of', (10, 11)), ('performed in', (39, 41))]","[('learning rate', (1, 3)), ('0.01', (6, 7)), ('training', (37, 38)), ('batches', (41, 42)), ('128 instances', (43, 45))]","[['learning rate', 'fixed to', '0.01'], ['training', 'performed in', 'batches'], ['batches', 'of', '128 instances']]",[],[],"[['Hyperparameters', 'has', 'learning rate'], ['Hyperparameters', 'has', 'training']]",[],[],[],[],[],relation_extraction,6,101
hyperparameters,We apply Dropout on the penultimate layer as well as on the embeddings layer with a probability of 0.5 .,"[('apply', (1, 2)), ('on', (3, 4)), ('as well as on', (7, 11)), ('with', (14, 15)), ('of', (17, 18))]","[('Dropout', (2, 3)), ('penultimate layer', (5, 7)), ('embeddings layer', (12, 14)), ('probability', (16, 17)), ('0.5', (18, 19))]","[['Dropout', 'on', 'penultimate layer'], ['penultimate layer', 'as well as on', 'embeddings layer'], ['embeddings layer', 'with', 'probability'], ['probability', 'of', '0.5']]",[],"[['Hyperparameters', 'apply', 'Dropout']]",[],[],[],[],[],[],relation_extraction,6,102
hyperparameters,We choose the size of the layers ( RNN layer size o = 256 ) and entity marker embeddings ( d = 3 ) with a random search on the validation set .,"[('choose', (1, 2)), ('of', (4, 5)), ('with', (24, 25)), ('on', (28, 29))]","[('size', (3, 4)), ('layers ( RNN layer size o = 256 )', (6, 15)), ('random search', (26, 28)), ('validation set', (30, 32))]","[['size', 'with', 'random search'], ['random search', 'on', 'validation set'], ['size', 'of', 'layers ( RNN layer size o = 256 )']]",[],"[['Hyperparameters', 'choose', 'size']]",[],[],[],[],[],[],relation_extraction,6,103
results,"The models that take the context into account perform similar to the baselines at the smallest recall numbers , but start to positively deviate from them at higher recall rates .",[],[],"[['models', 'that take', 'context'], ['context', 'into', 'account'], ['models', 'perform', 'similar'], ['similar', 'to', 'baselines'], ['baselines', 'at', 'smallest recall numbers'], ['models', 'perform', 'start'], ['start', 'to', 'positively deviate'], ['positively deviate', 'at', 'higher recall rates']]",[],[],"[['Results', 'has', 'models']]",[],[],[],[],[],relation_extraction,6,113
results,"In particular , the ContextAtt model performs better than any other system in our study over the entire recall range .","[('performs', (6, 7)), ('than', (8, 9)), ('over', (15, 16))]","[('ContextAtt model', (4, 6)), ('better', (7, 8)), ('any other system', (9, 12)), ('entire recall range', (17, 20))]","[['ContextAtt model', 'performs', 'better'], ['better', 'over', 'entire recall range'], ['better', 'than', 'any other system']]",[],[],"[['Results', 'has', 'ContextAtt model']]",[],[],[],[],[],relation_extraction,6,114
results,"Compared to the competitive LSTM - baseline that uses the same relation encoder , the ContextAtt model achieves a 24 % reduction of the average error : from 0.2096 0.002 to 0.1590 0.002 .","[('Compared to', (0, 2)), ('achieves', (17, 18)), ('of', (22, 23))]","[('competitive LSTM - baseline', (3, 7)), ('ContextAtt model', (15, 17)), ('24 % reduction', (19, 22)), ('average error', (24, 26))]","[['ContextAtt model', 'achieves', '24 % reduction'], ['24 % reduction', 'of', 'average error']]","[['competitive LSTM - baseline', 'has', 'ContextAtt model']]","[['Results', 'Compared to', 'competitive LSTM - baseline']]",[],[],[],[],[],[],relation_extraction,6,115
results,shows that the ContextAtt model performs best over all relation types .,"[('shows', (0, 1)), ('performs', (5, 6)), ('over', (7, 8))]","[('ContextAtt model', (3, 5)), ('best', (6, 7)), ('all relation types', (8, 11))]","[['ContextAtt model', 'over', 'all relation types'], ['ContextAtt model', 'performs', 'best']]",[],"[['Results', 'shows', 'ContextAtt model']]",[],[],[],[],[],[],relation_extraction,6,118
results,One can also see that the ContextSum does n't universally outperforms the LSTM - baseline .,"[('see that', (3, 5)), (""does n't universally outperforms"", (7, 11))]","[('ContextSum', (6, 7)), ('LSTM - baseline', (12, 15))]","[['ContextSum', ""does n't universally outperforms"", 'LSTM - baseline']]",[],"[['Results', 'see that', 'ContextSum']]",[],[],[],[],[],[],relation_extraction,6,119
results,It demonstrates again that using attention is crucial to extract relevant information from the context relations .,"[('using', (4, 5)), ('crucial to extract', (7, 10)), ('from', (12, 13))]","[('attention', (5, 6)), ('relevant information', (10, 12)), ('context relations', (14, 16))]","[['attention', 'crucial to extract', 'relevant information'], ['relevant information', 'from', 'context relations']]",[],"[['Results', 'using', 'attention']]",[],[],[],[],[],[],relation_extraction,6,120
results,"On the relation - specific results we observe that the context - enabled model demonstrates the most improvement on precision and seems to be especially useful for taxonomy relations ( see SUBCLASS OF , PART OF ) .","[('observe', (7, 8)), ('demonstrates', (14, 15)), ('on', (18, 19)), ('useful for', (25, 27))]","[('context - enabled model', (10, 14)), ('most improvement', (16, 18)), ('precision', (19, 20)), ('taxonomy relations', (27, 29))]","[['context - enabled model', 'demonstrates', 'most improvement'], ['most improvement', 'on', 'precision'], ['context - enabled model', 'useful for', 'taxonomy relations']]",[],"[['Results', 'observe', 'context - enabled model']]",[],[],[],[],[],[],relation_extraction,6,121
research-problem,Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,[],"[('Neural Relation Extraction', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Relation Extraction']]",[],[],[],[],relation_extraction,7,2
research-problem,Relation extraction aims to extract relations between pairs of marked entities in raw texts .,[],"[('Relation extraction', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation extraction']]",[],[],[],[],relation_extraction,7,13
research-problem,"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .","[('propose', (5, 6)), ('for', (12, 13)), ('by reducing', (17, 19)), ('improving', (22, 23)), ('against', (24, 25))]","[('novel word - level approach', (7, 12)), ('distant supervised relation extraction', (13, 17)), ('inner-sentence noise', (19, 21)), ('robustness', (23, 24)), ('noisy words', (25, 27))]","[['novel word - level approach', 'improving', 'robustness'], ['robustness', 'against', 'noisy words'], ['novel word - level approach', 'for', 'distant supervised relation extraction'], ['novel word - level approach', 'by reducing', 'inner-sentence noise']]",[],"[['Approach', 'propose', 'novel word - level approach']]",[],"[['Contribution', 'has research problem', 'distant supervised relation extraction']]",[],[],[],[],relation_extraction,7,36
approach,"To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .","[('To reduce', (0, 2)), ('utilize', (6, 7)), ('to remove', (17, 19)), ('by intercepting', (21, 23)), ('under', (25, 26))]","[('innersentence noise', (2, 4)), ('novel Sub - Tree Parse ( STP ) method', (8, 17)), ('irrelevant words', (19, 21)), ('subtree', (24, 25)), (""parent of entities ' lowest common ancestor"", (27, 34))]","[['innersentence noise', 'utilize', 'novel Sub - Tree Parse ( STP ) method'], ['novel Sub - Tree Parse ( STP ) method', 'by intercepting', 'subtree'], ['subtree', 'under', ""parent of entities ' lowest common ancestor""], ['novel Sub - Tree Parse ( STP ) method', 'to remove', 'irrelevant words']]",[],"[['Approach', 'To reduce', 'innersentence noise']]",[],[],[],[],[],[],relation_extraction,7,37
approach,"Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .","[('to alleviate', (9, 11)), ('in', (16, 17)), ('emphasize', (20, 21))]","[('entity - wise attention', (3, 7)), ('influence of noisy words', (12, 16)), ('subtree', (18, 19)), ('task - relevant features', (22, 26))]","[['entity - wise attention', 'to alleviate', 'influence of noisy words'], ['influence of noisy words', 'in', 'subtree'], ['entity - wise attention', 'emphasize', 'task - relevant features']]",[],[],"[['Approach', 'has', 'entity - wise attention']]",[],[],[],[],[],relation_extraction,7,39
approach,"To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .","[('initialize', (7, 8)), ('with', (11, 12)), ('learned from', (15, 17)), ('by', (22, 23))]","[('model parameters', (9, 11)), ('a priori knowledge', (12, 15)), ('entity type classification task', (18, 22)), ('transfer learning', (23, 25))]","[['model parameters', 'with', 'a priori knowledge'], ['a priori knowledge', 'learned from', 'entity type classification task'], ['entity type classification task', 'by', 'transfer learning']]",[],"[['Approach', 'initialize', 'model parameters']]",[],[],[],[],[],[],relation_extraction,7,40
experimental-setup,"In the experiment , we utilize word2vec 2 to train word embeddings on NYT corpus .","[('utilize', (5, 6)), ('to train', (8, 10)), ('on', (12, 13))]","[('word2vec', (6, 7)), ('word embeddings', (10, 12)), ('NYT corpus', (13, 15))]","[['word2vec', 'to train', 'word embeddings'], ['word embeddings', 'on', 'NYT corpus']]",[],"[['Experimental setup', 'utilize', 'word2vec']]",[],[],[],[],[],[],relation_extraction,7,191
experimental-setup,"The grid search approach is used to select optimal learning rate lr for Adam optimizer among { 0.1 , 0.001 , 0.0005 , 0.0001 } , GRU size m ? { 100 , 160 , 230 , 400 } , position embedding size l ? { 5 , 10 , 15 , 20}. shows all parameters for our task .","[('to select', (6, 8)), ('for', (12, 13)), ('among', (15, 16))]","[('grid search approach', (1, 4)), ('optimal learning rate lr', (8, 12)), ('Adam optimizer', (13, 15)), ('0.1 , 0.001 , 0.0005 , 0.0001', (17, 24)), ('GRU size m ?', (26, 30)), ('100 , 160 , 230 , 400', (31, 38)), ('position embedding size l ?', (40, 45))]","[['grid search approach', 'to select', 'position embedding size l ?'], ['grid search approach', 'to select', 'GRU size m ?'], ['grid search approach', 'to select', 'optimal learning rate lr'], ['optimal learning rate lr', 'among', '0.1 , 0.001 , 0.0005 , 0.0001'], ['optimal learning rate lr', 'for', 'Adam optimizer']]","[['GRU size m ?', 'has', '100 , 160 , 230 , 400']]",[],"[['Experimental setup', 'has', 'grid search approach']]",[],[],[],[],"[['position embedding size l ?', 'has', '5 , 10 , 15 , 20']]",relation_extraction,7,193
experimental-setup,GRU size m 230,"[('GRU size m', (0, 3))]","[('230', (3, 4))]",[],[],"[['Experimental setup', 'GRU size m', '230']]",[],[],[],[],[],[],relation_extraction,7,195
experimental-setup,Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ?,[],[],[],[],"[['Experimental setup', 'Word embedding dimension k', '50'], ['Experimental setup', 'POS embedding dimension l', '5'], ['Experimental setup', 'Batch size n', '50']]",[],[],[],[],[],"[['Experimental setup', 'Entity - Task weights', '0.5']]",relation_extraction,7,196
experimental-setup,"head , ? tail ) 0.5,0.5 Entity - Relation Task weight ?","[('Entity - Relation Task weight', (6, 11))]",[],[],[],[],[],[],[],[],[],"[['Experimental setup', 'Entity - Relation Task weight', '0.3']]",relation_extraction,7,197
experimental-setup,0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?,"[('Learning rate lr', (1, 4)), ('Dropout probability p', (5, 8)), ('l 2 penalty', (9, 12))]","[('0.3', (0, 1)), ('0.001', (4, 5)), ('0.5', (8, 9))]",[],[],"[['Experimental setup', 'Learning rate lr', '0.001'], ['Experimental setup', 'Dropout probability p', '0.5']]",[],[],[],[],[],"[['Experimental setup', 'l 2 penalty', '0.0001']]",relation_extraction,7,198
experimental-setup,0.0001,[],"[('0.0001', (0, 1))]",[],[],[],[],[],[],[],[],[],relation_extraction,7,199
results,"From , we can observe that the model with the STP performs best , and the SDP model obtains an even worse result than the pure one .","[('observe', (4, 5)), ('with', (8, 9)), ('performs', (11, 12)), ('obtains', (18, 19)), ('than', (23, 24))]","[('model', (7, 8)), ('STP', (10, 11)), ('best', (12, 13)), ('SDP model', (16, 18)), ('even worse result', (20, 23)), ('pure one', (25, 27))]","[['SDP model', 'obtains', 'even worse result'], ['even worse result', 'than', 'pure one'], ['model', 'with', 'STP'], ['model', 'performs', 'best']]",[],"[['Results', 'observe', 'model']]","[['Results', 'has', 'SDP model']]",[],[],[],[],[],relation_extraction,7,205
results,"The PR curve areas of BGRU + SDP and BGRU are about 0.332 and 0.337 respectively , while BGRU + STP increases it to 0.366 .","[('of', (4, 5)), ('are about', (10, 12)), ('increases it to', (21, 24))]","[('PR curve areas', (1, 4)), ('BGRU + SDP and BGRU', (5, 10)), ('0.332 and 0.337', (12, 15)), ('BGRU + STP', (18, 21)), ('0.366', (24, 25))]","[['PR curve areas', 'of', 'BGRU + SDP and BGRU'], ['BGRU + SDP and BGRU', 'are about', '0.332 and 0.337'], ['PR curve areas', 'of', 'BGRU + STP'], ['BGRU + STP', 'increases it to', '0.366']]",[],[],"[['Results', 'has', 'PR curve areas']]",[],[],[],[],[],relation_extraction,7,206
results,The result indicates : ( 1 ) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction .,"[('get rid of', (10, 13)), ('in', (15, 16)), ('obtain', (19, 20))]","[('Our STP', (7, 9)), ('irrelevant words', (13, 15)), ('each instance', (16, 18)), ('more precise sentence representation', (20, 24))]","[['Our STP', 'obtain', 'more precise sentence representation'], ['Our STP', 'get rid of', 'irrelevant words'], ['irrelevant words', 'in', 'each instance']]",[],[],"[['Results', 'has', 'Our STP']]",[],[],[],[],[],relation_extraction,7,207
results,( 2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .,"[('not appropriate to handle', (7, 11)), ('where', (15, 16)), ('not in', (20, 22))]","[('SDP method', (4, 6)), ('low - quality sentences', (11, 15)), ('key relation words', (16, 19)), ('SDP', (23, 24))]","[['SDP method', 'not appropriate to handle', 'low - quality sentences'], ['low - quality sentences', 'where', 'key relation words'], ['key relation words', 'not in', 'SDP']]",[],[],"[['Results', 'has', 'SDP method']]",[],[],[],[],[],relation_extraction,7,209
results,"From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , BGRU - WLA ( + STP ) + EWA outperforms BGRU (+ STP ) .","[('outperforms', (27, 28))]","[('BGRU - WLA ( + STP ) + EWA', (18, 27)), ('BGRU (+ STP )', (28, 32))]","[['BGRU - WLA ( + STP ) + EWA', 'outperforms', 'BGRU (+ STP )']]",[],[],"[['Results', 'has', 'BGRU - WLA ( + STP ) + EWA']]",[],[],[],[],[],relation_extraction,7,214
results,"To be more specific , the PR curve area has a relative improvement of over 2.3 % , which demonstrates that entity - wise hidden states in the BGRU present more precise relational features than other word states .","[('of', (13, 14))]","[('PR curve area', (6, 9)), ('relative improvement', (11, 13)), ('over 2.3 %', (14, 17))]","[['relative improvement', 'of', 'over 2.3 %']]","[['PR curve area', 'has', 'relative improvement']]",[],"[['Results', 'has', 'PR curve area']]",[],[],[],[],[],relation_extraction,7,215
results,"EWA achieves further improvements and outperforms the baseline by over 4.6 % , because it considers more information than entity or relational words alone .","[('achieves', (1, 2)), ('outperforms', (5, 6)), ('by over', (8, 10))]","[('EWA', (0, 1)), ('further improvements', (2, 4)), ('baseline', (7, 8)), ('4.6 %', (10, 12))]","[['EWA', 'achieves', 'further improvements'], ['EWA', 'outperforms', 'baseline'], ['baseline', 'by over', '4.6 %']]",[],[],"[['Results', 'has', 'EWA']]",[],[],[],[],[],relation_extraction,7,217
results,"( 1 ) Regardless of the dataset that we use , models with TL achieve better performance , which improve the PR curve area by over 4.7 % .","[('achieve', (14, 15)), ('improve', (19, 20)), ('by', (24, 25))]","[('models with TL', (11, 14)), ('better performance', (15, 17)), ('PR curve area', (21, 24)), ('over 4.7 %', (25, 28))]","[['models with TL', 'achieve', 'better performance'], ['models with TL', 'improve', 'PR curve area'], ['PR curve area', 'by', 'over 4.7 %']]",[],[],"[['Results', 'has', 'models with TL']]",[],[],[],[],[],relation_extraction,7,227
results,"( 2 ) BGRU + STP + TL achieves the best performance and increases the area to 0.383 , while areas of BGRU , BGRU + STP and BGRU + TL are 0.337 , 0.366 and 0.372 respectively .","[('achieves', (8, 9)), ('increases', (13, 14)), ('to', (16, 17))]","[('BGRU + STP + TL', (3, 8)), ('best performance', (10, 12)), ('area', (15, 16)), ('0.383', (17, 18))]","[['BGRU + STP + TL', 'increases', 'area'], ['area', 'to', '0.383'], ['BGRU + STP + TL', 'achieves', 'best performance']]",[],[],"[['Results', 'has', 'BGRU + STP + TL']]",[],[],[],[],[],relation_extraction,7,229
baselines,Mintz proposes the humandesigned feature model .,"[('proposes', (1, 2))]","[('Mintz', (0, 1)), ('humandesigned feature model', (3, 6))]","[['Mintz', 'proposes', 'humandesigned feature model']]",[],[],"[['Baselines', 'has', 'Mintz']]",[],[],[],[],[],relation_extraction,7,233
baselines,MultiR puts forward a graphical model .,"[('puts forward', (1, 3))]","[('MultiR', (0, 1)), ('graphical model', (4, 6))]","[['MultiR', 'puts forward', 'graphical model']]",[],[],"[['Baselines', 'has', 'MultiR']]",[],[],[],[],[],relation_extraction,7,234
baselines,MIML proposes a multi -instance multi-label model .,"[('proposes', (1, 2))]","[('MIML', (0, 1)), ('multi -instance multi-label model', (3, 7))]","[['MIML', 'proposes', 'multi -instance multi-label model']]",[],[],"[['Baselines', 'has', 'MIML']]",[],[],[],[],[],relation_extraction,7,235
baselines,PCNN puts forward a piecewise CNN for relation extraction .,"[('puts forward', (1, 3))]","[('PCNN', (0, 1)), ('piecewise CNN for relation extraction', (4, 9))]","[['PCNN', 'puts forward', 'piecewise CNN for relation extraction']]",[],[],"[['Baselines', 'has', 'PCNN']]",[],[],[],[],[],relation_extraction,7,236
baselines,PCNN + ATT proposes the selective attention mechanism with PCNN .,"[('proposes', (3, 4)), ('with', (8, 9))]","[('PCNN + ATT', (0, 3)), ('selective attention mechanism', (5, 8)), ('PCNN', (9, 10))]","[['PCNN + ATT', 'proposes', 'selective attention mechanism'], ['selective attention mechanism', 'with', 'PCNN']]",[],[],"[['Baselines', 'has', 'PCNN + ATT']]",[],[],[],[],[],relation_extraction,7,237
baselines,BGRU proposes a BGRU with the word - level attention mechanism .,"[('proposes', (1, 2)), ('with', (4, 5))]","[('BGRU', (0, 1)), ('word - level attention mechanism', (6, 11))]","[['BGRU', 'with', 'word - level attention mechanism']]",[],"[['Baselines', 'proposes', 'BGRU']]",[],[],[],[],[],[],relation_extraction,7,238
research-problem,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,[],"[('Distant Supervision for Relation Extraction', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Distant Supervision for Relation Extraction']]",[],[],[],[],relation_extraction,8,2
research-problem,"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .",[],"[('relation extraction', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'relation extraction']]",[],[],[],[],relation_extraction,8,15
model,"In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .","[('propose', (5, 6)), ('with', (19, 20))]","[('novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )', (7, 19)), ('multi-instance learning', (20, 22))]","[['novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )', 'with', 'multi-instance learning']]",[],"[['Model', 'propose', 'novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )']]",[],[],[],[],[],[],relation_extraction,8,36
research-problem,"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .","[('treated as', (11, 13))]","[('distant supervised relation extraction', (6, 10)), ('multi-instance problem', (14, 16))]","[['distant supervised relation extraction', 'treated as', 'multi-instance problem']]",[],[],"[['Model', 'has', 'distant supervised relation extraction']]","[['Contribution', 'has research problem', 'distant supervised relation extraction']]",[],[],[],[],relation_extraction,8,37
model,We design an objective function at the bag level .,"[('design', (1, 2)), ('at', (5, 6))]","[('objective function', (3, 5)), ('bag level', (7, 9))]","[['objective function', 'at', 'bag level']]",[],"[['Model', 'design', 'objective function']]",[],[],[],[],[],[],relation_extraction,8,40
model,"In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .","[('In', (0, 1)), ('of', (7, 8)), ('alleviates', (17, 18))]","[('learning process', (2, 4)), ('uncertainty', (6, 7)), ('instance labels', (8, 10)), ('wrong label problem', (19, 22))]","[['uncertainty', 'alleviates', 'wrong label problem'], ['uncertainty', 'of', 'instance labels']]","[['learning process', 'has', 'uncertainty']]","[['Model', 'In', 'learning process']]",[],[],[],[],[],[],relation_extraction,8,41
model,"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .","[('adopt', (7, 8)), ('to automatically learn', (10, 13)), ('without', (15, 16))]","[('convolutional architecture', (8, 10)), ('relevant features', (13, 15)), ('complicated NLP preprocessing', (16, 19))]","[['convolutional architecture', 'to automatically learn', 'relevant features'], ['relevant features', 'without', 'complicated NLP preprocessing']]",[],"[['Model', 'adopt', 'convolutional architecture']]",[],[],[],[],[],[],relation_extraction,8,42
model,"To capture structural and other latent information , we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer .","[('To capture', (0, 2)), ('divide', (9, 10)), ('into', (13, 14)), ('based on', (16, 18)), ('of', (20, 21)), ('devise', (26, 27)), ('instead of', (32, 34))]","[('structural and other latent information', (2, 7)), ('convolution results', (11, 13)), ('three segments', (14, 16)), ('positions', (19, 20)), ('two given entities', (22, 25)), ('piecewise max pooling layer', (28, 32)), ('single max pooling layer', (35, 39))]","[['structural and other latent information', 'divide', 'convolution results'], ['convolution results', 'into', 'three segments'], ['three segments', 'based on', 'positions'], ['positions', 'of', 'two given entities'], ['structural and other latent information', 'devise', 'piecewise max pooling layer'], ['piecewise max pooling layer', 'instead of', 'single max pooling layer']]",[],"[['Model', 'To capture', 'structural and other latent information']]",[],[],[],[],[],[],relation_extraction,8,51
model,The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence .,"[('returns', (5, 6)), ('in', (9, 10)), ('instead of', (12, 14)), ('over', (18, 19))]","[('piecewise max pooling procedure', (1, 5)), ('maximum value', (7, 9)), ('each segment', (10, 12)), ('single maximum value', (15, 18)), ('entire sentence', (20, 22))]","[['piecewise max pooling procedure', 'returns', 'maximum value'], ['maximum value', 'in', 'each segment'], ['each segment', 'instead of', 'single maximum value'], ['single maximum value', 'over', 'entire sentence']]",[],[],"[['Model', 'has', 'piecewise max pooling procedure']]",[],[],[],[],[],relation_extraction,8,52
experimental-setup,"In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .","[('use', (5, 6)), ('to train', (16, 18)), ('on', (21, 22))]","[('Skip - gram model ( word2 vec )', (7, 15)), ('word embeddings', (19, 21)), ('NYT corpus', (23, 25))]","[['Skip - gram model ( word2 vec )', 'to train', 'word embeddings'], ['word embeddings', 'on', 'NYT corpus']]",[],"[['Experimental setup', 'use', 'Skip - gram model ( word2 vec )']]",[],[],[],[],[],[],relation_extraction,8,202
experimental-setup,"Following , we tune all of the models using three - fold validation on the training set .","[('tune', (3, 4)), ('using', (8, 9)), ('on', (13, 14))]","[('models', (7, 8)), ('three - fold validation', (9, 13)), ('training set', (15, 17))]","[['models', 'using', 'three - fold validation'], ['three - fold validation', 'on', 'training set']]",[],"[['Experimental setup', 'tune', 'models']]",[],[],[],[],[],[],relation_extraction,8,213
experimental-setup,"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .","[('to determine', (5, 7)), ('manually specify', (11, 13)), ('of', (14, 15))]","[('grid search', (3, 5)), ('optimal parameters', (8, 10)), ('subsets', (13, 14)), ('parameter spaces', (16, 18)), ('w ? { 1 , 2 , 3 , , 7 }', (19, 31))]","[['grid search', 'to determine', 'optimal parameters'], ['grid search', 'manually specify', 'subsets'], ['subsets', 'of', 'parameter spaces']]","[['parameter spaces', 'has', 'w ? { 1 , 2 , 3 , , 7 }']]",[],"[['Experimental setup', 'use', 'grid search']]",[],[],[],[],"[['parameter spaces', 'has', 'n ? { 50 , 60 , , 300}']]",relation_extraction,8,214
experimental-setup,"Because the position dimension has little effect on the result , we heuristically choose d p = 5 .","[('heuristically choose', (12, 14)), ('=', (16, 17))]","[('d p', (14, 16)), ('5', (17, 18))]","[['d p', '=', '5']]",[],"[['Experimental setup', 'heuristically choose', 'd p']]",[],[],[],[],[],[],relation_extraction,8,215
experimental-setup,The batch size is fixed to 50 .,"[('fixed to', (4, 6))]","[('batch size', (1, 3)), ('50', (6, 7))]","[['batch size', 'fixed to', '50']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],relation_extraction,8,216
experimental-setup,"We use Adadelta in the update procedure ; it relies on two main parameters , ? and ? , which do not significantly affect the performance .","[('in', (3, 4))]","[('Adadelta', (2, 3)), ('update procedure', (5, 7))]","[['Adadelta', 'in', 'update procedure']]",[],[],"[['Experimental setup', 'use', 'Adadelta']]",[],[],[],[],[],relation_extraction,8,217
experimental-setup,"In the dropout operation , we randomly set the hidden unit activities to zero with a probability of 0.5 during training .","[('In', (0, 1)), ('randomly set', (6, 8)), ('to', (12, 13)), ('with', (14, 15)), ('of', (17, 18)), ('during', (19, 20))]","[('dropout operation', (2, 4)), ('hidden unit activities', (9, 12)), ('zero', (13, 14)), ('probability', (16, 17)), ('0.5', (18, 19)), ('training', (20, 21))]","[['dropout operation', 'randomly set', 'hidden unit activities'], ['hidden unit activities', 'with', 'probability'], ['probability', 'of', '0.5'], ['hidden unit activities', 'to', 'zero'], ['hidden unit activities', 'during', 'training']]",[],"[['Experimental setup', 'In', 'dropout operation']]",[],[],[],[],[],[],relation_extraction,8,219
baselines,Mintz represents a traditional distantsupervision - based model that was proposed by .,"[('represents', (1, 2))]","[('Mintz', (0, 1)), ('traditional distantsupervision - based model', (3, 8))]","[['Mintz', 'represents', 'traditional distantsupervision - based model']]",[],[],"[['Baselines', 'has', 'Mintz']]",[],[],[],[],[],relation_extraction,8,226
baselines,MultiR is a multi-instance learning method that was proposed by .,"[('is', (1, 2))]","[('MultiR', (0, 1)), ('multi-instance learning method', (3, 6))]","[['MultiR', 'is', 'multi-instance learning method']]",[],[],"[['Baselines', 'has', 'MultiR']]",[],[],[],[],[],relation_extraction,8,227
baselines,MIML is a multi-instance multilabel model that was proposed by .,"[('is', (1, 2))]","[('MIML', (0, 1)), ('multi-instance multilabel model', (3, 6))]","[['MIML', 'is', 'multi-instance multilabel model']]",[],[],"[['Baselines', 'has', 'MIML']]",[],[],[],[],[],relation_extraction,8,228
results,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves higher precision over the entire range of recall .","[('demonstrates', (19, 20)), ('achieves', (24, 25)), ('over', (27, 28))]","[('PCNNs + MIL', (11, 14)), ('higher precision', (25, 27)), ('entire range of recall', (29, 33))]","[['PCNNs + MIL', 'achieves', 'higher precision'], ['higher precision', 'over', 'entire range of recall']]",[],"[['Results', 'demonstrates', 'PCNNs + MIL']]",[],[],[],[],[],[],relation_extraction,8,229
results,PCNNs + MIL enhances the recall to ap - proximately 34 % without any loss of precision .,"[('enhances', (3, 4)), ('to', (6, 7)), ('without any loss of', (12, 16))]","[('PCNNs + MIL', (0, 3)), ('recall', (5, 6)), ('ap - proximately 34 %', (7, 12)), ('precision', (16, 17))]","[['PCNNs + MIL', 'enhances', 'recall'], ['recall', 'to', 'ap - proximately 34 %'], ['PCNNs + MIL', 'without any loss of', 'precision']]",[],[],"[['Results', 'has', 'PCNNs + MIL']]",[],[],[],[],[],relation_extraction,8,230
results,"In terms of both precision and recall , PCNNs + MIL outperforms all other evaluated approaches .","[('In terms of both', (0, 4)), ('outperforms', (11, 12))]","[('precision and recall', (4, 7)), ('PCNNs + MIL', (8, 11)), ('all other evaluated approaches', (12, 16))]","[['PCNNs + MIL', 'outperforms', 'all other evaluated approaches']]","[['precision and recall', 'has', 'PCNNs + MIL']]","[['Results', 'In terms of both', 'precision and recall']]",[],[],[],[],[],[],relation_extraction,8,231
results,Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction .,"[('Automatically learning', (0, 2)), ('via', (3, 4)), ('alleviate', (6, 7))]","[('features', (2, 3)), ('PCNNs', (4, 5)), ('error propagation', (8, 10))]","[['features', 'via', 'PCNNs'], ['PCNNs', 'alleviate', 'error propagation']]",[],"[['Results', 'Automatically learning', 'features']]",[],[],[],[],[],[],relation_extraction,8,235
results,Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem .,"[('Incorporating', (0, 1)), ('into', (3, 4)), ('effective means of addressing', (10, 14))]","[('multi-instance learning', (1, 3)), ('convolutional neural network', (5, 8)), ('wrong label problem', (15, 18))]","[['multi-instance learning', 'into', 'convolutional neural network'], ['convolutional neural network', 'effective means of addressing', 'wrong label problem']]",[],"[['Results', 'Incorporating', 'multi-instance learning']]",[],[],[],[],[],[],relation_extraction,8,236
research-problem,Relation Classification via Multi - Level Attention CNNs,[],"[('Relation Classification', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Relation Classification']]",[],[],[],[],relation_extraction,9,2
model,We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .,"[('propose', (1, 2))]","[('novel CNN architecture', (3, 6))]",[],[],"[['Model', 'propose', 'novel CNN architecture']]",[],[],[],[],[],[],relation_extraction,9,24
model,"Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .",[],[],"[['CNN architecture', 'relies on', 'novel multi-level attention mechanism'], ['novel multi-level attention mechanism', 'to capture', 'entity - specific attention'], ['primary attention', 'at', 'input level'], ['input level', 'with respect to', 'target entities'], ['novel multi-level attention mechanism', 'to capture', 'relation - specific pooling attention'], ['secondary attention', 'with respect to', 'target relations']]","[['entity - specific attention', 'has', 'primary attention'], ['relation - specific pooling attention', 'has', 'secondary attention']]",[],"[['Model', 'has', 'CNN architecture']]",[],[],[],[],[],relation_extraction,9,27
model,2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .,"[('introduce', (3, 4)), ('proves', (15, 16)), ('to', (17, 18))]","[('novel pair - wise margin - based objective function', (5, 14)), ('superior', (16, 17)), ('standard loss functions', (18, 21))]","[['novel pair - wise margin - based objective function', 'proves', 'superior'], ['superior', 'to', 'standard loss functions']]",[],"[['Model', 'introduce', 'novel pair - wise margin - based objective function']]",[],[],[],[],[],[],relation_extraction,9,29
experimental-setup,We use the word2 vec skip - gram model to learn initial word representations on Wikipedia .,"[('use', (1, 2)), ('to learn', (9, 11)), ('on', (14, 15))]","[('word2 vec skip - gram model', (3, 9)), ('initial word representations', (11, 14)), ('Wikipedia', (15, 16))]","[['word2 vec skip - gram model', 'to learn', 'initial word representations'], ['initial word representations', 'on', 'Wikipedia']]",[],"[['Experimental setup', 'use', 'word2 vec skip - gram model']]",[],[],[],[],[],[],relation_extraction,9,149
experimental-setup,Other matrices are initialized with random values following a Gaussian distribution .,"[('initialized with', (3, 5)), ('following', (7, 8))]","[('Other matrices', (0, 2)), ('random values', (5, 7)), ('Gaussian distribution', (9, 11))]","[['Other matrices', 'initialized with', 'random values'], ['random values', 'following', 'Gaussian distribution']]",[],[],"[['Experimental setup', 'has', 'Other matrices']]",[],[],[],[],[],relation_extraction,9,150
experimental-setup,We apply a cross-validation procedure on the training data to select suitable hyperparameters .,"[('apply', (1, 2)), ('on', (5, 6)), ('to select', (9, 11))]","[('cross-validation procedure', (3, 5)), ('training data', (7, 9)), ('suitable hyperparameters', (11, 13))]","[['cross-validation procedure', 'to select', 'suitable hyperparameters'], ['cross-validation procedure', 'on', 'training data']]",[],"[['Experimental setup', 'apply', 'cross-validation procedure']]",[],[],[],[],[],[],relation_extraction,9,151
results,We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,"[('observe that', (1, 3)), ('achieves', (7, 8))]","[('novel attentionbased architecture', (4, 7)), ('new state - of - the - art results', (8, 17))]","[['novel attentionbased architecture', 'achieves', 'new state - of - the - art results']]",[],"[['Results', 'observe that', 'novel attentionbased architecture']]",[],[],[],[],[],[],relation_extraction,9,153
results,"Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .","[('relies only on', (5, 8)), ('at', (11, 12)), ('performing', (16, 17)), ('after', (21, 22)), ('to generate', (25, 27))]","[('Att - Input - CNN', (0, 5)), ('primal attention', (9, 11)), ('input level', (13, 15)), ('standard max - pooling', (17, 21)), ('convolution layer', (23, 25)), ('network output w O', (28, 32))]","[['Att - Input - CNN', 'relies only on', 'primal attention'], ['primal attention', 'at', 'input level'], ['Att - Input - CNN', 'performing', 'standard max - pooling'], ['standard max - pooling', 'after', 'convolution layer'], ['convolution layer', 'to generate', 'network output w O']]",[],[],"[['Results', 'has', 'Att - Input - CNN']]",[],[],[],[],[],relation_extraction,9,154
results,"With Att - Input - CNN , we achieve an F1-score of 87.5 % , thus already outperforming not only the original winner of the SemEval task , an SVM - based approach ( 82.2 % ) , but also the wellknown CR - CNN model ( 84.1 % ) with a relative improvement of 4.04 % , and the newly released DRNNs ( 85.8 % ) with a relative improvement of 2.0 % , although the latter approach depends on the Stanford parser to obtain dependency parse information .","[('With', (0, 1)), ('achieve', (8, 9)), ('of', (11, 12)), ('outperforming', (17, 18))]","[('Att - Input - CNN', (1, 6)), ('F1-score', (10, 11)), ('87.5 %', (12, 14)), ('an SVM - based approach ( 82.2 % )', (28, 37)), ('wellknown CR - CNN model ( 84.1 % )', (41, 50)), ('newly released DRNNs ( 85.8 % )', (60, 67))]","[['Att - Input - CNN', 'achieve', 'F1-score'], ['F1-score', 'of', '87.5 %'], ['87.5 %', 'outperforming', 'an SVM - based approach ( 82.2 % )'], ['87.5 %', 'outperforming', 'wellknown CR - CNN model ( 84.1 % )'], ['87.5 %', 'outperforming', 'newly released DRNNs ( 85.8 % )']]",[],"[['Results', 'With', 'Att - Input - CNN']]",[],[],[],[],[],[],relation_extraction,9,155
results,Our full dual attention model Att - Pooling - CNN achieves an even more favorable F1- score of 88 % .,"[('achieves', (10, 11)), ('of', (17, 18))]","[('Our full dual attention model Att - Pooling - CNN', (0, 10)), ('even more favorable F1- score', (12, 17)), ('88 %', (18, 20))]","[['Our full dual attention model Att - Pooling - CNN', 'achieves', 'even more favorable F1- score'], ['even more favorable F1- score', 'of', '88 %']]",[],[],"[['Results', 'has', 'Our full dual attention model Att - Pooling - CNN']]",[],[],[],[],[],relation_extraction,9,156
ablation-analysis,"To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .","[('evaluating', (19, 20))]","[('model', (11, 12)), ('several simplified models', (20, 23))]",[],[],"[['Ablation analysis', 'evaluating', 'several simplified models']]",[],[],[],[],[],"[['several simplified models', 'has', 'first simplification'], ['several simplified models', 'has', 'third'], ['several simplified models', 'has', 'second']]",relation_extraction,9,159
ablation-analysis,The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .,"[('to use', (4, 6)), ('without', (8, 9)), ('with', (14, 15))]","[('first simplification', (1, 3)), ('input attention mechanism', (10, 13)), ('pooling attention layer', (16, 19))]",[],[],[],[],[],"[['model', 'with', 'pooling attention layer'], ['model', 'without', 'input attention mechanism']]","[['first simplification', 'to use', 'model']]",[],[],relation_extraction,9,160
ablation-analysis,The second removes both attention mechanisms .,"[('removes', (2, 3))]","[('second', (1, 2)), ('both attention mechanisms', (3, 6))]","[['second', 'removes', 'both attention mechanisms']]",[],[],[],[],[],[],[],[],relation_extraction,9,161
ablation-analysis,The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r w for a sentence representation r and relation class embedding w.,"[('removes', (2, 3)), ('uses', (9, 10)), ('based on', (14, 16)), ('for', (23, 24))]","[('third', (1, 2)), ('both forms of attention', (3, 7)), ('regular objective function', (11, 14)), ('inner product s = r w', (17, 23)), ('sentence representation r', (25, 28))]","[['third', 'uses', 'regular objective function'], ['regular objective function', 'based on', 'inner product s = r w'], ['inner product s = r w', 'for', 'sentence representation r'], ['third', 'removes', 'both forms of attention']]",[],[],[],[],[],"[['inner product s = r w', 'for', 'relation class embedding w']]",[],[],relation_extraction,9,162
ablation-analysis,We observe that all three of our components lead to noticeable improvements over these baselines .,"[('observe that', (1, 3)), ('lead to', (8, 10)), ('over', (12, 13))]","[('all three of our components', (3, 8)), ('noticeable improvements', (10, 12)), ('baselines', (14, 15))]","[['all three of our components', 'lead to', 'noticeable improvements'], ['noticeable improvements', 'over', 'baselines']]",[],"[['Ablation analysis', 'observe that', 'all three of our components']]",[],[],[],[],[],[],relation_extraction,9,163
research-problem,"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .",[],"[('sarcasm detection', (25, 27))]",[],[],[],[],"[['Contribution', 'has research problem', 'sarcasm detection']]",[],[],[],[],sarcasm_detection,0,4
dataset,"In this work , we make available the first corpus 1 for sarcasm detection that has both unbalanced and self - annotated labels and does not consist of short text snippets from Twitter 2 .","[('make available', (5, 7)), ('for', (11, 12))]","[('first corpus', (8, 10)), ('sarcasm detection', (12, 14)), ('unbalanced and self - annotated labels', (17, 23))]","[['first corpus', 'for', 'sarcasm detection']]","[['first corpus', 'has', 'unbalanced and self - annotated labels']]","[['Dataset', 'make available', 'first corpus']]",[],[],[],[],[],[],sarcasm_detection,0,12
dataset,"With more than a million examples of sarcastic statements , each provided with author , topic , and contex information , the dataset exceeds all previous sarcasm corpora by an order of magnitude in size .","[('With', (0, 1)), ('of', (6, 7)), ('each provided with', (10, 13)), ('exceeds', (23, 24)), ('by', (28, 29))]","[('more than a million examples', (1, 6)), ('sarcastic statements', (7, 9)), ('author , topic , and contex information', (13, 20)), ('previous sarcasm corpora', (25, 28)), ('an order of magnitude in size', (29, 35))]","[['more than a million examples', 'exceeds', 'previous sarcasm corpora'], ['previous sarcasm corpora', 'by', 'an order of magnitude in size'], ['more than a million examples', 'of', 'sarcastic statements'], ['sarcastic statements', 'each provided with', 'author , topic , and contex information']]",[],"[['Dataset', 'With', 'more than a million examples']]",[],[],[],[],[],[],sarcasm_detection,0,13
code,Code to reproduce our results is provided at https://github.com/NLPrinceton/,[],"[('https://github.com/NLPrinceton/', (8, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/NLPrinceton/']]",[],[],[],[],sarcasm_detection,0,120
results,"The baselines in perform reasonably well and much better than the random baseline , but none of them match human performance on either dataset .","[('perform', (3, 4)), ('than', (9, 10))]","[('reasonably well', (4, 6)), ('much better', (7, 9)), ('random baseline', (11, 13))]","[['much better', 'than', 'random baseline']]",[],"[['Results', 'perform', 'reasonably well'], ['Results', 'perform', 'much better']]",[],[],[],[],[],[],sarcasm_detection,0,169
results,"There is clear scope for improvement for machine learning methods , starting with the use of context provided to make better decisions about sarcasm .","[('for', (4, 5))]","[('clear scope', (2, 4)), ('improvement', (5, 6)), ('machine learning methods', (7, 10))]","[['clear scope', 'for', 'improvement'], ['clear scope', 'for', 'machine learning methods']]",[],[],"[['Results', 'has', 'clear scope']]",[],[],[],[],[],sarcasm_detection,0,170
research-problem,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,[],"[('Contextual Sarcasm Detection', (2, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Contextual Sarcasm Detection']]",[],[],[],[],sarcasm_detection,1,2
research-problem,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .",[],"[('automated sarcasm detection', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'automated sarcasm detection']]",[],[],[],[],sarcasm_detection,1,4
research-problem,"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",[],"[('sarcasm detection', (27, 29))]",[],[],[],[],"[['Contribution', 'has research problem', 'sarcasm detection']]",[],[],[],[],sarcasm_detection,1,6
model,"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .","[('propose', (3, 4)), ('named', (8, 9)), ('utilizes', (12, 13)), ('required for', (19, 21))]","[('hybrid network', (5, 7)), ('CASCADE', (9, 10)), ('content and contextual - information', (14, 19)), ('sarcasm detection', (21, 23))]","[['hybrid network', 'named', 'CASCADE'], ['hybrid network', 'utilizes', 'content and contextual - information'], ['content and contextual - information', 'required for', 'sarcasm detection']]",[],"[['Model', 'propose', 'hybrid network']]",[],[],[],[],[],[],sarcasm_detection,1,25
model,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .","[('performs', (3, 4)), ('to create', (6, 8)), ('that capture', (10, 12)), ('for', (15, 16))]","[('user profiling', (4, 6)), ('user embeddings', (8, 10)), ('indicative behavioral traits', (12, 15)), ('sarcasm', (16, 17))]","[['user profiling', 'to create', 'user embeddings'], ['user embeddings', 'that capture', 'indicative behavioral traits'], ['indicative behavioral traits', 'for', 'sarcasm']]",[],"[['Model', 'performs', 'user profiling']]",[],[],[],[],[],[],sarcasm_detection,1,27
model,"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .","[('makes use of', (1, 4)), ('to model', (8, 10)), ('fused into', (23, 25)), ('using', (28, 29))]","[(""users ' historical posts"", (4, 8)), ('writing style ( stylometry ) and personality indicators', (11, 19)), ('comprehensive user embeddings', (25, 28)), ('multi-view fusion approach', (30, 33)), ('Canonical Correlation Analysis ( CCA )', (34, 40))]","[[""users ' historical posts"", 'to model', 'writing style ( stylometry ) and personality indicators'], ['writing style ( stylometry ) and personality indicators', 'fused into', 'comprehensive user embeddings'], ['comprehensive user embeddings', 'using', 'multi-view fusion approach']]","[['multi-view fusion approach', 'name', 'Canonical Correlation Analysis ( CCA )']]","[['Model', 'makes use of', ""users ' historical posts""]]",[],[],[],[],[],[],sarcasm_detection,1,29
model,"Second , it extracts contextual information from the discourse of comments in the discussion forums .","[('extracts', (3, 4)), ('from', (6, 7)), ('of', (9, 10)), ('in', (11, 12))]","[('contextual information', (4, 6)), ('discourse', (8, 9)), ('comments', (10, 11)), ('discussion forums', (13, 15))]","[['contextual information', 'from', 'discourse'], ['discourse', 'in', 'discussion forums'], ['discourse', 'of', 'comments']]",[],"[['Model', 'extracts', 'contextual information']]",[],[],[],[],[],[],sarcasm_detection,1,30
model,This is done by document modeling of these consolidated comments belonging to the same forum .,"[('done by', (2, 4)), ('of', (6, 7)), ('belonging to', (10, 12))]","[('document modeling', (4, 6)), ('consolidated comments', (8, 10)), ('same forum', (13, 15))]","[['document modeling', 'of', 'consolidated comments'], ['consolidated comments', 'belonging to', 'same forum']]",[],"[['Model', 'done by', 'document modeling']]",[],[],[],[],[],[],sarcasm_detection,1,31
model,"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .","[('After', (0, 1)), ('provided with', (8, 10)), ('for', (12, 13))]","[('contextual modeling phase', (2, 5)), ('CASCADE', (6, 7)), ('comment', (11, 12)), ('sarcasm detection', (13, 15))]","[['CASCADE', 'provided with', 'comment'], ['comment', 'for', 'sarcasm detection']]","[['contextual modeling phase', 'has', 'CASCADE']]","[['Model', 'After', 'contextual modeling phase']]",[],[],[],[],[],[],sarcasm_detection,1,33
model,It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,"[('using', (5, 6)), ('to extract', (13, 15))]","[('content - modeling', (2, 5)), ('Convolutional Neural Network ( CNN )', (7, 13)), ('syntactic features', (16, 18))]","[['content - modeling', 'using', 'Convolutional Neural Network ( CNN )'], ['Convolutional Neural Network ( CNN )', 'to extract', 'syntactic features']]",[],[],"[['Model', 'performs', 'content - modeling']]",[],[],[],[],[],sarcasm_detection,1,34
model,This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,"[('concatenated with', (5, 7)), ('to get', (14, 16)), ('used for', (21, 23))]","[('CNN representation', (1, 3)), ('relevant user embedding and discourse features', (8, 14)), ('final representation', (17, 19)), ('classification', (23, 24))]","[['CNN representation', 'concatenated with', 'relevant user embedding and discourse features'], ['CNN representation', 'to get', 'final representation'], ['final representation', 'used for', 'classification']]",[],[],"[['Model', 'has', 'CNN representation']]",[],[],[],[],[],sarcasm_detection,1,35
hyperparameters,We holdout 10 % of the training data for validation .,"[('holdout', (1, 2)), ('of', (4, 5)), ('for', (8, 9))]","[('10 %', (2, 4)), ('training data', (6, 8)), ('validation', (9, 10))]","[['10 %', 'of', 'training data'], ['10 %', 'for', 'validation']]",[],"[['Hyperparameters', 'holdout', '10 %']]",[],[],[],[],[],[],sarcasm_detection,1,247
hyperparameters,"To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .","[('To optimize', (0, 2)), ('starting with', (17, 19)), ('of', (23, 24))]","[('parameters', (3, 4)), ('Adam optimizer', (5, 7)), ('initial learning rate', (20, 23)), ('1e ? 4', (24, 27))]","[['Adam optimizer', 'starting with', 'initial learning rate'], ['initial learning rate', 'of', '1e ? 4']]","[['parameters', 'has', 'Adam optimizer']]","[['Hyperparameters', 'To optimize', 'parameters']]",[],[],[],[],[],[],sarcasm_detection,1,249
hyperparameters,Training termination is decided using early stopping technique with a patience of 12 .,"[('decided using', (3, 5)), ('with', (8, 9)), ('of', (11, 12))]","[('Training termination', (0, 2)), ('early stopping technique', (5, 8)), ('patience', (10, 11)), ('12', (12, 13))]","[['Training termination', 'decided using', 'early stopping technique'], ['early stopping technique', 'with', 'patience'], ['patience', 'of', '12']]",[],[],"[['Hyperparameters', 'has', 'Training termination']]",[],[],[],[],[],sarcasm_detection,1,251
hyperparameters,"For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .","[('For', (0, 1)), ('of', (5, 6)), ('in', (7, 8)), ('is', (12, 13)), ('to', (17, 18)), ('for', (20, 21))]","[('batched - modeling', (2, 5)), ('comments', (6, 7)), ('CNNs', (8, 9)), ('each comment', (10, 12)), ('restricted or padded', (14, 17)), ('100 words', (18, 20)), ('uniformity', (21, 22))]","[['batched - modeling', 'of', 'comments'], ['comments', 'in', 'CNNs'], ['each comment', 'is', 'restricted or padded'], ['restricted or padded', 'for', 'uniformity'], ['restricted or padded', 'to', '100 words']]","[['comments', 'has', 'each comment']]","[['Hyperparameters', 'For', 'batched - modeling']]",[],[],[],[],[],[],sarcasm_detection,1,252
baselines,Bag - of - Words :,[],"[('Bag - of - Words', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Bag - of - Words']]",[],[],[],[],[],sarcasm_detection,1,258
baselines,This model uses a comment 's word - counts as features in a vector .,"[('uses', (2, 3)), ('as', (9, 10)), ('in', (11, 12))]","[(""comment 's"", (4, 6)), ('word - counts', (6, 9)), ('features', (10, 11)), ('vector', (13, 14))]","[['word - counts', 'as', 'features'], ['features', 'in', 'vector']]","[[""comment 's"", 'has', 'word - counts']]",[],[],[],"[['Bag - of - Words', 'uses', ""comment 's""]]",[],[],[],sarcasm_detection,1,259
baselines,CNN : We compare our model with this individual CNN version .,[],"[('CNN', (0, 1)), ('individual CNN version', (8, 11))]",[],"[['CNN', 'has', 'individual CNN version']]",[],"[['Baselines', 'has', 'CNN']]",[],[],[],[],[],sarcasm_detection,1,261
baselines,CNN - SVM :,[],"[('CNN - SVM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'CNN - SVM']]",[],[],[],[],[],sarcasm_detection,1,264
baselines,"This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .","[('consists of', (4, 6)), ('for', (8, 9)), ('for extracting', (15, 17)), ('from', (23, 24))]","[('CNN', (7, 8)), ('content modeling', (9, 11)), ('other pre-trained CNNs', (12, 15)), ('sentiment , emotion and personality features', (17, 23)), ('given comment', (25, 27))]","[['CNN', 'for', 'content modeling'], ['other pre-trained CNNs', 'for extracting', 'sentiment , emotion and personality features'], ['sentiment , emotion and personality features', 'from', 'given comment']]",[],[],[],[],"[['CNN - SVM', 'consists of', 'CNN'], ['CNN - SVM', 'consists of', 'other pre-trained CNNs']]",[],[],[],sarcasm_detection,1,265
baselines,CUE - CNN :,[],"[('CUE - CNN', (0, 3))]",[],[],[],"[['Baselines', 'has', 'CUE - CNN']]",[],[],[],[],[],sarcasm_detection,1,267
baselines,This method proposed by also models user embeddings with a method akin to ParagraphVector .,"[('models', (5, 6)), ('method akin to', (10, 13))]","[('user embeddings', (6, 8)), ('ParagraphVector', (13, 14))]","[['user embeddings', 'method akin to', 'ParagraphVector']]",[],[],[],[],"[['CUE - CNN', 'models', 'user embeddings']]",[],[],[],sarcasm_detection,1,268
results,CASCADE manages to achieve major improvement across all datasets with statistical significance .,"[('achieve', (3, 4)), ('across', (6, 7)), ('with', (9, 10))]","[('CASCADE', (0, 1)), ('major improvement', (4, 6)), ('all datasets', (7, 9)), ('statistical significance', (10, 12))]","[['CASCADE', 'achieve', 'major improvement'], ['major improvement', 'across', 'all datasets'], ['major improvement', 'with', 'statistical significance']]",[],[],"[['Results', 'has', 'CASCADE']]",[],[],[],[],[],sarcasm_detection,1,273
results,The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .,"[('obtained by', (4, 6))]","[('lowest performance', (1, 3)), ('Bag - of - words approach', (7, 13))]","[['lowest performance', 'obtained by', 'Bag - of - words approach']]",[],[],"[['Results', 'has', 'lowest performance']]",[],[],[],[],[],sarcasm_detection,1,274
results,"Amongst the neural networks , the CNN baseline receives the least performance .","[('Amongst', (0, 1)), ('receives', (8, 9))]","[('neural networks', (2, 4)), ('CNN baseline', (6, 8)), ('least performance', (10, 12))]","[['CNN baseline', 'receives', 'least performance']]","[['neural networks', 'has', 'CNN baseline']]","[['Results', 'Amongst', 'neural networks']]",[],[],[],[],[],[],sarcasm_detection,1,275
results,CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,"[('comfortably beats', (1, 3))]","[('state - of - the - art neural models', (4, 13)), ('CNN - SVM', (13, 16)), ('CUE - CNN', (17, 20))]",[],"[['state - of - the - art neural models', 'name', 'CNN - SVM'], ['state - of - the - art neural models', 'name', 'CUE - CNN']]",[],[],[],"[['CASCADE', 'comfortably beats', 'state - of - the - art neural models']]",[],[],[],sarcasm_detection,1,276
results,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,"[('on', (3, 4)), ('reflects', (9, 10)), ('towards', (12, 13)), ('establishes it as', (16, 19))]","[('improved performance', (1, 3)), ('Main imbalanced dataset', (5, 8)), ('robustness', (11, 12)), ('class imbalance', (13, 15)), ('real - world deployable network', (20, 25))]","[['improved performance', 'on', 'Main imbalanced dataset'], ['Main imbalanced dataset', 'establishes it as', 'real - world deployable network'], ['Main imbalanced dataset', 'reflects', 'robustness'], ['robustness', 'towards', 'class imbalance']]",[],[],"[['Results', 'has', 'improved performance']]",[],[],[],[],[],sarcasm_detection,1,277
results,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .","[('generates', (4, 5)), ('using', (8, 9)), ('similar to', (11, 13))]","[('CUE - CNN', (1, 4)), ('user embeddings', (6, 8)), ('method', (10, 11)), ('ParagraphVector', (14, 15))]","[['CUE - CNN', 'generates', 'user embeddings'], ['user embeddings', 'using', 'method'], ['method', 'similar to', 'ParagraphVector']]",[],[],"[['Results', 'has', 'CUE - CNN']]",[],[],[],[],[],sarcasm_detection,1,279
ablation-analysis,"First , we test performance for the content - based CNN only ( row 1 ) .","[('test', (3, 4)), ('for', (5, 6))]","[('performance', (4, 5)), ('content - based CNN only', (7, 12))]","[['performance', 'for', 'content - based CNN only']]",[],"[['Ablation analysis', 'test', 'performance']]",[],[],[],[],[],[],sarcasm_detection,1,286
ablation-analysis,This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .,"[('provides', (2, 3)), ('with', (7, 8)), ('than', (13, 14))]","[('worst relative performance', (4, 7)), ('almost 10 % lesser accuracy', (8, 13)), ('optimal', (14, 15))]","[['worst relative performance', 'with', 'almost 10 % lesser accuracy'], ['almost 10 % lesser accuracy', 'than', 'optimal']]",[],[],[],[],"[['content - based CNN only', 'provides', 'worst relative performance']]",[],[],[],sarcasm_detection,1,287
ablation-analysis,"Next , we include contextual features to this network .","[('include', (3, 4))]","[('contextual features', (4, 6))]",[],[],[],[],[],"[['content - based CNN only', 'include', 'contextual features']]",[],[],"[['contextual features', 'has', 'effect']]",sarcasm_detection,1,288
ablation-analysis,"Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .",[],[],"[['effect', 'of', 'discourse features'], ['discourse features', 'getting', 'increase'], ['increase', 'of', '3 % in F1'], ['discourse features', 'primarily seen in', 'Pol dataset']]",[],[],[],[],[],[],[],[],sarcasm_detection,1,289
ablation-analysis,A major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .,"[('in', (3, 4)), ('observed', (6, 7)), ('introduced', (20, 21))]","[('major boost', (1, 3)), ('performance', (4, 5)), ('user embeddings', (17, 19))]","[['user embeddings', 'observed', 'major boost'], ['major boost', 'in', 'performance']]",[],[],[],[],"[['content - based CNN only', 'introduced', 'user embeddings']]",[],[],[],sarcasm_detection,1,290
ablation-analysis,"Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .","[('consisting of', (3, 5)), ('with', (6, 7)), ('provide', (13, 14)), ('in', (17, 18))]","[('CASCADE', (2, 3)), ('CNN', (5, 6)), ('user embeddings and contextual discourse features', (7, 13)), ('best performance', (15, 17)), ('all three datasets', (18, 21))]","[['CASCADE', 'consisting of', 'CNN'], ['CNN', 'with', 'user embeddings and contextual discourse features'], ['user embeddings and contextual discourse features', 'provide', 'best performance'], ['best performance', 'in', 'all three datasets']]",[],[],"[['Ablation analysis', 'has', 'CASCADE']]",[],[],[],[],[],sarcasm_detection,1,292
ablation-analysis,We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .,"[('use of', (3, 5)), ('for', (6, 7)), ('of', (9, 10)), ('replace it with', (14, 17))]","[('CCA', (5, 6)), ('generation', (8, 9)), ('user embeddings', (10, 12)), ('simple concatenation', (17, 19))]","[['CCA', 'for', 'generation'], ['generation', 'of', 'user embeddings'], ['user embeddings', 'replace it with', 'simple concatenation']]",[],"[['Ablation analysis', 'use of', 'CCA']]",[],[],[],[],[],[],sarcasm_detection,1,293
ablation-analysis,This however causes a significant drop in performance ( row 3 ) .,"[('causes', (2, 3)), ('in', (6, 7))]","[('significant drop', (4, 6)), ('performance', (7, 8))]","[['significant drop', 'in', 'performance']]",[],[],[],[],"[['simple concatenation', 'causes', 'significant drop']]",[],[],[],sarcasm_detection,1,294
research-problem,Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task,[],"[('Complex and Cross - Domain Semantic Parsing', (11, 18)), ('Text - to - SQL', (19, 24))]",[],[],[],[],"[['Contribution', 'has research problem', 'Complex and Cross - Domain Semantic Parsing'], ['Contribution', 'has research problem', 'Text - to - SQL']]",[],[],[],[],semantic_parsing,0,2
code,Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,[],"[('https://yale-lily. github.io/ spider', (8, 11))]",[],[],[],[],"[['Contribution', 'Code', 'https://yale-lily. github.io/ spider']]",[],[],[],[],semantic_parsing,0,11
research-problem,Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .,[],"[('Semantic parsing ( SP )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic parsing ( SP )']]",[],[],[],[],semantic_parsing,0,13
research-problem,Existing datasets for SP have two shortcomings .,[],"[('SP', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'SP']]",[],[],[],[],semantic_parsing,0,19
dataset,"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .","[('for', (4, 5)), ('introduce', (23, 24)), ('consists of', (27, 29)), ('with', (31, 32))]","[('new complex and cross-domain semantic parsing task', (14, 21)), ('Spider', (24, 25)), ('200 databases', (29, 31)), ('multiple tables', (32, 34)), ('10,181 questions', (35, 37)), ('5,693 corresponding complex SQL queries', (39, 44))]","[['new complex and cross-domain semantic parsing task', 'introduce', 'Spider'], ['Spider', 'consists of', '200 databases'], ['200 databases', 'with', 'multiple tables'], ['200 databases', 'with', '10,181 questions'], ['200 databases', 'with', '5,693 corresponding complex SQL queries']]",[],"[['Dataset', 'for', 'new complex and cross-domain semantic parsing task']]",[],[],[],[],[],[],semantic_parsing,0,34
dataset,"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .","[('including', (9, 10)), ('creates and annotates', (15, 18)), ('such as', (27, 29))]","[('questions', (19, 20)), ('SQL queries', (21, 23)), ('different SQL clauses', (24, 27)), ('joining and nested query', (29, 33))]","[['SQL queries', 'including', 'different SQL clauses'], ['different SQL clauses', 'such as', 'joining and nested query']]",[],"[['Dataset', 'creates and annotates', 'questions'], ['Dataset', 'creates and annotates', 'SQL queries']]",[],[],[],[],[],[],semantic_parsing,0,35
results,"The performances of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .","[('of', (2, 3)), ('including', (9, 10)), ('are', (20, 21))]","[('performances', (1, 2)), ('Seq2Seq - based basic models', (4, 9)), ('Seq2Seq', (10, 11)), ('Seq2Seq + Attention', (12, 15)), ('Seq2Seq + Copying', (17, 20)), ('very low', (21, 23))]","[['performances', 'of', 'Seq2Seq - based basic models'], ['Seq2Seq - based basic models', 'including', 'Seq2Seq'], ['Seq2Seq - based basic models', 'including', 'Seq2Seq + Attention'], ['Seq2Seq - based basic models', 'including', 'Seq2Seq + Copying'], ['Seq2Seq - based basic models', 'are', 'very low']]",[],[],"[['Results', 'has', 'performances']]",[],[],[],[],[],semantic_parsing,0,248
results,"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the SQL generation process significantly outperform other Seq2Seq models .","[('utilize', (7, 8)), ('to guide', (11, 13)), ('significantly outperform', (17, 19))]","[('SQLNet and TypeSQL', (3, 6)), ('SQL structure information', (8, 11)), ('SQL generation process', (14, 17)), ('other Seq2Seq models', (19, 22))]","[['SQLNet and TypeSQL', 'utilize', 'SQL structure information'], ['SQL structure information', 'to guide', 'SQL generation process'], ['SQLNet and TypeSQL', 'significantly outperform', 'other Seq2Seq models']]",[],[],[],[],[],[],"[['performances', 'of', 'SQLNet and TypeSQL']]",[],semantic_parsing,0,253
results,"As Component Matching results in shows , all models struggle with WHERE clause prediction the most .","[('struggle with', (9, 11))]","[('all models', (7, 9)), ('WHERE clause prediction', (11, 14))]","[['all models', 'struggle with', 'WHERE clause prediction']]",[],[],"[['Results', 'has', 'all models']]",[],[],[],[],[],semantic_parsing,0,255
results,"In general , the over all performances of all models are low , indicating that our task is challenging and there is still a large room for improvement .","[('of', (7, 8)), ('are', (10, 11))]","[('over all performances', (4, 7)), ('all models', (8, 10)), ('low', (11, 12))]","[['over all performances', 'of', 'all models'], ['all models', 'are', 'low']]",[],[],"[['Results', 'has', 'over all performances']]",[],[],[],[],[],semantic_parsing,0,258
research-problem,TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,[],"[('Semantic Parsing and Code Generation', (11, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Parsing and Code Generation']]",[],[],[],[],semantic_parsing,1,2
model,"Inspired by this existing research , we have developed TRANX , a TRANsition - based abstract syntaX parser for semantic parsing and code generation .","[('developed', (8, 9)), ('for', (18, 19))]","[('TRANX', (9, 10)), ('TRANsition - based abstract syntaX parser', (12, 18)), ('semantic parsing and code generation', (19, 24))]","[['TRANX', 'for', 'semantic parsing and code generation']]","[['TRANX', 'name', 'TRANsition - based abstract syntaX parser']]","[['Model', 'developed', 'TRANX']]",[],[],[],[],[],[],semantic_parsing,1,18
model,TRANX is designed with the following principles in mind :,"[('designed with', (2, 4))]","[('TRANX', (0, 1)), ('following principles', (5, 7))]","[['TRANX', 'designed with', 'following principles']]",[],[],"[['Model', 'has', 'TRANX']]",[],[],[],[],"[['following principles', 'has', 'Generalization ability']]",semantic_parsing,1,19
model,"Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .",[],[],"[['Generalization ability', 'employs', 'ASTs'], ['ASTs', 'as', 'general - purpose intermediate meaning representation'], ['task - dependent grammar', 'provided to', 'system'], ['system', 'as', 'external knowledge'], ['external knowledge', 'to guide', 'parsing process']]","[['Generalization ability', 'has', 'task - dependent grammar']]",[],[],[],[],[],[],[],semantic_parsing,1,20
model,Extensibility TRANX uses a simple transition system to parse NL utterances into tree -,"[('uses', (2, 3)), ('to parse', (7, 9))]","[('Extensibility', (0, 1)), ('simple transition system', (4, 7)), ('NL utterances', (9, 11))]","[['Extensibility', 'uses', 'simple transition system'], ['simple transition system', 'to parse', 'NL utterances']]",[],[],[],[],[],[],"[['following principles', 'has', 'Extensibility']]",[],semantic_parsing,1,21
model,Effectiveness,[],"[('Effectiveness', (0, 1))]",[],[],[],[],[],[],[],"[['following principles', 'has', 'Effectiveness']]",[],semantic_parsing,1,24
model,"We test TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .","[('test', (1, 2)), ('on', (3, 4))]","[('TRANX', (2, 3)), ('semantic parsing', (5, 7)), ('ATIS', (8, 9)), ('GEO', (10, 11)), ('code generation', (13, 15)), ('DJANGO', (16, 17)), ('WIKISQL', (18, 19))]","[['TRANX', 'on', 'code generation'], ['TRANX', 'on', 'semantic parsing']]","[['code generation', 'name', 'DJANGO'], ['code generation', 'name', 'WIKISQL'], ['semantic parsing', 'name', 'ATIS'], ['semantic parsing', 'name', 'GEO']]",[],[],[],"[['Effectiveness', 'test', 'TRANX']]",[],[],[],semantic_parsing,1,25
results,Semantic Parsing Tab.,[],"[('Semantic Parsing', (0, 2))]",[],[],[],"[['Results', 'has', 'Semantic Parsing']]",[],[],[],[],"[['Semantic Parsing', 'has', 'Our system']]",semantic_parsing,1,126
results,Our system outperforms existing neural network - based approaches .,"[('outperforms', (2, 3))]","[('Our system', (0, 2)), ('existing neural network - based approaches', (3, 9))]","[['Our system', 'outperforms', 'existing neural network - based approaches']]",[],[],[],[],[],[],[],[],semantic_parsing,1,129
results,"Interestingly , we found the model without parent feeding achieves slightly better accuracy on GEO , probably because that its relative simple grammar does not require extra handling of parent information .","[('without', (6, 7)), ('achieves', (9, 10)), ('on', (13, 14))]","[('model', (5, 6)), ('parent feeding', (7, 9)), ('slightly better accuracy', (10, 13)), ('GEO', (14, 15))]","[['model', 'achieves', 'slightly better accuracy'], ['slightly better accuracy', 'on', 'GEO'], ['model', 'without', 'parent feeding']]",[],[],[],[],[],[],"[['Semantic Parsing', 'has', 'model']]",[],semantic_parsing,1,131
results,Code Generation Tab.,[],"[('Code Generation', (0, 2))]",[],[],[],"[['Results', 'has', 'Code Generation']]",[],[],[],[],"[['Code Generation', 'has', 'TRANX']]",semantic_parsing,1,132
results,2 lists the results on DJANGO .,"[('on', (4, 5))]","[('DJANGO', (5, 6))]",[],[],[],[],[],"[['state - of - the - art results', 'on', 'DJANGO']]",[],[],[],semantic_parsing,1,133
results,TRANX achieves state - of - the - art results on DJANGO .,"[('achieves', (1, 2))]","[('TRANX', (0, 1)), ('state - of - the - art results', (2, 10))]","[['TRANX', 'achieves', 'state - of - the - art results']]",[],[],[],[],[],[],[],[],semantic_parsing,1,134
results,"We also find parent feeding yields + 1 point gain in accuracy , suggesting the importance of modeling parental connections in ASTs with complex domain grammars ( e.g. , Python ) .","[('find', (2, 3)), ('yields', (5, 6)), ('in', (10, 11))]","[('parent feeding', (3, 5)), ('+ 1 point gain', (6, 10)), ('accuracy', (11, 12))]","[['parent feeding', 'yields', '+ 1 point gain'], ['+ 1 point gain', 'in', 'accuracy']]",[],[],[],[],"[['Code Generation', 'find', 'parent feeding']]",[],[],[],semantic_parsing,1,135
results,"3 . We find TRANX , although just with simple extensions to adapt to this dataset , achieves impressive results and outperforms many task - specific methods .","[('achieves', (17, 18)), ('outperforms', (21, 22))]","[('TRANX', (4, 5)), ('impressive results', (18, 20)), ('many task - specific methods', (22, 27))]","[['TRANX', 'achieves', 'impressive results'], ['TRANX', 'outperforms', 'many task - specific methods']]",[],[],[],[],[],[],"[['Code Generation', 'find', 'TRANX']]",[],semantic_parsing,1,138
research-problem,Coarse - to - Fine Decoding for Neural Semantic Parsing,[],"[('Neural Semantic Parsing', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Semantic Parsing']]",[],[],[],[],semantic_parsing,2,2
research-problem,Semantic parsing aims at mapping natural language utterances into structured meaning representations .,[],"[('Semantic parsing', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic parsing']]",[],[],[],[],semantic_parsing,2,4
model,"In this work , we propose to decompose the decoding process into two stages .","[('propose to decompose', (5, 8)), ('into', (11, 12))]","[('decoding process', (9, 11)), ('two stages', (12, 14))]","[['decoding process', 'into', 'two stages']]",[],"[['Model', 'propose to decompose', 'decoding process']]",[],[],[],[],[],[],semantic_parsing,2,14
model,"The first decoder focuses on predicting a rough sketch of the meaning representation , which omits low - level details , such as arguments and variable names .","[('focuses on', (3, 5)), ('of', (9, 10))]","[('first decoder', (1, 3)), ('predicting', (5, 6)), ('rough sketch', (7, 9)), ('meaning representation', (11, 13))]","[['first decoder', 'focuses on', 'predicting'], ['rough sketch', 'of', 'meaning representation']]","[['predicting', 'has', 'rough sketch']]",[],"[['Model', 'has', 'first decoder']]",[],[],[],[],[],semantic_parsing,2,15
model,"Then , a second decoder fills in missing details by conditioning on the natural language input and the sketch itself .","[('fills in', (5, 7)), ('by conditioning on', (9, 12))]","[('second decoder', (3, 5)), ('missing details', (7, 9)), ('natural language input', (13, 16)), ('sketch', (18, 19))]","[['second decoder', 'fills in', 'missing details'], ['missing details', 'by conditioning on', 'natural language input'], ['missing details', 'by conditioning on', 'sketch']]",[],[],"[['Model', 'has', 'second decoder']]",[],[],[],[],[],semantic_parsing,2,17
model,"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .","[('constrains', (4, 5)), ('encoded into', (10, 12)), ('to guide', (13, 15))]","[('sketch', (3, 4)), ('generation process', (6, 8)), ('vectors', (12, 13)), ('decoding', (15, 16))]","[['sketch', 'encoded into', 'vectors'], ['vectors', 'to guide', 'decoding'], ['sketch', 'constrains', 'generation process']]",[],[],"[['Model', 'has', 'sketch']]",[],[],[],[],[],semantic_parsing,2,18
model,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .","[('disentangles', (4, 5)), ('enables', (16, 17)), ('to model', (19, 21)), ('at', (22, 23))]","[('decomposition', (3, 4)), ('high - level from low - level semantic information', (5, 14)), ('decoders', (18, 19)), ('meaning', (21, 22)), ('different levels of granularity', (23, 27))]","[['decomposition', 'disentangles', 'high - level from low - level semantic information'], ['high - level from low - level semantic information', 'enables', 'decoders'], ['decoders', 'to model', 'meaning'], ['meaning', 'at', 'different levels of granularity']]",[],[],"[['Model', 'has', 'decomposition']]",[],[],[],[],[],semantic_parsing,2,20
model,"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .","[('of', (8, 9)), ('for', (11, 12))]","[('explicitly share', (5, 7)), ('knowledge', (7, 8)), ('coarse structures', (9, 11)), ('examples that have the same sketch ( i.e. , basic meaning )', (13, 25))]","[['knowledge', 'of', 'coarse structures'], ['coarse structures', 'for', 'examples that have the same sketch ( i.e. , basic meaning )']]","[['explicitly share', 'has', 'knowledge']]",[],"[['Model', 'has', 'explicitly share']]",[],[],[],[],[],semantic_parsing,2,22
model,"Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .","[('knows', (9, 10)), ('use it', (24, 26)), ('as', (26, 27)), ('to improve', (29, 31)), ('of', (33, 34))]","[('generating', (3, 4)), ('sketch', (5, 6)), ('decoder', (8, 9)), ('basic meaning of the utterance', (12, 17)), ('model', (22, 23)), ('global context', (27, 29)), ('prediction', (32, 33)), ('final details', (35, 37))]","[['decoder', 'knows', 'basic meaning of the utterance'], ['sketch', 'use it', 'model'], ['model', 'as', 'global context'], ['global context', 'to improve', 'prediction'], ['prediction', 'of', 'final details']]","[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]",[],"[['Model', 'has', 'generating']]",[],[],[],[],[],semantic_parsing,2,23
code,Our implementation and pretrained models are available at https :// github.com/donglixp/coarse2fine.,[],[],[],[],[],[],[],[],[],[],[],semantic_parsing,2,230
hyperparameters,"Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .","[('of', (1, 2)), ('selected from', (8, 10))]","[('Dimensions', (0, 1)), ('hidden vectors and word embeddings', (2, 7)), ('{ 250 , 300 } and { 150 , 200 , 250 , 300 }', (10, 25))]","[['Dimensions', 'of', 'hidden vectors and word embeddings'], ['hidden vectors and word embeddings', 'selected from', '{ 250 , 300 } and { 150 , 200 , 250 , 300 }']]",[],[],"[['Hyperparameters', 'has', 'Dimensions']]",[],[],[],[],[],semantic_parsing,2,237
hyperparameters,"The dropout rate was selected from { 0.3 , 0.5 } .","[('selected from', (4, 6))]","[('dropout rate', (1, 3)), ('{ 0.3 , 0.5 }', (6, 11))]","[['dropout rate', 'selected from', '{ 0.3 , 0.5 }']]",[],[],"[['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],semantic_parsing,2,238
hyperparameters,Label smoothing was employed for GEO and ATIS .,"[('employed for', (3, 5))]","[('Label smoothing', (0, 2)), ('GEO', (5, 6)), ('ATIS', (7, 8))]","[['Label smoothing', 'employed for', 'GEO'], ['Label smoothing', 'employed for', 'ATIS']]",[],[],"[['Hyperparameters', 'has', 'Label smoothing']]",[],[],[],[],[],semantic_parsing,2,239
hyperparameters,The smoothing parameter was set to 0.1 .,"[('set to', (4, 6))]","[('smoothing parameter', (1, 3)), ('0.1', (6, 7))]","[['smoothing parameter', 'set to', '0.1']]",[],[],"[['Hyperparameters', 'has', 'smoothing parameter']]",[],[],[],[],[],semantic_parsing,2,240
hyperparameters,"Word embeddings were initialized by GloVe , and were shared by table encoder and input encoder in Section 4.3 .","[('initialized by', (3, 5)), ('shared by', (9, 11))]","[('Word embeddings', (0, 2)), ('GloVe', (5, 6)), ('table encoder', (11, 13)), ('input encoder', (14, 16))]","[['Word embeddings', 'shared by', 'table encoder'], ['Word embeddings', 'shared by', 'input encoder'], ['Word embeddings', 'initialized by', 'GloVe']]",[],[],"[['Hyperparameters', 'has', 'Word embeddings']]",[],[],[],[],[],semantic_parsing,2,242
hyperparameters,We appended 10 - dimensional part - of - speech tag vectors to embeddings of the question words in WIKISQL .,"[('appended', (1, 2)), ('to', (12, 13)), ('of', (14, 15)), ('in', (18, 19))]","[('10 - dimensional part - of - speech tag vectors', (2, 12)), ('embeddings', (13, 14)), ('question words', (16, 18)), ('WIKISQL', (19, 20))]","[['10 - dimensional part - of - speech tag vectors', 'to', 'embeddings'], ['embeddings', 'of', 'question words'], ['question words', 'in', 'WIKISQL']]",[],"[['Hyperparameters', 'appended', '10 - dimensional part - of - speech tag vectors']]",[],[],[],[],[],[],semantic_parsing,2,243
hyperparameters,The part - of - speech tags were obtained by the spaCy toolkit .,"[('obtained by', (8, 10))]","[('part - of - speech tags', (1, 7)), ('spaCy toolkit', (11, 13))]","[['part - of - speech tags', 'obtained by', 'spaCy toolkit']]",[],[],"[['Hyperparameters', 'has', 'part - of - speech tags']]",[],[],[],[],[],semantic_parsing,2,244
hyperparameters,We used the RMSProp optimizer to train the models .,"[('used', (1, 2)), ('to train', (5, 7))]","[('RMSProp optimizer', (3, 5)), ('models', (8, 9))]","[['RMSProp optimizer', 'to train', 'models']]",[],"[['Hyperparameters', 'used', 'RMSProp optimizer']]",[],[],[],[],[],[],semantic_parsing,2,245
hyperparameters,"The learning rate was selected from { 0.002 , 0.005 } .","[('selected from', (4, 6))]","[('learning rate', (1, 3)), ('{ 0.002 , 0.005 }', (6, 11))]","[['learning rate', 'selected from', '{ 0.002 , 0.005 }']]",[],[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],semantic_parsing,2,246
hyperparameters,"The batch size was 200 for WIKISQL , and was 64 for other datasets .",[],[],"[['batch size', 'was', '200'], ['200', 'for', 'WIKISQL'], ['batch size', 'was', '64'], ['64', 'for', 'other datasets']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],semantic_parsing,2,247
hyperparameters,Early stopping was used to determine the number of epochs .,"[('to determine', (4, 6))]","[('Early stopping', (0, 2)), ('number of epochs', (7, 10))]","[['Early stopping', 'to determine', 'number of epochs']]",[],[],"[['Hyperparameters', 'has', 'Early stopping']]",[],[],[],[],[],semantic_parsing,2,248
results,"Overall , we observe that COARSE2FINE outperforms ONESTAGE , which suggests that disentangling high - level from low - level information dur - 62.3 SNM + COPY 71 and .","[('observe', (3, 4)), ('outperforms', (6, 7)), ('suggests', (10, 11)), ('from', (16, 17))]","[('COARSE2FINE', (5, 6)), ('ONESTAGE', (7, 8)), ('disentangling', (12, 13)), ('high - level', (13, 16)), ('low - level information', (17, 21))]","[['COARSE2FINE', 'suggests', 'disentangling'], ['high - level', 'from', 'low - level information'], ['COARSE2FINE', 'outperforms', 'ONESTAGE']]","[['disentangling', 'has', 'high - level']]","[['Results', 'observe', 'COARSE2FINE']]",[],[],[],[],[],[],semantic_parsing,2,257
results,"Compared with previous neural models that utilize syntax or grammatical information ( SEQ2 TREE , ASN ; the second block in ) , our method performs competitively despite the use of relatively simple decoders .","[('performs', (25, 26)), ('despite the use of', (27, 31))]","[('our method', (23, 25)), ('competitively', (26, 27)), ('relatively simple decoders', (31, 34))]","[['our method', 'performs', 'competitively'], ['competitively', 'despite the use of', 'relatively simple decoders']]",[],[],"[['Results', 'has', 'our method']]",[],[],[],[],[],semantic_parsing,2,260
results,"As can be seen , predicting the sketch correctly boosts performance .","[('predicting', (5, 6)), ('boosts', (9, 10))]","[('sketch', (7, 8)), ('correctly', (8, 9)), ('performance', (10, 11))]","[['sketch', 'boosts', 'performance']]","[['sketch', 'has', 'correctly']]","[['Results', 'predicting', 'sketch']]",[],[],[],[],[],[],semantic_parsing,2,262
results,Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle .,"[('is', (7, 8)), ('there is', (11, 13)), ('in accuracy between', (17, 20))]","[('sketch encoder', (5, 7)), ('beneficial', (8, 9)), ('8.9 point difference', (14, 17)), ('COARSE2FINE and the oracle', (20, 24))]","[['sketch encoder', 'there is', '8.9 point difference'], ['8.9 point difference', 'in accuracy between', 'COARSE2FINE and the oracle'], ['sketch encoder', 'is', 'beneficial']]",[],[],"[['Results', 'observe', 'sketch encoder']]",[],[],[],[],[],semantic_parsing,2,267
results,Our model is superior to ONESTAGE as well as to previous best performing systems .,"[('superior to', (3, 5))]","[('Our model', (0, 2)), ('ONESTAGE', (5, 6)), ('previous best performing systems', (10, 14))]","[['Our model', 'superior to', 'ONESTAGE'], ['Our model', 'superior to', 'previous best performing systems']]",[],[],"[['Results', 'has', 'Our model']]",[],[],[],[],[],semantic_parsing,2,269
results,"COARSE2FINE 's accuracies on aggregation agg op and agg col are 90.2 % and 92.0 % , respectively , which is comparable to SQLNET .","[('on', (3, 4)), ('are', (10, 11)), ('comparable to', (21, 23))]","[(""COARSE2FINE 's accuracies"", (0, 3)), ('aggregation agg op and agg col', (4, 10)), ('90.2 % and 92.0 %', (11, 16)), ('SQLNET', (23, 24))]","[[""COARSE2FINE 's accuracies"", 'on', 'aggregation agg op and agg col'], ['aggregation agg op and agg col', 'are', '90.2 % and 92.0 %'], ['90.2 % and 92.0 %', 'comparable to', 'SQLNET']]",[],[],"[['Results', 'has', ""COARSE2FINE 's accuracies""]]",[],[],[],[],[],semantic_parsing,2,270
results,So the most gain is obtained by the improved decoder of the WHERE clause .,"[('obtained by', (5, 7)), ('of', (10, 11))]","[('most gain', (2, 4)), ('improved decoder', (8, 10)), ('WHERE clause', (12, 14))]","[['most gain', 'obtained by', 'improved decoder'], ['improved decoder', 'of', 'WHERE clause']]",[],[],"[['Results', 'has', 'most gain']]",[],[],[],[],[],semantic_parsing,2,271
results,"We also find that a tableaware input encoder is critical for doing well on this task , since the same question might lead to different SQL queries depending on the table schemas .","[('find', (2, 3)), ('is', (8, 9)), ('for', (10, 11)), ('on', (13, 14))]","[('tableaware input encoder', (5, 8)), ('critical', (9, 10)), ('doing well', (11, 13)), ('this task', (14, 16))]","[['tableaware input encoder', 'is', 'critical'], ['critical', 'for', 'doing well'], ['doing well', 'on', 'this task']]",[],"[['Results', 'find', 'tableaware input encoder']]",[],[],[],[],[],[],semantic_parsing,2,272
results,Sketches produced by COARSE2FINE are more accurate across the board .,"[('produced by', (1, 3)), ('are', (4, 5))]","[('Sketches', (0, 1)), ('COARSE2FINE', (3, 4)), ('more accurate', (5, 7))]","[['Sketches', 'produced by', 'COARSE2FINE'], ['COARSE2FINE', 'are', 'more accurate']]",[],[],"[['Results', 'has', 'Sketches']]",[],[],[],[],[],semantic_parsing,2,279
results,"On WIKISQL , the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE .","[('On', (0, 1)), ('predicted by', (5, 7)), ('are', (8, 9)), ('compared with', (11, 13))]","[('WIKISQL', (1, 2)), ('sketches', (4, 5)), ('COARSE2FINE', (7, 8)), ('marginally better', (9, 11)), ('ONESTAGE', (13, 14))]","[['sketches', 'are', 'marginally better'], ['marginally better', 'compared with', 'ONESTAGE'], ['sketches', 'predicted by', 'COARSE2FINE']]","[['WIKISQL', 'has', 'sketches']]","[['Results', 'On', 'WIKISQL']]",[],[],[],[],[],[],semantic_parsing,2,282
research-problem,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,[],"[('Neural Semantic Role Labeling', (6, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,0,2
research-problem,"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . """,[],"[('Semantic role labeling ( SRL )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic role labeling ( SRL )']]",[],[],[],[],semantic_role_labeling,0,10
research-problem,"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .",[],"[('SRL', (4, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],semantic_role_labeling,0,11
model,We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,"[('propose', (1, 2)), ('for predicting', (9, 11)), ('in', (18, 19))]","[('end - to - end approach', (3, 9)), ('all the predicates', (11, 14)), ('argument spans', (16, 18)), ('one forward pass', (19, 22))]","[['end - to - end approach', 'in', 'one forward pass'], ['one forward pass', 'for predicting', 'all the predicates'], ['one forward pass', 'for predicting', 'argument spans']]",[],"[['Model', 'propose', 'end - to - end approach']]",[],[],[],[],[],[],semantic_role_labeling,0,13
model,"Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .","[('builds on', (2, 4)), ('by making', (10, 12)), ('use of', (13, 15))]","[('recent coreference resolution model', (5, 9)), ('central', (12, 13)), ('learned , contextualized span representations', (15, 20))]","[['recent coreference resolution model', 'by making', 'central'], ['central', 'use of', 'learned , contextualized span representations']]",[],"[['Model', 'builds on', 'recent coreference resolution model']]",[],[],[],[],[],[],semantic_role_labeling,0,14
model,We use these representations to predict SRL graphs directly over text spans .,"[('use', (1, 2)), ('to predict', (4, 6)), ('directly over', (8, 10))]","[('representations', (3, 4)), ('SRL graphs', (6, 8)), ('text spans', (10, 12))]","[['representations', 'to predict', 'SRL graphs'], ['SRL graphs', 'directly over', 'text spans']]",[],"[['Model', 'use', 'representations']]",[],[],[],[],[],[],semantic_role_labeling,0,15
model,"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .","[('identified by', (3, 5)), ('which', (7, 8)), ('holds between', (13, 15))]","[('Each edge', (0, 2)), ('independently predicting', (5, 7)), ('role', (8, 9)), ('every possible pair of text spans', (15, 21)), ('https://github.com/luheng/lsgn', (31, 32))]","[['Each edge', 'identified by', 'independently predicting'], ['independently predicting', 'which', 'role'], ['role', 'holds between', 'every possible pair of text spans']]",[],[],"[['Model', 'has', 'Each edge']]","[['Contribution', 'Code', 'https://github.com/luheng/lsgn']]",[],[],[],[],semantic_role_labeling,0,16
model,The final graph is simply the union of predicted SRL roles ( edges ) and their associated text spans ( nodes ) .,"[('union of', (6, 8))]","[('final graph', (1, 3)), ('predicted SRL roles ( edges )', (8, 14)), ('associated text spans ( nodes )', (16, 22))]","[['final graph', 'union of', 'predicted SRL roles ( edges )'], ['final graph', 'union of', 'associated text spans ( nodes )']]",[],[],"[['Model', 'has', 'final graph']]",[],[],[],[],[],semantic_role_labeling,0,17
model,"The span representations also generalize the token - level representations in BIObased models , letting the model dynamically decide which spans and roles to include , without using previously standard syntactic features .","[('generalize', (4, 5)), ('in', (10, 11)), ('letting', (14, 15)), ('dynamically decide', (17, 19)), ('without using', (26, 28))]","[('span representations', (1, 3)), ('token - level representations', (6, 10)), ('BIObased models', (11, 13)), ('model', (16, 17)), ('which spans and roles to include', (19, 25)), ('previously standard syntactic features', (28, 32))]","[['span representations', 'generalize', 'token - level representations'], ['token - level representations', 'in', 'BIObased models'], ['token - level representations', 'letting', 'model'], ['model', 'dynamically decide', 'which spans and roles to include'], ['which spans and roles to include', 'without using', 'previously standard syntactic features']]",[],[],"[['Model', 'has', 'span representations']]",[],[],[],[],[],semantic_role_labeling,0,19
model,"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .","[('is', (8, 9)), ('that does not assume', (16, 20))]","[('first span - based SRL model', (10, 16)), ('predicates are given', (21, 24))]","[['first span - based SRL model', 'that does not assume', 'predicates are given']]",[],"[['Model', 'is', 'first span - based SRL model']]",[],[],[],[],[],[],semantic_role_labeling,0,20
results,"As shown in , 2 our joint model outperforms the previous best pipeline system by an F1 difference of anywhere between 1.3 and 6.0 in every setting .","[('outperforms', (8, 9)), ('by', (14, 15)), ('of', (18, 19))]","[('joint model', (6, 8)), ('previous best pipeline system', (10, 14)), ('F1 difference', (16, 18)), ('anywhere between 1.3 and 6.0', (19, 24))]","[['joint model', 'by', 'F1 difference'], ['F1 difference', 'of', 'anywhere between 1.3 and 6.0'], ['joint model', 'outperforms', 'previous best pipeline system']]",[],[],"[['Results', 'has', 'joint model']]",[],[],[],[],[],semantic_role_labeling,0,71
results,"On all datasets , our model is able to predict over 40 % of the sentences completely correctly .","[('On', (0, 1)), ('able to', (7, 9)), ('over', (10, 11)), ('of', (13, 14))]","[('all datasets', (1, 3)), ('our model', (4, 6)), ('predict', (9, 10)), ('40 %', (11, 13)), ('sentences', (15, 16)), ('completely correctly', (16, 18))]","[['our model', 'able to', 'predict'], ['predict', 'over', '40 %'], ['40 %', 'of', 'sentences']]","[['all datasets', 'has', 'our model'], ['40 %', 'has', 'completely correctly']]","[['Results', 'On', 'all datasets']]",[],[],[],[],[],[],semantic_role_labeling,0,73
research-problem,Linguistically - Informed Self - Attention for Semantic Role Labeling,[],"[('Semantic Role Labeling', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,1,2
research-problem,Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .,[],"[('semantic role labeling ( SRL )', (8, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic role labeling ( SRL )']]",[],[],[],[],semantic_role_labeling,1,4
research-problem,"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .",[],"[('SRL', (13, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],semantic_role_labeling,1,5
model,"In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .","[('propose', (4, 5)), ('trained to', (33, 35)), ('to', (58, 59))]","[('linguistically - informed self - attention ( LISA )', (5, 14)), ('jointly predict', (39, 41)), ('parts of speech and predicates', (41, 46)), ('perform parsing', (50, 52)), ('attend', (57, 58)), ('syntactic parse parents', (59, 62)), ('assigning', (67, 68)), ('semantic role labels', (68, 71))]","[['attend', 'to', 'syntactic parse parents']]","[['jointly predict', 'has', 'parts of speech and predicates'], ['assigning', 'has', 'semantic role labels']]","[['Model', 'trained to', 'jointly predict'], ['Model', 'trained to', 'perform parsing'], ['Model', 'trained to', 'attend'], ['Model', 'trained to', 'assigning'], ['Model', 'propose', 'linguistically - informed self - attention ( LISA )']]",[],[],[],[],[],[],semantic_role_labeling,1,21
model,"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .","[('trained to predict', (38, 41)), ('supplied to', (55, 57)), ('for', (59, 60))]","[('end - to - end', (29, 34)), ('earlier layers', (35, 37)), ('prerequisite parts - of - speech and predicates', (41, 49)), ('latter', (51, 52)), ('later layers', (57, 59)), ('scoring', (60, 61))]","[['earlier layers', 'trained to predict', 'prerequisite parts - of - speech and predicates'], ['latter', 'supplied to', 'later layers'], ['later layers', 'for', 'scoring']]","[['end - to - end', 'has', 'earlier layers'], ['end - to - end', 'has', 'latter']]",[],"[['Model', 'has', 'end - to - end']]",[],[],[],[],[],semantic_role_labeling,1,22
model,"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .","[('predict', (7, 8)), ('encode', (25, 26)), ('then predict', (46, 48))]","[('each sentence', (4, 6)), ('only once', (28, 30)), ('predicates', (33, 34)), ('part - of - speech tags', (35, 41)), ('labeled syntactic parse', (42, 45)), ('semantic roles', (49, 51))]","[['each sentence', 'predict', 'predicates'], ['predicates', 'then predict', 'semantic roles'], ['each sentence', 'predict', 'part - of - speech tags'], ['each sentence', 'predict', 'labeled syntactic parse']]","[['each sentence', 'has', 'only once']]","[['Model', 'encode', 'each sentence']]",[],[],[],[],[],[],semantic_role_labeling,1,23
hyperparameters,"We train the model using Nadam ( Dozat , 2016 ) SGD combined with the learning rate schedule in .","[('train', (1, 2)), ('using', (4, 5))]","[('model', (3, 4)), ('Nadam ( Dozat , 2016 ) SGD', (5, 12))]","[['model', 'using', 'Nadam ( Dozat , 2016 ) SGD']]",[],"[['Hyperparameters', 'train', 'model']]",[],[],[],[],[],[],semantic_role_labeling,1,128
hyperparameters,"In addition to MTL , we regularize our model using dropout .","[('regularize', (6, 7)), ('using', (9, 10))]","[('our model', (7, 9)), ('dropout', (10, 11))]","[['our model', 'using', 'dropout']]",[],"[['Hyperparameters', 'regularize', 'our model']]",[],[],[],[],[],[],semantic_role_labeling,1,129
hyperparameters,We use gradient clipping to avoid exploding gradients .,"[('use', (1, 2)), ('to avoid', (4, 6))]","[('gradient clipping', (2, 4)), ('exploding gradients', (6, 8))]","[['gradient clipping', 'to avoid', 'exploding gradients']]",[],"[['Hyperparameters', 'use', 'gradient clipping']]",[],[],[],[],[],[],semantic_role_labeling,1,130
results,"We present results on the CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0 , achieving state - of - the - art results for a single model with predicted predicates on both corpora .",[],[],"[['CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0', 'achieving', 'state - of - the - art results'], ['state - of - the - art results', 'for', 'single model'], ['single model', 'with', 'predicted predicates'], ['single model', 'on', 'both corpora']]",[],"[['Results', 'on', 'CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0']]",[],[],[],[],[],[],semantic_role_labeling,1,151
results,"We demonstrate that our models benefit from injecting state - of - the - art predicted parses at test time ( + D&M ) by fixing the attention to parses predicted by Dozat and Manning ( 2017 ) , the winner of the 2017 CoNLL shared task which we re-train using ELMo embeddings .","[('demonstrate that', (1, 3)), ('benefit from', (5, 7)), ('at', (17, 18))]","[('our models', (3, 5)), ('injecting', (7, 8)), ('state - of - the - art predicted parses', (8, 17)), ('test time', (18, 20))]","[['our models', 'benefit from', 'injecting'], ['injecting', 'at', 'test time']]","[['injecting', 'has', 'state - of - the - art predicted parses']]","[['Results', 'demonstrate that', 'our models']]",[],[],[],[],[],[],semantic_role_labeling,1,159
results,"For models using GloVe embeddings , our syntax - free SA model already achieves a new state - of - the - art by jointly predicting predicates , POS and SRL .","[('For', (0, 1)), ('using', (2, 3)), ('achieves', (13, 14)), ('by jointly predicting', (23, 26))]","[('models', (1, 2)), ('GloVe embeddings', (3, 5)), ('our syntax - free SA model', (6, 12)), ('new state - of - the - art', (15, 23)), ('predicates', (26, 27)), ('POS', (28, 29)), ('SRL', (30, 31))]","[['models', 'using', 'GloVe embeddings'], ['our syntax - free SA model', 'achieves', 'new state - of - the - art'], ['new state - of - the - art', 'by jointly predicting', 'predicates'], ['new state - of - the - art', 'by jointly predicting', 'POS'], ['new state - of - the - art', 'by jointly predicting', 'SRL']]","[['models', 'has', 'our syntax - free SA model']]","[['Results', 'For', 'models']]",[],[],[],[],[],[],semantic_role_labeling,1,165
results,"LISA with it s own parses performs comparably to SA , but when supplied with D&M parses LISA out - performs the previous state - of - the - art by 2.5 F1 points .","[('with', (1, 2)), ('performs', (6, 7)), ('to', (8, 9)), ('when supplied with', (12, 15)), ('out - performs', (18, 21)), ('by', (30, 31))]","[('LISA', (0, 1)), ('own parses', (4, 6)), ('comparably', (7, 8)), ('SA', (9, 10)), ('D&M parses', (15, 17)), ('previous state - of - the - art', (22, 30)), ('2.5 F1 points', (31, 34))]","[['LISA', 'with', 'own parses'], ['own parses', 'performs', 'comparably'], ['comparably', 'to', 'SA'], ['LISA', 'when supplied with', 'D&M parses'], ['D&M parses', 'out - performs', 'previous state - of - the - art'], ['previous state - of - the - art', 'by', '2.5 F1 points']]",[],[],"[['Results', 'has', 'LISA']]",[],[],[],[],[],semantic_role_labeling,1,166
results,"On the out - ofdomain Brown test set , LISA also performs comparably to its syntax - free counterpart with its own parses , but with D&M parses LISA performs exceptionally well , more than 3.5 F1 points higher than He et al ..",[],[],"[['LISA', 'performs', 'comparably'], ['comparably', 'with', 'its own parses'], ['comparably', 'to', 'syntax - free counterpart'], ['LISA', 'with', 'D&M parses'], ['D&M parses', 'performs', 'exceptionally well']]","[['out - ofdomain Brown test set', 'has', 'LISA']]","[['Results', 'On', 'out - ofdomain Brown test set']]",[],[],[],[],[],[],semantic_role_labeling,1,167
results,Incorporating ELMo em-beddings improves all scores .,"[('Incorporating', (0, 1)), ('improves', (3, 4))]","[('ELMo em-beddings', (1, 3)), ('all scores', (4, 6))]","[['ELMo em-beddings', 'improves', 'all scores']]",[],"[['Results', 'Incorporating', 'ELMo em-beddings']]",[],[],[],[],[],[],semantic_role_labeling,1,168
research-problem,Deep Semantic Role Labeling : What Works and What 's Next,[],"[('Deep Semantic Role Labeling', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Deep Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,2,2
research-problem,"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations .",[],"[('semantic role labeling ( SRL )', (8, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic role labeling ( SRL )']]",[],[],[],[],semantic_role_labeling,2,4
research-problem,Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .,[],"[('SRL', (11, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],semantic_role_labeling,2,10
model,"In this paper , we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding , again significantly moving the state of the art ( another 2 points on CoNLL 2005 ) .","[('using', (13, 14)), ('with', (18, 19))]","[('deep highway bidirectional LSTMs', (14, 18)), ('constrained decoding', (19, 21))]","[['deep highway bidirectional LSTMs', 'with', 'constrained decoding']]",[],"[['Model', 'using', 'deep highway bidirectional LSTMs']]",[],[],[],[],[],[],semantic_role_labeling,2,11
model,"Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .","[('treat', (5, 6)), ('as', (7, 8)), ('use', (13, 14))]","[('SRL', (6, 7)), ('BIO tagging problem', (9, 12)), ('deep bidirectional LSTMs', (14, 17))]","[['SRL', 'as', 'BIO tagging problem']]",[],"[['Model', 'treat', 'SRL'], ['Model', 'use', 'deep bidirectional LSTMs']]",[],[],[],[],[],[],semantic_role_labeling,2,14
model,"However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .",[],[],"[['ensembling', 'with', 'product of experts'], ['decoding', 'with', 'BIOconstraints']]","[['using', 'has', 'recurrent dropout'], ['introducing', 'has', 'highway connections'], ['simplifying', 'has', 'input and output layers']]",[],[],[],"[['deep bidirectional LSTMs', 'differ by', 'using'], ['deep bidirectional LSTMs', 'differ by', 'ensembling'], ['deep bidirectional LSTMs', 'differ by', 'introducing'], ['deep bidirectional LSTMs', 'differ by', 'decoding'], ['deep bidirectional LSTMs', 'differ by', 'simplifying']]",[],[],[],semantic_role_labeling,2,15
hyperparameters,"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a softmax layer for predicting the output distribution .","[('consists of', (2, 4)), ('with', (16, 17)), ('for predicting', (26, 28))]","[('network', (1, 2)), ('8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )', (4, 16)), ('300 dimensional hidden units', (17, 21)), ('softmax layer', (24, 26)), ('output distribution', (29, 31))]","[['network', 'consists of', '8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )'], ['8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )', 'with', '300 dimensional hidden units'], ['network', 'consists of', 'softmax layer'], ['softmax layer', 'for predicting', 'output distribution']]",[],[],"[['Hyperparameters', 'has', 'network']]",[],[],[],[],[],semantic_role_labeling,2,87
hyperparameters,"All the weight matrices in BiL - STMs are initialized with random orthonormal matrices as described in. ,","[('in', (4, 5)), ('initialized with', (9, 11))]","[('weight matrices', (2, 4)), ('BiL - STMs', (5, 8)), ('random orthonormal matrices', (11, 14))]","[['weight matrices', 'in', 'BiL - STMs'], ['BiL - STMs', 'initialized with', 'random orthonormal matrices']]",[],[],"[['Hyperparameters', 'has', 'weight matrices']]",[],[],[],[],[],semantic_role_labeling,2,89
hyperparameters,All tokens are lower - cased and initialized with 100 - dimensional GloVe embeddings pre-trained on 6B tokens and updated during training .,"[('are', (2, 3)), ('initialized with', (7, 9)), ('pre-trained on', (14, 16)), ('updated during', (19, 21))]","[('tokens', (1, 2)), ('lower - cased', (3, 6)), ('100 - dimensional GloVe embeddings', (9, 14)), ('6B tokens', (16, 18)), ('training', (21, 22))]","[['tokens', 'are', 'lower - cased'], ['tokens', 'initialized with', '100 - dimensional GloVe embeddings'], ['100 - dimensional GloVe embeddings', 'pre-trained on', '6B tokens'], ['100 - dimensional GloVe embeddings', 'updated during', 'training']]",[],[],"[['Hyperparameters', 'has', 'tokens']]",[],[],[],[],[],semantic_role_labeling,2,91
hyperparameters,Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding .,"[('not covered by', (3, 6)), ('replaced with', (8, 10))]","[('Tokens', (0, 1)), ('GloVe', (6, 7)), ('randomly initialized UNK embedding', (11, 15))]","[['Tokens', 'not covered by', 'GloVe'], ['GloVe', 'replaced with', 'randomly initialized UNK embedding']]",[],[],"[['Hyperparameters', 'has', 'Tokens']]",[],[],[],[],[],semantic_role_labeling,2,92
hyperparameters,"Training We use Adadelta ( Zeiler , 2012 ) with = 1e ?6 and ? = 0.95 and mini-batches of size 80 .","[('use', (2, 3)), ('with', (9, 10)), ('size', (20, 21))]","[('Adadelta', (3, 4)), ('1e ?6 and ? = 0.95', (11, 17)), ('mini-batches', (18, 19)), ('80', (21, 22))]","[['Adadelta', 'with', '1e ?6 and ? = 0.95'], ['mini-batches', 'size', '80']]",[],"[['Hyperparameters', 'use', 'Adadelta'], ['Hyperparameters', 'use', 'mini-batches']]",[],[],[],[],[],[],semantic_role_labeling,2,93
hyperparameters,We set RNN - dropout probability to 0.1 and clip gradients with norm larger than 1 .,"[('set', (1, 2)), ('to', (6, 7)), ('clip', (9, 10)), ('with', (11, 12)), ('larger than', (13, 15))]","[('RNN - dropout probability', (2, 6)), ('0.1', (7, 8)), ('gradients', (10, 11)), ('norm', (12, 13)), ('1', (15, 16))]","[['RNN - dropout probability', 'to', '0.1'], ['gradients', 'with', 'norm'], ['norm', 'larger than', '1']]",[],"[['Hyperparameters', 'set', 'RNN - dropout probability'], ['Hyperparameters', 'clip', 'gradients']]",[],[],[],[],[],[],semantic_role_labeling,2,94
hyperparameters,All the models are trained for 500 epochs with early stopping based on development results .,"[('trained for', (4, 6)), ('with', (8, 9)), ('based on', (11, 13))]","[('models', (2, 3)), ('500 epochs', (6, 8)), ('early stopping', (9, 11)), ('development results', (13, 15))]","[['models', 'trained for', '500 epochs'], ['500 epochs', 'with', 'early stopping'], ['early stopping', 'based on', 'development results']]",[],[],"[['Hyperparameters', 'has', 'models']]",[],[],[],[],[],semantic_role_labeling,2,95
results,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,"[('of', (9, 10)), ('on', (12, 13)), ('over', (19, 20))]","[('Our ensemble ( PoE )', (0, 5)), ('an absolute improvement', (6, 9)), ('2.1 F1', (10, 12)), ('CoNLL 2005', (14, 16)), ('CoNLL 2012', (17, 19)), ('previous state of the art', (21, 26))]","[['an absolute improvement', 'over', 'previous state of the art'], ['previous state of the art', 'of', '2.1 F1'], ['previous state of the art', 'on', 'CoNLL 2005'], ['previous state of the art', 'on', 'CoNLL 2012']]","[['Our ensemble ( PoE )', 'has', 'an absolute improvement']]",[],"[['Results', 'has', 'Our ensemble ( PoE )']]",[],[],[],[],[],semantic_role_labeling,2,103
results,Our single model also achieves more than a 0.4 improvement on both datasets .,"[('achieves', (4, 5)), ('on', (10, 11))]","[('Our single model', (0, 3)), ('more than a 0.4 improvement', (5, 10)), ('both datasets', (11, 13))]","[['Our single model', 'achieves', 'more than a 0.4 improvement'], ['more than a 0.4 improvement', 'on', 'both datasets']]",[],[],"[['Results', 'has', 'Our single model']]",[],[],[],[],[],semantic_role_labeling,2,104
results,"In comparison with the best reported results , our percentage of completely correct predicates improves by 5.9 points .","[('In comparison with', (0, 3)), ('improves by', (14, 16))]","[('best reported results', (4, 7)), ('our percentage of completely correct predicates', (8, 14)), ('5.9 points', (16, 18))]","[['our percentage of completely correct predicates', 'improves by', '5.9 points']]","[['best reported results', 'has', 'our percentage of completely correct predicates']]","[['Results', 'In comparison with', 'best reported results']]",[],[],[],[],[],[],semantic_role_labeling,2,105
results,"While the continuing trend of improving SRL without syntax seems to suggest that neural end - to - end systems no longer needs parsers , our analysis in Section 4.4 will show that accurate syntactic information can improve these deep models .","[('show', (31, 32)), ('improve', (37, 38))]","[('accurate syntactic information', (33, 36)), ('deep models', (39, 41))]","[['accurate syntactic information', 'improve', 'deep models']]",[],"[['Results', 'show', 'accurate syntactic information']]",[],[],[],[],[],[],semantic_role_labeling,2,106
ablation-analysis,"Without dropout , the model overfits at around 300 epochs at 78 F1 .","[('overfits', (5, 6)), ('at', (6, 7))]","[('Without dropout', (0, 2)), ('model', (4, 5)), ('around 300 epochs', (7, 10)), ('78 F1', (11, 13))]","[['model', 'overfits', 'around 300 epochs'], ['around 300 epochs', 'at', '78 F1']]","[['Without dropout', 'has', 'model']]",[],"[['Ablation analysis', 'has', 'Without dropout']]",[],[],[],[],[],semantic_role_labeling,2,108
ablation-analysis,"Orthonormal parameter initialization is surprisingly important - without this , the model achieves only 65 F1 within the first 50 epochs .","[('is', (3, 4)), ('without this', (7, 9)), ('achieves', (12, 13)), ('within', (16, 17))]","[('Orthonormal parameter initialization', (0, 3)), ('surprisingly important', (4, 6)), ('model', (11, 12)), ('only 65 F1', (13, 16)), ('first 50 epochs', (18, 21))]","[['Orthonormal parameter initialization', 'is', 'surprisingly important'], ['Orthonormal parameter initialization', 'without this', 'model'], ['model', 'achieves', 'only 65 F1'], ['only 65 F1', 'within', 'first 50 epochs']]",[],[],"[['Ablation analysis', 'has', 'Orthonormal parameter initialization']]",[],[],[],[],[],semantic_role_labeling,2,109
ablation-analysis,All 8 layer ablations suffer a loss of more than 1.7 in absolute F 1 compared to the full model .,"[('suffer', (4, 5)), ('more than', (8, 10)), ('in', (11, 12)), ('compared to', (15, 17))]","[('8 layer ablations', (1, 4)), ('loss', (6, 7)), ('1.7', (10, 11)), ('absolute F 1', (12, 15)), ('full model', (18, 20))]","[['8 layer ablations', 'suffer', 'loss'], ['loss', 'more than', '1.7'], ['loss', 'in', 'absolute F 1'], ['loss', 'compared to', 'full model']]",[],[],"[['Ablation analysis', 'has', '8 layer ablations']]",[],[],[],[],[],semantic_role_labeling,2,110
research-problem,Deep Semantic Role Labeling with Self - Attention,[],"[('Deep Semantic Role Labeling', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Deep Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,3,2
research-problem,Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,[],"[('Semantic Role Labeling ( SRL )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Role Labeling ( SRL )']]",[],[],[],[],semantic_role_labeling,3,4
research-problem,"In this paper , we present a simple and effective architecture for SRL which aims to address these problems .",[],"[('SRL', (12, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],semantic_role_labeling,3,7
research-problem,"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" .",[],"[('Semantic Role Labeling', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,3,12
model,"To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .","[('present', (7, 8)), ('for', (16, 17))]","[('deep attentional neural network ( DEEPATT )', (9, 16)), ('SRL', (20, 21))]","[['deep attentional neural network ( DEEPATT )', 'for', 'SRL']]",[],"[['Model', 'present', 'deep attentional neural network ( DEEPATT )']]",[],[],[],[],[],[],semantic_role_labeling,3,29
model,Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .,"[('rely on', (2, 4)), ('directly draws', (10, 12)), ('of', (15, 16))]","[('self - attention mechanism', (5, 9)), ('global dependencies', (13, 15)), ('inputs', (17, 18))]","[['self - attention mechanism', 'directly draws', 'global dependencies'], ['global dependencies', 'of', 'inputs']]",[],"[['Model', 'rely on', 'self - attention mechanism']]",[],[],[],[],[],[],semantic_role_labeling,3,30
model,"In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .","[('major advantage of', (6, 9)), ('conducts', (15, 16)), ('between', (18, 19)), ('in', (22, 23))]","[('self - attention', (9, 12)), ('direct connections', (16, 18)), ('two arbitrary tokens', (19, 22)), ('sentence', (24, 25))]","[['self - attention', 'conducts', 'direct connections'], ['direct connections', 'between', 'two arbitrary tokens'], ['two arbitrary tokens', 'in', 'sentence']]",[],"[['Model', 'major advantage of', 'self - attention']]",[],[],[],[],[],[],semantic_role_labeling,3,31
model,"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .","[('can interact with', (4, 7)), ('by', (9, 10)), ('allows', (25, 26)), ('through', (29, 30))]","[('distant elements', (2, 4)), ('each other', (7, 9)), ('shorter paths', (10, 12)), ('unimpeded information flow', (26, 29)), ('network', (31, 32))]","[['distant elements', 'can interact with', 'each other'], ['each other', 'allows', 'unimpeded information flow'], ['unimpeded information flow', 'through', 'network'], ['each other', 'by', 'shorter paths']]",[],[],[],[],[],[],"[['self - attention', 'has', 'distant elements']]",[],semantic_role_labeling,3,32
model,"Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .","[('provides', (4, 5)), ('to select , represent and synthesize', (9, 15)), ('of', (17, 18)), ('complementary to', (22, 24))]","[('more flexible way', (6, 9)), ('information', (16, 17)), ('inputs', (19, 20)), ('RNN based models', (24, 27))]","[['more flexible way', 'to select , represent and synthesize', 'information'], ['information', 'of', 'inputs'], ['information', 'complementary to', 'RNN based models']]",[],[],[],[],"[['self - attention', 'provides', 'more flexible way']]",[],[],[],semantic_role_labeling,3,33
model,"Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .","[('Along with', (0, 2)), ('comes with', (9, 11)), ('uses', (14, 15)), ('to further enhance', (33, 36))]","[('self - attention', (2, 5)), ('DEEP - ATT', (6, 9)), ('three variants', (11, 13)), ('recurrent ( RNN )', (15, 19)), ('convolutional ( CNN )', (20, 24)), ('feed - forward ( FFN ) neural network', (25, 33)), ('representations', (37, 38))]","[['DEEP - ATT', 'comes with', 'three variants'], ['three variants', 'to further enhance', 'representations'], ['representations', 'uses', 'recurrent ( RNN )'], ['representations', 'uses', 'convolutional ( CNN )'], ['representations', 'uses', 'feed - forward ( FFN ) neural network']]","[['self - attention', 'has', 'DEEP - ATT']]","[['Model', 'Along with', 'self - attention']]",[],[],[],[],[],[],semantic_role_labeling,3,34
hyperparameters,We initialize the weights of all sub-layers as random orthogonal matrices .,"[('initialize', (1, 2)), ('of', (4, 5)), ('as', (7, 8))]","[('weights', (3, 4)), ('all sub-layers', (5, 7)), ('random orthogonal matrices', (8, 11))]","[['weights', 'of', 'all sub-layers'], ['all sub-layers', 'as', 'random orthogonal matrices']]",[],"[['Hyperparameters', 'initialize', 'weights']]",[],[],[],[],[],[],semantic_role_labeling,3,157
hyperparameters,"For other parameters , we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 ? d .","[('by sampling', (7, 9)), ('from', (11, 12)), ('with', (15, 16))]","[('other parameters', (1, 3)), ('each element', (9, 11)), ('Gaussian distribution', (13, 15)), ('mean 0 and variance 1 ? d', (16, 23))]","[['other parameters', 'by sampling', 'each element'], ['each element', 'from', 'Gaussian distribution'], ['Gaussian distribution', 'with', 'mean 0 and variance 1 ? d']]",[],[],"[['Hyperparameters', 'initialize', 'other parameters']]",[],[],[],[],[],semantic_role_labeling,3,158
hyperparameters,The embedding layer can be initialized randomly or using pre-trained word embeddings .,"[('can be', (3, 5))]","[('embedding layer', (1, 3)), ('initialized randomly', (5, 7)), ('pre-trained word embeddings', (9, 12))]","[['embedding layer', 'can be', 'initialized randomly'], ['embedding layer', 'can be', 'pre-trained word embeddings']]",[],[],"[['Hyperparameters', 'has', 'embedding layer']]",[],[],[],[],[],semantic_role_labeling,3,159
hyperparameters,The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10 .,[],[],"[['number of hidden layers', 'set to', '10'], ['dimension', 'of', 'word embeddings and predicate mask embeddings'], ['word embeddings and predicate mask embeddings', 'set to', '100']]",[],[],"[['Hyperparameters', 'has', 'number of hidden layers'], ['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],semantic_role_labeling,3,164
hyperparameters,We set the number of hidden units d to 200 .,"[('set', (1, 2)), ('to', (8, 9))]","[('number of hidden units d', (3, 8)), ('200', (9, 10))]","[['number of hidden units d', 'to', '200']]",[],"[['Hyperparameters', 'set', 'number of hidden units d']]",[],[],[],[],[],[],semantic_role_labeling,3,165
hyperparameters,The number of heads h is set to 8 .,"[('set to', (6, 8))]","[('number of heads h', (1, 5)), ('8', (8, 9))]","[['number of heads h', 'set to', '8']]",[],[],"[['Hyperparameters', 'has', 'number of heads h']]",[],[],[],[],[],semantic_role_labeling,3,166
hyperparameters,Dropout layers are added before residual connections with a keep probability of 0.8 .,"[('added before', (3, 5)), ('with', (7, 8)), ('of', (11, 12))]","[('Dropout layers', (0, 2)), ('residual connections', (5, 7)), ('keep probability', (9, 11)), ('0.8', (12, 13))]","[['Dropout layers', 'added before', 'residual connections'], ['residual connections', 'with', 'keep probability'], ['keep probability', 'of', '0.8']]",[],[],"[['Hyperparameters', 'has', 'Dropout layers']]",[],[],[],[],[],semantic_role_labeling,3,168
hyperparameters,"Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .","[('applied before', (3, 5)), ('set to', (23, 25))]","[('Dropout', (0, 1)), ('attention softmax layer', (6, 9)), ('feed - froward ReLU hidden layer', (11, 17)), ('keep probabilities', (20, 22)), ('0.9', (25, 26))]","[['Dropout', 'applied before', 'attention softmax layer'], ['Dropout', 'applied before', 'feed - froward ReLU hidden layer'], ['Dropout', 'applied before', 'keep probabilities'], ['keep probabilities', 'set to', '0.9']]",[],[],"[['Hyperparameters', 'has', 'Dropout']]",[],[],[],[],[],semantic_role_labeling,3,169
hyperparameters,We also employ label smoothing technique with a smoothing value of 0.1 during training .,"[('employ', (2, 3)), ('with', (6, 7)), ('of', (10, 11)), ('during', (12, 13))]","[('label smoothing technique', (3, 6)), ('smoothing value', (8, 10)), ('0.1', (11, 12)), ('training', (13, 14))]","[['label smoothing technique', 'with', 'smoothing value'], ['smoothing value', 'of', '0.1'], ['0.1', 'during', 'training']]",[],"[['Hyperparameters', 'employ', 'label smoothing technique']]",[],[],[],[],[],[],semantic_role_labeling,3,170
hyperparameters,Learning Parameter optimization is performed using stochastic gradient descent .,"[('performed using', (4, 6))]","[('Learning Parameter optimization', (0, 3)), ('stochastic gradient descent', (6, 9))]","[['Learning Parameter optimization', 'performed using', 'stochastic gradient descent']]",[],[],"[['Hyperparameters', 'has', 'Learning Parameter optimization']]",[],[],[],[],[],semantic_role_labeling,3,171
hyperparameters,We adopt Adadelta ) ( = 10 6 and ? = 0.95 ) as the optimizer .,"[('adopt', (1, 2)), ('as', (13, 14))]","[('Adadelta', (2, 3)), ('optimizer', (15, 16))]","[['Adadelta', 'as', 'optimizer']]",[],"[['Hyperparameters', 'adopt', 'Adadelta']]",[],[],[],[],[],[],semantic_role_labeling,3,172
hyperparameters,"To avoid exploding gradients problem , we clip the norm of gradients with a predefined threshold 1.0 .","[('To avoid', (0, 2)), ('clip', (7, 8)), ('of', (10, 11)), ('with', (12, 13))]","[('exploding gradients problem', (2, 5)), ('norm', (9, 10)), ('gradients', (11, 12)), ('predefined threshold 1.0', (14, 17))]","[['norm', 'of', 'gradients'], ['gradients', 'with', 'predefined threshold 1.0'], ['norm', 'To avoid', 'exploding gradients problem']]",[],"[['Hyperparameters', 'clip', 'norm']]",[],[],[],[],[],[],semantic_role_labeling,3,173
hyperparameters,Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL - 2005 dataset and 8192 tokens for the CoNLL - 2012 dataset .,[],[],"[['SGD', 'contains', 'mini-batch'], ['mini-batch', 'of approximately', '4096 tokens'], ['4096 tokens', 'for', 'CoNLL - 2005 dataset'], ['mini-batch', 'of approximately', '8192 tokens'], ['8192 tokens', 'for', 'CoNLL - 2012 dataset']]",[],[],"[['Hyperparameters', 'has', 'SGD']]",[],[],[],[],[],semantic_role_labeling,3,174
hyperparameters,The learning rate is initialized to 1.0 .,"[('initialized to', (4, 6))]","[('learning rate', (1, 3)), ('1.0', (6, 7))]","[['learning rate', 'initialized to', '1.0']]",[],[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],semantic_role_labeling,3,175
hyperparameters,"After training 400 k steps , we halve the learning rate every 100 K steps .","[('After training', (0, 2)), ('halve', (7, 8)), ('every', (11, 12))]","[('400 k steps', (2, 5)), ('learning rate', (9, 11)), ('100 K steps', (12, 15))]","[['400 k steps', 'halve', 'learning rate'], ['learning rate', 'every', '100 K steps']]",[],"[['Hyperparameters', 'After training', '400 k steps']]",[],[],[],[],[],[],semantic_role_labeling,3,176
hyperparameters,We train all models for 600 K steps .,"[('train', (1, 2)), ('for', (4, 5))]","[('all models', (2, 4)), ('600 K steps', (5, 8))]","[['all models', 'for', '600 K steps']]",[],"[['Hyperparameters', 'train', 'all models']]",[],[],[],[],[],[],semantic_role_labeling,3,177
hyperparameters,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .","[('For', (0, 1)), ('with', (4, 5)), ('takes', (14, 15)), ('to', (18, 19)), ('on', (20, 21)), ('which is', (27, 29))]","[('DEEP - ATT', (1, 4)), ('FFN sub - layers', (5, 9)), ('whole training stage', (11, 14)), ('two days', (16, 18)), ('finish', (19, 20)), ('single Titan X GPU', (22, 26)), ('2.5 times faster', (29, 32))]","[['DEEP - ATT', 'with', 'FFN sub - layers'], ['whole training stage', 'takes', 'two days'], ['two days', 'to', 'finish'], ['finish', 'on', 'single Titan X GPU'], ['two days', 'which is', '2.5 times faster']]","[['DEEP - ATT', 'has', 'whole training stage']]","[['Hyperparameters', 'For', 'DEEP - ATT']]",[],[],[],[],[],[],semantic_role_labeling,3,178
results,"In Remarkably , we get 74.1 F 1 score on the out - of - domain dataset , which outperforms the previous state - of - the - art system by 2.0 F 1 score .","[('get', (4, 5)), ('on', (9, 10))]","[('74.1 F 1 score', (5, 9)), ('out - of - domain dataset', (11, 17))]","[['74.1 F 1 score', 'on', 'out - of - domain dataset']]",[],"[['Results', 'get', '74.1 F 1 score']]",[],[],[],[],[],[],semantic_role_labeling,3,180
results,"When ensembling 5 models with FFN nonlinear sub - layers , our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively , which has an absolute improvement of 1.4 and 0.5 over the previous state - of - the - art .",[],[],"[['5 models', 'with', 'FFN nonlinear sub - layers'], ['our approach', 'achieves', 'F 1 score'], ['F 1 score', 'of', '84.6 and 83.9'], ['absolute improvement', 'over', 'previous state - of - the - art'], ['absolute improvement', 'of', '1.4 and 0.5']]","[['FFN nonlinear sub - layers', 'has', 'our approach'], ['our approach', 'has', 'absolute improvement']]","[['Results', 'When ensembling', '5 models']]",[],[],[],[],[],[],semantic_role_labeling,3,182
results,These results are consistent with our intuition that the self - attention layers is helpful to capture structural information and long distance dependencies .,"[('that', (7, 8)), ('helpful to capture', (14, 17))]","[('results', (1, 2)), ('self - attention layers', (9, 13)), ('structural information', (17, 19)), ('long distance dependencies', (20, 23))]","[['results', 'that', 'self - attention layers'], ['self - attention layers', 'helpful to capture', 'structural information'], ['self - attention layers', 'helpful to capture', 'long distance dependencies']]",[],[],"[['Results', 'has', 'results']]",[],[],[],[],[],semantic_role_labeling,3,183
research-problem,A Span Selection Model for Semantic Role Labeling,[],"[('Semantic Role Labeling', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Semantic Role Labeling']]",[],[],[],[],semantic_role_labeling,4,2
research-problem,We present a simple and accurate span - based model for semantic role labeling ( SRL ) .,[],"[('semantic role labeling ( SRL )', (11, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'semantic role labeling ( SRL )']]",[],[],[],[],semantic_role_labeling,4,4
research-problem,"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .",[],"[('SRL', (8, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'SRL']]",[],[],[],[],semantic_role_labeling,4,11
model,"To fill this gap , this paper presents a simple and accurate span - based model .","[('presents', (7, 8))]","[('simple and accurate span - based model', (9, 16))]",[],[],"[['Model', 'presents', 'simple and accurate span - based model']]",[],[],[],[],[],[],semantic_role_labeling,4,23
model,"Inspired by recent span - based models in syntactic parsing and coreference resolution , our model directly scores all possible labeled spans based on span representations induced from neural networks .","[('directly scores', (16, 18)), ('based on', (22, 24)), ('induced from', (26, 28))]","[('all possible labeled spans', (18, 22)), ('span representations', (24, 26)), ('neural networks', (28, 30))]","[['all possible labeled spans', 'based on', 'span representations'], ['span representations', 'induced from', 'neural networks']]",[],"[['Model', 'directly scores', 'all possible labeled spans']]",[],[],[],[],[],[],semantic_role_labeling,4,24
model,"At decoding time , we greedily select higher scoring labeled spans .","[('At', (0, 1)), ('greedily select', (5, 7))]","[('decoding time', (1, 3)), ('higher scoring labeled spans', (7, 11))]","[['decoding time', 'greedily select', 'higher scoring labeled spans']]",[],"[['Model', 'At', 'decoding time']]",[],[],[],[],[],[],semantic_role_labeling,4,25
model,The model parameters are learned by optimizing loglikelihood of correct labeled spans .,"[('learned by', (4, 6)), ('of', (8, 9))]","[('model parameters', (1, 3)), ('optimizing loglikelihood', (6, 8)), ('correct labeled spans', (9, 12))]","[['model parameters', 'learned by', 'optimizing loglikelihood'], ['optimizing loglikelihood', 'of', 'correct labeled spans']]",[],[],"[['Model', 'has', 'model parameters']]",[],[],[],[],[],semantic_role_labeling,4,26
baselines,"For comparison , as a model based on BIO tagging approaches , we use the BiLSTM - CRF model proposed by .","[('use', (13, 14))]","[('BiLSTM - CRF model', (15, 19))]",[],[],"[['Baselines', 'use', 'BiLSTM - CRF model']]",[],[],[],[],[],[],semantic_role_labeling,4,157
hyperparameters,"As the base function f base , we use 4 BiLSTM layers with 300 dimensional hidden units .","[('As', (0, 1)), ('use', (8, 9)), ('with', (12, 13))]","[('base function f base', (2, 6)), ('4 BiLSTM layers', (9, 12)), ('300 dimensional hidden units', (13, 17))]","[['base function f base', 'use', '4 BiLSTM layers'], ['4 BiLSTM layers', 'with', '300 dimensional hidden units']]",[],"[['Hyperparameters', 'As', 'base function f base']]",[],[],[],[],[],[],semantic_role_labeling,4,160
hyperparameters,"To optimize the model parameters , we use Adam .","[('optimize', (1, 2)), ('use', (7, 8))]","[('model parameters', (3, 5)), ('Adam', (8, 9))]","[['model parameters', 'use', 'Adam']]",[],"[['Hyperparameters', 'optimize', 'model parameters']]",[],[],[],[],[],[],semantic_role_labeling,4,161
hyperparameters,"To validate the model performance , we use two types of word embeddings .","[('To validate', (0, 2)), ('use', (7, 8))]","[('model performance', (3, 5)), ('two types of word embeddings', (8, 13))]","[['model performance', 'use', 'two types of word embeddings']]",[],"[['Hyperparameters', 'To validate', 'model performance']]",[],[],[],[],[],[],semantic_role_labeling,4,165
hyperparameters,"Typical word embeddings , SENNA 6 ( Collobert et al. , 2011 ) Contextualized word embeddings , ELMo 7 SENNA and ELMo can be regarded as different types of embeddings in terms of the context sensitivity .",[],"[('Typical word embeddings', (0, 3)), ('SENNA', (4, 5)), ('ELMo', (17, 18))]",[],"[['Typical word embeddings', 'has', 'SENNA'], ['Typical word embeddings', 'has', 'ELMo']]",[],"[['Hyperparameters', 'has', 'Typical word embeddings']]",[],[],[],[],[],semantic_role_labeling,4,166
hyperparameters,These embeddings are fixed during training .,"[('fixed during', (3, 5))]","[('embeddings', (1, 2)), ('training', (5, 6))]","[['embeddings', 'fixed during', 'training']]",[],[],"[['Hyperparameters', 'has', 'embeddings']]",[],[],[],[],[],semantic_role_labeling,4,171
hyperparameters,"As the objective function , we use the crossentropy L ? in Eq. 3 with L2 weight decay ,","[('use', (6, 7)), ('with', (14, 15))]","[('objective function', (2, 4)), ('crossentropy L', (8, 10)), ('L2 weight decay', (15, 18))]","[['objective function', 'use', 'crossentropy L'], ['crossentropy L', 'with', 'L2 weight decay']]",[],[],"[['Hyperparameters', 'As', 'objective function']]",[],[],[],[],[],semantic_role_labeling,4,173
results,We report averaged scores across five different runs of the model training .,"[('report', (1, 2)), ('across', (4, 5)), ('of', (8, 9))]","[('averaged scores', (2, 4)), ('five different runs', (5, 8)), ('model', (10, 11))]","[['averaged scores', 'across', 'five different runs'], ['five different runs', 'of', 'model']]",[],"[['Results', 'report', 'averaged scores']]",[],[],[],[],[],[],semantic_role_labeling,4,177
results,"Overall , our span - based ensemble model using ELMo achieved the best F1 scores , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and CoNLL - 2012 datasets , respectively .","[('using', (8, 9)), ('achieved', (10, 11)), ('on', (21, 22))]","[('span - based ensemble model', (3, 8)), ('ELMo', (9, 10)), ('best F1 scores', (12, 15)), ('87.4 F1 and 87.0 F1', (16, 21)), ('CoNLL - 2005 and CoNLL - 2012 datasets', (23, 31))]","[['span - based ensemble model', 'using', 'ELMo'], ['ELMo', 'achieved', 'best F1 scores'], ['best F1 scores', 'on', 'CoNLL - 2005 and CoNLL - 2012 datasets']]","[['best F1 scores', 'has', '87.4 F1 and 87.0 F1']]",[],"[['Results', 'has', 'span - based ensemble model']]",[],[],[],[],[],semantic_role_labeling,4,179
results,"In comparison with the CRF - based single model , our span - based single model consistently yielded better F 1 scores regardless of the word embeddings , SENNA and ELMO .","[('In comparison with', (0, 3)), ('consistently yielded', (16, 18)), ('regardless of', (22, 24))]","[('CRF - based single model', (4, 9)), ('our span - based single model', (10, 16)), ('better F 1 scores', (18, 22)), ('word embeddings , SENNA and ELMO', (25, 31))]","[['our span - based single model', 'consistently yielded', 'better F 1 scores'], ['better F 1 scores', 'regardless of', 'word embeddings , SENNA and ELMO']]","[['CRF - based single model', 'has', 'our span - based single model']]","[['Results', 'In comparison with', 'CRF - based single model']]",[],[],[],[],[],[],semantic_role_labeling,4,180
results,Our single and ensemble models using ELMO achieved the best F 1 scores on all the test sets except the Brown test set .,"[('achieved', (7, 8)), ('on', (13, 14)), ('except', (18, 19))]","[('Our single and ensemble models', (0, 5)), ('best F 1 scores', (9, 13)), ('all the test sets', (14, 18)), ('Brown test set', (20, 23))]","[['Our single and ensemble models', 'achieved', 'best F 1 scores'], ['best F 1 scores', 'on', 'all the test sets'], ['all the test sets', 'except', 'Brown test set']]",[],[],"[['Results', 'has', 'Our single and ensemble models']]",[],[],[],[],[],semantic_role_labeling,4,183
research-problem,Structural Scaffolds for Citation Intent Classification in Scientific Publications,[],"[('Citation Intent Classification in Scientific Publications', (3, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Citation Intent Classification in Scientific Publications']]",[],[],[],[],sentence_classification,0,2
research-problem,"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .",[],"[('Identifying the intent of a citation in scientific papers', (0, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Identifying the intent of a citation in scientific papers']]",[],[],[],[],sentence_classification,0,4
code,Our code and data are available at : https://github.com/ allenai/scicite .,[],"[('https://github.com/ allenai/scicite', (8, 10))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/ allenai/scicite']]",[],[],[],[],sentence_classification,0,8
research-problem,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",[],"[('citation intent classification', (9, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'citation intent classification']]",[],[],[],[],sentence_classification,0,19
model,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .","[('propose', (5, 6)), ('to incorporate', (11, 13)), ('into', (14, 15)), ('from', (16, 17))]","[('neural multitask learning framework', (7, 11)), ('knowledge', (13, 14)), ('citations', (15, 16)), ('structure of scientific papers', (18, 22))]","[['neural multitask learning framework', 'to incorporate', 'knowledge'], ['knowledge', 'into', 'citations'], ['citations', 'from', 'structure of scientific papers']]",[],"[['Model', 'propose', 'neural multitask learning framework']]",[],[],[],[],[],[],sentence_classification,0,23
model,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .","[('as', (8, 9)), ('to improve', (11, 13))]","[('two auxiliary tasks', (5, 8)), ('structural scaffolds', (9, 11)), ('citation intent prediction', (13, 16))]","[['two auxiliary tasks', 'as', 'structural scaffolds'], ['structural scaffolds', 'to improve', 'citation intent prediction']]",[],[],"[['Model', 'propose', 'two auxiliary tasks']]",[],[],[],[],[],sentence_classification,0,24
model,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .","[('On', (0, 1)), ('show that', (5, 7)), ('outperforms', (12, 13)), ('by', (15, 16))]","[('two datasets', (1, 3)), ('proposed neural scaffold model', (8, 12)), ('existing methods', (13, 15)), ('large margins', (16, 18))]","[['two datasets', 'show that', 'proposed neural scaffold model'], ['proposed neural scaffold model', 'by', 'large margins'], ['proposed neural scaffold model', 'outperforms', 'existing methods']]",[],"[['Model', 'On', 'two datasets']]",[],[],[],[],[],[],sentence_classification,0,26
model,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .","[('for', (13, 14)), ('to incorporate into', (17, 20)), ('from', (22, 23))]","[('neural scaffold framework', (10, 13)), ('citation intent classification', (14, 17)), ('citations', (20, 21)), ('knowledge', (21, 22)), ('structure of scientific papers', (23, 27))]","[['neural scaffold framework', 'for', 'citation intent classification'], ['citation intent classification', 'to incorporate into', 'citations'], ['knowledge', 'from', 'structure of scientific papers']]","[['citations', 'has', 'knowledge']]",[],"[['Model', 'propose', 'neural scaffold framework']]",[],[],[],[],[],sentence_classification,0,27
dataset,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .","[('introduce', (6, 7)), ('of', (14, 15)), ('that is', (17, 19)), ('compared with', (28, 30))]","[('Sci - Cite', (7, 10)), ('new dataset', (12, 14)), ('citation intents', (15, 17)), ('significantly larger , more coarse - grained and generaldomain', (19, 28)), ('existing datasets', (30, 32))]","[['new dataset', 'that is', 'significantly larger , more coarse - grained and generaldomain'], ['significantly larger , more coarse - grained and generaldomain', 'compared with', 'existing datasets'], ['new dataset', 'of', 'citation intents']]","[['Sci - Cite', 'has', 'new dataset']]","[['Dataset', 'introduce', 'Sci - Cite']]",[],[],[],[],[],[],sentence_classification,0,121
dataset,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .","[('consider', (1, 2))]","[('three intent categories', (2, 5)), ('BACK - GROUND', (8, 11)), ('METHOD', (12, 13)), ('RESULTCOMPARISON', (14, 15))]",[],"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","[['Dataset', 'consider', 'three intent categories']]",[],[],[],[],[],[],sentence_classification,0,125
dataset,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,"[('of', (2, 3)), ('labeled through', (6, 8))]","[('Citation intent', (0, 2)), ('sentence extractions', (3, 5)), ('crowdsourcing platform', (9, 11))]","[['Citation intent', 'of', 'sentence extractions'], ['sentence extractions', 'labeled through', 'crowdsourcing platform']]",[],[],"[['Dataset', 'has', 'Citation intent']]",[],[],[],[],[],sentence_classification,0,128
dataset,"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .","[('annotated by', (3, 5)), ('made', (9, 10)), ('individually made', (16, 18))]","[('Citation contexts', (0, 2)), ('850 crowdsource workers', (5, 8)), ('total of 29,926 annotations', (11, 15)), ('4 and 240 annotations', (19, 23))]","[['Citation contexts', 'annotated by', '850 crowdsource workers'], ['850 crowdsource workers', 'individually made', '4 and 240 annotations'], ['850 crowdsource workers', 'made', 'total of 29,926 annotations']]",[],[],"[['Dataset', 'has', 'Citation contexts']]",[],[],[],[],[],sentence_classification,0,142
dataset,"Each sentence was annotated , on average , 3.74 times .","[('was', (2, 3)), ('on average', (5, 7))]","[('sentence', (1, 2)), ('annotated', (3, 4)), ('3.74 times', (8, 10))]","[['sentence', 'was', 'annotated'], ['annotated', 'on average', '3.74 times']]",[],[],[],[],[],[],"[['Citation contexts', 'has', 'sentence']]",[],sentence_classification,0,143
dataset,"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .","[('resulted in', (1, 3)), ('divided to', (10, 12)), ('with', (16, 17)), ('of', (19, 20)), ('used for', (22, 24))]","[('total 9,159', (4, 6)), ('crowdsourced instances', (6, 8)), ('training and validation sets', (12, 16)), ('90 %', (17, 19)), ('data', (21, 22)), ('training set', (25, 27))]","[['crowdsourced instances', 'divided to', 'training and validation sets'], ['training and validation sets', 'with', '90 %'], ['90 %', 'of', 'data'], ['data', 'used for', 'training set']]","[['total 9,159', 'has', 'crowdsourced instances']]",[],[],[],"[['Citation contexts', 'resulted in', 'total 9,159']]",[],[],[],sentence_classification,0,144
hyperparameters,We implement our proposed scaffold framework using the AllenNLP library .,"[('implement', (1, 2)), ('using', (6, 7))]","[('proposed scaffold framework', (3, 6)), ('AllenNLP library', (8, 10))]","[['proposed scaffold framework', 'using', 'AllenNLP library']]",[],"[['Hyperparameters', 'implement', 'proposed scaffold framework']]",[],[],[],[],[],[],sentence_classification,0,155
hyperparameters,"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .","[('For', (0, 1)), ('use', (5, 6)), ('trained on', (11, 13)), ('of', (15, 16)), ('from', (18, 19))]","[('word representations', (1, 3)), ('100 - dimensional GloVe vectors', (6, 11)), ('corpus', (14, 15)), ('6B tokens', (16, 18)), ('Wikipedia and Gigaword', (19, 22))]","[['word representations', 'use', '100 - dimensional GloVe vectors'], ['100 - dimensional GloVe vectors', 'trained on', 'corpus'], ['corpus', 'of', '6B tokens'], ['corpus', 'from', 'Wikipedia and Gigaword']]",[],"[['Hyperparameters', 'For', 'word representations']]",[],[],[],[],[],[],sentence_classification,0,156
hyperparameters,"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .",[],[],"[['contextual representations', 'use', 'ELMo vectors'], ['ELMo vectors', 'with', 'output dimension size'], ['output dimension size', 'of', '1,024'], ['ELMo vectors', 'trained on', 'dataset'], ['dataset', 'of', '5.5 B tokens']]",[],[],"[['Hyperparameters', 'For', 'contextual representations']]",[],[],[],[],[],sentence_classification,0,157
hyperparameters,We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,"[('use', (1, 2)), ('with', (7, 8)), ('of', (12, 13))]","[('single - layer BiLSTM', (3, 7)), ('hidden dimension size', (9, 12)), ('50', (13, 14))]","[['single - layer BiLSTM', 'with', 'hidden dimension size'], ['hidden dimension size', 'of', '50']]",[],"[['Hyperparameters', 'use', 'single - layer BiLSTM']]",[],[],[],[],[],[],sentence_classification,0,158
hyperparameters,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .","[('use', (7, 8)), ('with', (13, 14)), ('of', (24, 25)), ('between', (26, 27))]","[('each of scaffold tasks', (1, 5)), ('single - layer MLP', (9, 13)), ('20 hidden nodes', (14, 17)), ('ReLU activation', (18, 20)), ('Dropout rate', (22, 24)), ('0.2', (25, 26)), ('hidden and input layers', (28, 32))]","[['each of scaffold tasks', 'use', 'single - layer MLP'], ['single - layer MLP', 'between', 'hidden and input layers'], ['hidden and input layers', 'with', '20 hidden nodes'], ['hidden and input layers', 'with', 'ReLU activation'], ['hidden and input layers', 'with', 'Dropout rate'], ['Dropout rate', 'of', '0.2']]",[],[],"[['Hyperparameters', 'For', 'each of scaffold tasks']]",[],[],[],[],[],sentence_classification,0,159
hyperparameters,Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,[],[],"[['Batch size', 'is', '8'], ['8', 'for', 'ACL - ARC dataset'], ['Batch size', 'is', '32'], ['32', 'for', 'SciCite dataset']]",[],[],"[['Hyperparameters', 'has', 'Batch size']]",[],[],[],[],[],sentence_classification,0,164
hyperparameters,We use Beaker 12 for running the experiments .,"[('for', (4, 5))]","[('Beaker', (2, 3)), ('running the experiments', (5, 8))]","[['Beaker', 'for', 'running the experiments']]",[],[],"[['Hyperparameters', 'use', 'Beaker']]",[],[],[],[],[],sentence_classification,0,165
baselines,BiLSTM Attention ( with and without ELMo ) .,[],"[('BiLSTM Attention ( with and without ELMo )', (0, 8))]",[],[],[],"[['Baselines', 'has', 'BiLSTM Attention ( with and without ELMo )']]",[],[],[],[],[],sentence_classification,0,173
baselines,"This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .","[('uses', (2, 3)), ('to', (6, 7)), ('optimizes', (18, 19)), ('for', (21, 22)), ('regarding', (25, 26))]","[('similar architecture', (4, 6)), ('proposed neural multitask learning framework', (8, 13)), ('network', (20, 21)), ('main loss', (23, 25)), ('citation intent classification ( L 1 )', (27, 34))]","[['network', 'for', 'main loss'], ['main loss', 'regarding', 'citation intent classification ( L 1 )'], ['similar architecture', 'to', 'proposed neural multitask learning framework']]",[],[],[],[],"[['BiLSTM Attention ( with and without ELMo )', 'optimizes', 'network'], ['BiLSTM Attention ( with and without ELMo )', 'uses', 'similar architecture']]",[],[],[],sentence_classification,0,174
results,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,"[('observe', (1, 2)), ('achieve', (8, 9)), ('over', (11, 12))]","[('scaffold - enhanced models', (4, 8)), ('clear improvements', (9, 11)), ('state - of - the - art approach', (13, 21))]","[['scaffold - enhanced models', 'achieve', 'clear improvements'], ['clear improvements', 'over', 'state - of - the - art approach']]",[],"[['Results', 'observe', 'scaffold - enhanced models']]",[],[],[],[],[],[],sentence_classification,0,181
results,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .","[('Starting with', (0, 2)), ('with', (9, 10)), ('of', (14, 15)), ('adding', (17, 18)), ('improves', (32, 33)), ('to', (36, 37))]","[('BiLSTM - Attn', (4, 7)), ('macro F1 score', (11, 14)), ('51.8', (15, 16)), (""first scaffold task in ' BiLSTM - Attn + section title scaffold '"", (19, 32)), ('F1 score', (34, 36)), ('56.9 (?= 5.1 )', (37, 41))]","[['BiLSTM - Attn', 'with', 'macro F1 score'], ['macro F1 score', 'of', '51.8'], ['BiLSTM - Attn', 'adding', ""first scaffold task in ' BiLSTM - Attn + section title scaffold '""], [""first scaffold task in ' BiLSTM - Attn + section title scaffold '"", 'improves', 'F1 score'], ['F1 score', 'to', '56.9 (?= 5.1 )']]",[],"[['Results', 'Starting with', 'BiLSTM - Attn']]",[],[],[],[],[],[],sentence_classification,0,182
results,Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,"[('Adding', (0, 1)), ('in', (4, 5)), ('results in', (15, 17))]","[('second scaffold', (2, 4)), ('BiLSTM - Attn + citation worthiness scaffold', (6, 13)), ('similar improvements', (17, 19)), ('56.3 (?= 4.5 )', (20, 24))]","[['second scaffold', 'in', 'BiLSTM - Attn + citation worthiness scaffold'], ['BiLSTM - Attn + citation worthiness scaffold', 'results in', 'similar improvements']]","[['similar improvements', 'has', '56.3 (?= 4.5 )']]","[['Results', 'Adding', 'second scaffold']]",[],[],[],[],[],[],sentence_classification,0,183
results,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .","[('used simultaneously in', (4, 7)), ('improves to', (20, 22))]","[('both scaffolds', (1, 3)), ('BiLSTM - Attn + both scaffolds', (8, 14)), ('F1 score', (17, 19)), ('63.1 ( ?= 11.3 )', (22, 27))]","[['both scaffolds', 'used simultaneously in', 'BiLSTM - Attn + both scaffolds'], ['F1 score', 'improves to', '63.1 ( ?= 11.3 )']]","[['both scaffolds', 'has', 'F1 score']]",[],"[['Results', 'has', 'both scaffolds']]",[],[],[],[],[],sentence_classification,0,184
results,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .",[],[],"[['ELMo vectors', 'achieving', 'F1'], ['F1', 'of', '67.9'], ['major improvement', 'from', 'previous state - of - the - art results'], ['previous state - of - the - art results', 'of', '54.6 ( ?= 13.3 )'], ['ELMo vectors', 'to', 'input representations'], ['input representations', 'in', 'BiLSTM - Attn w / ELMo + both scaffolds']]","[['67.9', 'has', 'major improvement']]",[],[],[],"[['best results', 'add', 'ELMo vectors']]",[],[],[],sentence_classification,0,185
results,"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .","[('note', (1, 2)), ('provide', (6, 7)), ('on top of', (9, 12))]","[('scaffold tasks', (4, 6)), ('major contributions', (7, 9)), ('ELMo - enabled baseline ( ?= 13.6 )', (13, 21))]","[['scaffold tasks', 'provide', 'major contributions'], ['major contributions', 'on top of', 'ELMo - enabled baseline ( ?= 13.6 )']]",[],"[['Results', 'note', 'scaffold tasks']]",[],[],[],[],[],[],sentence_classification,0,186
results,"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .","[('experimented with', (2, 4)), ('used in', (6, 8)), ('observed', (24, 25)), ('in', (30, 31))]","[('adding features', (4, 6)), ('our best model', (9, 12)), ('at least 1.7 % decline', (25, 30)), ('performance', (31, 32))]","[['adding features', 'used in', 'our best model'], ['adding features', 'observed', 'at least 1.7 % decline'], ['at least 1.7 % decline', 'in', 'performance']]",[],"[['Results', 'experimented with', 'adding features']]",[],[],[],[],[],[],sentence_classification,0,188
results,Each scaffold task improves model performance .,"[('improves', (3, 4))]","[('Each scaffold task', (0, 3)), ('model performance', (4, 6))]","[['Each scaffold task', 'improves', 'model performance']]",[],[],"[['Results', 'has', 'Each scaffold task']]",[],[],[],[],[],sentence_classification,0,191
results,Adding both scaffolds results in further improvements .,"[('results in', (3, 5))]","[('both scaffolds', (1, 3)), ('further improvements', (5, 7))]","[['both scaffolds', 'results in', 'further improvements']]",[],[],"[['Results', 'Adding', 'both scaffolds']]",[],[],[],[],[],sentence_classification,0,192
results,And the best results are obtained by using ELMo representation in addition to both scaffolds .,"[('by using', (6, 8)), ('in addition to', (10, 13))]","[('best results', (2, 4)), ('ELMo representation', (8, 10)), ('both scaffolds', (13, 15))]","[['best results', 'by using', 'ELMo representation'], ['ELMo representation', 'in addition to', 'both scaffolds']]",[],[],[],[],[],[],[],[],sentence_classification,0,193
results,Generally we observe that results on categories with more number of instances are higher .,"[('on', (5, 6)), ('with', (7, 8)), ('are', (12, 13))]","[('results', (4, 5)), ('categories', (6, 7)), ('more number of instances', (8, 12)), ('higher', (13, 14))]","[['results', 'on', 'categories'], ['categories', 'with', 'more number of instances'], ['more number of instances', 'are', 'higher']]",[],[],"[['Results', 'observe', 'results']]",[],[],[],[],[],sentence_classification,0,197
results,"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .",[],[],"[['results', 'on', 'BACKGROUND category'], ['BACKGROUND category', 'are', 'highest'], ['highest', 'as', 'category'], ['category', 'is', 'most common']]","[['ACL - ARC', 'has', 'results']]","[['Results', 'on', 'ACL - ARC']]",[],[],[],[],[],[],sentence_classification,0,198
results,"Conversely , the results on the FUTUREWORK category are the lowest .","[('are', (8, 9))]","[('FUTUREWORK category', (6, 8)), ('lowest', (10, 11))]","[['FUTUREWORK category', 'are', 'lowest']]",[],[],[],[],[],[],"[['results', 'on', 'FUTUREWORK category']]",[],sentence_classification,0,199
results,This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,[],[],"[['fewest data points', 'thus', 'harder'], ['harder', 'for', 'model'], ['model', 'to learn', 'optimal parameters'], ['optimal parameters', 'for', 'correct classification']]",[],[],[],[],[],[],"[['FUTUREWORK category', 'has', 'fewest data points']]",[],sentence_classification,0,200
research-problem,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,[],"[('Sequential Sentence Classification in Medical Scientific Abstracts', (4, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sequential Sentence Classification in Medical Scientific Abstracts']]",[],[],[],[],sentence_classification,1,2
research-problem,"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .",[],"[('sequential sentence classification', (11, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'sequential sentence classification']]",[],[],[],[],sentence_classification,1,5
model,"In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .","[('present', (5, 6)), ('for', (11, 12)), ('call', (20, 21))]","[('hierarchical neural network model', (7, 11)), ('sequential sentence classification task', (13, 17)), ('hierarchical sequential labeling network ( HSLN )', (22, 29))]","[['hierarchical neural network model', 'call', 'hierarchical sequential labeling network ( HSLN )'], ['hierarchical neural network model', 'for', 'sequential sentence classification task']]",[],"[['Model', 'present', 'hierarchical neural network model']]",[],[],[],[],[],[],sentence_classification,1,21
model,"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .",[],[],"[['single - hidden - layer feed - forward network', 'to transform', 'sentence representation'], ['sentence representation', 'to', 'probability vector'], ['predicted label sequence jointly', 'via', 'CRF layer'], ['another bi - LSTM layer', 'output', 'contextualized sentence representation'], ['another bi - LSTM layer', 'take as input', 'individual sentence representation'], ['RNN or CNN layer', 'to individually encode', 'sentence representation'], ['sentence representation', 'from', 'sequence of word embeddings']]",[],"[['Model', 'subsequently uses', 'single - hidden - layer feed - forward network'], ['Model', 'optimizes', 'predicted label sequence jointly'], ['Model', 'uses', 'another bi - LSTM layer'], ['Model', 'first uses', 'RNN or CNN layer']]",[],[],[],[],[],[],sentence_classification,1,22
hyperparameters,"The token embeddings were pre-trained on a large corpus combining Wikipedia , PubMed , and PMC texts ( Moen and Ananiadou , 2013 ) using the word2vec tool 4 ( denoted as "" Word2vec- wiki+P.M. "" ) .","[('pre-trained on', (4, 6)), ('combining', (9, 10)), ('using', (24, 25))]","[('token embeddings', (1, 3)), ('large corpus', (7, 9)), ('Wikipedia , PubMed , and PMC texts', (10, 17)), ('word2vec tool', (26, 28))]","[['token embeddings', 'using', 'word2vec tool'], ['token embeddings', 'pre-trained on', 'large corpus'], ['large corpus', 'combining', 'Wikipedia , PubMed , and PMC texts']]",[],[],"[['Hyperparameters', 'has', 'token embeddings']]",[],[],[],[],[],sentence_classification,1,110
hyperparameters,They are fixed during the training phase to avoid over-fitting .,"[('fixed during', (2, 4)), ('to avoid', (7, 9))]","[('training phase', (5, 7)), ('over-fitting', (9, 10))]","[['training phase', 'to avoid', 'over-fitting']]",[],"[['Hyperparameters', 'fixed during', 'training phase']]",[],[],[],[],[],[],sentence_classification,1,111
hyperparameters,"The model is trained using the Adam optimization method ( Kingma and Ba , 2014 ) .","[('trained using', (3, 5))]","[('Adam optimization method', (6, 9))]",[],[],"[['Hyperparameters', 'trained using', 'Adam optimization method']]",[],[],[],[],[],[],sentence_classification,1,115
hyperparameters,The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .,"[('initially set', (4, 6)), ('decayed by', (9, 11)), ('after', (12, 13))]","[('learning rate', (1, 3)), ('0.003', (7, 8)), ('0.9', (11, 12)), ('each epoch', (13, 15))]","[['learning rate', 'initially set', '0.003'], ['learning rate', 'decayed by', '0.9'], ['0.9', 'after', 'each epoch']]",[],[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],sentence_classification,1,116
hyperparameters,"For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .","[('For', (0, 1)), ('applied to', (12, 14))]","[('regularization', (1, 2)), ('dropout', (3, 4)), ('each layer', (14, 16))]","[['dropout', 'applied to', 'each layer']]","[['regularization', 'has', 'dropout']]","[['Hyperparameters', 'For', 'regularization']]",[],[],[],[],[],[],sentence_classification,1,117
hyperparameters,"To reduce this gap , we adopted the dropout with expectation - linear regularization introduced by to explicitly control the inference gap and thus improve the generaliza - tion performance .","[('adopted', (6, 7)), ('with', (9, 10)), ('to explicitly control', (16, 19)), ('improve', (24, 25))]","[('dropout', (8, 9)), ('expectation - linear regularization', (10, 14)), ('inference gap', (20, 22)), ('generaliza - tion performance', (26, 30))]","[['dropout', 'with', 'expectation - linear regularization'], ['expectation - linear regularization', 'improve', 'generaliza - tion performance'], ['expectation - linear regularization', 'to explicitly control', 'inference gap']]",[],"[['Hyperparameters', 'adopted', 'dropout']]",[],[],[],[],[],[],sentence_classification,1,119
hyperparameters,Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in .,"[('optimized via', (2, 4)), ('based on', (6, 8))]","[('grid search', (4, 6)), ('validation set', (9, 11))]","[['grid search', 'based on', 'validation set']]",[],"[['Hyperparameters', 'optimized via', 'grid search']]",[],[],[],[],[],[],sentence_classification,1,120
hyperparameters,"The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .","[('of', (3, 4)), ('in', (7, 8)), ('are', (12, 13))]","[('window sizes', (1, 3)), ('CNN encoder', (5, 7)), ('sentence encoding layer', (9, 12)), ('2 , 3 , 4 and 5', (13, 20))]","[['window sizes', 'of', 'CNN encoder'], ['CNN encoder', 'in', 'sentence encoding layer'], ['sentence encoding layer', 'are', '2 , 3 , 4 and 5']]",[],[],"[['Hyperparameters', 'has', 'window sizes']]",[],[],[],[],[],sentence_classification,1,121
ablation-analysis,"As can be seen from , our HSLN - CNN model uni-formly suffers a little more from the component removal than the HSLN - RNN model , indicating that the HSLN - RNN model is more robust .","[('suffers a little more from', (12, 17)), ('than', (20, 21))]","[('HSLN - CNN model', (7, 11)), ('component removal', (18, 20)), ('HSLN - RNN model', (22, 26))]","[['HSLN - CNN model', 'suffers a little more from', 'component removal'], ['HSLN - CNN model', 'than', 'HSLN - RNN model']]",[],[],"[['Ablation analysis', 'has', 'HSLN - CNN model']]",[],[],[],[],[],sentence_classification,1,129
ablation-analysis,"When the context enriching layer is removed , both models experience the most significant performance drop and can only be on par with the previous stateof - the - art results , strongly demonstrating that this proposed component is the key to the performance improvement of our model .","[('When', (0, 1)), ('is', (5, 6)), ('experience', (10, 11)), ('on par with', (20, 23))]","[('context enriching layer', (2, 5)), ('removed', (6, 7)), ('both models', (8, 10)), ('most significant performance drop', (12, 16)), ('previous stateof - the - art results', (24, 31))]","[['context enriching layer', 'is', 'removed'], ['both models', 'experience', 'most significant performance drop'], ['both models', 'on par with', 'previous stateof - the - art results']]","[['removed', 'has', 'both models']]","[['Ablation analysis', 'When', 'context enriching layer']]",[],[],[],[],[],[],sentence_classification,1,130
ablation-analysis,"Furthermore , even without the label sequence optimization layer , our model still significantly outperforms the best published methods that are empowered by this layer , indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences .","[('even without', (2, 4)), ('significantly outperforms', (13, 15)), ('empowered by', (21, 23))]","[('label sequence optimization layer', (5, 9)), ('our model', (10, 12)), ('best published methods', (16, 19)), ('this layer', (23, 25))]","[['our model', 'significantly outperforms', 'best published methods'], ['best published methods', 'empowered by', 'this layer']]","[['label sequence optimization layer', 'has', 'our model']]","[['Ablation analysis', 'even without', 'label sequence optimization layer']]",[],[],[],[],[],[],sentence_classification,1,131
ablation-analysis,"Last but not the least , the dropout regularization and attention - based pooling components we add to our system can help further improve the model in a limited extent . :","[('add to', (16, 18)), ('further improve', (22, 24)), ('in', (26, 27))]","[('dropout regularization and attention - based pooling components', (7, 15)), ('our system', (18, 20)), ('model', (25, 26)), ('limited extent', (28, 30))]","[['dropout regularization and attention - based pooling components', 'further improve', 'model'], ['model', 'in', 'limited extent'], ['dropout regularization and attention - based pooling components', 'add to', 'our system']]",[],[],"[['Ablation analysis', 'has', 'dropout regularization and attention - based pooling components']]",[],[],[],[],[],sentence_classification,1,132
research-problem,Translations as Additional Contexts for Sentence Classification,[],"[('Sentence Classification', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Classification']]",[],[],[],[],sentence_classification,2,2
approach,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .","[('propose', (5, 6)), ('as', (10, 11))]","[('usage of translations', (7, 10)), ('compelling and effective domain - free contexts', (11, 18))]","[['usage of translations', 'as', 'compelling and effective domain - free contexts']]",[],"[['Approach', 'propose', 'usage of translations']]",[],[],[],[],[],[],sentence_classification,2,20
approach,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .","[('to mitigate', (8, 10)), ('when using', (13, 15)), ('as', (17, 18))]","[('method', (7, 8)), ('possible problems', (11, 13)), ('translated sentences', (15, 17)), ('context', (18, 19))]","[['method', 'to mitigate', 'possible problems'], ['possible problems', 'when using', 'translated sentences'], ['translated sentences', 'as', 'context']]",[],[],"[['Approach', 'propose', 'method']]",[],[],[],[],[],sentence_classification,2,37
approach,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .","[('present', (6, 7))]","[('neural attentionbased multiple context fixing attachment ( MCFA )', (8, 17))]",[],[],"[['Approach', 'present', 'neural attentionbased multiple context fixing attachment ( MCFA )']]",[],[],[],[],[],[],sentence_classification,2,42
approach,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .","[('is', (1, 2)), ('uses', (7, 8)), ('as', (22, 23)), ('to fix', (24, 26))]","[('MCFA', (0, 1)), ('series of modules', (3, 6)), ('all the sentence vectors', (8, 12)), ('context', (23, 24)), ('a sentence vector', (26, 29))]","[['MCFA', 'is', 'series of modules'], ['series of modules', 'uses', 'all the sentence vectors'], ['all the sentence vectors', 'as', 'context'], ['all the sentence vectors', 'to fix', 'a sentence vector']]",[],[],"[['Approach', 'has', 'MCFA']]",[],[],[],[],[],sentence_classification,2,43
approach,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,"[('computes', (1, 2)), ('to control', (6, 8)), ('weighs', (24, 25)), ('of using', (27, 29)), ('in solving', (31, 33))]","[('two sentence usability metrics', (2, 6)), ('noise when fixing vectors', (9, 13)), ('self usability', (17, 19)), ('confidence', (26, 27)), ('sentence a', (29, 31)), ('task', (34, 35))]","[['two sentence usability metrics', 'to control', 'noise when fixing vectors'], ['self usability', 'weighs', 'confidence'], ['confidence', 'of using', 'sentence a'], ['sentence a', 'in solving', 'task']]","[['two sentence usability metrics', 'has', 'self usability']]",[],[],[],"[['MCFA', 'computes', 'two sentence usability metrics']]",[],[],[],sentence_classification,2,46
approach,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.","[('weighs', (12, 13)), ('using', (16, 17)), ('fixing', (20, 21))]","[('relative usability', (3, 5)), ('confidence of', (14, 16)), ('sentence a', (17, 19))]","[['relative usability', 'weighs', 'confidence of'], ['confidence of', 'using', 'sentence a']]",[],[],[],[],[],"[['sentence a', 'fixing', 'sentence b']]","[['two sentence usability metrics', 'has', 'relative usability']]",[],sentence_classification,2,47
approach,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .","[('attached after', (5, 7)), ('makes it widely adaptable', (12, 16))]","[('encoding the sentence', (7, 10)), ('to other models', (16, 19))]","[['encoding the sentence', 'makes it widely adaptable', 'to other models']]",[],[],[],[],"[['MCFA', 'attached after', 'encoding the sentence']]",[],[],[],sentence_classification,2,49
approach,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .","[('moves', (4, 5)), ('inside', (7, 8)), ('preserves', (13, 14)), ('of', (16, 17))]","[('vectors', (6, 7)), ('same space', (9, 11)), ('meaning', (15, 16)), ('vector dimensions', (17, 19))]","[['vectors', 'preserves', 'meaning'], ['meaning', 'of', 'vector dimensions'], ['vectors', 'inside', 'same space']]",[],[],[],[],"[['MCFA', 'moves', 'vectors']]",[],[],[],sentence_classification,2,51
hyperparameters,Tokenization is done using the polyglot library 7 .,"[('done using', (2, 4))]","[('Tokenization', (0, 1)), ('polyglot library', (5, 7))]","[['Tokenization', 'done using', 'polyglot library']]",[],[],"[['Hyperparameters', 'has', 'Tokenization']]",[],[],[],[],[],sentence_classification,2,150
hyperparameters,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,"[('experiment on using', (1, 4))]","[('only one additional context ( N = 1 )', (4, 13)), ('all ten languages at once ( N = 10 )', (15, 25))]",[],[],"[['Hyperparameters', 'experiment on using', 'only one additional context ( N = 1 )'], ['Hyperparameters', 'experiment on using', 'all ten languages at once ( N = 10 )']]",[],[],[],[],[],[],sentence_classification,2,151
hyperparameters,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .","[('For', (0, 1))]","[('our CNN', (1, 3))]",[],[],"[['Hyperparameters', 'For', 'our CNN']]",[],[],[],[],[],"[['our CNN', 'use', 'rectified linear units'], ['our CNN', 'use', 'three filters']]",sentence_classification,2,153
hyperparameters,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","[('concatenate', (7, 8)), ('to get', (11, 13))]","[('final sentence vector', (2, 5)), ('feature maps', (9, 11)), ('300 - dimension vector', (14, 18))]","[['final sentence vector', 'concatenate', 'feature maps'], ['feature maps', 'to get', '300 - dimension vector']]",[],[],"[['Hyperparameters', 'For', 'final sentence vector']]",[],[],[],[],[],sentence_classification,2,154
hyperparameters,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,"[('use', (1, 2)), ('on', (3, 4)), ('with', (7, 8)), ('of', (11, 12))]","[('dropout', (2, 3)), ('all nonlinear connections', (4, 7)), ('dropout rate', (9, 11)), ('0.5', (12, 13))]","[['dropout', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5'], ['dropout', 'on', 'all nonlinear connections']]",[],"[['Hyperparameters', 'use', 'dropout']]",[],[],[],[],[],[],sentence_classification,2,155
hyperparameters,"We also use an l 2 constraint of 3 , following for accurate comparisons .","[('of', (7, 8))]","[('l 2 constraint', (4, 7)), ('3', (8, 9))]","[['l 2 constraint', 'of', '3']]",[],[],"[['Hyperparameters', 'use', 'l 2 constraint']]",[],[],[],[],[],sentence_classification,2,156
hyperparameters,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,"[('for', (6, 7))]","[('FastText pre-trained vectors', (2, 5)), ('all our data sets', (7, 11))]","[['FastText pre-trained vectors', 'for', 'all our data sets']]",[],[],"[['Hyperparameters', 'use', 'FastText pre-trained vectors']]",[],[],[],[],[],sentence_classification,2,157
hyperparameters,"During training , we use mini-batch size of 50 .","[('During', (0, 1)), ('use', (4, 5)), ('of', (7, 8))]","[('training', (1, 2)), ('mini-batch size', (5, 7)), ('50', (8, 9))]","[['training', 'use', 'mini-batch size'], ['mini-batch size', 'of', '50']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],sentence_classification,2,158
hyperparameters,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,"[('done via', (2, 4)), ('over', (7, 8)), ('with', (10, 11))]","[('Training', (0, 1)), ('stochastic gradient descent', (4, 7)), ('shuffled mini-batches', (8, 10)), ('Adadelta update rule', (12, 15))]","[['Training', 'done via', 'stochastic gradient descent'], ['stochastic gradient descent', 'over', 'shuffled mini-batches'], ['shuffled mini-batches', 'with', 'Adadelta update rule']]",[],[],"[['Hyperparameters', 'has', 'Training']]",[],[],[],[],[],sentence_classification,2,159
hyperparameters,We perform early stopping using a random 10 % of the training set as the development set .,"[('perform', (1, 2)), ('using', (4, 5)), ('of', (9, 10)), ('as', (13, 14))]","[('early stopping', (2, 4)), ('random 10 %', (6, 9)), ('training set', (11, 13)), ('development set', (15, 17))]","[['early stopping', 'using', 'random 10 %'], ['random 10 %', 'as', 'development set'], ['random 10 %', 'of', 'training set']]",[],"[['Hyperparameters', 'perform', 'early stopping']]",[],[],[],[],[],[],sentence_classification,2,160
results,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,[],[],"[['CNN + MCFA', 'performs', 'competitively'], ['competitively', 'on', 'one data set'], ['CNN + MCFA', 'achieves', 'state of the art performance'], ['state of the art performance', 'on', 'three of the four data sets']]",[],"[['Results', 'show', 'CNN + MCFA']]",[],[],[],[],[],[],sentence_classification,2,169
results,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .","[('When', (0, 1)), ('increases', (6, 7)), ('of', (9, 10)), ('from', (13, 14)), ('to', (15, 16)), ('beating', (18, 19)), ('on', (25, 26))]","[('N = 1', (1, 4)), ('MCFA', (5, 6)), ('performance', (8, 9)), ('normal CNN', (11, 13)), ('85.0', (14, 15)), ('87.6', (16, 17)), ('current state of the art', (20, 25)), ('CR data set', (27, 30))]","[['MCFA', 'increases', 'performance'], ['performance', 'of', 'normal CNN'], ['normal CNN', 'beating', 'current state of the art'], ['current state of the art', 'on', 'CR data set'], ['normal CNN', 'from', '85.0'], ['85.0', 'to', '87.6']]","[['N = 1', 'has', 'MCFA']]","[['Results', 'When', 'N = 1']]",[],[],[],[],[],[],sentence_classification,2,170
results,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .","[('additionally beats', (6, 8)), ('on', (13, 14))]","[('N = 10', (1, 4)), ('MCFA', (5, 6)), ('state of the art', (9, 13)), ('TREC data set', (15, 18))]","[['MCFA', 'additionally beats', 'state of the art'], ['state of the art', 'on', 'TREC data set']]","[['N = 10', 'has', 'MCFA']]",[],"[['Results', 'When', 'N = 10']]",[],[],[],[],[],sentence_classification,2,171
results,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .","[('additionally outperforms', (5, 7)), ('on', (10, 11))]","[('our ensemble classifier', (2, 5)), ('all competing models', (7, 10)), ('MR data set', (12, 15))]","[['our ensemble classifier', 'additionally outperforms', 'all competing models'], ['all competing models', 'on', 'MR data set']]",[],[],"[['Results', 'has', 'our ensemble classifier']]",[],[],[],[],[],sentence_classification,2,172
results,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .","[('On', (0, 1)), ('except', (4, 5)), ('of', (9, 10)), ('decreases from', (13, 15))]","[('all data sets', (1, 4)), ('SUBJ', (5, 6)), ('accuracy', (8, 9)), ('CNN + B1', (10, 13)), ('base CNN accuracy', (16, 19))]","[['all data sets', 'except', 'SUBJ'], ['accuracy', 'of', 'CNN + B1'], ['CNN + B1', 'decreases from', 'base CNN accuracy']]","[['all data sets', 'has', 'accuracy']]","[['Results', 'On', 'all data sets']]",[],[],[],[],[],[],sentence_classification,2,186
results,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",[],"[('topics', (10, 11)), ('translations', (15, 16))]",[],[],[],"[['Results', 'conclude that', 'translations']]",[],[],[],[],[],sentence_classification,2,188
results,"Overall , we conclude that translations are better additional contexts than topics .","[('conclude that', (3, 5)), ('are', (6, 7)), ('than', (10, 11))]","[('better additional contexts', (7, 10))]",[],[],[],[],[],"[['translations', 'are', 'better additional contexts']]","[['better additional contexts', 'than', 'topics']]",[],[],sentence_classification,2,189
results,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .","[('using', (1, 2)), ('always outperform', (23, 25))]","[('single context', (3, 5)), ('translations', (22, 23)), ('topics', (25, 26))]","[['translations', 'always outperform', 'topics']]","[['single context', 'has', 'translations']]","[['Results', 'using', 'single context']]",[],[],[],[],[],[],sentence_classification,2,190
results,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .","[('Using', (0, 1)), ('as', (2, 3)), ('decreases', (6, 7)), ('of', (9, 10)), ('on', (13, 14))]","[('topics', (1, 2)), ('additional context', (3, 5)), ('performance', (8, 9)), ('CNN classifier', (11, 13)), ('most data sets', (14, 17))]","[['topics', 'as', 'additional context'], ['topics', 'decreases', 'performance'], ['performance', 'of', 'CNN classifier'], ['CNN classifier', 'on', 'most data sets']]",[],"[['Results', 'Using', 'topics']]",[],[],[],[],[],[],sentence_classification,2,191
research-problem,Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains,[],"[('Sentence Compression', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Compression']]",[],[],[],[],sentence_compression,0,2
model,"To this end , we extend the deletionbased LSTM model for sentence compression by .","[('extend', (5, 6)), ('for', (10, 11))]","[('deletionbased LSTM model', (7, 10)), ('sentence compression', (11, 13))]","[['deletionbased LSTM model', 'for', 'sentence compression']]",[],"[['Model', 'extend', 'deletionbased LSTM model']]",[],[],[],[],[],[],sentence_compression,0,28
model,"Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .","[('propose', (3, 4)), ('explicitly introduce', (13, 15)), ('into', (21, 22))]","[('two major changes', (4, 7)), ('POS embeddings and dependency relation embeddings', (15, 21)), ('neural network model', (23, 26))]","[['two major changes', 'explicitly introduce', 'POS embeddings and dependency relation embeddings'], ['POS embeddings and dependency relation embeddings', 'into', 'neural network model']]",[],"[['Model', 'propose', 'two major changes']]",[],[],[],[],[],[],sentence_compression,0,35
model,"( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .","[('formulate', (10, 11)), ('as', (14, 15)), ('to incorporate', (20, 22)), ('based on', (23, 25)), ('between', (27, 28)), ('of', (32, 33))]","[('final predictions', (12, 14)), ('Integer Linear Programming problem', (16, 20)), ('constraints', (22, 23)), ('syntactic relations', (25, 27)), ('words and expected lengths', (28, 32)), ('compressed sentences', (34, 36))]","[['final predictions', 'as', 'Integer Linear Programming problem'], ['Integer Linear Programming problem', 'to incorporate', 'constraints'], ['constraints', 'based on', 'syntactic relations'], ['syntactic relations', 'between', 'words and expected lengths'], ['words and expected lengths', 'of', 'compressed sentences']]",[],[],[],[],"[['two major changes', 'formulate', 'final predictions']]",[],[],[],sentence_compression,0,36
model,"In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .","[('use', (11, 12)), ('to include', (14, 16)), ('from', (18, 19))]","[('bi-directional LSTM', (12, 14)), ('contextual information', (16, 18)), ('both directions into the model', (19, 24))]","[['bi-directional LSTM', 'to include', 'contextual information'], ['contextual information', 'from', 'both directions into the model']]",[],"[['Model', 'use', 'bi-directional LSTM']]",[],[],[],[],[],[],sentence_compression,0,37
hyperparameters,"In the experiments , our model was trained using the Adam algorithm with a learning rate initialized at 0.001 .","[('trained using', (7, 9)), ('with', (12, 13)), ('initialized at', (16, 18))]","[('our model', (4, 6)), ('Adam algorithm', (10, 12)), ('learning rate', (14, 16)), ('0.001', (18, 19))]","[['our model', 'trained using', 'Adam algorithm'], ['Adam algorithm', 'with', 'learning rate'], ['learning rate', 'initialized at', '0.001']]",[],[],"[['Hyperparameters', 'has', 'our model']]",[],[],[],[],[],sentence_compression,0,165
hyperparameters,The dimension of the hidden layers of bi - LSTM is 100 .,[],[],"[['dimension', 'of', 'hidden layers'], ['hidden layers', 'of', 'bi - LSTM'], ['bi - LSTM', 'is', '100']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentence_compression,0,166
hyperparameters,Word embeddings are initialized from GloVe 100 dimensional pre-trained embeddings .,"[('initialized from', (3, 5))]","[('Word embeddings', (0, 2)), ('GloVe 100 dimensional pre-trained embeddings', (5, 10))]","[['Word embeddings', 'initialized from', 'GloVe 100 dimensional pre-trained embeddings']]",[],[],"[['Hyperparameters', 'has', 'Word embeddings']]",[],[],[],[],[],sentence_compression,0,167
hyperparameters,POS and dependency embeddings are randomly initialized with 40 - dimensional vectors .,"[('randomly initialized with', (5, 8))]","[('POS and dependency embeddings', (0, 4)), ('40 - dimensional vectors', (8, 12))]","[['POS and dependency embeddings', 'randomly initialized with', '40 - dimensional vectors']]",[],[],"[['Hyperparameters', 'has', 'POS and dependency embeddings']]",[],[],[],[],[],sentence_compression,0,168
hyperparameters,The embeddings are all updated during training .,"[('updated during', (4, 6))]","[('embeddings', (1, 2)), ('training', (6, 7))]","[['embeddings', 'updated during', 'training']]",[],[],"[['Hyperparameters', 'has', 'embeddings']]",[],[],[],[],[],sentence_compression,0,169
hyperparameters,Dropping probability for dropout layers between stacked LSTM layers is 0.5 .,"[('for', (2, 3)), ('between', (5, 6)), ('is', (9, 10))]","[('Dropping probability', (0, 2)), ('dropout layers', (3, 5)), ('stacked LSTM layers', (6, 9)), ('0.5', (10, 11))]","[['Dropping probability', 'for', 'dropout layers'], ['dropout layers', 'between', 'stacked LSTM layers'], ['stacked LSTM layers', 'is', '0.5']]",[],[],"[['Hyperparameters', 'has', 'Dropping probability']]",[],[],[],[],[],sentence_compression,0,170
hyperparameters,The batch size is set as 30 .,"[('set as', (4, 6))]","[('batch size', (1, 3)), ('30', (6, 7))]","[['batch size', 'set as', '30']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],sentence_compression,0,171
hyperparameters,We utilize an open source ILP solver 4 in our method .,"[('utilize', (1, 2))]","[('open source ILP solver', (3, 7))]",[],[],"[['Hyperparameters', 'utilize', 'open source ILP solver']]",[],[],[],[],[],[],sentence_compression,0,174
baselines,LSTM : This is the basic LSTM - based deletion method proposed by .,"[('is', (3, 4))]","[('LSTM', (0, 1)), ('basic LSTM - based deletion method', (5, 11))]","[['LSTM', 'is', 'basic LSTM - based deletion method']]",[],[],"[['Baselines', 'has', 'LSTM']]",[],[],[],[],[],sentence_compression,0,176
baselines,"LSTM + : This is advanced version of the model proposed by , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word .",[],[],"[['LSTM +', 'incorporated', 'dependency parse tree information'], ['dependency parse tree information', 'into', 'LSTM model'], ['LSTM +', 'used', 'prediction'], ['prediction', 'on', 'previous word'], ['previous word', 'to help', 'prediction'], ['prediction', 'on', 'current word']]",[],[],"[['Baselines', 'has', 'LSTM +']]",[],[],[],[],[],sentence_compression,0,178
baselines,Traditional ILP :,[],"[('Traditional ILP', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Traditional ILP']]",[],[],[],[],[],sentence_compression,0,179
baselines,This is the ILP - based method proposed by .,"[('is', (1, 2))]","[('ILP - based method', (3, 7))]",[],[],[],[],[],"[['Traditional ILP', 'is', 'ILP - based method']]",[],[],[],sentence_compression,0,180
baselines,Abstractive seq2seq :,[],"[('Abstractive seq2seq', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Abstractive seq2seq']]",[],[],[],[],[],sentence_compression,0,183
baselines,This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .,"[('is', (1, 2)), ('trained on', (10, 12))]","[('abstractive sequence - to - sequence model', (3, 10)), ('3.8 million Gigaword title - article pairs', (12, 19))]","[['abstractive sequence - to - sequence model', 'trained on', '3.8 million Gigaword title - article pairs']]",[],[],[],[],"[['Abstractive seq2seq', 'is', 'abstractive sequence - to - sequence model']]",[],[],[],sentence_compression,0,184
results,We can see that indeed this abstractive method performed poorly in cross - domain settings .,"[('see that', (2, 4)), ('performed', (8, 9)), ('in', (10, 11))]","[('abstractive method', (6, 8)), ('poorly', (9, 10)), ('cross - domain settings', (11, 15))]","[['abstractive method', 'performed', 'poorly'], ['poorly', 'in', 'cross - domain settings']]",[],"[['Results', 'see that', 'abstractive method']]",[],[],[],[],[],[],sentence_compression,0,197
results,"( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .","[('In', (3, 4)), ('with', (10, 11)), ('performs', (40, 41)), ('than', (45, 46)), ('in terms of', (53, 56))]","[('in - domain setting', (5, 9)), ('our BiLSTM method', (21, 24)), ('syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP )', (25, 40)), ('similarly to or better', (41, 45)), ('LSTM + method', (47, 50)), ('both F1 and accuracy', (56, 60))]","[['our BiLSTM method', 'with', 'syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP )'], ['our BiLSTM method', 'performs', 'similarly to or better'], ['similarly to or better', 'than', 'LSTM + method'], ['similarly to or better', 'in terms of', 'both F1 and accuracy']]","[['in - domain setting', 'has', 'our BiLSTM method']]","[['Results', 'In', 'in - domain setting']]",[],[],[],[],[],[],sentence_compression,0,198
results,This shows that our method is comparable to the LSTM + method in the in - domain setting .,"[('comparable to', (6, 8)), ('in', (12, 13))]","[('our method', (3, 5)), ('LSTM + method', (9, 12)), ('in - domain setting', (14, 18))]","[['our method', 'comparable to', 'LSTM + method'], ['LSTM + method', 'in', 'in - domain setting']]",[],[],"[['Results', 'has', 'our method']]",[],[],[],[],[],sentence_compression,0,199
results,"( 4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .","[('clearly outperform', (19, 21))]","[('out - of - domain setting', (5, 11)), ('our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods', (12, 19)), ('LSTM and LSTM + methods', (22, 27))]","[['our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods', 'clearly outperform', 'LSTM and LSTM + methods']]","[['out - of - domain setting', 'has', 'our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods']]",[],"[['Results', 'In', 'out - of - domain setting']]",[],[],[],[],[],sentence_compression,0,202
results,( 5 ) The Traditional ILP method also works better than the LSTM and LSTM + methods in the out - of - domain setting .,"[('works', (8, 9)), ('than', (10, 11)), ('in', (17, 18))]","[('Traditional ILP method', (4, 7)), ('better', (9, 10)), ('LSTM and LSTM + methods', (12, 17)), ('out - of - domain setting', (19, 25))]","[['Traditional ILP method', 'works', 'better'], ['better', 'than', 'LSTM and LSTM + methods'], ['LSTM and LSTM + methods', 'in', 'out - of - domain setting']]",[],[],"[['Results', 'has', 'Traditional ILP method']]",[],[],[],[],[],sentence_compression,0,204
results,But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .,"[('performs', (5, 6)), ('in', (7, 8)), ('than both', (13, 15))]","[('worse', (6, 7)), ('in - domain setting', (9, 13)), ('LSTM and LSTM + methods and our methods', (16, 24))]","[['worse', 'in', 'in - domain setting'], ['worse', 'than both', 'LSTM and LSTM + methods and our methods']]",[],[],[],[],"[['Traditional ILP method', 'performs', 'worse']]",[],[],[],sentence_compression,0,206
results,"Therefore , our method works reasonably well for both in - domain and out - ofdomain data .","[('works', (4, 5)), ('for both', (7, 9))]","[('reasonably well', (5, 7)), ('in - domain and out - ofdomain data', (9, 17))]","[['reasonably well', 'for both', 'in - domain and out - ofdomain data']]",[],[],[],[],"[['our method', 'works', 'reasonably well']]",[],[],[],sentence_compression,0,208
results,"We also notice that on Google News , adding the ILP layer decreased the sentence compression performance .","[('on', (4, 5)), ('adding', (8, 9)), ('decreased', (12, 13))]","[('Google News', (5, 7)), ('ILP layer', (10, 12)), ('sentence compression performance', (14, 17))]","[['Google News', 'adding', 'ILP layer'], ['ILP layer', 'decreased', 'sentence compression performance']]",[],"[['Results', 'on', 'Google News']]",[],[],[],[],[],[],sentence_compression,0,209
results,"We can see that in the in - domain setting , our method does not have any advantage over the LSTM + method .","[('in', (4, 5)), ('does not have', (13, 16)), ('over', (18, 19))]","[('in - domain setting', (6, 10)), ('our method', (11, 13)), ('any advantage', (16, 18)), ('LSTM + method', (20, 23))]","[['our method', 'does not have', 'any advantage'], ['any advantage', 'over', 'LSTM + method']]",[],"[['Results', 'in', 'in - domain setting']]",[],[],[],[],[],[],sentence_compression,0,216
results,"But in the cross - domain setting , our method that uses ILP to impose syntax - based constraints clearly performs better than LSTM + when the amount of training data is relatively small .","[('uses', (11, 12)), ('to impose', (13, 15)), ('performs', (20, 21)), ('than', (22, 23)), ('when', (25, 26)), ('is', (31, 32))]","[('cross - domain setting', (3, 7)), ('our method', (8, 10)), ('ILP', (12, 13)), ('syntax - based constraints', (15, 19)), ('better', (21, 22)), ('LSTM +', (23, 25)), ('amount of training data', (27, 31)), ('relatively small', (32, 34))]","[['our method', 'performs', 'better'], ['better', 'than', 'LSTM +'], ['better', 'when', 'amount of training data'], ['amount of training data', 'is', 'relatively small'], ['our method', 'uses', 'ILP'], ['ILP', 'to impose', 'syntax - based constraints']]","[['cross - domain setting', 'has', 'our method']]",[],"[['Results', 'in', 'cross - domain setting']]",[],[],[],[],[],sentence_compression,0,217
research-problem,Sentence Compression by Deletion with LSTMs,[],"[('Sentence Compression by Deletion', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Compression by Deletion']]",[],[],[],[],sentence_compression,1,2
research-problem,"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .",[],"[('deletion - based sentence compression', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'deletion - based sentence compression']]",[],[],[],[],sentence_compression,1,4
research-problem,Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .,[],"[('Sentence compression', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence compression']]",[],[],[],[],sentence_compression,1,9
model,"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .","[('benefits from', (9, 11)), ('uses', (19, 20)), ('to output', (31, 33))]","[('very recent advances in deep learning', (12, 18)), ('word embeddings and Long Short Term Memory models ( LSTMs )', (20, 31)), ('surprisingly readable and informative compressions', (33, 38))]","[['word embeddings and Long Short Term Memory models ( LSTMs )', 'to output', 'surprisingly readable and informative compressions']]",[],"[['Model', 'benefits from', 'very recent advances in deep learning'], ['Model', 'uses', 'word embeddings and Long Short Term Memory models ( LSTMs )']]",[],[],[],[],[],[],sentence_compression,1,19
model,"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .","[('Trained on', (0, 2)), ('of', (4, 5)), ('automatically extracted', (9, 11)), ('using', (14, 15)), ('to obtain', (18, 20))]","[('corpus', (3, 4)), ('less than two million', (5, 9)), ('parallel sentences', (11, 13)), ('standard tool', (16, 18)), ('word embeddings', (20, 22))]","[['corpus', 'using', 'standard tool'], ['standard tool', 'to obtain', 'word embeddings'], ['corpus', 'of', 'less than two million'], ['less than two million', 'automatically extracted', 'parallel sentences']]",[],"[['Model', 'Trained on', 'corpus']]",[],[],[],[],[],[],sentence_compression,1,20
results,"There is a significant difference in performance of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .","[('in', (5, 6)), ('of', (7, 8)), ('in terms of', (17, 20))]","[('significant difference', (3, 5)), ('performance', (6, 7)), ('MIRA baseline and the LSTM models', (9, 15)), ('F1 - score and in accuracy', (20, 26))]","[['significant difference', 'in', 'performance'], ['performance', 'of', 'MIRA baseline and the LSTM models'], ['performance', 'in terms of', 'F1 - score and in accuracy']]",[],[],"[['Results', 'has', 'significant difference']]",[],[],[],[],[],sentence_compression,1,144
results,More than 30 % of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20 % of MIRA .,[],[],"[['More than 30 %', 'which is in', 'sharp contrast'], ['sharp contrast', 'with', '20 %'], ['20 %', 'of', 'MIRA'], ['More than 30 %', 'of', 'golden compressions'], ['golden compressions', 'could be', 'regenerated'], ['regenerated', 'by', 'LSTM systems']]",[],[],"[['Results', 'has', 'More than 30 %']]",[],[],[],[],[],sentence_compression,1,145
results,"The differences in F- score between the three versions of LSTM are not significant , all scores are close to 0.81 .","[('in', (2, 3)), ('between', (5, 6)), ('are', (11, 12)), ('close to', (18, 20))]","[('differences', (1, 2)), ('F- score', (3, 5)), ('three versions of LSTM', (7, 11)), ('not significant', (12, 14)), ('scores', (16, 17)), ('0.81', (20, 21))]","[['scores', 'close to', '0.81'], ['differences', 'in', 'F- score'], ['F- score', 'between', 'three versions of LSTM'], ['three versions of LSTM', 'are', 'not significant']]",[],[],"[['Results', 'has', 'scores'], ['Results', 'has', 'differences']]",[],[],[],[],[],sentence_compression,1,146
research-problem,Improving sentence compression by learning to predict gaze,[],"[('sentence compression', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentence compression']]",[],[],[],[],sentence_compression,2,2
model,We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,[],[],"[['eye - tracking recordings', 'to induce', 'better models'], ['better models', 'for', 'sentence compression'], ['sentence compression', 'for', 'text simplification']]",[],"[['Model', 'suggesting that', 'eye - tracking recordings']]",[],[],[],[],[],[],sentence_compression,2,12
model,"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .","[('show how to use', (3, 7)), ('to improve', (12, 14)), ('of', (16, 17)), ('for', (26, 27))]","[('existing eye - tracking recordings', (7, 12)), ('induction', (15, 16)), ('Long Short - Term Memory models ( LSTMs )', (17, 26)), ('sentence compression', (27, 29))]","[['existing eye - tracking recordings', 'to improve', 'induction'], ['induction', 'of', 'Long Short - Term Memory models ( LSTMs )'], ['Long Short - Term Memory models ( LSTMs )', 'for', 'sentence compression']]",[],"[['Model', 'show how to use', 'existing eye - tracking recordings']]",[],[],[],[],[],[],sentence_compression,2,13
model,Our proposed model does not require that the gaze data and the compression data come from the same source .,"[('does not require', (3, 6)), ('come from', (14, 16))]","[('Our proposed model', (0, 3)), ('gaze data and the compression data', (8, 14)), ('same source', (17, 19))]","[['Our proposed model', 'does not require', 'gaze data and the compression data'], ['gaze data and the compression data', 'come from', 'same source']]",[],[],"[['Model', 'has', 'Our proposed model']]",[],[],[],[],[],sentence_compression,2,14
model,"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .","[('use', (6, 7)), ('from', (9, 10)), ('of', (11, 12)), ('to improve', (15, 17)), ('on', (20, 21))]","[('gaze data', (7, 9)), ('readers', (10, 11)), ('Dundee Corpus', (13, 15)), ('sentence compression results', (17, 20)), ('several datasets', (21, 23))]","[['gaze data', 'to improve', 'sentence compression results'], ['sentence compression results', 'on', 'several datasets'], ['gaze data', 'from', 'readers'], ['readers', 'of', 'Dundee Corpus']]",[],"[['Model', 'use', 'gaze data']]",[],[],[],[],[],[],sentence_compression,2,15
model,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .","[('of', (8, 9)), ('in deriving', (12, 14)), ('that are', (17, 19)), ('for', (20, 21)), ('based on', (24, 26))]","[('intriguing potential', (6, 8)), ('this work', (9, 11)), ('sentence simplification models', (14, 17)), ('personalized', (19, 20)), ('individual users', (21, 23)), ('their reading behavior', (26, 29))]","[['intriguing potential', 'of', 'this work'], ['this work', 'in deriving', 'sentence simplification models'], ['sentence simplification models', 'that are', 'personalized'], ['personalized', 'based on', 'their reading behavior'], ['personalized', 'for', 'individual users']]",[],[],"[['Model', 'has', 'intriguing potential']]",[],[],[],[],[],sentence_compression,2,16
hyperparameters,Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .,"[('are', (6, 7)), ('trained for', (14, 16)), ('with', (18, 19))]","[('Both the baseline and our systems', (0, 6)), ('three - layer bi - LSTM models', (7, 14)), ('30 iterations', (16, 18)), ('pretrained ( SENNA ) embeddings', (19, 24))]","[['Both the baseline and our systems', 'are', 'three - layer bi - LSTM models'], ['three - layer bi - LSTM models', 'trained for', '30 iterations'], ['30 iterations', 'with', 'pretrained ( SENNA ) embeddings']]",[],[],"[['Hyperparameters', 'has', 'Both the baseline and our systems']]",[],[],[],[],[],sentence_compression,2,85
hyperparameters,"The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .","[('are', (5, 6)), ('predict', (15, 16)), ('of', (17, 18))]","[('input and hidden layers', (1, 5)), ('50 dimensions', (6, 8)), ('output layer', (12, 14)), ('sequences', (16, 17)), ('two labels', (18, 20))]","[['input and hidden layers', 'are', '50 dimensions'], ['output layer', 'predict', 'sequences'], ['sequences', 'of', 'two labels']]",[],[],"[['Hyperparameters', 'has', 'input and hidden layers'], ['Hyperparameters', 'has', 'output layer']]",[],[],[],[],[],sentence_compression,2,86
baselines,Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/,"[('is', (7, 8))]","[('BASELINE - LSTM', (3, 6)), ('multi - task learning', (9, 13))]","[['BASELINE - LSTM', 'is', 'multi - task learning']]",[],[],"[['Baselines', 'has', 'BASELINE - LSTM']]",[],[],[],[],[],sentence_compression,2,87
baselines,bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .,"[('predicting both', (2, 4)), ('at', (13, 14))]","[('CCG supertags and sentence compression', (4, 9)), ('outer layer', (15, 17))]","[['CCG supertags and sentence compression', 'at', 'outer layer']]",[],[],[],[],"[['multi - task learning', 'predicting both', 'CCG supertags and sentence compression']]",[],[],[],sentence_compression,2,88
results,"We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .","[('across', (3, 4)), ('including', (8, 9)), ('lead to', (17, 19)), ('over', (20, 21))]","[('all three datasets', (4, 7)), ('all three annotations of BROADCAST', (9, 14)), ('gaze features', (15, 17)), ('improvements', (19, 20)), ('baseline 3 - layer bi - LSTM', (22, 29))]","[['all three datasets', 'including', 'all three annotations of BROADCAST'], ['gaze features', 'lead to', 'improvements'], ['improvements', 'over', 'baseline 3 - layer bi - LSTM']]","[['all three datasets', 'has', 'gaze features']]","[['Results', 'across', 'all three datasets']]",[],[],[],[],[],[],sentence_compression,2,93
results,"Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .","[('is', (5, 6)), ('than', (8, 9))]","[('CASCADED - LSTM', (2, 5)), ('consistently better', (6, 8)), ('MULTITASK - LSTM', (9, 12))]","[['CASCADED - LSTM', 'is', 'consistently better'], ['consistently better', 'than', 'MULTITASK - LSTM']]",[],[],[],[],[],[],"[['all three datasets', 'has', 'CASCADED - LSTM']]",[],sentence_compression,2,94
results,"For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .","[('For', (0, 1)), ('of', (7, 8)), ('leads to', (24, 26)), ('over', (27, 28))]","[('all three datasets', (1, 4)), ('inclusion', (6, 7)), ('gaze measures', (8, 10)), ('first pass duration ( FP )', (11, 17)), ('regression duration ( Regr. )', (18, 23)), ('improvements', (26, 27)), ('baseline', (29, 30))]","[['inclusion', 'of', 'gaze measures'], ['gaze measures', 'leads to', 'improvements'], ['improvements', 'over', 'baseline']]","[['all three datasets', 'has', 'inclusion'], ['gaze measures', 'name', 'first pass duration ( FP )'], ['gaze measures', 'name', 'regression duration ( Regr. )']]","[['Results', 'For', 'all three datasets']]",[],[],[],[],[],[],sentence_compression,2,95
results,"With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .","[('With', (0, 1)), ('impact of', (6, 8)), ('becomes', (11, 12)), ('favouring', (15, 16)), ('with', (21, 22)), ('using', (23, 24))]","[('harder datasets', (2, 4)), ('gaze information', (9, 11)), ('stronger', (12, 13)), ('cascaded architecture', (17, 19)), ('improvements', (22, 23)), ('first pass duration', (25, 28)), ('regression duration', (29, 31))]","[['harder datasets', 'impact of', 'gaze information'], ['gaze information', 'with', 'improvements'], ['improvements', 'using', 'first pass duration'], ['improvements', 'using', 'regression duration'], ['gaze information', 'favouring', 'cascaded architecture'], ['gaze information', 'becomes', 'stronger']]",[],"[['Results', 'With', 'harder datasets']]",[],[],[],[],[],[],sentence_compression,2,100
research-problem,A Language Model based Evaluator for Sentence Compression,[],"[('Sentence Compression', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Compression']]",[],[],[],[],sentence_compression,3,2
research-problem,"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .",[],"[('deletion - based sentence compression', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'deletion - based sentence compression']]",[],[],[],[],sentence_compression,3,4
model,"To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .","[('trained on', (14, 16)), ('as', (20, 21))]","[('syntax - based neural language model', (7, 13)), ('large - scale datasets', (16, 20)), ('readability evaluator', (22, 24))]","[['syntax - based neural language model', 'trained on', 'large - scale datasets'], ['large - scale datasets', 'as', 'readability evaluator']]",[],[],"[['Model', 'has', 'syntax - based neural language model']]",[],[],[],[],[],sentence_compression,3,19
model,The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .,"[('to learn', (6, 8)), ('in terms of', (12, 15))]","[('neural language model', (1, 4)), ('correct word collocations', (9, 12)), ('syntax', (16, 17)), ('semantics', (18, 19))]","[['neural language model', 'to learn', 'correct word collocations'], ['correct word collocations', 'in terms of', 'syntax'], ['correct word collocations', 'in terms of', 'semantics']]",[],[],"[['Model', 'has', 'neural language model']]",[],[],[],[],[],sentence_compression,3,20
model,"Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .","[('formulate', (3, 4)), ('series of', (10, 12)), ('through', (17, 18))]","[('deletionbased sentence compression', (5, 8)), ('trialand - error deletion operations', (12, 17)), ('reinforcement learning framework', (19, 22))]","[['deletionbased sentence compression', 'series of', 'trialand - error deletion operations'], ['trialand - error deletion operations', 'through', 'reinforcement learning framework']]",[],"[['Model', 'formulate', 'deletionbased sentence compression']]",[],[],[],[],[],[],sentence_compression,3,21
model,"The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .","[('performs either', (3, 5)), ('to form', (9, 11)), ('receives', (15, 16)), ('to update', (24, 26))]","[('policy network', (1, 3)), ('RETAIN', (5, 6)), ('REMOVE', (7, 8)), ('compression', (12, 13)), ('reward ( e.g. , readability score )', (17, 24)), ('network', (27, 28))]","[['policy network', 'to form', 'compression'], ['compression', 'performs either', 'RETAIN'], ['compression', 'performs either', 'REMOVE'], ['policy network', 'receives', 'reward ( e.g. , readability score )'], ['reward ( e.g. , readability score )', 'to update', 'network']]",[],[],"[['Model', 'has', 'policy network']]",[],[],[],[],[],sentence_compression,3,22
baselines,We choose several strong baselines ; the first one is the dependency - tree - based method that considers the sentence compression task as an optimization problem by using integer linear programming 5 .,"[('considers', (18, 19)), ('as', (23, 24)), ('by using', (27, 29))]","[('dependency - tree - based method', (11, 17)), ('sentence compression task', (20, 23)), ('optimization problem', (25, 27)), ('integer linear programming', (29, 32))]","[['dependency - tree - based method', 'considers', 'sentence compression task'], ['sentence compression task', 'as', 'optimization problem'], ['optimization problem', 'by using', 'integer linear programming']]",[],[],"[['Baselines', 'has', 'dependency - tree - based method']]",[],[],[],[],[],sentence_compression,3,72
baselines,The second method is the long short - term memory networks ( LSTMs ) which showed strong promise in sentence compression by .,"[('showed', (15, 16)), ('in', (18, 19))]","[('long short - term memory networks ( LSTMs )', (5, 14)), ('strong promise', (16, 18)), ('sentence compression', (19, 21))]","[['long short - term memory networks ( LSTMs )', 'showed', 'strong promise'], ['strong promise', 'in', 'sentence compression']]",[],[],"[['Baselines', 'has', 'long short - term memory networks ( LSTMs )']]",[],[],[],[],[],sentence_compression,3,82
hyperparameters,"The embedding size for word , part - of - speech tag , and the dependency relation is 128 .","[('for', (3, 4)), ('is', (17, 18))]","[('embedding size', (1, 3)), ('word', (4, 5)), ('part - of - speech tag', (6, 12)), ('dependency relation', (15, 17)), ('128', (18, 19))]","[['embedding size', 'for', 'word'], ['embedding size', 'for', 'part - of - speech tag'], ['embedding size', 'for', 'dependency relation'], ['embedding size', 'is', '128']]",[],[],"[['Hyperparameters', 'has', 'embedding size']]",[],[],[],[],[],sentence_compression,3,90
hyperparameters,We employed the vanilla RNN with a hidden size of 512 for both the policy network and neural language model .,"[('employed', (1, 2)), ('with', (5, 6)), ('of', (9, 10)), ('for', (11, 12))]","[('vanilla RNN', (3, 5)), ('hidden size', (7, 9)), ('512', (10, 11)), ('policy network', (14, 16)), ('neural language model', (17, 20))]","[['vanilla RNN', 'with', 'hidden size'], ['hidden size', 'of', '512'], ['hidden size', 'for', 'policy network'], ['hidden size', 'for', 'neural language model']]",[],"[['Hyperparameters', 'employed', 'vanilla RNN']]",[],[],[],[],[],[],sentence_compression,3,91
hyperparameters,"The mini - batch size was chosen from [ 5 , 50 , 100 ] .","[('chosen from', (6, 8))]","[('mini - batch size', (1, 5)), ('[ 5 , 50 , 100 ]', (8, 15))]","[['mini - batch size', 'chosen from', '[ 5 , 50 , 100 ]']]",[],[],"[['Hyperparameters', 'has', 'mini - batch size']]",[],[],[],[],[],sentence_compression,3,92
hyperparameters,"Vocabulary size was 50,000 .","[('was', (2, 3))]","[('Vocabulary size', (0, 2)), ('50,000', (3, 4))]","[['Vocabulary size', 'was', '50,000']]",[],[],"[['Hyperparameters', 'has', 'Vocabulary size']]",[],[],[],[],[],sentence_compression,3,93
hyperparameters,"The learning rate for neural language model is 2.5 e - 4 , and 1e - 05 for the policy network .",[],[],"[['learning rate', 'for', 'neural language model'], ['neural language model', 'is', '2.5 e - 4'], ['1e - 05', 'for', 'policy network']]","[['learning rate', 'has', '1e - 05']]",[],"[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],sentence_compression,3,94
hyperparameters,"For policy learning , we used the REINFORCE algorithm to update the parameters of the policy network and find an policy that maximizes the reward .","[('For', (0, 1)), ('used', (5, 6)), ('to update', (9, 11)), ('of', (13, 14)), ('find', (18, 19)), ('maximizes', (22, 23))]","[('policy learning', (1, 3)), ('REINFORCE algorithm', (7, 9)), ('parameters', (12, 13)), ('policy network', (15, 17)), ('policy', (20, 21)), ('reward', (24, 25))]","[['policy learning', 'find', 'policy'], ['policy', 'maximizes', 'reward'], ['policy learning', 'used', 'REINFORCE algorithm'], ['REINFORCE algorithm', 'to update', 'parameters'], ['parameters', 'of', 'policy network']]",[],"[['Hyperparameters', 'For', 'policy learning']]",[],[],[],[],[],[],sentence_compression,3,95
code,6 https://github.com/code4conference/code4sc,[],"[('https://github.com/code4conference/code4sc', (1, 2))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/code4conference/code4sc']]",[],[],[],[],sentence_compression,3,99
results,"( 1 ) As shown in , our Evaluator - SLMbased method yields a large improvement over the baselines , demonstrating that the language - modelbased evaluator is effective as a post-hoc grammar checker for the compressed sentences .","[('yields', (12, 13)), ('over', (16, 17))]","[('Evaluator - SLMbased method', (8, 12)), ('large improvement', (14, 16)), ('baselines', (18, 19))]","[['Evaluator - SLMbased method', 'yields', 'large improvement'], ['large improvement', 'over', 'baselines']]",[],[],"[['Results', 'has', 'Evaluator - SLMbased method']]",[],[],[],[],[],sentence_compression,3,107
results,"( 3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .","[('for', (4, 5)), ('is', (19, 20)), ('incorporating', (27, 28))]","[('Google news dataset', (5, 8)), ('LSTMs ( LSTM + pos+dep )', (9, 15)), ('relatively strong baseline', (21, 24)), ('dependency relations', (28, 30)), ('part - of - speech tags', (31, 37))]","[['LSTMs ( LSTM + pos+dep )', 'is', 'relatively strong baseline'], ['relatively strong baseline', 'incorporating', 'dependency relations'], ['relatively strong baseline', 'incorporating', 'part - of - speech tags']]","[['Google news dataset', 'has', 'LSTMs ( LSTM + pos+dep )']]","[['Results', 'for', 'Google news dataset']]",[],[],[],[],[],[],sentence_compression,3,109
results,"When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .","[('applying', (2, 3)), ('observed', (12, 13))]","[('Evaluator - SLM', (3, 6)), ('only a tiny improvement', (7, 11))]","[['Evaluator - SLM', 'observed', 'only a tiny improvement']]",[],"[['Results', 'applying', 'Evaluator - SLM']]",[],[],[],[],[],[],sentence_compression,3,110
results,"For Gigaword dataset with 1.02 million instances , the perplexity of the language model is 20.3 , while for the Google news dataset with 0.2 million instances , the perplexity is 76.5 .",[],[],"[['perplexity', 'is', '76.5'], ['Gigaword dataset', 'with', '1.02 million instances'], ['perplexity', 'of', 'language model'], ['language model', 'is', '20.3']]","[['Gigaword dataset', 'has', 'perplexity']]","[['Results', 'For', 'Gigaword dataset']]",[],[],"[['Google news dataset', 'with', '0.2 million instances']]",[],"[['Google news dataset', 'has', 'perplexity']]",[],sentence_compression,3,112
results,"The results shows that small improvements are observed on two datasets ( # 4 vs # 5 ; & 4 vs & 5 ) , suggesting that incorporating syntactic knowledge may help evaluator to encourage more unseen but reasonable word collocations .","[('shows that', (2, 4)), ('observed on', (7, 9))]","[('small improvements', (4, 6)), ('two datasets', (9, 11))]","[['small improvements', 'observed on', 'two datasets']]",[],"[['Results', 'shows that', 'small improvements']]",[],[],[],[],[],[],sentence_compression,3,114
research-problem,MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT,[],"[('SPEECH EMOTION RECOGNITION', (1, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'SPEECH EMOTION RECOGNITION']]",[],[],[],[],sentiment_analysis,0,2
model,"To overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .","[('uses', (10, 11)), ('as well as', (17, 20)), ('to utilize', (26, 28)), ('contained within', (30, 32))]","[('high - level text transcription', (11, 16)), ('low - level audio signals', (20, 25)), ('information', (29, 30)), ('low - resource datasets', (32, 36))]","[['high - level text transcription', 'to utilize', 'information'], ['information', 'contained within', 'low - resource datasets'], ['high - level text transcription', 'as well as', 'low - level audio signals']]",[],"[['Model', 'uses', 'high - level text transcription']]",[],[],[],[],[],[],sentiment_analysis,0,23
model,"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .","[('propose', (5, 6)), ('simultaneously utilizes', (14, 16)), ('in recognizing', (20, 22)), ('from', (23, 24))]","[('novel deep dual recurrent encoder model', (7, 13)), ('audio and text data', (16, 20)), ('emotions', (22, 23)), ('speech', (24, 25))]","[['novel deep dual recurrent encoder model', 'simultaneously utilizes', 'audio and text data'], ['audio and text data', 'in recognizing', 'emotions'], ['emotions', 'from', 'speech']]",[],"[['Model', 'propose', 'novel deep dual recurrent encoder model']]",[],[],[],[],[],[],sentiment_analysis,0,27
hyperparameters,"Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .","[('of', (3, 4)), ('use', (9, 10)), ('as they yield', (11, 14)), ('to that of', (16, 19)), ('include', (22, 23))]","[('GRUs', (10, 11)), ('comparable performance', (14, 16)), ('LSTM', (20, 21)), ('smaller number', (24, 26)), ('weight parameters', (27, 29))]","[['GRUs', 'include', 'smaller number'], ['smaller number', 'of', 'weight parameters'], ['GRUs', 'as they yield', 'comparable performance'], ['comparable performance', 'to that of', 'LSTM']]",[],"[['Hyperparameters', 'use', 'GRUs']]",[],[],[],[],[],[],sentiment_analysis,0,123
hyperparameters,"We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .",[],[],"[['max encoder step', 'of', '750'], ['750', 'for', 'audio input'], ['max encoder step', 'of', '128'], ['128', 'for', 'text input'], ['text input', 'covers', 'maximum length'], ['maximum length', 'of', 'transcripts']]",[],[],"[['Hyperparameters', 'use', 'max encoder step']]",[],[],[],[],[],sentiment_analysis,0,124
hyperparameters,"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .","[('of', (3, 4)), ('is', (6, 7)), ('including', (9, 10)), ('represents', (17, 18)), ('used to indicate', (30, 33)), ('added while preparing', (35, 38))]","[('vocabulary size', (1, 3)), ('dataset', (5, 6)), ('3,747', (7, 8)), ('"" UNK "" token', (11, 15)), ('unknown words', (18, 20)), ('"" PAD "" token', (23, 27)), ('padding information', (33, 35)), ('mini-batch data', (38, 40))]","[['vocabulary size', 'of', 'dataset'], ['dataset', 'is', '3,747'], ['3,747', 'including', '"" PAD "" token'], ['"" PAD "" token', 'used to indicate', 'padding information'], ['padding information', 'added while preparing', 'mini-batch data'], ['3,747', 'including', '"" UNK "" token'], ['"" UNK "" token', 'represents', 'unknown words']]",[],[],"[['Hyperparameters', 'has', 'vocabulary size']]",[],[],[],[],[],sentiment_analysis,0,125
hyperparameters,"The number of hidden units and the number of layers in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .","[('in', (10, 11)), ('for', (13, 14)), ('selected based on', (26, 29))]","[('number of hidden units and the number of layers', (1, 10)), ('RNN', (12, 13)), ('each model ( ARE , TRE , MDRE and MDREA )', (14, 25)), ('extensive hyperparameter search experiments', (29, 33))]","[['number of hidden units and the number of layers', 'in', 'RNN'], ['RNN', 'for', 'each model ( ARE , TRE , MDRE and MDREA )'], ['each model ( ARE , TRE , MDRE and MDREA )', 'selected based on', 'extensive hyperparameter search experiments']]",[],[],"[['Hyperparameters', 'has', 'number of hidden units and the number of layers']]",[],[],[],[],[],sentiment_analysis,0,126
,The weights of the hidden units are initialized using orthogonal ,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,0,127
hyperparameters,"weights ] , and the text embedding layer is initialized from pretrained word - embedding vectors .","[('initialized from', (9, 11))]","[('text embedding layer', (5, 8)), ('pretrained word - embedding vectors', (11, 16))]","[['text embedding layer', 'initialized from', 'pretrained word - embedding vectors']]",[],[],"[['Hyperparameters', 'has', 'text embedding layer']]",[],[],[],[],[],sentiment_analysis,0,133
hyperparameters,"In preparing the textual dataset , we first use the released transcripts of the IEMOCAP dataset for simplicity .","[('of', (12, 13)), ('for', (16, 17))]","[('released transcripts', (10, 12)), ('IEMOCAP dataset', (14, 16)), ('simplicity', (17, 18))]","[['released transcripts', 'of', 'IEMOCAP dataset'], ['IEMOCAP dataset', 'for', 'simplicity']]",[],[],"[['Hyperparameters', 'use', 'released transcripts']]",[],[],[],[],[],sentiment_analysis,0,134
results,"First , our ARE model shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .","[('shows', (5, 6)), ('use', (11, 12)), ('such as', (16, 18)), ('with', (23, 24))]","[('ARE model', (3, 5)), ('baseline performance', (7, 9)), ('minimal audio features', (12, 15)), ('MFCC and prosodic features', (19, 23)), ('simple architectures', (24, 26))]","[['ARE model', 'shows', 'baseline performance'], ['baseline performance', 'use', 'minimal audio features'], ['minimal audio features', 'such as', 'MFCC and prosodic features'], ['MFCC and prosodic features', 'with', 'simple architectures']]",[],[],"[['Results', 'has', 'ARE model']]",[],[],[],[],[],sentiment_analysis,0,144
results,"On the other hand , the TRE model shows higher performance gain compared to the ARE .","[('shows', (8, 9)), ('compared to', (12, 14))]","[('TRE model', (6, 8)), ('higher performance gain', (9, 12)), ('ARE', (15, 16))]","[['TRE model', 'shows', 'higher performance gain'], ['higher performance gain', 'compared to', 'ARE']]",[],[],"[['Results', 'has', 'TRE model']]",[],[],[],[],[],sentiment_analysis,0,145
results,"From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .","[('note', (5, 6)), ('are', (9, 10)), ('in', (11, 12)), ('is', (21, 22)), ('in understanding', (23, 25))]","[('textual data', (7, 9)), ('informative', (10, 11)), ('emotion prediction tasks', (12, 15)), ('recurrent encoder model', (18, 21)), ('effective', (22, 23)), ('types of sequential data', (26, 30))]","[['recurrent encoder model', 'is', 'effective'], ['effective', 'in understanding', 'types of sequential data'], ['textual data', 'are', 'informative'], ['informative', 'in', 'emotion prediction tasks']]",[],"[['Results', 'note', 'recurrent encoder model'], ['Results', 'note', 'textual data']]",[],[],[],[],[],[],sentiment_analysis,0,146
results,"Second , the newly proposed model , MDRE , shows a substantial performance gain .","[('shows', (9, 10))]","[('MDRE', (7, 8)), ('substantial performance gain', (11, 14))]","[['MDRE', 'shows', 'substantial performance gain']]",[],[],"[['Results', 'has', 'MDRE']]",[],[],[],[],[],sentiment_analysis,0,147
results,It thus achieves the state - of - the - art performance with a WAP value of 0.718 .,"[('achieves', (2, 3)), ('with', (12, 13)), ('of', (16, 17))]","[('state - of - the - art performance', (4, 12)), ('WAP value', (14, 16)), ('0.718', (17, 18))]","[['state - of - the - art performance', 'with', 'WAP value'], ['WAP value', 'of', '0.718']]",[],[],[],[],"[['MDRE', 'achieves', 'state - of - the - art performance']]",[],[],[],sentiment_analysis,0,148
results,"Lastly , the attention model , MDREA , also outperforms the best existing research results ( WAP 0.690 to 0.688 ) .","[('outperforms', (9, 10))]","[('MDREA', (6, 7)), ('best existing research results ( WAP 0.690 to 0.688 )', (11, 21))]","[['MDREA', 'outperforms', 'best existing research results ( WAP 0.690 to 0.688 )']]",[],[],"[['Results', 'has', 'MDREA']]",[],[],[],[],[],sentiment_analysis,0,150
results,The label accuracy of the processed transcripts is 5.53 % WER .,"[('of', (3, 4)), ('is', (7, 8))]","[('label accuracy', (1, 3)), ('processed transcripts', (5, 7)), ('5.53 % WER', (8, 11))]","[['label accuracy', 'of', 'processed transcripts'], ['processed transcripts', 'is', '5.53 % WER']]",[],[],"[['Results', 'has', 'label accuracy']]",[],[],[],[],[],sentiment_analysis,0,156
results,"The TRE - ASR , MDRE - ASR and MDREA - ASR models reflect degraded performance compared to that of the TRE , MDRE and MDREA models .","[('reflect', (13, 14)), ('compared to', (16, 18))]","[('TRE - ASR , MDRE - ASR and MDREA - ASR models', (1, 13)), ('degraded performance', (14, 16)), ('TRE , MDRE and MDREA models', (21, 27))]","[['TRE - ASR , MDRE - ASR and MDREA - ASR models', 'reflect', 'degraded performance'], ['degraded performance', 'compared to', 'TRE , MDRE and MDREA models']]",[],[],"[['Results', 'has', 'TRE - ASR , MDRE - ASR and MDREA - ASR models']]",[],[],[],[],[],sentiment_analysis,0,157
ablation-analysis,"The ARE model ( ) incorrectly classifies most instances of happy as neutral ( 43.51 % ) ; thus , it shows reduced accuracy ( 35.15 % ) in predicting the the happy class .","[('incorrectly classifies', (5, 7)), ('as', (11, 12))]","[('ARE model', (1, 3)), ('most instances of happy', (7, 11)), ('neutral ( 43.51 % )', (12, 17))]","[['ARE model', 'incorrectly classifies', 'most instances of happy'], ['most instances of happy', 'as', 'neutral ( 43.51 % )']]",[],[],"[['Ablation analysis', 'has', 'ARE model']]",[],[],[],[],[],sentiment_analysis,0,162
ablation-analysis,"Overall , most of the emotion classes are frequently confused with the neutral class .","[('frequently confused with', (8, 11))]","[('emotion classes', (5, 7)), ('neutral class', (12, 14))]","[['emotion classes', 'frequently confused with', 'neutral class']]",[],[],[],[],[],[],"[['ARE model', 'has', 'emotion classes']]",[],sentiment_analysis,0,163
ablation-analysis,"Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .",[],[],"[['TRE model', 'shows', 'greater prediction gains'], ['greater prediction gains', 'in predicting', 'happy class']]",[],[],"[['Ablation analysis', 'has', 'TRE model']]",[],[],"[['happy class', 'compared to', 'ARE model ( 35.15 % to 75. 73 % ) ']]",[],[],sentiment_analysis,0,165
ablation-analysis,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,"[('compensates for', (4, 6)), ('of', (8, 9)), ('benefits from', (19, 21)), ('to', (23, 24))]","[('MDRE model', (1, 3)), ('weaknesses', (7, 8)), ('previous two models ( ARE and TRE )', (10, 18)), ('strengths', (22, 23)), ('surprising degree', (25, 27))]","[['MDRE model', 'benefits from', 'strengths'], ['strengths', 'to', 'surprising degree'], ['MDRE model', 'compensates for', 'weaknesses'], ['weaknesses', 'of', 'previous two models ( ARE and TRE )']]",[],[],"[['Ablation analysis', 'has', 'MDRE model']]",[],[],[],[],[],sentiment_analysis,0,168
ablation-analysis,"Furthermore , the occurrence of the incorrect "" sad - to - happy "" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .","[('of', (4, 5)), ('in', (15, 16)), ('reduced from', (20, 22))]","[('occurrence', (3, 4)), ('incorrect "" sad - to - happy "" cases', (6, 15)), ('TRE model', (17, 19)), ('16 . 20 % to 9.15 %', (22, 29))]","[['occurrence', 'of', 'incorrect "" sad - to - happy "" cases'], ['incorrect "" sad - to - happy "" cases', 'in', 'TRE model'], ['TRE model', 'reduced from', '16 . 20 % to 9.15 %']]",[],[],[],[],[],[],"[['MDRE model', 'has', 'occurrence']]",[],sentiment_analysis,0,170
research-problem,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,[],"[('EEG - Based Emotion Recognition', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'EEG - Based Emotion Recognition']]",[],[],[],[],sentiment_analysis,1,2
research-problem,"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .",[],"[('E MOTION recognition', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'E MOTION recognition']]",[],[],[],[],sentiment_analysis,1,15
model,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .","[('propose', (5, 6))]","[('regularized graph neural network ( RGNN )', (7, 14))]",[],[],"[['Model', 'propose', 'regularized graph neural network ( RGNN )']]",[],[],[],[],[],[],sentiment_analysis,1,49
model,"Inspired by , , we consider each channel in EEG signals as a node in our graph .",[],[],"[['each channel', 'in', 'EEG signals'], ['EEG signals', 'as', 'node'], ['node', 'in', 'our graph']]",[],"[['Model', 'consider', 'each channel']]",[],[],[],[],[],[],sentiment_analysis,1,52
model,"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .","[('extends', (3, 4)), ('leverages', (13, 14)), ('of', (17, 18)), ('according to', (23, 25)), ('propose', (33, 34)), ('to capture', (40, 42))]","[('RGNN model', (1, 3)), ('simple graph convolution network ( SGC )', (5, 12)), ('topological structure', (15, 17)), ('EEG signals', (18, 20)), ('economy of brain network organization', (26, 31)), ('biologically supported sparse adjacency matrix', (35, 40)), ('local and global inter-channel relations', (43, 48))]","[['RGNN model', 'extends', 'simple graph convolution network ( SGC )'], ['RGNN model', 'leverages', 'topological structure'], ['topological structure', 'of', 'EEG signals'], ['EEG signals', 'according to', 'economy of brain network organization'], ['economy of brain network organization', 'propose', 'biologically supported sparse adjacency matrix'], ['biologically supported sparse adjacency matrix', 'to capture', 'local and global inter-channel relations']]",[],[],"[['Model', 'has', 'RGNN model']]",[],[],[],[],[],sentiment_analysis,1,53
model,"Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .","[('connect', (3, 4)), ('reveal', (10, 11)), ('at', (13, 14))]","[('Local interchannel relations', (0, 3)), ('nearby groups of neurons', (4, 8)), ('anatomical connectivity', (11, 13)), ('macroscale', (14, 15))]","[['Local interchannel relations', 'reveal', 'anatomical connectivity'], ['anatomical connectivity', 'at', 'macroscale'], ['Local interchannel relations', 'connect', 'nearby groups of neurons']]",[],[],"[['Model', 'has', 'Local interchannel relations']]",[],[],[],[],[],sentiment_analysis,1,54
model,"Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .","[('connect', (3, 4)), ('between', (8, 9)), ('may reveal', (15, 17))]","[('Global inter-channel relations', (0, 3)), ('distant groups of neurons', (4, 8)), ('left and right hemispheres', (10, 14)), ('emotion - related functional connectivity', (17, 22))]","[['Global inter-channel relations', 'connect', 'distant groups of neurons'], ['distant groups of neurons', 'between', 'left and right hemispheres'], ['Global inter-channel relations', 'may reveal', 'emotion - related functional connectivity']]",[],[],"[['Model', 'has', 'Global inter-channel relations']]",[],[],[],[],[],sentiment_analysis,1,55
hyperparameters,"For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .",[],[],"[['RGNN', 'empirically', 'set'], ['number', 'of', 'convolutional layers L = 2'], ['batch size', 'of', '16'], ['dropout rate', 'of', '0.7'], ['0.7', 'at', 'output fully - connected layer']]","[['set', 'has', 'number'], ['set', 'has', 'batch size'], ['set', 'has', 'dropout rate']]",[],"[['Hyperparameters', 'has', 'RGNN']]",[],[],[],[],[],sentiment_analysis,1,312
hyperparameters,"We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .","[('use', (1, 2)), ('with', (4, 5)), ('i.e.', (8, 9))]","[('Adam optimization', (2, 4)), ('default values', (5, 7)), ('? 1 = 0.9 and ? 2 = 0.999', (10, 19))]","[['Adam optimization', 'with', 'default values'], ['default values', 'i.e.', '? 1 = 0.9 and ? 2 = 0.999']]",[],"[['Hyperparameters', 'use', 'Adam optimization']]",[],[],[],[],[],[],sentiment_analysis,1,313
hyperparameters,"We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .","[('tune', (2, 3))]","[('output feature dimension d', (4, 8)), ('label noise level', (9, 12)), ('learning rate ?', (13, 16)), ('L1 regularization factor ?', (17, 21)), ('L2 regularization', (23, 25))]",[],[],"[['Hyperparameters', 'tune', 'output feature dimension d'], ['Hyperparameters', 'tune', 'label noise level'], ['Hyperparameters', 'tune', 'learning rate ?'], ['Hyperparameters', 'tune', 'L1 regularization factor ?'], ['Hyperparameters', 'tune', 'L2 regularization']]",[],[],[],[],[],[],sentiment_analysis,1,314
results,It is encouraging to see that our model achieves superior performance on both datasets as compared to all baselines including the stateof - the - art BiHDM when DE features from all frequency bands are used .,"[('achieves', (8, 9)), ('on', (11, 12)), ('compared to', (15, 17)), ('including', (19, 20))]","[('our model', (6, 8)), ('superior performance', (9, 11)), ('both datasets', (12, 14)), ('all baselines', (17, 19)), ('stateof - the - art BiHDM when DE features from all frequency bands are used', (21, 36))]","[['our model', 'achieves', 'superior performance'], ['superior performance', 'on', 'both datasets'], ['both datasets', 'compared to', 'all baselines'], ['all baselines', 'including', 'stateof - the - art BiHDM when DE features from all frequency bands are used']]",[],[],"[['Results', 'has', 'our model']]",[],[],[],[],[],sentiment_analysis,1,322
results,It is worth noting that our model improves the accuracy of the state - of - the - art model on SEED - IV by around 5 % .,"[('improves', (7, 8)), ('of', (10, 11)), ('on', (20, 21)), ('by', (24, 25))]","[('accuracy', (9, 10)), ('state - of - the - art model', (12, 20)), ('SEED - IV', (21, 24)), ('around 5 %', (25, 28))]","[['accuracy', 'of', 'state - of - the - art model'], ['state - of - the - art model', 'on', 'SEED - IV'], ['SEED - IV', 'by', 'around 5 %']]",[],[],[],[],"[['our model', 'improves', 'accuracy']]",[],[],[],sentiment_analysis,1,323
results,"In particular , our model performs better than DGCNN , which is another GNN - based model that leverages the topological structure in EEG signals .","[('performs', (5, 6)), ('than', (7, 8)), ('another', (12, 13)), ('leverages', (18, 19)), ('in', (22, 23))]","[('better', (6, 7)), ('DGCNN', (8, 9)), ('GNN - based model', (13, 17)), ('topological structure', (20, 22)), ('EEG signals', (23, 25))]","[['better', 'than', 'DGCNN'], ['DGCNN', 'another', 'GNN - based model'], ['GNN - based model', 'leverages', 'topological structure'], ['topological structure', 'in', 'EEG signals']]",[],[],[],[],"[['our model', 'performs', 'better']]",[],[],[],sentiment_analysis,1,324
results,"In subject - dependent experiments on SEED , STRNN achieves the highest accuracy in delta , theta and alpha bands , BiDANN performs best in beta band , and our model performs best in gamma band .",[],[],"[['our model', 'performs', 'best'], ['subject - dependent experiments', 'on', 'SEED'], ['BiDANN', 'performs', 'best'], ['best', 'in', 'beta band'], ['STRNN', 'achieves', 'highest accuracy'], ['highest accuracy', 'in', 'delta , theta and alpha bands'], ['our model', 'performs', 'best'], ['best', 'in', 'gamma band']]","[['SEED', 'has', 'our model'], ['SEED', 'has', 'BiDANN'], ['SEED', 'has', 'STRNN'], ['SEED', 'has', 'our model'], ['SEED', 'has', 'our model']]","[['Results', 'In', 'subject - dependent experiments']]",[],[],[],[],[],[],sentiment_analysis,1,329
results,"In subject - independent experiments on SEED , BiDANN - S achieves the highest accuracy in theta and alpha bands , and our model performs best in delta , beta and gamma bands .",[],[],"[['subject - independent experiments', 'on', 'SEED'], ['BiDANN - S', 'achieves', 'highest accuracy'], ['highest accuracy', 'in', 'theta and alpha bands'], ['best', 'in', 'delta , beta and gamma bands']]","[['SEED', 'has', 'BiDANN - S']]",[],"[['Results', 'In', 'subject - independent experiments']]",[],[],[],[],[],sentiment_analysis,1,330
results,"For both subject - dependent and subjectindependent settings on SEED , we compare the performance of each model across different frequency bands .","[('For', (0, 1)), ('on', (8, 9))]","[('subject - dependent and subjectindependent settings', (2, 8)), ('SEED', (9, 10))]","[['subject - dependent and subjectindependent settings', 'on', 'SEED']]",[],"[['Results', 'For', 'subject - dependent and subjectindependent settings']]",[],[],[],[],[],[],sentiment_analysis,1,332
results,"In general , most models including our model achieve better performance on beta and gamma bands than delta , theta and alpha bands , with one exception of STRNN , which performs the worst on gamma band .",[],[],"[['our model', 'achieve', 'better performance'], ['better performance', 'with one exception of', 'STRNN'], ['STRNN', 'performs', 'worst'], ['worst', 'on', 'gamma band'], ['better performance', 'on', 'beta and gamma bands'], ['beta and gamma bands', 'than', 'delta , theta and alpha bands']]",[],[],[],[],[],[],[],[],sentiment_analysis,1,333
results,"One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .","[('in', (16, 17)), ('than', (19, 20))]","[('consistently better', (14, 16)), ('gamma band', (17, 19)), ('beta band', (20, 22))]","[['consistently better', 'in', 'gamma band'], ['gamma band', 'than', 'beta band']]",[],[],[],[],[],[],"[['our model', 'performs', 'consistently better']]",[],sentiment_analysis,1,335
ablation-analysis,"The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .","[('in', (4, 5)), ('i.e.', (10, 11)), ('helpful in', (21, 23))]","[('two major designs', (1, 4)), ('adjacency matrix A', (6, 9)), ('global connection and symmetric adjacency matrix designs', (12, 19)), ('recognizing emotions', (23, 25))]","[['two major designs', 'in', 'adjacency matrix A'], ['adjacency matrix A', 'helpful in', 'recognizing emotions'], ['adjacency matrix A', 'i.e.', 'global connection and symmetric adjacency matrix designs']]",[],[],"[['Ablation analysis', 'has', 'two major designs']]",[],[],[],[],[],sentiment_analysis,1,349
ablation-analysis,"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .","[('models', (3, 4)), ('between', (7, 8)), ('in', (10, 11)), ('shown to reveal', (19, 22))]","[('global connection', (1, 3)), ('asymmetric difference', (5, 7)), ('neuronal activities', (8, 10)), ('left and right hemispheres', (12, 16)), ('certain emotions', (22, 24))]","[['global connection', 'models', 'asymmetric difference'], ['asymmetric difference', 'between', 'neuronal activities'], ['neuronal activities', 'in', 'left and right hemispheres'], ['global connection', 'shown to reveal', 'certain emotions']]",[],[],"[['Ablation analysis', 'has', 'global connection']]",[],[],[],[],[],sentiment_analysis,1,350
ablation-analysis,"Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .","[('on', (8, 9)), ('of', (11, 12))]","[('NodeDAT regularizer', (1, 3)), ('noticeable positive impact', (5, 8)), ('performance', (10, 11)), ('our model', (12, 14))]","[['noticeable positive impact', 'on', 'performance'], ['performance', 'of', 'our model'], ['performance', 'of', 'our model'], ['performance', 'of', 'our model']]","[['NodeDAT regularizer', 'has', 'noticeable positive impact']]",[],"[['Ablation analysis', 'has', 'NodeDAT regularizer']]",[],[],[],[],[],sentiment_analysis,1,352
ablation-analysis,"In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .",[],[],"[['NodeDAT', 'is', 'removed']]","[['removed', 'has', 'performance'], ['our model', 'has', 'greater variance']]","[['Ablation analysis', 'if', 'NodeDAT']]",[],[],[],[],[],[],sentiment_analysis,1,355
ablation-analysis,DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .,[],[],"[['DL regularizer', 'improves', 'performance'], ['our model', 'by', 'around 3 %'], ['around 3 %', 'in', 'accuracy'], ['accuracy', 'on', 'both datasets']]",[],[],"[['Ablation analysis', 'has', 'DL regularizer']]",[],[],[],[],[],sentiment_analysis,1,357
research-problem,DialogueRNN : An Attentive RNN for Emotion Detection in Conversations,[],"[('Emotion Detection in Conversations', (6, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion Detection in Conversations']]",[],[],[],[],sentiment_analysis,10,2
model,Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,"[('employs', (4, 5))]","[('DialogueRNN system', (2, 4)), ('three gated recurrent units ( GRU )', (5, 12))]","[['DialogueRNN system', 'employs', 'three gated recurrent units ( GRU )']]",[],[],"[['Model', 'has', 'DialogueRNN system']]",[],[],[],[],[],sentiment_analysis,10,18
model,"The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .","[('fed into', (4, 6)), ('called', (8, 9))]","[('incoming utterance', (1, 3)), ('two GRUs', (6, 8)), ('global GRU and party GRU', (9, 14))]","[['incoming utterance', 'fed into', 'two GRUs'], ['two GRUs', 'called', 'global GRU and party GRU']]",[],[],"[['Model', 'has', 'incoming utterance']]",[],[],[],[],[],sentiment_analysis,10,19
model,The global GRU encodes corresponding party information while encoding an utterance .,"[('encodes', (3, 4)), ('while encoding', (7, 9))]","[('global GRU', (1, 3)), ('corresponding party information', (4, 7)), ('utterance', (10, 11))]","[['global GRU', 'encodes', 'corresponding party information'], ['corresponding party information', 'while encoding', 'utterance']]",[],[],"[['Model', 'has', 'global GRU']]",[],[],[],[],[],sentiment_analysis,10,22
model,Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .,"[('Attending over', (0, 2)), ('has information of', (8, 11)), ('by', (14, 15)), ('in', (17, 18))]","[('gives contextual representation', (4, 7)), ('all preceding utterances', (11, 14)), ('different parties', (15, 17)), ('conversation', (19, 20))]","[['gives contextual representation', 'has information of', 'all preceding utterances'], ['all preceding utterances', 'by', 'different parties'], ['different parties', 'in', 'conversation']]",[],[],[],[],"[['global GRU', 'Attending over', 'gives contextual representation']]",[],[],[],sentiment_analysis,10,23
model,The speaker state depends on this context through attention and the speaker 's previous state .,"[('depends on', (3, 5)), ('through', (7, 8))]","[('speaker state', (1, 3)), ('context', (6, 7)), (""attention and the speaker 's previous state"", (8, 15))]","[['speaker state', 'depends on', 'context'], ['context', 'through', ""attention and the speaker 's previous state""]]",[],[],"[['Model', 'has', 'speaker state']]",[],[],[],[],[],sentiment_analysis,10,24
model,"This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .","[('at time t', (3, 6)), ('directly gets information from', (10, 14)), ('which has information on', (22, 26))]","[('speaker state', (8, 10)), (""speaker 's previous state"", (15, 19)), ('global GRU', (20, 22)), ('preceding parties', (27, 29))]","[['speaker state', 'directly gets information from', ""speaker 's previous state""], ['speaker state', 'directly gets information from', 'global GRU'], ['global GRU', 'which has information on', 'preceding parties']]",[],"[['Model', 'at time t', 'speaker state']]",[],[],[],[],[],[],sentiment_analysis,10,25
model,"Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .","[('fed into', (7, 9)), ('to decode', (12, 14)), ('of', (17, 18)), ('used for', (24, 26))]","[('updated speaker state', (3, 6)), ('emotion GRU', (10, 12)), ('emotion representation', (15, 17)), ('given utterance', (19, 21)), ('emotion classification', (26, 28))]","[['updated speaker state', 'fed into', 'emotion GRU'], ['emotion GRU', 'to decode', 'emotion representation'], ['emotion representation', 'of', 'given utterance'], ['given utterance', 'used for', 'emotion classification']]",[],[],"[['Model', 'has', 'updated speaker state']]",[],[],[],[],[],sentiment_analysis,10,26
model,"At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .","[('At time t', (0, 3)), ('gets', (8, 9))]","[('emotion GRU cell', (5, 8)), ('emotion representation of t ? 1 and the speaker state of t', (10, 22))]","[['emotion GRU cell', 'gets', 'emotion representation of t ? 1 and the speaker state of t']]",[],"[['Model', 'At time t', 'emotion GRU cell']]",[],[],[],[],[],[],sentiment_analysis,10,27
baselines,c - LSTM : Biredectional LSTM is used to capture the context from the surrounding utterances to generate contextaware utterance representation .,"[('is', (6, 7)), ('used to capture', (7, 10)), ('from', (12, 13)), ('to generate', (16, 18))]","[('c - LSTM', (0, 3)), ('Biredectional LSTM', (4, 6)), ('context', (11, 12)), ('surrounding utterances', (14, 16)), ('contextaware utterance representation', (18, 21))]","[['c - LSTM', 'is', 'Biredectional LSTM'], ['Biredectional LSTM', 'used to capture', 'context'], ['context', 'from', 'surrounding utterances'], ['surrounding utterances', 'to generate', 'contextaware utterance representation']]",[],[],"[['Baselines', 'has', 'c - LSTM']]",[],[],[],[],[],sentiment_analysis,10,149
baselines,c- LSTM+ Att :,[],"[('c- LSTM+ Att', (0, 3))]",[],[],[],"[['Baselines', 'has', 'c- LSTM+ Att']]",[],[],[],[],"[['c- LSTM+ Att', 'has', 'variant attention']]",sentiment_analysis,10,151
baselines,In this variant attention is applied applied to the c - LSTM output at each timestamp by following Eqs. and .,"[('applied to', (6, 8)), ('at', (13, 14))]","[('variant attention', (2, 4)), ('c - LSTM output', (9, 13)), ('each timestamp', (14, 16))]","[['variant attention', 'at', 'each timestamp'], ['variant attention', 'applied to', 'c - LSTM output']]",[],[],[],[],[],[],[],[],sentiment_analysis,10,152
baselines,TFN :,[],"[('TFN', (0, 1))]",[],[],[],"[['Baselines', 'has', 'TFN']]",[],[],[],[],"[['TFN', 'has', 'Tensor outer product']]",sentiment_analysis,10,154
baselines,This is specific to multimodal scenario .,"[('specific to', (2, 4))]","[('multimodal scenario', (4, 6))]",[],[],[],[],[],"[['TFN', 'specific to', 'multimodal scenario']]",[],[],[],sentiment_analysis,10,155
baselines,Tensor outer product is used to capture intermodality and intra-modality interactions .,"[('used to capture', (4, 7))]","[('Tensor outer product', (0, 3)), ('intermodality and intra-modality interactions', (7, 11))]","[['Tensor outer product', 'used to capture', 'intermodality and intra-modality interactions']]",[],[],[],[],[],[],[],[],sentiment_analysis,10,156
baselines,"MFN ) : Specific to multimodal scenario , this model utilizes multi-view learning by modeling view - specific and cross - view interactions .","[('Specific to', (3, 5)), ('utilizes', (10, 11)), ('by modeling', (13, 15))]","[('MFN', (0, 1)), ('multimodal scenario', (5, 7)), ('multi-view learning', (11, 13)), ('view - specific and cross - view interactions', (15, 23))]","[['MFN', 'Specific to', 'multimodal scenario'], ['MFN', 'utilizes', 'multi-view learning'], ['multi-view learning', 'by modeling', 'view - specific and cross - view interactions']]",[],[],"[['Baselines', 'has', 'MFN']]",[],[],[],[],[],sentiment_analysis,10,158
baselines,CNN : This is identical to our textual feature extractor network ( Section 3.2 ) and it does not use contextual information from the surrounding utterances .,"[('identical to', (4, 6)), ('does not use', (17, 20))]","[('CNN', (0, 1)), ('our textual feature extractor network', (6, 11)), ('contextual information', (20, 22))]","[['CNN', 'identical to', 'our textual feature extractor network'], ['CNN', 'does not use', 'contextual information']]",[],[],"[['Baselines', 'has', 'CNN']]",[],[],[],[],[],sentiment_analysis,10,160
baselines,"Memnet : As described in , the current utterance is fed to a memory network , where the memories correspond to preceding utterances .","[('fed to', (10, 12)), ('where', (16, 17)), ('correspond to', (19, 21))]","[('Memnet', (0, 1)), ('current utterance', (7, 9)), ('memory network', (13, 15)), ('memories', (18, 19)), ('preceding utterances', (21, 23))]","[['current utterance', 'fed to', 'memory network'], ['memory network', 'where', 'memories'], ['memories', 'correspond to', 'preceding utterances']]","[['Memnet', 'has', 'current utterance']]",[],"[['Baselines', 'has', 'Memnet']]",[],[],[],[],"[['Memnet', 'has', 'output']]",sentiment_analysis,10,161
baselines,The output from the memory network is used as the final utterance representation for emotion classification .,"[('from', (2, 3)), ('used as', (7, 9)), ('for', (13, 14))]","[('output', (1, 2)), ('memory network', (4, 6)), ('final utterance representation', (10, 13)), ('emotion classification', (14, 16))]","[['output', 'from', 'memory network'], ['memory network', 'used as', 'final utterance representation'], ['final utterance representation', 'for', 'emotion classification']]",[],[],[],[],[],[],[],[],sentiment_analysis,10,162
baselines,CMN : This state - of - the - art method models utterance context from dialogue history using two distinct GRUs for two speakers .,"[('models', (11, 12)), ('from', (14, 15)), ('using', (17, 18)), ('for', (21, 22))]","[('CMN', (0, 1)), ('state - of - the - art method', (3, 11)), ('utterance context', (12, 14)), ('dialogue history', (15, 17)), ('two distinct GRUs', (18, 21)), ('two speakers', (22, 24))]","[['state - of - the - art method', 'models', 'utterance context'], ['utterance context', 'from', 'dialogue history'], ['dialogue history', 'using', 'two distinct GRUs'], ['two distinct GRUs', 'for', 'two speakers']]","[['CMN', 'has', 'state - of - the - art method']]",[],"[['Baselines', 'has', 'CMN']]",[],[],[],[],[],sentiment_analysis,10,163
results,"As expected , on average Di - alogue RNN outperforms all the baseline methods , including the state - of - the - art CMN , on both of the datasets .","[('on average', (3, 5)), ('outperforms', (9, 10)), ('including', (15, 16))]","[('Di - alogue RNN', (5, 9)), ('all the baseline methods', (10, 14)), ('state - of - the - art CMN', (17, 25))]","[['Di - alogue RNN', 'outperforms', 'all the baseline methods'], ['all the baseline methods', 'including', 'state - of - the - art CMN']]",[],"[['Results', 'on average', 'Di - alogue RNN']]",[],[],[],[],[],[],sentiment_analysis,10,170
results,"As evidenced by , for IEMOCAP dataset , our model surpasses the state - of - the - art method CMN by 2.77 % accuracy and 3.76 % f 1 - score on average .","[('by', (2, 3)), ('for', (4, 5)), ('surpasses', (10, 11))]","[('IEMOCAP dataset', (5, 7)), ('our model', (8, 10)), ('state - of - the - art method CMN', (12, 21)), ('2.77 % accuracy', (22, 25)), ('3.76 % f 1 - score', (26, 32))]","[['our model', 'surpasses', 'state - of - the - art method CMN'], ['state - of - the - art method CMN', 'by', '2.77 % accuracy'], ['state - of - the - art method CMN', 'by', '3.76 % f 1 - score']]","[['IEMOCAP dataset', 'has', 'our model']]","[['Results', 'for', 'IEMOCAP dataset']]",[],[],[],[],[],[],sentiment_analysis,10,174
results,"AVEC DialogueRNN outperforms CMN for valence , arousal , expectancy , and power attributes ; see .","[('outperforms', (2, 3)), ('for', (4, 5))]","[('AVEC', (0, 1)), ('DialogueRNN', (1, 2)), ('CMN', (3, 4)), ('valence , arousal , expectancy , and power attributes', (5, 14))]","[['DialogueRNN', 'outperforms', 'CMN'], ['CMN', 'for', 'valence , arousal , expectancy , and power attributes']]","[['AVEC', 'has', 'DialogueRNN']]",[],"[['Results', 'has', 'AVEC']]",[],[],[],[],[],sentiment_analysis,10,182
results,DialogueRNN vs. DialogueRNN Variants,[],"[('DialogueRNN vs. DialogueRNN Variants', (0, 4))]",[],[],[],"[['Results', 'has', 'DialogueRNN vs. DialogueRNN Variants']]",[],[],[],[],"[['DialogueRNN vs. DialogueRNN Variants', 'has', 'DialogueRNN l']]",sentiment_analysis,10,185
results,DialogueRNN l :,[],"[('DialogueRNN l', (0, 2))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,10,187
results,"Following , using explicit listener state update yields slightly worse performance than regular DialogueRNN .","[('using', (2, 3)), ('yields', (7, 8)), ('than', (11, 12))]","[('explicit listener state update', (3, 7)), ('slightly worse performance', (8, 11)), ('regular DialogueRNN', (12, 14))]","[['explicit listener state update', 'yields', 'slightly worse performance'], ['slightly worse performance', 'than', 'regular DialogueRNN']]",[],[],[],[],"[['DialogueRNN l', 'using', 'explicit listener state update']]",[],[],[],sentiment_analysis,10,188
results,"BiDialogueRNN : Since BiDialogueRNN captures context from the future utterances , we expect improved performance from it over DialogueRNN .",[],"[('BiDialogueRNN', (0, 1))]",[],[],[],[],[],[],[],"[['DialogueRNN vs. DialogueRNN Variants', 'has', 'BiDialogueRNN']]",[],sentiment_analysis,10,193
results,"This is confirmed in , where BiDialogueRNN outperforms Dialogue RNN on average on both datasets .","[('outperforms', (7, 8)), ('on', (10, 11))]","[('Dialogue RNN', (8, 10)), ('both datasets', (13, 15))]","[['Dialogue RNN', 'on', 'both datasets']]",[],[],[],[],"[['BiDialogueRNN', 'outperforms', 'Dialogue RNN']]",[],[],[],sentiment_analysis,10,194
research-problem,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,[],"[('Emotion Recognition in Dyadic Dialogue Videos', (4, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion Recognition in Dyadic Dialogue Videos']]",[],[],[],[],sentiment_analysis,11,2
research-problem,Emotion recognition in conversations is crucial for the development of empathetic machines .,[],"[('Emotion recognition in conversations', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion recognition in conversations']]",[],[],[],[],sentiment_analysis,11,4
research-problem,"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .",[],"[('Emotion detection', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion detection']]",[],[],[],[],sentiment_analysis,11,16
research-problem,"In this paper , we analyze emotion detection in videos of dyadic conversations .",[],"[('emotion detection in videos of dyadic conversations', (6, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'emotion detection in videos of dyadic conversations']]",[],[],[],[],sentiment_analysis,11,17
model,"We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .","[('propose', (1, 2)), ('uses', (11, 12)), ('for', (15, 16)), ('in', (18, 19)), ('of', (31, 32))]","[('conversational memory network ( CMN )', (3, 9)), ('multimodal approach', (13, 15)), ('emotion detection', (16, 18)), ('utterances ( a unit of speech bound by breathes or pauses )', (19, 31)), ('conversational videos', (33, 35))]","[['conversational memory network ( CMN )', 'uses', 'multimodal approach'], ['multimodal approach', 'for', 'emotion detection'], ['emotion detection', 'in', 'utterances ( a unit of speech bound by breathes or pauses )'], ['utterances ( a unit of speech bound by breathes or pauses )', 'of', 'conversational videos']]",[],"[['Model', 'propose', 'conversational memory network ( CMN )']]",[],[],[],[],[],[],sentiment_analysis,11,19
model,Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .,"[('by using', (6, 8)), ('present in', (11, 13))]","[('proposed CMN', (1, 3)), ('emotional context information', (8, 11)), ('conversation history', (14, 16))]","[['proposed CMN', 'by using', 'emotional context information'], ['emotional context information', 'present in', 'conversation history']]",[],[],"[['Model', 'has', 'proposed CMN']]",[],[],[],[],[],sentiment_analysis,11,28
model,It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,[],[],"[['speakerbased emotion modeling', 'by using', 'memory networks'], ['memory networks', 'efficient in capturing', 'long - term dependencies']]",[],[],[],[],"[['proposed CMN', 'improves', 'speakerbased emotion modeling']]","[['memory networks', 'summarizing', 'task - specific details ']]",[],[],sentiment_analysis,11,29
model,"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .","[('of', (5, 6)), ('are', (7, 8)), ('store', (11, 12)), ('found in', (15, 17))]","[('memory cells', (3, 5)), ('CMN', (6, 7)), ('continuous vectors', (8, 10)), ('context information', (13, 15)), ('utterance histories', (18, 20))]","[['memory cells', 'of', 'CMN'], ['CMN', 'are', 'continuous vectors'], ['continuous vectors', 'store', 'context information'], ['context information', 'found in', 'utterance histories']]",[],[],"[['Model', 'has', 'memory cells']]",[],[],[],[],[],sentiment_analysis,11,30
model,CMN also models interplay of these memories to capture interspeaker dependencies .,"[('models', (2, 3)), ('of', (4, 5)), ('to capture', (7, 9))]","[('CMN', (0, 1)), ('interplay', (3, 4)), ('memories', (6, 7)), ('interspeaker dependencies', (9, 11))]","[['CMN', 'models', 'interplay'], ['interplay', 'of', 'memories'], ['memories', 'to capture', 'interspeaker dependencies']]",[],[],"[['Model', 'has', 'CMN']]",[],[],[],[],[],sentiment_analysis,11,31
model,"CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .","[('extracts', (2, 3)), ('for', (13, 14)), ('in', (16, 17))]","[('multimodal features ( audio , visual , and text )', (3, 13)), ('all utterances', (14, 16)), ('video', (18, 19))]","[['multimodal features ( audio , visual , and text )', 'for', 'all utterances'], ['all utterances', 'in', 'video']]",[],[],[],[],"[['CMN', 'extracts', 'multimodal features ( audio , visual , and text )']]",[],[],[],sentiment_analysis,11,32
hyperparameters,We use 10 % of the training set as a held - out validation set for hyperparameter tuning .,"[('use', (1, 2)), ('of', (4, 5)), ('as', (8, 9)), ('for', (15, 16))]","[('10 %', (2, 4)), ('training set', (6, 8)), ('held - out validation set', (10, 15)), ('hyperparameter tuning', (16, 18))]","[['10 %', 'of', 'training set'], ['training set', 'as', 'held - out validation set'], ['held - out validation set', 'for', 'hyperparameter tuning']]",[],"[['Hyperparameters', 'use', '10 %']]",[],[],[],[],[],[],sentiment_analysis,11,245
hyperparameters,"To optimize the parameters , we use Stochastic Gradient Descent ( SGD ) optimizer , starting with an initial learning Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person , is counted in case 1 or 2 , respectively .","[('starting with', (15, 17)), ('whose history has', (21, 24))]","[('Stochastic Gradient Descent ( SGD ) optimizer', (7, 14)), ('initial learning Utterances', (18, 21)), ('atleast 3 similar emotion labels', (24, 29))]","[['Stochastic Gradient Descent ( SGD ) optimizer', 'starting with', 'initial learning Utterances'], ['initial learning Utterances', 'whose history has', 'atleast 3 similar emotion labels']]",[],[],"[['Hyperparameters', 'use', 'Stochastic Gradient Descent ( SGD ) optimizer']]",[],[],[],[],[],sentiment_analysis,11,246
hyperparameters,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,"[('halves', (3, 4)), ('every', (6, 7)), ('decided using', (12, 14)), ('with', (19, 20)), ('of', (22, 23)), ('by monitoring', (24, 26))]","[('annealing approach', (1, 3)), ('lr', (5, 6)), ('20 epochs', (7, 9)), ('termination', (10, 11)), ('early - stop measure', (15, 19)), ('patience', (21, 22)), ('12', (23, 24)), ('validation loss', (27, 29))]","[['annealing approach', 'halves', 'lr'], ['lr', 'every', '20 epochs'], ['termination', 'decided using', 'early - stop measure'], ['early - stop measure', 'with', 'patience'], ['patience', 'of', '12'], ['12', 'by monitoring', 'validation loss']]","[['annealing approach', 'has', 'termination']]",[],"[['Hyperparameters', 'has', 'annealing approach']]",[],[],[],[],[],sentiment_analysis,11,249
hyperparameters,Gradient clipping is used for regularization with a norm set to 40 .,"[('used for', (3, 5)), ('with', (6, 7)), ('set to', (9, 11))]","[('Gradient clipping', (0, 2)), ('regularization', (5, 6)), ('norm', (8, 9)), ('40', (11, 12))]","[['Gradient clipping', 'used for', 'regularization'], ['regularization', 'with', 'norm'], ['norm', 'set to', '40']]",[],[],"[['Hyperparameters', 'has', 'Gradient clipping']]",[],[],[],[],[],sentiment_analysis,11,250
hyperparameters,Hyperparameters are decided using a Random Search .,"[('decided using', (2, 4))]","[('Random Search', (5, 7))]",[],[],"[['Hyperparameters', 'decided using', 'Random Search']]",[],[],[],[],[],[],sentiment_analysis,11,251
hyperparameters,"Based on validation performance , context window length K is set to be 40 and the number of hops R is fixed at 3 hops .","[('Based on', (0, 2)), ('set to', (10, 12)), ('fixed at', (21, 23))]","[('validation performance', (2, 4)), ('context window length K', (5, 9)), ('40', (13, 14)), ('number of hops R', (16, 20)), ('3 hops', (23, 25))]","[['number of hops R', 'fixed at', '3 hops'], ['context window length K', 'set to', '40']]","[['validation performance', 'has', 'number of hops R'], ['validation performance', 'has', 'context window length K']]","[['Hyperparameters', 'Based on', 'validation performance']]",[],[],[],[],[],[],sentiment_analysis,11,252
hyperparameters,The dimension size of the memory cells d is set as 50 .,"[('of', (3, 4)), ('set as', (9, 11))]","[('dimension size', (1, 3)), ('memory cells d', (5, 8)), ('50', (11, 12))]","[['dimension size', 'of', 'memory cells d'], ['memory cells d', 'set as', '50']]",[],[],"[['Hyperparameters', 'has', 'dimension size']]",[],[],[],[],[],sentiment_analysis,11,254
baselines,SVM - ensemble :,[],"[('SVM - ensemble', (0, 3))]",[],[],[],"[['Baselines', 'has', 'SVM - ensemble']]",[],[],[],[],"[['SVM - ensemble', 'has', 'strong context - free benchmark model']]",sentiment_analysis,11,257
baselines,A strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .,"[('which uses', (7, 9)), ('on', (12, 13))]","[('strong context - free benchmark model', (1, 7)), ('similar multimodal approach', (9, 12)), ('ensemble of trees', (14, 17))]","[['strong context - free benchmark model', 'which uses', 'similar multimodal approach'], ['similar multimodal approach', 'on', 'ensemble of trees']]",[],[],[],[],[],[],[],[],sentiment_analysis,11,258
baselines,bc - LSTM :,[],"[('bc - LSTM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'bc - LSTM']]",[],[],[],[],"[['bc - LSTM', 'has', 'bi-directional LSTM']]",sentiment_analysis,11,260
baselines,"A bi-directional LSTM equipped with hierarchical fusion , proposed by .","[('equipped with', (3, 5))]","[('bi-directional LSTM', (1, 3)), ('hierarchical fusion', (5, 7))]","[['bi-directional LSTM', 'equipped with', 'hierarchical fusion']]",[],[],[],[],[],[],[],[],sentiment_analysis,11,261
baselines,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .","[('generates', (15, 16)), ('for', (19, 20)), ('using', (23, 24)), ('without', (34, 35))]","[('Memn2n', (0, 1)), ('memory representations', (17, 19)), ('each historical utterance', (20, 23)), ('embedding matrix B', (25, 28)), ('sequential modeling', (35, 37))]","[['Memn2n', 'generates', 'memory representations'], ['memory representations', 'for', 'each historical utterance'], ['each historical utterance', 'using', 'embedding matrix B'], ['embedding matrix B', 'without', 'sequential modeling']]",[],[],"[['Baselines', 'has', 'Memn2n']]",[],[],[],[],[],sentiment_analysis,11,266
baselines,"[ 1 , K ] } for ? ? {a , b}. CMN Self :",[],"[('CMN Self', (12, 14))]",[],[],[],"[['Baselines', 'has', 'CMN Self']]",[],[],[],[],[],sentiment_analysis,11,271
baselines,"In this baseline , we use only self history for classifying emotion of utterance u i .","[('use', (5, 6)), ('for classifying', (9, 11)), ('of', (12, 13))]","[('only self history', (6, 9)), ('emotion', (11, 12)), ('utterance u i', (13, 16))]","[['only self history', 'for classifying', 'emotion'], ['emotion', 'of', 'utterance u i']]",[],[],[],[],"[['CMN Self', 'use', 'only self history']]",[],[],[],sentiment_analysis,11,272
baselines,CMN N A : Single layer variant of the CMN with no attention module .,"[('of', (7, 8)), ('with', (10, 11))]","[('CMN N A', (0, 3)), ('Single layer variant', (4, 7)), ('CMN', (9, 10)), ('no attention module', (11, 14))]","[['Single layer variant', 'of', 'CMN'], ['CMN', 'with', 'no attention module']]","[['CMN N A', 'has', 'Single layer variant']]",[],"[['Baselines', 'has', 'CMN N A']]",[],[],[],[],[],sentiment_analysis,11,275
results,This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations .,"[('suggests', (1, 2)), ('through', (6, 7)), ('is', (9, 10)), ('over', (14, 15))]","[('gathering contexts temporally', (3, 6)), ('sequential processing', (7, 9)), ('superior method', (12, 14)), ('non-temporal memory representations', (15, 18))]","[['gathering contexts temporally', 'through', 'sequential processing'], ['sequential processing', 'is', 'superior method'], ['superior method', 'over', 'non-temporal memory representations']]",[],"[['Results', 'suggests', 'gathering contexts temporally']]",[],[],[],[],[],[],sentiment_analysis,11,289
results,CMN self which uses only single history channel also provides lesser performance when compared to CMN .,"[('uses only', (3, 5)), ('provides', (9, 10)), ('compared to', (13, 15))]","[('CMN self', (0, 2)), ('single history channel', (5, 8)), ('lesser performance', (10, 12)), ('CMN', (15, 16))]","[['CMN self', 'provides', 'lesser performance'], ['lesser performance', 'compared to', 'CMN'], ['CMN self', 'uses only', 'single history channel']]",[],[],"[['Results', 'has', 'CMN self']]",[],[],[],[],[],sentiment_analysis,11,290
results,"Overall , predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN 's ability to model emotional dynamics .","[('on', (3, 4)), ('show', (9, 10))]","[('predictions', (2, 3)), ('valence and arousal levels', (4, 8)), ('similar results', (10, 12))]","[['predictions', 'on', 'valence and arousal levels'], ['valence and arousal levels', 'show', 'similar results']]",[],[],"[['Results', 'has', 'predictions']]",[],[],[],[],[],sentiment_analysis,11,292
research-problem,Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis,[],"[('Aspect - Level Sentiment Analysis', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Level Sentiment Analysis']]",[],[],[],[],sentiment_analysis,12,2
research-problem,"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .",[],"[('aspect - level sentiment classification ( ASC )', (1, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect - level sentiment classification ( ASC )']]",[],[],[],[],sentiment_analysis,12,4
research-problem,"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .",[],"[('neural ASC', (15, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural ASC']]",[],[],[],[],sentiment_analysis,12,6
research-problem,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .",[],"[('Aspect - level sentiment classification ( ASC )', (0, 8)), ('sentiment analysis', (14, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level sentiment classification ( ASC )'], ['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,12,13
research-problem,"However , the existing attention mechanism in ASC suffers from a major drawback .",[],"[('ASC', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'ASC']]",[],[],[],[],sentiment_analysis,12,18
approach,"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .","[('propose', (5, 6)), ('for', (15, 16))]","[('novel progressive self - supervised attention learning approach', (7, 15)), ('neural ASC models', (16, 19))]","[['novel progressive self - supervised attention learning approach', 'for', 'neural ASC models']]",[],"[['Approach', 'propose', 'novel progressive self - supervised attention learning approach']]",[],[],[],[],[],[],sentiment_analysis,12,30
approach,"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .","[('able to automatically and incrementally mine', (3, 9)), ('from', (12, 13)), ('exploited to guide', (20, 23)), ('of', (25, 26)), ('in', (28, 29))]","[('attention supervision information', (9, 12)), ('training corpus', (14, 16)), ('training', (24, 25)), ('attention mechanisms', (26, 28)), ('ASC models', (29, 31))]","[['attention supervision information', 'exploited to guide', 'training'], ['training', 'of', 'attention mechanisms'], ['attention mechanisms', 'in', 'ASC models'], ['attention supervision information', 'from', 'training corpus']]",[],"[['Approach', 'able to automatically and incrementally mine', 'attention supervision information']]",[],[],[],[],[],[],sentiment_analysis,12,31
approach,The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,"[('roots in', (6, 8)), ('with', (15, 16)), ('on', (24, 25)), ('of', (28, 29))]","[('context word', (13, 15)), ('maximum attention weight', (17, 20)), ('greatest impact', (22, 24)), ('sentiment prediction', (26, 28)), ('input sentence', (30, 32))]","[['context word', 'with', 'maximum attention weight'], ['greatest impact', 'on', 'sentiment prediction'], ['sentiment prediction', 'of', 'input sentence']]","[['maximum attention weight', 'has', 'greatest impact']]","[['Approach', 'roots in', 'context word']]",[],[],[],[],[],[],sentiment_analysis,12,32
hyperparameters,We used pre-trained Glo Ve vectors to initialize the word embeddings with vector dimension 300 .,"[('used', (1, 2)), ('to initialize', (6, 8)), ('with', (11, 12))]","[('pre-trained Glo Ve vectors', (2, 6)), ('word embeddings', (9, 11)), ('vector dimension', (12, 14)), ('300', (14, 15))]","[['pre-trained Glo Ve vectors', 'to initialize', 'word embeddings'], ['word embeddings', 'with', 'vector dimension']]","[['vector dimension', 'has', '300']]","[['Hyperparameters', 'used', 'pre-trained Glo Ve vectors']]",[],[],[],[],[],[],sentiment_analysis,12,150
hyperparameters,"For out - of - vocabulary words , we randomly sampled their embeddings from the uniform distribution , as implemented in .","[('For', (0, 1)), ('randomly sampled', (9, 11)), ('from', (13, 14))]","[('out - of - vocabulary words', (1, 7)), ('embeddings', (12, 13)), ('uniform distribution', (15, 17))]","[['out - of - vocabulary words', 'randomly sampled', 'embeddings'], ['embeddings', 'from', 'uniform distribution']]",[],"[['Hyperparameters', 'For', 'out - of - vocabulary words']]",[],[],[],[],[],[],sentiment_analysis,12,151
hyperparameters,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .","[('To alleviate', (0, 2)), ('employed', (5, 6)), ('on', (15, 16)), ('of', (20, 21))]","[('overfitting', (2, 3)), ('dropout strategy ( Hinton et al. , 2012 )', (6, 15)), ('input word embeddings', (17, 20)), ('LSTM', (22, 23))]","[['overfitting', 'employed', 'dropout strategy ( Hinton et al. , 2012 )'], ['dropout strategy ( Hinton et al. , 2012 )', 'on', 'input word embeddings'], ['input word embeddings', 'of', 'LSTM']]",[],"[['Hyperparameters', 'To alleviate', 'overfitting']]",[],[],[],[],[],[],sentiment_analysis,12,153
hyperparameters,"Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .","[('adopted', (9, 10)), ('with', (13, 14))]","[('Adam ( Kingma and Ba , 2015 )', (0, 8)), ('optimizer', (12, 13)), ('learning rate', (15, 17)), ('0.001', (17, 18))]","[['Adam ( Kingma and Ba , 2015 )', 'adopted', 'optimizer'], ['optimizer', 'with', 'learning rate']]","[['learning rate', 'has', '0.001']]",[],"[['Hyperparameters', 'has', 'Adam ( Kingma and Ba , 2015 )']]",[],[],[],[],[],sentiment_analysis,12,154
hyperparameters,"When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .",[],[],"[['our approach', 'empirically set', 'maximum iteration number K'], ['maximum iteration number K', 'as', '5'], ['maximum iteration number K', 'as', '0.1'], ['0.1', 'on', 'LAPTOP data set'], ['maximum iteration number K', 'as', '0.5'], ['0.5', 'on', 'REST data set'], ['maximum iteration number K', 'as', '0.1'], ['0.1', 'on', 'TWITTER data set']]",[],"[['Hyperparameters', 'When implementing', 'our approach']]",[],[],[],[],[],[],sentiment_analysis,12,155
hyperparameters,All hyper - parameters were tuned on 20 % randomly held - out training data .,"[('tuned on', (5, 7))]","[('All hyper - parameters', (0, 4)), ('20 % randomly held - out training data', (7, 15))]","[['All hyper - parameters', 'tuned on', '20 % randomly held - out training data']]",[],[],"[['Hyperparameters', 'has', 'All hyper - parameters']]",[],[],[],[],[],sentiment_analysis,12,156
results,"First , both of our reimplemented MN and TNet are comparable to their original models reported in .","[('comparable to', (10, 12))]","[('both of our reimplemented MN and TNet', (2, 9)), ('original models', (13, 15))]","[['both of our reimplemented MN and TNet', 'comparable to', 'original models']]",[],[],"[['Results', 'has', 'both of our reimplemented MN and TNet']]",[],[],[],[],[],sentiment_analysis,12,174
results,"When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .",[],[],"[['TNet - ATT', 'slightly inferior to', 'TNet'], ['TNet - ATT', 'replace', 'CNN'], ['CNN', 'of', 'TNet'], ['TNet', 'with', 'attention mechanism']]",[],[],"[['Results', 'has', 'TNet - ATT']]",[],[],[],[],[],sentiment_analysis,12,176
results,"Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .","[('perform', (4, 5)), ('of', (9, 10)), ('on', (11, 12)), ('performance', (16, 17))]","[('additional K+1 - iteration', (5, 9)), ('training', (10, 11)), ('not changed significantly', (18, 21)), ('neural ASC models', (35, 38))]","[['additional K+1 - iteration', 'of', 'training'], ['training', 'on', 'neural ASC models'], ['neural ASC models', 'performance', 'not changed significantly']]",[],"[['Results', 'perform', 'additional K+1 - iteration']]",[],[],[],[],[],[],sentiment_analysis,12,177
results,"Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms MN on all test sets .","[('use', (4, 5)), ('remarkably outperforms', (23, 25)), ('on', (26, 27))]","[('both kinds of attention supervision information', (5, 11)), ('MN ( + AS )', (18, 23)), ('MN', (25, 26)), ('all test sets', (27, 30))]","[['MN ( + AS )', 'remarkably outperforms', 'MN'], ['MN', 'on', 'all test sets']]","[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","[['Results', 'use', 'both kinds of attention supervision information']]",[],[],[],[],[],[],sentiment_analysis,12,183
research-problem,BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,[],"[('Aspect - based Sentiment Analysis', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,13,2
research-problem,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .",[],"[('aspect extraction', (25, 27)), ('aspect sentiment classification', (28, 31)), ('aspect - based sentiment analysis', (32, 37))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect extraction'], ['Contribution', 'has research problem', 'aspect sentiment classification'], ['Contribution', 'has research problem', 'aspect - based sentiment analysis']]",[],[],[],[],sentiment_analysis,13,10
dataset,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .","[('first builds', (2, 4)), ('called', (7, 8)), ('using', (10, 11)), ('from', (12, 13)), ('is a', (20, 22)), ('for', (24, 25)), ('in', (33, 34))]","[('RRC dataset', (5, 7)), ('ReviewRC', (8, 9)), ('reviews', (11, 12)), ('SemEval 2016 Task 5', (13, 17)), ('popular dataset', (22, 24)), ('aspect - based sentiment analysis ( ABSA )', (25, 33)), ('domains of laptop and restaurant', (35, 40))]","[['RRC dataset', 'called', 'ReviewRC'], ['ReviewRC', 'using', 'reviews'], ['reviews', 'from', 'SemEval 2016 Task 5'], ['SemEval 2016 Task 5', 'is a', 'popular dataset'], ['popular dataset', 'for', 'aspect - based sentiment analysis ( ABSA )'], ['aspect - based sentiment analysis ( ABSA )', 'in', 'domains of laptop and restaurant']]",[],"[['Dataset', 'first builds', 'RRC dataset']]",[],[],[],[],[],[],sentiment_analysis,13,40
model,This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,"[('adopts', (2, 3)), ('as', (5, 6))]","[('BERT', (3, 4)), ('base model', (7, 9))]","[['BERT', 'as', 'base model']]",[],"[['Model', 'adopts', 'BERT']]",[],[],[],[],[],[],sentiment_analysis,13,43
model,"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .",[],[],"[['BERT', 'with', 'both domain knowledge and task ( MRC ) knowledge'], ['both domain knowledge and task ( MRC ) knowledge', 'before', 'fine - tuning'], ['fine - tuning', 'using', 'domain end task annotated data'], ['domain end task annotated data', 'for', 'domain RRC'], ['novel joint post - training technique', 'takes', ""BERT 's pre-trained weights""], [""BERT 's pre-trained weights"", 'as', 'initialization'], ['initialization', 'for', 'basic language understanding']]",[],"[['Model', 'adapt', 'BERT'], ['Model', 'propose', 'novel joint post - training technique']]",[],[],[],[],[],[],sentiment_analysis,13,48
model,"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .","[('leverages', (2, 3)), ('from', (4, 5))]","[('knowledge', (3, 4)), ('two sources', (5, 7)), ('unsupervised domain reviews', (8, 11)), ('supervised ( yet out - of - domain ) MRC data', (12, 23))]","[['knowledge', 'from', 'two sources']]","[['two sources', 'name', 'unsupervised domain reviews'], ['two sources', 'name', 'supervised ( yet out - of - domain ) MRC data']]","[['Model', 'leverages', 'knowledge']]",[],[],[],[],[],[],sentiment_analysis,13,49
hyperparameters,We adopt BERT BASE ( uncased ) as the basis for all experiments,"[('adopt', (1, 2)), ('as', (7, 8)), ('for', (10, 11))]","[('BERT BASE ( uncased )', (2, 7)), ('basis', (9, 10)), ('all experiments', (11, 13))]","[['BERT BASE ( uncased )', 'as', 'basis'], ['basis', 'for', 'all experiments']]",[],"[['Hyperparameters', 'adopt', 'BERT BASE ( uncased )']]",[],[],[],[],[],[],sentiment_analysis,13,210
hyperparameters,"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .","[('leverage', (21, 22)), ('to reduce', (25, 27)), ('of both', (29, 31)), ('of', (36, 37))]","[('FP16 computation', (22, 24)), ('size', (28, 29)), ('model', (32, 33)), ('hidden representations', (34, 36)), ('data', (37, 38))]","[['FP16 computation', 'to reduce', 'size'], ['size', 'of both', 'model'], ['size', 'of both', 'hidden representations'], ['hidden representations', 'of', 'data']]",[],"[['Hyperparameters', 'leverage', 'FP16 computation']]",[],[],[],[],[],[],sentiment_analysis,13,211
hyperparameters,"We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .","[('set', (1, 2)), ('of', (6, 7)), ('in', (8, 9))]","[('static loss scale', (3, 6)), ('2', (7, 8))]","[['static loss scale', 'of', '2']]",[],"[['Hyperparameters', 'set', 'static loss scale']]",[],[],[],"[['static loss scale', 'in', 'FP']]",[],[],sentiment_analysis,13,212
hyperparameters,The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .,[],[],"[['maximum length', 'of', 'post -training'], ['post -training', 'set to', '320'], ['320', 'with', 'batch size'], ['batch size', 'of', '16'], ['16', 'for each type of', 'knowledge']]",[],[],"[['Hyperparameters', 'has', 'maximum length']]",[],[],[],[],[],sentiment_analysis,13,213
hyperparameters,"The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .","[('set to', (6, 8)), ('good enough to store', (12, 16)), ('into', (21, 22)), ('of', (25, 26))]","[('number of subbatch u', (1, 5)), ('2', (8, 9)), ('sub - batch iteration', (17, 21)), ('GPU memory', (23, 25)), ('11G', (26, 27))]","[['number of subbatch u', 'set to', '2'], ['2', 'good enough to store', 'sub - batch iteration'], ['sub - batch iteration', 'into', 'GPU memory'], ['GPU memory', 'of', '11G']]",[],[],"[['Hyperparameters', 'has', 'number of subbatch u']]",[],[],[],[],[],sentiment_analysis,13,214
hyperparameters,We use Adam optimizer and set the learning rate to be 3e - 5 .,"[('use', (1, 2)), ('set', (5, 6)), ('to be', (9, 11))]","[('Adam optimizer', (2, 4)), ('learning rate', (7, 9)), ('3e - 5', (11, 14))]","[['Adam optimizer', 'set', 'learning rate'], ['learning rate', 'to be', '3e - 5']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],[],[],[],sentiment_analysis,13,215
hyperparameters,"We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .",[],[],"[['140,000 steps', 'for', 'restaurant domain'], ['70,000 steps', 'for', 'laptop domain']]",[],"[['Hyperparameters', 'train', '140,000 steps'], ['Hyperparameters', 'train', '70,000 steps']]",[],[],[],[],[],[],sentiment_analysis,13,216
results,"To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .","[('observed', (5, 6)), ('over', (22, 23)), ('in', (25, 26)), ('show', (30, 31)), ('of having', (33, 35))]","[('proposed joint post - training ( BERT - PT )', (8, 18)), ('best performance', (20, 22)), ('all tasks', (23, 25)), ('all domains', (26, 28)), ('benefits', (32, 33)), ('two types of knowledge', (35, 39))]","[['best performance', 'over', 'all tasks'], ['all tasks', 'in', 'all domains'], ['best performance', 'show', 'benefits'], ['benefits', 'of having', 'two types of knowledge']]","[['proposed joint post - training ( BERT - PT )', 'has', 'best performance']]","[['Results', 'observed', 'proposed joint post - training ( BERT - PT )']]",[],[],[],[],[],[],sentiment_analysis,13,250
results,"Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .","[('found that', (21, 23)), ('of', (27, 28)), ('do not work', (29, 32)), ('for', (33, 34))]","[('vanilla pre-trained weights', (24, 27)), ('BERT', (28, 29)), ('well', (32, 33)), ('review - based tasks', (34, 38))]","[['vanilla pre-trained weights', 'of', 'BERT'], ['BERT', 'do not work', 'well'], ['well', 'for', 'review - based tasks']]",[],"[['Results', 'found that', 'vanilla pre-trained weights']]",[],[],[],[],[],[],sentiment_analysis,13,252
results,"To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .","[('noticed that', (5, 7)), ('of', (9, 10)), ('vary for', (15, 17))]","[('roles', (8, 9)), ('domain knowledge and task knowledge', (10, 15)), ('different tasks and domains', (17, 21))]","[['roles', 'of', 'domain knowledge and task knowledge'], ['domain knowledge and task knowledge', 'vary for', 'different tasks and domains']]",[],"[['Results', 'noticed that', 'roles']]",[],[],[],[],[],[],sentiment_analysis,13,254
results,"For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .","[('For', (0, 1)), ('found that', (4, 6)), ('of', (9, 10)), ('mostly comes from', (13, 16))]","[('RRC', (1, 2)), ('performance gain', (7, 9)), ('BERT - PT', (10, 13)), ('task - awareness ( MRC ) post -training', (16, 24))]","[['RRC', 'found that', 'performance gain'], ['performance gain', 'of', 'BERT - PT'], ['BERT - PT', 'mostly comes from', 'task - awareness ( MRC ) post -training']]",[],"[['Results', 'For', 'RRC']]",[],[],[],[],[],[],sentiment_analysis,13,255
results,We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,"[('further investigated', (1, 3)), ('improved', (5, 6)), ('found that', (11, 13)), ('were', (22, 23))]","[('examples', (4, 5)), ('BERT - MRC', (7, 10)), ('boundaries of spans ( especially short spans )', (14, 22)), ('greatly improved', (23, 25))]","[['BERT - MRC', 'improved', 'examples'], ['examples', 'found that', 'boundaries of spans ( especially short spans )'], ['boundaries of spans ( especially short spans )', 'were', 'greatly improved']]",[],"[['Results', 'further investigated', 'BERT - MRC']]",[],[],[],[],[],[],sentiment_analysis,13,258
results,"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .","[('found that', (4, 6)), ('comes mostly from', (9, 12))]","[('AE', (1, 2)), ('great performance boost', (6, 9)), ('domain knowledge posttraining', (12, 15))]","[['AE', 'found that', 'great performance boost'], ['great performance boost', 'comes mostly from', 'domain knowledge posttraining']]",[],[],"[['Results', 'For', 'AE']]",[],[],[],[],[],sentiment_analysis,13,259
results,"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .","[('has almost no', (3, 6)), ('on', (7, 8))]","[('BERT - MRC', (0, 3)), ('improvement', (6, 7)), ('restaurant', (8, 9))]","[['BERT - MRC', 'has almost no', 'improvement'], ['improvement', 'on', 'restaurant']]",[],[],"[['Results', 'has', 'BERT - MRC']]",[],[],[],[],[],sentiment_analysis,13,260
results,"For ASC , we observed that large - scale annotated MRC data is very useful .","[('observed that', (4, 6)), ('is', (12, 13))]","[('ASC', (1, 2)), ('large - scale annotated MRC data', (6, 12)), ('very useful', (13, 15))]","[['ASC', 'observed that', 'large - scale annotated MRC data'], ['large - scale annotated MRC data', 'is', 'very useful']]",[],[],"[['Results', 'For', 'ASC']]",[],[],[],[],[],sentiment_analysis,13,262
results,The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .,[],[],"[['errors', 'on', 'RRC'], ['RRC', 'come from', 'incorrect location'], ['incorrect location', 'of', 'spans'], ['spans', 'that may have', 'certain nearby words'], ['certain nearby words', 'related to', 'question'], ['RRC', 'come from', 'boundaries'], ['boundaries', 'of', 'spans'], ['spans', 'that are not', 'concise enough']]",[],[],"[['Results', 'has', 'errors']]",[],[],[],[],[],sentiment_analysis,13,267
results,"For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .","[('mostly come from', (4, 7)), ('of', (11, 12))]","[('errors', (3, 4)), ('annotation inconsistency', (7, 9)), ('boundaries', (10, 11)), ('aspects', (12, 13))]","[['errors', 'mostly come from', 'annotation inconsistency'], ['errors', 'mostly come from', 'boundaries'], ['boundaries', 'of', 'aspects']]",[],[],[],[],[],[],"[['AE', 'has', 'errors']]",[],sentiment_analysis,13,269
results,"ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .","[('tends to have', (1, 4)), ('as', (6, 7)), ('between', (10, 11)), ('is', (16, 17))]","[('ASC', (0, 1)), ('errors', (5, 6)), ('decision boundary', (8, 10)), ('negative and neutral examples', (12, 16)), ('unclear', (17, 18))]","[['ASC', 'tends to have', 'errors'], ['errors', 'as', 'decision boundary'], ['decision boundary', 'between', 'negative and neutral examples'], ['negative and neutral examples', 'is', 'unclear']]",[],[],"[['Results', 'has', 'ASC']]",[],[],[],[],[],sentiment_analysis,13,271
research-problem,Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training,[],"[('Learning Generalized Emotion Representation', (3, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Learning Generalized Emotion Representation']]",[],[],[],[],sentiment_analysis,14,2
model,This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .,"[('demonstrates', (2, 3)), ('of incorporating', (5, 7)), ('in', (9, 10)), ('for', (13, 14)), ('compared to', (18, 20))]","[('effectiveness', (4, 5)), ('sentiment labels', (7, 9)), ('wordlevel information', (11, 13)), ('sentiment - related tasks', (14, 18)), ('other word embeddings', (20, 23))]","[['effectiveness', 'of incorporating', 'sentiment labels'], ['sentiment labels', 'in', 'wordlevel information'], ['wordlevel information', 'for', 'sentiment - related tasks'], ['sentiment - related tasks', 'compared to', 'other word embeddings']]",[],"[['Model', 'demonstrates', 'effectiveness']]",[],[],[],[],[],[],sentiment_analysis,14,14
model,"1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .","[('propose', (3, 4)), ('are', (7, 8)), ('that encode', (12, 14)), ('into', (16, 17))]","[('Emo2Vec', (4, 5)), ('word - level representations', (8, 12)), ('emotional semantics', (14, 16)), ('fixed - sized , real - valued vectors', (17, 25))]","[['Emo2Vec', 'are', 'word - level representations'], ['word - level representations', 'that encode', 'emotional semantics'], ['emotional semantics', 'into', 'fixed - sized , real - valued vectors']]",[],"[['Model', 'propose', 'Emo2Vec']]",[],[],[],[],[],[],sentiment_analysis,14,24
model,2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .,"[('propose to learn', (3, 6)), ('with', (7, 8)), ('by including', (12, 14))]","[('Emo2Vec', (6, 7)), ('multi-task learning framework', (9, 12)), ('six different emotion - related tasks', (14, 20))]","[['Emo2Vec', 'with', 'multi-task learning framework'], ['multi-task learning framework', 'by including', 'six different emotion - related tasks']]",[],"[['Model', 'propose to learn', 'Emo2Vec']]",[],[],[],[],[],[],sentiment_analysis,14,25
hyperparameters,Pre-training Emo2Vec,[],"[('Pre-training Emo2Vec', (0, 2))]",[],[],[],"[['Hyperparameters', 'has', 'Pre-training Emo2Vec']]",[],[],[],[],"[['Pre-training Emo2Vec', 'has', 'Emo2 Vec embedding matrix and the CNN model']]",sentiment_analysis,14,87
hyperparameters,Emo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .,"[('pre-trained using', (9, 11))]","[('Emo2 Vec embedding matrix and the CNN model', (0, 8)), ('hashtag corpus', (11, 13))]","[['Emo2 Vec embedding matrix and the CNN model', 'pre-trained using', 'hashtag corpus']]",[],[],[],[],[],[],[],[],sentiment_analysis,14,88
hyperparameters,Parameters of T and CNN are randomly initialized and Adam is used for optimization .,"[('are', (5, 6)), ('used for', (11, 13))]","[('Parameters of T and CNN', (0, 5)), ('randomly initialized', (6, 8)), ('Adam', (9, 10)), ('optimization', (13, 14))]","[['Adam', 'used for', 'optimization'], ['Parameters of T and CNN', 'are', 'randomly initialized']]",[],[],[],[],[],[],"[['Pre-training Emo2Vec', 'has', 'Adam'], ['Pre-training Emo2Vec', 'has', 'Parameters of T and CNN']]",[],sentiment_analysis,14,89
hyperparameters,"For the best model , we use the batch size of 16 , embedding size of 100 , 1024 filters and filter sizes are 1 , 3 ,5 and 7 respectively .",[],[],"[['best model', 'use', 'batch size'], ['batch size', 'of', '16'], ['best model', 'use', 'embedding size'], ['embedding size', 'of', '100'], ['filter sizes', 'are', '1 , 3 ,5 and 7']]","[['best model', 'has', '1024 filters'], ['1024 filters', 'has', 'filter sizes']]",[],[],[],"[['Pre-training Emo2Vec', 'For', 'best model']]",[],[],[],sentiment_analysis,14,91
hyperparameters,Multi - task training,[],"[('Multi - task training', (0, 4))]",[],[],[],"[['Hyperparameters', 'has', 'Multi - task training']]",[],[],[],[],[],sentiment_analysis,14,94
hyperparameters,"We tune our parameters of learning rate , L2 regularization , whether to pre-train our model and batch size with the average accuracy of the development set of all datasets .","[('tune', (1, 2)), ('of', (4, 5))]","[('our parameters', (2, 4)), ('learning rate', (5, 7)), ('L2 regularization', (8, 10))]","[['our parameters', 'of', 'learning rate'], ['our parameters', 'of', 'L2 regularization']]",[],[],[],[],"[['Multi - task training', 'tune', 'our parameters']]",[],[],[],sentiment_analysis,14,95
hyperparameters,We early stop our model when the averaged dev accuracy stop increasing .,"[('early stop', (1, 3)), ('when', (5, 6)), ('stop', (10, 11))]","[('our model', (3, 5)), ('averaged dev accuracy', (7, 10)), ('increasing', (11, 12))]","[['our model', 'when', 'averaged dev accuracy'], ['averaged dev accuracy', 'stop', 'increasing']]",[],[],[],[],"[['Multi - task training', 'early stop', 'our model']]",[],[],[],sentiment_analysis,14,96
hyperparameters,"Our best model uses learning rate of 0.001 , L2 regularization of 1.0 , batch size of 32 .",[],[],"[['best model', 'uses', 'L2 regularization'], ['L2 regularization', 'of', '1.0'], ['best model', 'uses', 'learning rate'], ['learning rate', 'of', '0.001'], ['best model', 'uses', 'batch size'], ['batch size', 'of', '32']]",[],[],[],[],[],[],"[['Multi - task training', 'has', 'best model']]",[],sentiment_analysis,14,97
hyperparameters,We save the best model and take the embedding layer as Emo2Vec vectors .,"[('save', (1, 2)), ('take', (6, 7)), ('as', (10, 11))]","[('best model', (3, 5)), ('embedding layer', (8, 10)), ('Emo2Vec vectors', (11, 13))]","[['best model', 'take', 'embedding layer'], ['embedding layer', 'as', 'Emo2Vec vectors']]",[],[],[],[],"[['Multi - task training', 'save', 'best model']]",[],[],[],sentiment_analysis,14,98
results,"Compared with CNN embedding : Emo2 Vec works better than CNN embedding on 14 / 18 datasets , giving 2.6 % absolute accuracy improvement for the sentiment task and 1.6 % absolute f1score improvement on the other tasks .",[],[],"[['Emo2 Vec', 'works better than', 'CNN embedding'], ['CNN embedding', 'giving', '2.6 % absolute accuracy improvement'], ['2.6 % absolute accuracy improvement', 'for', 'sentiment task'], ['CNN embedding', 'giving', '1.6 % absolute f1score improvement'], ['1.6 % absolute f1score improvement', 'on', 'other tasks'], ['CNN embedding', 'on', '14 / 18 datasets']]",[],[],"[['Results', 'has', 'Emo2 Vec']]",[],[],[],[],[],sentiment_analysis,14,115
results,It shows multi-task training helps to create better generalized word emotion representations than just using a single task .,"[('shows', (1, 2)), ('helps to create', (4, 7)), ('than just using', (12, 15))]","[('multi-task training', (2, 4)), ('better generalized word emotion representations', (7, 12)), ('single task', (16, 18))]","[['multi-task training', 'helps to create', 'better generalized word emotion representations'], ['better generalized word emotion representations', 'than just using', 'single task']]",[],"[['Results', 'shows', 'multi-task training']]",[],[],[],[],[],[],sentiment_analysis,14,116
results,"Compared with SSWE : Emo2 Vec works much better on all datasets except SS - T datasets , which gives 3.3 % accuracy improvement and 4.7 % f 1 score improvement respectively on sentiment and other tasks .",[],[],"[['much better', 'on', 'all datasets'], ['all datasets', 'except', 'SS - T datasets'], ['all datasets', 'on', 'sentiment and other tasks'], ['sentiment and other tasks', 'gives', '3.3 % accuracy improvement'], ['sentiment and other tasks', 'gives', '4.7 % f 1 score improvement']]",[],[],[],[],"[['Emo2 Vec', 'works', 'much better']]",[],[],[],sentiment_analysis,14,117
results,"On average , it gives 1.3 % improvement in accuracy for the sentiment task and 1.1 % improvement of f 1 - score on the other tasks .","[('gives', (4, 5)), ('in', (8, 9)), ('for', (10, 11)), ('of', (18, 19)), ('on', (23, 24))]","[('1.3 % improvement', (5, 8)), ('accuracy', (9, 10)), ('sentiment task', (12, 14)), ('1.1 % improvement', (15, 18)), ('f 1 - score', (19, 23)), ('other tasks', (25, 27))]","[['1.3 % improvement', 'in', 'accuracy'], ['accuracy', 'for', 'sentiment task'], ['1.1 % improvement', 'of', 'f 1 - score'], ['f 1 - score', 'on', 'other tasks']]",[],"[['Results', 'gives', '1.3 % improvement'], ['Results', 'gives', '1.1 % improvement']]",[],[],[],[],[],[],sentiment_analysis,14,121
results,"Since Emo2 Vec is not trained by predicting contextual words , it is weak on capturing synthetic and semantic meaning .","[('is', (3, 4)), ('not trained by', (4, 7)), ('on capturing', (14, 16))]","[('predicting contextual words', (7, 10)), ('weak', (13, 14)), ('synthetic and semantic meaning', (16, 20))]","[['predicting contextual words', 'is', 'weak'], ['weak', 'on capturing', 'synthetic and semantic meaning']]",[],[],[],[],"[['Emo2 Vec', 'not trained by', 'predicting contextual words']]",[],[],[],sentiment_analysis,14,124
results,"Here , we want to highlight that solely using a simple classifier with good word representation can achieve promising results .","[('solely using', (7, 9)), ('with', (12, 13)), ('can achieve', (16, 18))]","[('simple classifier', (10, 12)), ('good word representation', (13, 16)), ('promising results', (18, 20))]","[['simple classifier', 'with', 'good word representation'], ['good word representation', 'can achieve', 'promising results']]",[],"[['Results', 'solely using', 'simple classifier']]",[],[],[],[],[],[],sentiment_analysis,14,128
results,"Compared with GloVe+ DeepMoji , GloVe + Emo2 Vec achieves same or better results on 11 / 14 datasets , which on average gives 1.0 % improvement .","[('Compared with', (0, 2)), ('achieves', (9, 10)), ('on', (14, 15)), ('on average gives', (21, 24))]","[('GloVe+ DeepMoji , GloVe + Emo2 Vec', (2, 9)), ('same or better results', (10, 14)), ('11 / 14 datasets', (15, 19)), ('1.0 % improvement', (24, 27))]","[['GloVe+ DeepMoji , GloVe + Emo2 Vec', 'achieves', 'same or better results'], ['same or better results', 'on average gives', '1.0 % improvement'], ['same or better results', 'on', '11 / 14 datasets']]",[],"[['Results', 'Compared with', 'GloVe+ DeepMoji , GloVe + Emo2 Vec']]",[],[],[],[],[],[],sentiment_analysis,14,131
results,"GloVe + Emo2 Vec achieves better performances on SOTA results on three datasets ( SE0714 , stress and tube tablet ) and comparable result to SOTA on dataset Previous SOTA results",[],[],"[['GloVe + Emo2 Vec', 'achieves', 'comparable result'], ['comparable result', 'to', 'SOTA'], ['SOTA', 'on', 'dataset'], ['GloVe + Emo2 Vec', 'achieves', 'better performances'], ['better performances', 'on', 'SOTA results'], ['SOTA results', 'on', 'three datasets ( SE0714 , stress and tube tablet )']]",[],[],"[['Results', 'has', 'GloVe + Emo2 Vec']]",[],[],[],[],[],sentiment_analysis,14,132
results,"Thus , to detect the corresponding emotion , more attention needs to be paid to words .","[('to detect', (2, 4)), ('more attention needs to be paid to', (8, 15))]","[('corresponding emotion', (5, 7)), ('words', (15, 16))]","[['corresponding emotion', 'more attention needs to be paid to', 'words']]",[],"[['Results', 'to detect', 'corresponding emotion']]",[],[],[],[],[],[],sentiment_analysis,14,136
research-problem,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,[],"[('Sentiment Treebank', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment Treebank']]",[],[],[],[],sentiment_analysis,15,2
research-problem,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,[],"[('richer supervised training and evaluation resources', (12, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'richer supervised training and evaluation resources']]",[],[],[],[],sentiment_analysis,15,5
dataset,The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .,[],[],"[['Stanford Sentiment Treebank', 'is', 'first corpus'], ['first corpus', 'with', 'fully labeled parse trees'], ['fully labeled parse trees', 'allows for', 'complete analysis'], ['complete analysis', 'of', 'compositional effects'], ['compositional effects', 'of', 'sentiment in language']]",[],[],"[['Dataset', 'name', 'Stanford Sentiment Treebank']]",[],[],[],[],[],sentiment_analysis,15,20
dataset,"The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .","[('based on', (3, 5)), ('consists of', (10, 12)), ('extracted from', (15, 17))]","[('corpus', (1, 2)), ('dataset', (6, 7)), ('11,855 single sentences', (12, 15)), ('movie reviews', (17, 19))]","[['corpus', 'based on', 'dataset'], ['dataset', 'consists of', '11,855 single sentences'], ['11,855 single sentences', 'extracted from', 'movie reviews']]",[],[],"[['Dataset', 'has', 'corpus']]",[],[],[],[],[],sentiment_analysis,15,21
dataset,"It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .","[('with', (3, 4)), ('total of', (10, 12)), ('from', (15, 16)), ('annotated by', (21, 23))]","[('parsed', (2, 3)), ('Stanford parser', (5, 7)), ('215,154 unique phrases', (12, 15)), ('parse trees', (17, 19)), ('3 human judges', (23, 26))]","[['parsed', 'with', 'Stanford parser'], ['parsed', 'total of', '215,154 unique phrases'], ['215,154 unique phrases', 'from', 'parse trees'], ['parse trees', 'annotated by', '3 human judges']]",[],[],"[['Dataset', 'has', 'parsed']]",[],[],[],[],[],sentiment_analysis,15,22
dataset,This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .,"[('to analyze', (5, 7)), ('of', (9, 10)), ('to capture', (12, 14))]","[('new dataset', (1, 3)), ('intricacies', (8, 9)), ('sentiment', (10, 11)), ('complex linguistic phenomena', (14, 17))]","[['new dataset', 'to analyze', 'intricacies'], ['intricacies', 'of', 'sentiment'], ['new dataset', 'to capture', 'complex linguistic phenomena']]",[],[],"[['Dataset', 'has', 'new dataset']]",[],[],[],[],[],sentiment_analysis,15,23
dataset,The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .,"[('enable', (8, 9)), ('to train', (11, 13)), ('based on', (17, 19))]","[('granularity and size', (1, 4)), ('community', (10, 11)), ('compositional models', (13, 15)), ('supervised and structured machine learning techniques', (19, 25))]","[['granularity and size', 'enable', 'community'], ['community', 'to train', 'compositional models'], ['compositional models', 'based on', 'supervised and structured machine learning techniques']]",[],[],"[['Dataset', 'has', 'granularity and size']]",[],[],[],[],[],sentiment_analysis,15,25
model,"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .","[('called', (16, 17))]","[('Recursive Neural Tensor Network ( RNTN )', (18, 25))]",[],[],"[['Model', 'called', 'Recursive Neural Tensor Network ( RNTN )']]",[],[],[],[],[],[],sentiment_analysis,15,27
model,Recursive Neural Tensor Networks take as input phrases of any length .,"[('take as input', (4, 7)), ('of', (8, 9))]","[('Recursive Neural Tensor Networks', (0, 4)), ('phrases', (7, 8)), ('any length', (9, 11))]","[['Recursive Neural Tensor Networks', 'take as input', 'phrases'], ['phrases', 'of', 'any length']]",[],[],"[['Model', 'has', 'Recursive Neural Tensor Networks']]",[],[],[],[],[],sentiment_analysis,15,28
model,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,"[('represent', (1, 2)), ('through', (4, 5)), ('compute', (13, 14)), ('for', (15, 16)), ('in', (18, 19)), ('using', (21, 22))]","[('phrase', (3, 4)), ('word vectors and a parse tree', (5, 11)), ('vectors', (14, 15)), ('higher nodes', (16, 18)), ('tree', (20, 21)), ('same tensor - based composition function', (23, 29))]","[['vectors', 'for', 'higher nodes'], ['higher nodes', 'in', 'tree'], ['tree', 'using', 'same tensor - based composition function'], ['phrase', 'through', 'word vectors and a parse tree']]",[],[],[],[],"[['Recursive Neural Tensor Networks', 'compute', 'vectors'], ['Recursive Neural Tensor Networks', 'represent', 'phrase']]",[],[],[],sentiment_analysis,15,29
hyperparameters,Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30 .,[],[],"[['Optimal performance', 'for', 'all models'], ['all models', 'achieved at', 'word vector sizes'], ['word vector sizes', 'between', '25 and 35 dimensions'], ['all models', 'achieved at', 'batch sizes'], ['batch sizes', 'between', '20 and 30']]",[],[],"[['Hyperparameters', 'has', 'Optimal performance']]",[],[],[],[],[],sentiment_analysis,15,202
hyperparameters,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours .,"[('usually achieve', (3, 5)), ('on', (8, 9)), ('after training for', (12, 15))]","[('RNTN', (1, 2)), ('best performance', (6, 8)), ('dev set', (10, 12)), ('3 - 5 hours', (15, 19))]","[['RNTN', 'usually achieve', 'best performance'], ['best performance', 'on', 'dev set'], ['dev set', 'after training for', '3 - 5 hours']]",[],[],"[['Hyperparameters', 'has', 'RNTN']]",[],[],[],[],[],sentiment_analysis,15,206
hyperparameters,We use f = tanh in all experiments .,"[('use', (1, 2)), ('in', (5, 6))]","[('f = tanh', (2, 5)), ('all experiments', (6, 8))]","[['f = tanh', 'in', 'all experiments']]",[],"[['Hyperparameters', 'use', 'f = tanh']]",[],[],[],[],[],[],sentiment_analysis,15,208
baselines,"We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .",[],[],"[['commonly used methods', 'that use', 'bag of words features'], ['bag of words features', 'with', 'Naive Bayes and SVMs'], ['bag of words features', 'with', 'Naive Bayes'], ['Naive Bayes', 'with', 'bag of bigram features']]",[],"[['Baselines', 'compare to', 'commonly used methods']]",[],[],[],[],[],[],sentiment_analysis,15,209
baselines,We also compare to a model that averages neural word vectors and ignores word order ( VecAvg ) .,"[('averages', (7, 8)), ('ignores', (12, 13))]","[('model', (5, 6)), ('neural word vectors', (8, 11)), ('word order ( VecAvg )', (13, 18))]","[['model', 'ignores', 'word order ( VecAvg )'], ['model', 'averages', 'neural word vectors']]",[],[],"[['Baselines', 'compare to', 'model']]",[],[],[],[],[],sentiment_analysis,15,211
hyperparameters,"The sentences in the treebank were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .","[('in', (2, 3)), ('were split into', (5, 8))]","[('sentences', (1, 2)), ('treebank', (4, 5)), ('train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )', (9, 24))]","[['sentences', 'in', 'treebank'], ['treebank', 'were split into', 'train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )']]",[],[],"[['Hyperparameters', 'has', 'sentences']]",[],[],[],[],[],sentiment_analysis,15,212
results,showed that a fine grained classification into 5 classes is a reasonable approximation to capture most of the data variation .,"[('showed', (0, 1)), ('into', (6, 7)), ('is', (9, 10)), ('to capture', (13, 15))]","[('fine grained classification', (3, 6)), ('5 classes', (7, 9)), ('reasonable approximation', (11, 13)), ('most of the data variation', (15, 20))]","[['fine grained classification', 'into', '5 classes'], ['5 classes', 'is', 'reasonable approximation'], ['reasonable approximation', 'to capture', 'most of the data variation']]",[],"[['Results', 'showed', 'fine grained classification']]",[],[],[],[],[],[],sentiment_analysis,15,217
results,"The RNTN gets the highest performance , followed by the MV - RNN and RNN .","[('gets', (2, 3)), ('followed by', (7, 9))]","[('RNTN', (1, 2)), ('highest performance', (4, 6)), ('MV - RNN and RNN', (10, 15))]","[['RNTN', 'followed by', 'MV - RNN and RNN'], ['RNTN', 'gets', 'highest performance']]",[],[],"[['Results', 'has', 'RNTN']]",[],[],[],[],[],sentiment_analysis,15,219
results,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .","[('work', (3, 4)), ('on', (6, 7)), ('where', (10, 11)), ('are', (14, 15)), ('perform', (22, 23)), ('with', (25, 26))]","[('recursive models', (1, 3)), ('very well', (4, 6)), ('shorter phrases', (7, 9)), ('negation and composition', (11, 14)), ('important', (15, 16)), ('bag of features baselines', (18, 22)), ('well', (23, 24)), ('longer sentences', (26, 28))]","[['recursive models', 'work', 'very well'], ['very well', 'on', 'shorter phrases'], ['shorter phrases', 'where', 'negation and composition'], ['negation and composition', 'are', 'important'], ['bag of features baselines', 'perform', 'well'], ['well', 'with', 'longer sentences']]",[],[],"[['Results', 'has', 'recursive models'], ['Results', 'has', 'bag of features baselines']]",[],[],[],[],[],sentiment_analysis,15,220
results,The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4 % .,"[('combination of', (1, 3)), ('pushes', (10, 11)), ('on', (16, 17)), ('up to', (19, 21))]","[('new sentiment treebank and the RNTN', (4, 10)), ('state of the art', (12, 16)), ('short phrases', (17, 19)), ('85.4 %', (21, 23))]","[['new sentiment treebank and the RNTN', 'pushes', 'state of the art'], ['state of the art', 'on', 'short phrases'], ['short phrases', 'up to', '85.4 %']]",[],"[['Results', 'combination of', 'new sentiment treebank and the RNTN']]",[],[],[],[],[],[],sentiment_analysis,15,229
ablation-analysis,"The RNTN has the highest reversal accuracy , showing its ability to structurally learn negation of positive sentences .","[('showing its ability to', (8, 12))]","[('RNTN', (1, 2)), ('highest reversal accuracy', (4, 7)), ('structurally learn negation of positive sentences', (12, 18))]","[['highest reversal accuracy', 'showing its ability to', 'structurally learn negation of positive sentences']]","[['RNTN', 'has', 'highest reversal accuracy']]",[],"[['Ablation analysis', 'has', 'RNTN']]",[],[],[],[],[],sentiment_analysis,15,247
ablation-analysis,shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were negative .,"[('shows', (0, 1)), ('in which', (4, 6)), ('made', (8, 9)), ('by switching', (11, 13)), ('from', (16, 17))]","[('typical case', (2, 4)), ('sentiment', (6, 7)), ('more positive', (9, 11)), ('main class', (14, 16)), ('negative to neutral', (17, 20))]","[['typical case', 'in which', 'sentiment'], ['sentiment', 'made', 'more positive'], ['more positive', 'by switching', 'main class'], ['main class', 'from', 'negative to neutral']]",[],"[['Ablation analysis', 'shows', 'typical case']]",[],[],[],[],[],[],sentiment_analysis,15,255
ablation-analysis,Therefore we can conclude that the RNTN is best able to identify the effect of negations upon both positive and negative sentiment sentences . :,"[('conclude', (3, 4)), ('best able to identify', (8, 12)), ('upon', (16, 17))]","[('RNTN', (6, 7)), ('effect of negations', (13, 16)), ('positive and negative sentiment sentences', (18, 23))]","[['RNTN', 'best able to identify', 'effect of negations'], ['effect of negations', 'upon', 'positive and negative sentiment sentences']]",[],"[['Ablation analysis', 'conclude', 'RNTN']]",[],[],[],[],[],[],sentiment_analysis,15,259
research-problem,Target - Sensitive Memory Networks for Aspect Sentiment Classification,[],"[('Aspect Sentiment Classification', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Sentiment Classification']]",[],[],[],[],sentiment_analysis,16,2
research-problem,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,[],"[('Aspect sentiment classification ( ASC )', (0, 6)), ('sentiment analysis', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect sentiment classification ( ASC )'], ['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,16,4
research-problem,"However , we found an important problem with the current MNs in performing the ASC task .",[],"[('ASC', (14, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'ASC']]",[],[],[],[],sentiment_analysis,16,8
model,"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .","[('propose', (6, 7)), ('can capture', (17, 19)), ('between', (22, 23))]","[('target - sensitive memory networks ( TMNs )', (7, 15)), ('sentiment interaction', (20, 22)), ('targets and contexts', (23, 26))]","[['target - sensitive memory networks ( TMNs )', 'can capture', 'sentiment interaction'], ['sentiment interaction', 'between', 'targets and contexts']]",[],"[['Model', 'propose', 'target - sensitive memory networks ( TMNs )']]",[],[],[],[],[],[],sentiment_analysis,16,51
baselines,AMN : A state - of - the - art memory network used for ASC .,"[('used for', (12, 14))]","[('AMN', (0, 1)), ('state - of - the - art memory network', (3, 12)), ('ASC', (14, 15))]","[['state - of - the - art memory network', 'used for', 'ASC']]","[['AMN', 'has', 'state - of - the - art memory network']]",[],"[['Baselines', 'has', 'AMN']]",[],[],[],[],[],sentiment_analysis,16,229
baselines,"BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .","[('does not use', (14, 17)), ('for capturing', (20, 22))]","[('BL - MN', (0, 3)), ('Our basic memory network', (4, 8)), ('proposed techniques', (18, 20)), ('target - sensitive sentiments', (22, 26))]","[['Our basic memory network', 'does not use', 'proposed techniques'], ['proposed techniques', 'for capturing', 'target - sensitive sentiments']]","[['BL - MN', 'has', 'Our basic memory network']]",[],"[['Baselines', 'has', 'BL - MN']]",[],[],[],[],[],sentiment_analysis,16,231
baselines,AE - LSTM : RNN / LSTM is another popular attention based neural model .,[],"[('AE - LSTM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'AE - LSTM']]",[],[],[],[],[],sentiment_analysis,16,232
baselines,"Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .","[('compare with', (2, 4)), ('for', (16, 17))]","[('state - of - the - art attention - based LSTM', (5, 16)), ('ASC', (17, 18))]","[['state - of - the - art attention - based LSTM', 'for', 'ASC']]",[],[],[],[],"[['AE - LSTM', 'compare with', 'state - of - the - art attention - based LSTM']]",[],[],[],sentiment_analysis,16,233
baselines,ATAE - LSTM :,[],"[('ATAE - LSTM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],"[['ATAE - LSTM', 'has', 'attention - based LSTM']]",sentiment_analysis,16,234
baselines,Another attention - based LSTM for ASC reported in .,"[('for', (5, 6))]","[('attention - based LSTM', (1, 5)), ('ASC', (6, 7))]","[['attention - based LSTM', 'for', 'ASC']]",[],[],[],[],[],[],[],[],sentiment_analysis,16,235
baselines,Target - sensitive Memory Networks ( TMNs ) :,[],"[('Target - sensitive Memory Networks ( TMNs )', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Target - sensitive Memory Networks ( TMNs )']]",[],[],[],[],"[['Target - sensitive Memory Networks ( TMNs )', 'has', 'six proposed techniques']]",sentiment_analysis,16,236
baselines,"The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .","[('give', (17, 18))]","[('six proposed techniques', (1, 4)), ('NP', (5, 6)), ('CNP', (7, 8)), ('IT', (9, 10)), ('CI', (11, 12)), ('JCI', (13, 14)), ('JPI', (16, 17)), ('six target - sensitive memory networks', (18, 24))]","[['six proposed techniques', 'give', 'six target - sensitive memory networks']]","[['six proposed techniques', 'name', 'NP'], ['six proposed techniques', 'name', 'CNP'], ['six proposed techniques', 'name', 'IT'], ['six proposed techniques', 'name', 'CI'], ['six proposed techniques', 'name', 'JCI'], ['six proposed techniques', 'name', 'JPI']]",[],[],[],[],[],[],[],sentiment_analysis,16,237
hyperparameters,We use the open - domain word embeddings 1 for the initialization of word vectors .,"[('use', (1, 2)), ('for', (9, 10)), ('of', (12, 13))]","[('open - domain word embeddings', (3, 8)), ('initialization', (11, 12)), ('word vectors', (13, 15))]","[['open - domain word embeddings', 'for', 'initialization'], ['initialization', 'of', 'word vectors']]",[],"[['Hyperparameters', 'use', 'open - domain word embeddings']]",[],[],[],[],[],[],sentiment_analysis,16,247
hyperparameters,"We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .","[('initialize', (1, 2)), ('from', (5, 6))]","[('other model parameters', (2, 5)), ('uniform distribution U ( - 0.05 , 0.05 )', (7, 16))]","[['other model parameters', 'from', 'uniform distribution U ( - 0.05 , 0.05 )']]",[],"[['Hyperparameters', 'initialize', 'other model parameters']]",[],[],[],[],[],[],sentiment_analysis,16,248
hyperparameters,The dimension of the word embedding and the size of the hidden layers are 300 .,"[('of', (2, 3)), ('are', (13, 14))]","[('dimension', (1, 2)), ('word embedding', (4, 6)), ('size of the hidden layers', (8, 13)), ('300', (14, 15))]","[['dimension', 'are', '300'], ['300', 'of', 'word embedding'], ['300', 'of', 'size of the hidden layers']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,16,249
hyperparameters,The learning rate is set to 0.01 and the dropout rate is set to 0.1 .,[],[],"[['learning rate', 'set to', '0.01'], ['dropout rate', 'set to', '0.1']]",[],[],"[['Hyperparameters', 'has', 'learning rate'], ['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],sentiment_analysis,16,250
hyperparameters,Stochastic gradient descent is used as our optimizer .,"[('used as', (4, 6))]","[('Stochastic gradient descent', (0, 3)), ('our optimizer', (6, 8))]","[['Stochastic gradient descent', 'used as', 'our optimizer']]",[],[],"[['Hyperparameters', 'has', 'Stochastic gradient descent']]",[],[],[],[],[],sentiment_analysis,16,251
hyperparameters,"We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .","[('compare', (2, 3)), ('in', (6, 7)), ('set to', (24, 26))]","[('memory networks', (4, 6)), ('multiple computational layers version ( i.e. , multiple hops )', (8, 18)), ('number of hops', (20, 23)), ('3', (26, 27))]","[['memory networks', 'in', 'multiple computational layers version ( i.e. , multiple hops )'], ['number of hops', 'set to', '3']]","[['memory networks', 'has', 'number of hops']]","[['Hyperparameters', 'compare', 'memory networks']]",[],[],[],[],[],[],sentiment_analysis,16,253
hyperparameters,"We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.","[('implemented', (1, 2)), ('in', (4, 5)), ('using', (8, 9))]","[('all models', (2, 4)), ('TensorFlow environment', (6, 8)), ('same input , embedding size , dropout rate , optimizer , etc.', (9, 21))]","[['all models', 'in', 'TensorFlow environment'], ['TensorFlow environment', 'using', 'same input , embedding size , dropout rate , optimizer , etc.']]",[],"[['Hyperparameters', 'implemented', 'all models']]",[],[],[],[],[],[],sentiment_analysis,16,254
results,"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .","[('Comparing', (0, 1)), ('see', (14, 15)), ('achieved by', (18, 20)), ('on', (28, 29)), ('each of them has', (33, 37)), ('over', (40, 41)), ('from', (49, 50)), ('using', (53, 54))]","[('1 - hop memory networks', (2, 7)), ('significant performance gains', (15, 18)), ('CNP , CI , JCI , and JPI', (20, 28)), ('both datasets', (29, 31)), ('p < 0.01', (37, 40)), ('strongest baseline ( BL - MN )', (42, 49)), ('paired t- test', (50, 53)), ('F1 - Macro', (54, 57))]","[['1 - hop memory networks', 'see', 'significant performance gains'], ['significant performance gains', 'achieved by', 'CNP , CI , JCI , and JPI'], ['significant performance gains', 'on', 'both datasets'], ['both datasets', 'each of them has', 'p < 0.01'], ['p < 0.01', 'over', 'strongest baseline ( BL - MN )'], ['strongest baseline ( BL - MN )', 'from', 'paired t- test'], ['p < 0.01', 'using', 'F1 - Macro']]",[],"[['Results', 'Comparing', '1 - hop memory networks']]",[],[],[],[],[],[],sentiment_analysis,16,265
results,"2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .","[('In', (2, 3)), ('achieve', (10, 11)), ('on', (14, 15))]","[('3 - hop setting', (4, 8)), ('TMNs', (9, 10)), ('much better results', (11, 14)), ('Restaurant', (15, 16))]","[['TMNs', 'achieve', 'much better results'], ['much better results', 'on', 'Restaurant']]","[['3 - hop setting', 'has', 'TMNs']]","[['Results', 'In', '3 - hop setting']]",[],[],[],[],[],[],sentiment_analysis,16,268
results,"JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .","[('achieve', (6, 7)), ('outperforming', (11, 12)), ('by', (16, 17))]","[('JCI , IT , and CI', (0, 6)), ('best scores', (8, 10)), ('strongest baseline AMN', (13, 16)), ('2.38 % , 2.18 % , and 2.03 %', (17, 26))]","[['JCI , IT , and CI', 'achieve', 'best scores'], ['JCI , IT , and CI', 'outperforming', 'strongest baseline AMN'], ['strongest baseline AMN', 'by', '2.38 % , 2.18 % , and 2.03 %']]",[],[],[],[],[],[],"[['3 - hop setting', 'has', 'JCI , IT , and CI']]",[],sentiment_analysis,16,269
results,"On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .","[('On', (0, 1)), ('perform', (15, 16))]","[('Laptop', (1, 2)), ('BL - MN and most TMNs', (3, 9)), ('similarly', (16, 17))]","[['BL - MN and most TMNs', 'perform', 'similarly']]","[['Laptop', 'has', 'BL - MN and most TMNs']]","[['Results', 'On', 'Laptop']]",[],[],[],[],[],[],sentiment_analysis,16,270
results,"3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .","[('see that', (7, 9)), ('works', (10, 11))]","[('all TMNs', (3, 5)), ('JCI', (9, 10)), ('best', (12, 13))]","[['all TMNs', 'see that', 'JCI'], ['JCI', 'works', 'best']]",[],[],"[['Results', 'Comparing', 'all TMNs']]",[],[],[],[],[],sentiment_analysis,16,272
results,CI and JPI also perform well in most cases .,"[('perform', (4, 5)), ('in', (6, 7))]","[('CI and JPI', (0, 3)), ('well', (5, 6)), ('most cases', (7, 9))]","[['CI and JPI', 'perform', 'well'], ['well', 'in', 'most cases']]",[],[],"[['Results', 'has', 'CI and JPI']]",[],[],[],[],[],sentiment_analysis,16,273
results,"IT , NP , and CNP can achieve very good scores in some cases but are less stable .","[('achieve', (7, 8)), ('in', (11, 12)), ('are', (15, 16))]","[('IT , NP , and CNP', (0, 6)), ('very good scores', (8, 11)), ('some cases', (12, 14)), ('less stable', (16, 18))]","[['IT , NP , and CNP', 'achieve', 'very good scores'], ['very good scores', 'in', 'some cases'], ['very good scores', 'are', 'less stable']]",[],[],"[['Results', 'has', 'IT , NP , and CNP']]",[],[],[],[],[],sentiment_analysis,16,274
research-problem,Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks,[],"[('Improved Semantic Representations', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Improved Semantic Representations']]",[],[],[],[],sentiment_analysis,17,2
research-problem,"Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .",[],"[('predicting the semantic relatedness of two sentences', (15, 22)), ('sentiment classification', (31, 33))]",[],[],[],[],"[['Contribution', 'has research problem', 'predicting the semantic relatedness of two sentences'], ['Contribution', 'has research problem', 'sentiment classification']]",[],[],[],[],sentiment_analysis,17,8
model,"In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .","[('introduce', (5, 6)), ('to', (13, 14)), ('show', (20, 21)), ('for representing', (23, 25)), ('over', (27, 28))]","[('generalization of the standard LSTM architecture', (7, 13)), ('tree - structured network topologies', (14, 19)), ('superiority', (22, 23)), ('sentence meaning', (25, 27)), ('sequential LSTM', (29, 31))]","[['generalization of the standard LSTM architecture', 'to', 'tree - structured network topologies'], ['tree - structured network topologies', 'show', 'superiority'], ['superiority', 'for representing', 'sentence meaning'], ['sentence meaning', 'over', 'sequential LSTM']]",[],"[['Model', 'introduce', 'generalization of the standard LSTM architecture']]",[],[],[],[],[],[],sentiment_analysis,17,33
model,"While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .","[('composes', (4, 5)), ('from', (8, 9)), ('of', (20, 21))]","[('state', (7, 8)), ('tree - structured LSTM , or Tree - LSTM', (31, 40)), ('input vector', (46, 48)), ('hidden states', (50, 52)), ('arbitrarily many child units', (53, 57))]","[['tree - structured LSTM , or Tree - LSTM', 'composes', 'state'], ['state', 'from', 'input vector'], ['state', 'from', 'hidden states'], ['hidden states', 'of', 'arbitrarily many child units']]",[],[],"[['Model', 'has', 'tree - structured LSTM , or Tree - LSTM']]",[],[],[],[],[],sentiment_analysis,17,34
code,Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,17,39
hyperparameters,The hyperparameters for our models were tuned on the development set for each task .,"[('tuned on', (6, 8))]","[('development set', (9, 11))]",[],[],"[['Hyperparameters', 'tuned on', 'development set']]",[],[],[],[],[],[],sentiment_analysis,17,182
hyperparameters,We initialized our word representations using publicly available 300 - dimensional Glove vectors,"[('initialized', (1, 2)), ('using', (5, 6))]","[('our word representations', (2, 5)), ('publicly available 300 - dimensional Glove vectors', (6, 13))]","[['our word representations', 'using', 'publicly available 300 - dimensional Glove vectors']]",[],"[['Hyperparameters', 'initialized', 'our word representations']]",[],[],[],[],[],[],sentiment_analysis,17,183
hyperparameters,"For the sentiment classification task , word representations were updated during training with a learning rate of 0.1 .","[('For', (0, 1)), ('updated', (9, 10)), ('with', (12, 13)), ('of', (16, 17))]","[('sentiment classification task', (2, 5)), ('word representations', (6, 8)), ('during training', (10, 12)), ('learning rate', (14, 16)), ('0.1', (17, 18))]","[['word representations', 'updated', 'during training'], ['during training', 'with', 'learning rate'], ['learning rate', 'of', '0.1']]","[['sentiment classification task', 'has', 'word representations']]","[['Hyperparameters', 'For', 'sentiment classification task']]",[],[],[],[],[],[],sentiment_analysis,17,185
hyperparameters,"For the semantic relatedness task , word representations were held fixed as we did not observe any significant improvement when the representations were tuned .","[('held', (9, 10))]","[('semantic relatedness task', (2, 5)), ('word representations', (6, 8)), ('fixed', (10, 11))]","[['word representations', 'held', 'fixed']]","[['semantic relatedness task', 'has', 'word representations']]",[],"[['Hyperparameters', 'For', 'semantic relatedness task']]",[],[],[],[],[],sentiment_analysis,17,186
hyperparameters,Our models were trained using AdaGrad with a learning rate of 0.05 and a minibatch size of 25 .,[],[],"[['Our models', 'trained using', 'AdaGrad'], ['AdaGrad', 'with', 'minibatch size'], ['minibatch size', 'of', '25'], ['AdaGrad', 'with', 'learning rate'], ['learning rate', 'of', '0.05']]",[],[],"[['Hyperparameters', 'has', 'Our models']]",[],[],[],[],[],sentiment_analysis,17,187
hyperparameters,The model parameters were regularized with a per-minibatch L2 regularization strength of 10 ?4 .,"[('regularized with', (4, 6)), ('of', (11, 12))]","[('model parameters', (1, 3)), ('per-minibatch L2 regularization strength', (7, 11)), ('10 ?4', (12, 14))]","[['model parameters', 'regularized with', 'per-minibatch L2 regularization strength'], ['per-minibatch L2 regularization strength', 'of', '10 ?4']]",[],[],"[['Hyperparameters', 'has', 'model parameters']]",[],[],[],[],[],sentiment_analysis,17,188
hyperparameters,The sentiment classifier was additionally regularized using dropout with a dropout rate of 0.5 .,"[('additionally regularized using', (4, 7)), ('with', (8, 9)), ('of', (12, 13))]","[('sentiment classifier', (1, 3)), ('dropout', (7, 8)), ('dropout rate', (10, 12)), ('0.5', (13, 14))]","[['sentiment classifier', 'additionally regularized using', 'dropout'], ['dropout', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5']]",[],[],"[['Hyperparameters', 'has', 'sentiment classifier']]",[],[],[],[],[],sentiment_analysis,17,189
research-problem,Effective Attention Modeling for Aspect - Level Sentiment Classification,[],"[('Aspect - Level Sentiment Classification', (4, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,18,2
research-problem,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,[],"[('fine - grained sentiment analysis', (10, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'fine - grained sentiment analysis']]",[],[],[],[],sentiment_analysis,18,16
approach,We propose two novel approaches for improving the effectiveness of attention models .,"[('propose', (1, 2)), ('for improving', (5, 7))]","[('two novel approaches', (2, 5)), ('effectiveness of attention models', (8, 12))]","[['two novel approaches', 'for improving', 'effectiveness of attention models']]",[],"[['Approach', 'propose', 'two novel approaches']]",[],[],[],[],[],[],sentiment_analysis,18,27
approach,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,"[('of', (7, 8))]","[('first approach', (1, 3))]",[],[],[],"[['Approach', 'has', 'first approach']]",[],[],[],[],[],sentiment_analysis,18,28
approach,"To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .","[('model', (10, 11)), ('as', (13, 14))]","[('each target', (11, 13)), ('mixture', (15, 16)), ('K aspect embeddings', (17, 20))]","[['each target', 'as', 'mixture']]","[['mixture', 'of', 'K aspect embeddings']]",[],[],[],"[['first approach', 'model', 'each target']]",[],[],[],sentiment_analysis,18,40
approach,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .,"[('use', (1, 2)), ('to learn', (5, 7)), ('as', (19, 20)), ('of', (23, 24))]","[('autoencoder structure', (3, 5)), ('both the aspect embeddings as well as the representation of the target', (7, 19)), ('weighted combination', (21, 23)), ('aspect embeddings', (25, 27))]","[['autoencoder structure', 'to learn', 'both the aspect embeddings as well as the representation of the target'], ['both the aspect embeddings as well as the representation of the target', 'as', 'weighted combination'], ['weighted combination', 'of', 'aspect embeddings']]",[],[],[],[],"[['first approach', 'use', 'autoencoder structure']]",[],[],[],sentiment_analysis,18,41
approach,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,"[('jointly trained with', (4, 7)), ('to provide', (14, 16)), ('as well as', (20, 23)), ('on', (26, 27))]","[('autoencoder structure', (1, 3)), ('neural attention - based sentiment classifier', (8, 14)), ('good target representation', (17, 20)), ('high accuracy', (24, 26)), ('predicted sentiment', (28, 30))]","[['autoencoder structure', 'jointly trained with', 'neural attention - based sentiment classifier'], ['neural attention - based sentiment classifier', 'to provide', 'good target representation'], ['good target representation', 'as well as', 'high accuracy'], ['high accuracy', 'on', 'predicted sentiment']]",[],[],[],[],[],[],"[['first approach', 'has', 'autoencoder structure']]",[],sentiment_analysis,18,43
approach,Our second approach exploits syntactic information to construct a syntax - based attention model .,"[('exploits', (3, 4)), ('to construct', (6, 8))]","[('second approach', (1, 3)), ('syntactic information', (4, 6)), ('syntax - based attention model', (9, 14))]","[['second approach', 'exploits', 'syntactic information'], ['syntactic information', 'to construct', 'syntax - based attention model']]",[],[],"[['Approach', 'has', 'second approach']]",[],[],[],[],[],sentiment_analysis,18,46
approach,"Instead , our syntax - based attention mechanism selectively focuses on a small subset of context words that are close to the target on the syntactic path which is obtained by applying a dependency parser on the review sentence .",[],[],"[['syntax - based attention mechanism', 'selectively focuses on', 'small subset of context words'], ['small subset of context words', 'close to', 'target'], ['target', 'on', 'syntactic path'], ['syntactic path', 'obtained by', 'applying a dependency parser'], ['applying a dependency parser', 'on', 'review sentence']]",[],[],[],[],[],[],"[['second approach', 'has', 'syntax - based attention mechanism']]",[],sentiment_analysis,18,50
baselines,( 1 ) Feature - based SVM :,[],"[('Feature - based SVM', (3, 7))]",[],[],[],"[['Baselines', 'has', 'Feature - based SVM']]",[],[],[],[],[],sentiment_analysis,18,165
baselines,We compare with the reported results of a top system in SemEval 2014 .,"[('compare with', (1, 3)), ('of', (6, 7)), ('in', (10, 11))]","[('reported results', (4, 6)), ('top system', (8, 10)), ('SemEval 2014', (11, 13))]","[['reported results', 'of', 'top system'], ['top system', 'in', 'SemEval 2014']]",[],[],[],[],"[['Feature - based SVM', 'compare with', 'reported results']]",[],[],[],sentiment_analysis,18,166
baselines,( 2 ) LSTM : An LSTM network is built on top of word embeddings .,"[('built on', (9, 11))]","[('LSTM', (3, 4)), ('top of word embeddings', (11, 15))]","[['LSTM', 'built on', 'top of word embeddings']]",[],[],"[['Baselines', 'has', 'LSTM']]",[],[],[],[],[],sentiment_analysis,18,168
results,"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .","[('achieves', (15, 16)), ('on', (18, 19)), ('without relying on', (22, 25))]","[('our best model', (12, 15)), ('competitive results', (16, 18)), ('D1 and D2', (19, 22)), ('so many manually - designed features and external resources', (25, 34))]","[['our best model', 'achieves', 'competitive results'], ['competitive results', 'on', 'D1 and D2'], ['D1 and D2', 'without relying on', 'so many manually - designed features and external resources']]",[],[],"[['Results', 'has', 'our best model']]",[],[],[],[],[],sentiment_analysis,18,179
results,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .","[('Compared with', (2, 4)), ('achieves', (12, 13)), ('on', (21, 22)), ('for', (29, 30))]","[('all other neural baselines', (4, 8)), ('our full model', (9, 12)), ('statistically significant improvements ( p < 0.05 )', (13, 21)), ('both accuracies and macro - F1 scores', (22, 29)), ('D1 , D3 , D4', (30, 35))]","[['our full model', 'achieves', 'statistically significant improvements ( p < 0.05 )'], ['statistically significant improvements ( p < 0.05 )', 'on', 'both accuracies and macro - F1 scores'], ['both accuracies and macro - F1 scores', 'for', 'D1 , D3 , D4']]","[['all other neural baselines', 'has', 'our full model']]","[['Results', 'Compared with', 'all other neural baselines']]",[],[],[],[],[],[],sentiment_analysis,18,180
results,"3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .","[('of', (11, 12)), ('are able to achieve', (14, 18)), ('on', (26, 27))]","[('LSTM + ATT', (4, 7)), ('all three settings', (8, 11)), ('our model', (12, 14)), ('statistically significant improvements ( p < 0.05 )', (18, 26)), ('all datasets', (27, 29))]","[['all three settings', 'of', 'our model'], ['our model', 'are able to achieve', 'statistically significant improvements ( p < 0.05 )'], ['statistically significant improvements ( p < 0.05 )', 'on', 'all datasets']]","[['LSTM + ATT', 'has', 'all three settings']]",[],"[['Results', 'Compared with', 'LSTM + ATT']]",[],[],[],[],[],sentiment_analysis,18,181
results,4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .,"[('achieves', (8, 9)), ('compared to using', (12, 15))]","[('integrated full model over all', (3, 8)), ('best performance', (10, 12)), ('only one of the two proposed approaches', (15, 22))]","[['integrated full model over all', 'achieves', 'best performance'], ['best performance', 'compared to using', 'only one of the two proposed approaches']]",[],[],"[['Results', 'has', 'integrated full model over all']]",[],[],[],[],[],sentiment_analysis,18,183
results,"5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .","[('is', (6, 7)), ('on', (9, 10)), ('than', (20, 21))]","[('proposed target representation', (3, 6)), ('more helpful', (7, 9)), ('restaurant domain ( D1 , D3 , and D4 )', (10, 20)), ('laptop domain ( D2 )', (21, 26))]","[['proposed target representation', 'is', 'more helpful'], ['more helpful', 'than', 'laptop domain ( D2 )'], ['more helpful', 'on', 'restaurant domain ( D1 , D3 , and D4 )']]",[],[],"[['Results', 'has', 'proposed target representation']]",[],[],[],[],[],sentiment_analysis,18,185
research-problem,Improved Sentence Modeling using Suffix Bidirectional LSTM,[],"[('Improved Sentence Modeling', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Improved Sentence Modeling']]",[],[],[],[],sentiment_analysis,19,2
research-problem,"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .",[],"[('computing representations of sequential data', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'computing representations of sequential data']]",[],[],[],[],sentiment_analysis,19,4
research-problem,Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .,[],"[('fine - grained sentiment classification', (14, 19)), ('question classification', (20, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'fine - grained sentiment classification'], ['Contribution', 'has research problem', 'question classification']]",[],[],[],[],sentiment_analysis,19,12
research-problem,Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .,[],"[('modeling sequential data', (14, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'modeling sequential data']]",[],[],[],[],sentiment_analysis,19,18
model,"In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .","[('propose', (5, 6)), ('to compute', (13, 15)), ('that capture', (17, 19))]","[('simple , general and effective technique', (7, 13)), ('contextual representations', (15, 17)), ('long range dependencies', (19, 22))]","[['simple , general and effective technique', 'to compute', 'contextual representations'], ['contextual representations', 'that capture', 'long range dependencies']]",[],"[['Model', 'propose', 'simple , general and effective technique']]",[],[],[],[],[],[],sentiment_analysis,19,30
model,"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .","[('For', (0, 1)), ('encode both', (6, 8)), ('in both', (12, 14))]","[('each token t', (1, 4)), ('prefix and suffix', (9, 12)), ('forward and reverse direction', (15, 19))]","[['each token t', 'encode both', 'prefix and suffix'], ['prefix and suffix', 'in both', 'forward and reverse direction']]",[],"[['Model', 'For', 'each token t']]",[],[],[],[],[],[],sentiment_analysis,19,31
model,"Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .","[('combine', (3, 4)), ('by', (9, 10)), ('to produce', (16, 18)), ('in both', (24, 26))]","[('prefix and suffix representations', (5, 9)), ('simple max - pooling operation', (11, 16)), ('richer contextual representation', (19, 22)), ('forward and reverse direction', (27, 31))]","[['prefix and suffix representations', 'by', 'simple max - pooling operation'], ['simple max - pooling operation', 'to produce', 'richer contextual representation'], ['richer contextual representation', 'in both', 'forward and reverse direction']]",[],"[['Model', 'combine', 'prefix and suffix representations']]",[],[],[],[],[],[],sentiment_analysis,19,34
model,We call our model Suffix BiLSTM or SuBiLSTM in short .,"[('call', (1, 2)), ('in', (8, 9))]","[('Suffix BiLSTM', (4, 6)), ('SuBiLSTM', (7, 8)), ('short', (9, 10))]","[['SuBiLSTM', 'in', 'short']]",[],"[['Model', 'call', 'Suffix BiLSTM'], ['Model', 'call', 'SuBiLSTM']]",[],[],[],[],[],[],sentiment_analysis,19,35
baselines,"For each of the tasks , we compare SuBiLSTM and SuBiLSTM - Tied with a single - layer BiLSTM and a 2 - layer BiLSTM encoder with the same hidden dimension .",[],[],"[['SuBiLSTM and SuBiLSTM - Tied', 'with', 'single - layer BiLSTM and a 2 - layer BiLSTM encoder'], ['single - layer BiLSTM and a 2 - layer BiLSTM encoder', 'with', 'same hidden dimension']]",[],"[['Baselines', 'compare', 'SuBiLSTM and SuBiLSTM - Tied']]",[],[],[],[],[],[],sentiment_analysis,19,115
results,"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .","[('of', (3, 4)), ('are', (11, 12))]","[('relative performance', (1, 3)), ('SuBiL - STM and SuBiLSTM - Tied', (4, 11)), ('fairly close', (12, 14))]","[['relative performance', 'of', 'SuBiL - STM and SuBiLSTM - Tied'], ['SuBiL - STM and SuBiLSTM - Tied', 'are', 'fairly close']]",[],[],"[['Results', 'has', 'relative performance']]",[],[],[],[],[],sentiment_analysis,19,125
results,"SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .","[('works', (3, 4)), ('on', (5, 6))]","[('SuBiLSTM - Tied', (0, 3)), ('better', (4, 5)), ('small datasets ( SST and TREC )', (6, 13))]","[['SuBiLSTM - Tied', 'works', 'better'], ['better', 'on', 'small datasets ( SST and TREC )']]",[],[],"[['Results', 'has', 'SuBiLSTM - Tied']]",[],[],[],[],[],sentiment_analysis,19,126
results,"For the larger datasets ( SNLI and QUORA ) , SuBILSTM slightly edges out the tied version owing to its larger capacity .","[('For', (0, 1)), ('edges out', (12, 14)), ('owing to', (17, 19))]","[('larger datasets ( SNLI and QUORA )', (2, 9)), ('SuBILSTM', (10, 11)), ('tied version', (15, 17)), ('larger capacity', (20, 22))]","[['SuBILSTM', 'edges out', 'tied version'], ['SuBILSTM', 'owing to', 'larger capacity']]","[['larger datasets ( SNLI and QUORA )', 'has', 'SuBILSTM']]","[['Results', 'For', 'larger datasets ( SNLI and QUORA )']]",[],[],[],[],[],[],sentiment_analysis,19,127
results,"The training complexity for both the models is similar and hence , with half the parameters , SuBILSTM - Tied should be the more favored model for sentence modeling tasks .","[('for both', (3, 5)), ('is', (7, 8)), ('with', (12, 13)), ('should be', (20, 22)), ('for', (26, 27))]","[('training complexity', (1, 3)), ('models', (6, 7)), ('similar', (8, 9)), ('half the parameters', (13, 16)), ('SuBILSTM - Tied', (17, 20)), ('more favored model', (23, 26)), ('sentence modeling tasks', (27, 30))]","[['training complexity', 'for both', 'models'], ['models', 'is', 'similar'], ['SuBILSTM - Tied', 'with', 'half the parameters'], ['half the parameters', 'should be', 'more favored model'], ['more favored model', 'for', 'sentence modeling tasks']]","[['models', 'has', 'SuBILSTM - Tied']]",[],"[['Results', 'has', 'training complexity']]",[],[],[],[],[],sentiment_analysis,19,128
research-problem,A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis,[],"[('Aspect - level Sentiment Analysis', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level Sentiment Analysis']]",[],[],[],[],sentiment_analysis,2,2
research-problem,"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .",[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,2,18
model,"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .","[('propose', (10, 11)), ('based on', (21, 23))]","[('position - aware bidirectional attention network ( PBAN )', (12, 21)), ('bidirectional Gated Recurrent Units ( Bi - GRU )', (23, 32))]","[['position - aware bidirectional attention network ( PBAN )', 'based on', 'bidirectional Gated Recurrent Units ( Bi - GRU )']]",[],"[['Model', 'propose', 'position - aware bidirectional attention network ( PBAN )']]",[],[],[],[],[],[],sentiment_analysis,2,39
,"In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .",[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,2,40
model,"1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .","[('of', (5, 6)), ('in', (8, 9)), ('based on', (11, 13)), ('converting', (19, 20)), ('into', (23, 24))]","[('Obtaining position information', (2, 5)), ('each word', (6, 8)), ('corresponding sentence', (9, 11)), ('current aspect term', (14, 17)), ('position information', (21, 23)), ('position embedding', (24, 26))]","[['Obtaining position information', 'of', 'each word'], ['each word', 'in', 'corresponding sentence'], ['corresponding sentence', 'based on', 'current aspect term'], ['current aspect term', 'converting', 'position information'], ['position information', 'into', 'position embedding']]",[],[],"[['Model', 'has', 'Obtaining position information']]",[],[],[],[],[],sentiment_analysis,2,42
model,2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .,"[('composes of', (4, 6)), ('focusing on extracting', (11, 14))]","[('PBAN', (3, 4)), ('two Bi - GRU networks', (6, 11)), ('aspectlevel features', (15, 17)), ('sentence - level features', (18, 22))]","[['PBAN', 'composes of', 'two Bi - GRU networks'], ['two Bi - GRU networks', 'focusing on extracting', 'aspectlevel features'], ['two Bi - GRU networks', 'focusing on extracting', 'sentence - level features']]",[],[],"[['Model', 'has', 'PBAN']]",[],[],[],[],[],sentiment_analysis,2,43
model,3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .,"[('to model', (7, 9)), ('between', (12, 13))]","[('bidirectional attention mechanism', (4, 7)), ('mutual relation', (10, 12)), ('aspect term and its corresponding sentence', (13, 19))]","[['bidirectional attention mechanism', 'to model', 'mutual relation'], ['mutual relation', 'between', 'aspect term and its corresponding sentence']]",[],[],"[['Model', 'has', 'bidirectional attention mechanism']]",[],[],[],[],[],sentiment_analysis,2,44
hyperparameters,"In our experiments , all word embedding are initialized by the pre-trained Glove vector 2 .","[('initialized by', (8, 10))]","[('word embedding', (5, 7)), ('pre-trained Glove vector', (11, 14))]","[['word embedding', 'initialized by', 'pre-trained Glove vector']]",[],[],"[['Hyperparameters', 'has', 'word embedding']]",[],[],[],[],[],sentiment_analysis,2,120
hyperparameters,"All the weight matrices are given the initial value by sampling from the uniform distribution U ( ?0.1 , 0.1 ) , and all the biases are set to zero .","[('given', (5, 6)), ('by', (9, 10)), ('from', (11, 12)), ('are set to', (26, 29))]","[('weight matrices', (2, 4)), ('initial value', (7, 9)), ('sampling', (10, 11)), ('uniform distribution U ( ?0.1 , 0.1 )', (13, 21)), ('biases', (25, 26)), ('zero', (29, 30))]","[['biases', 'are set to', 'zero'], ['weight matrices', 'given', 'initial value'], ['initial value', 'by', 'sampling'], ['sampling', 'from', 'uniform distribution U ( ?0.1 , 0.1 )']]",[],[],"[['Hyperparameters', 'has', 'biases'], ['Hyperparameters', 'has', 'weight matrices']]",[],[],[],[],[],sentiment_analysis,2,121
hyperparameters,"The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .",[],[],"[['word embedding and aspect term embedding', 'set to', '300'], ['hidden units', 'set to', '200']]",[],[],"[['Hyperparameters', 'has', 'hidden units']]",[],[],[],"[['dimension', 'of', 'word embedding and aspect term embedding']]",[],sentiment_analysis,2,122
hyperparameters,"The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .","[('of', (2, 3)), ('set to', (6, 8)), ('which is', (10, 12)), ('during', (16, 17))]","[('dimension', (1, 2)), ('position embedding', (3, 5)), ('100', (8, 9)), ('randomly initialized and updated', (12, 16)), ('training process', (18, 20))]","[['dimension', 'of', 'position embedding'], ['position embedding', 'set to', '100'], ['100', 'which is', 'randomly initialized and updated'], ['randomly initialized and updated', 'during', 'training process']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,2,123
hyperparameters,"We use Tensorflow to implement our proposed model and employ the Momentum as the training method , whose momentum parameter ? is set to 0.9 , ? is set to 10 ? 6 , and the initial learning rate is set to 0.01 .",[],[],"[['Momentum', 'as', 'training method'], ['Momentum', 'whose', 'momentum parameter ?'], ['momentum parameter ?', 'set to', '0.9'], ['Momentum', 'whose', 'initial learning rate'], ['initial learning rate', 'set to', '0.01'], ['Tensorflow', 'to implement', 'our proposed model']]",[],"[['Hyperparameters', 'employ', 'Momentum'], ['Hyperparameters', 'use', 'Tensorflow']]",[],[],[],[],[],[],sentiment_analysis,2,124
baselines,LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .,[],[],"[['LSTM', 'takes', 'sentence'], ['sentence', 'as', 'input'], ['input', 'to get', 'hidden representation'], ['hidden representation', 'of', 'each word']]",[],[],"[['Baselines', 'has', 'LSTM']]",[],[],[],[],[],sentiment_analysis,2,134
baselines,"Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .",[],[],"[['average value', 'of', 'hidden states'], ['representation of sentence', 'puts it into', 'softmax layer'], ['softmax layer', 'to predict', 'probability'], ['probability', 'of', 'each sentiment polarity']]","[['hidden states', 'as', 'representation of sentence']]",[],[],[],"[['LSTM', 'regards', 'average value']]",[],[],[],sentiment_analysis,2,135
baselines,"AE - LSTM : AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity .","[('models', (8, 9)), ('in', (11, 12)), ('via', (13, 14)), ('concatenate', (17, 18)), ('to', (21, 22)), ('for calculating', (26, 28)), ('employed to produce', (34, 37)), ('for', (40, 41)), ('to judge', (44, 46))]","[('AE - LSTM', (0, 3)), ('words', (10, 11)), ('sentence', (12, 13)), ('LSTM network', (14, 16)), ('aspect embedding', (19, 21)), ('hidden contextual representation', (23, 26)), ('attention weights', (29, 31)), ('final representation', (38, 40)), ('input sentence', (42, 44)), ('sentiment polarity', (47, 49))]","[['AE - LSTM', 'models', 'words'], ['words', 'in', 'sentence'], ['sentence', 'via', 'LSTM network'], ['AE - LSTM', 'concatenate', 'aspect embedding'], ['aspect embedding', 'to', 'hidden contextual representation'], ['hidden contextual representation', 'for calculating', 'attention weights'], ['attention weights', 'employed to produce', 'final representation'], ['final representation', 'for', 'input sentence'], ['input sentence', 'to judge', 'sentiment polarity']]",[],[],"[['Baselines', 'has', 'AE - LSTM']]",[],[],[],[],[],sentiment_analysis,2,137
baselines,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .","[('extended', (7, 8)), ('by appending', (11, 13)), ('to', (16, 17)), ('to represent', (22, 24))]","[('ATAE - LSTM', (0, 3)), ('AE - LSTM', (8, 11)), ('aspect embedding', (14, 16)), ('each word embedding', (17, 20)), ('input sentence', (25, 27))]","[['ATAE - LSTM', 'extended', 'AE - LSTM'], ['AE - LSTM', 'by appending', 'aspect embedding'], ['aspect embedding', 'to', 'each word embedding'], ['each word embedding', 'to represent', 'input sentence']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,2,138
baselines,IAN : IAN considers the separate modeling of aspect terms and sentences respectively .,"[('considers', (3, 4)), ('of', (7, 8))]","[('IAN', (0, 1)), ('separate modeling', (5, 7)), ('aspect terms and sentences', (8, 12))]","[['IAN', 'considers', 'separate modeling'], ['separate modeling', 'of', 'aspect terms and sentences']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,2,140
baselines,"MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .","[('applies', (3, 4)), ('on', (7, 8)), ('so that', (12, 14)), ('could be selected from', (17, 21))]","[('MemNet', (0, 1)), ('attention', (4, 5)), ('multiple times', (5, 7)), ('word embedding', (9, 11)), ('more abstractive evidences', (14, 17)), ('external memory', (22, 24))]","[['MemNet', 'applies', 'attention'], ['multiple times', 'so that', 'more abstractive evidences'], ['more abstractive evidences', 'could be selected from', 'external memory'], ['multiple times', 'on', 'word embedding']]","[['attention', 'has', 'multiple times']]",[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,2,143
results,shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .,"[('on', (10, 11))]","[('datasets Restaurant and Laptop', (11, 15))]",[],[],"[['Results', 'on', 'datasets Restaurant and Laptop']]",[],[],[],[],[],[],sentiment_analysis,2,145
results,We can observe that our proposed PBAN model achieves the best performance among all methods .,"[('observe', (2, 3)), ('achieves', (8, 9)), ('among', (12, 13))]","[('our proposed PBAN model', (4, 8)), ('best performance', (10, 12)), ('methods', (14, 15))]","[['our proposed PBAN model', 'achieves', 'best performance'], ['best performance', 'among', 'methods']]",[],[],[],[],"[['datasets Restaurant and Laptop', 'observe', 'our proposed PBAN model']]",[],[],[],sentiment_analysis,2,146
results,"Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .","[('by integrating', (3, 5)), ('achieves', (15, 16)), ('effectively judge', (29, 31)), ('of', (34, 35)), ('in', (38, 39)), ('to improve', (44, 46))]","[('position information and the bidirectional attention mechanism', (6, 13)), ('PBAN', (14, 15)), ('state - of - the - art performances', (17, 25)), ('sentiment polarity', (32, 34)), ('different aspect term', (35, 38)), ('corresponding sentence', (40, 42)), ('classification accuracy', (47, 49))]","[['PBAN', 'effectively judge', 'sentiment polarity'], ['sentiment polarity', 'of', 'different aspect term'], ['different aspect term', 'in', 'corresponding sentence'], ['corresponding sentence', 'to improve', 'classification accuracy'], ['PBAN', 'achieves', 'state - of - the - art performances']]","[['position information and the bidirectional attention mechanism', 'has', 'PBAN']]",[],[],[],"[['datasets Restaurant and Laptop', 'by integrating', 'position information and the bidirectional attention mechanism']]",[],[],[],sentiment_analysis,2,160
research-problem,DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis,[],"[('Message - level and Topic - based Sentiment Analysis', (11, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'Message - level and Topic - based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,20,2
research-problem,"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" .",[],"[('Sentiment Analysis in Twitter', (19, 23))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment Analysis in Twitter']]",[],[],[],[],sentiment_analysis,20,4
research-problem,"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .",[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,20,12
model,"In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .","[('present', (5, 6)), ('competed at', (12, 14))]","[('two deep - learning systems', (6, 11)), ('SemEval - 2017 Task 4', (14, 19))]","[['two deep - learning systems', 'competed at', 'SemEval - 2017 Task 4']]",[],"[['Model', 'present', 'two deep - learning systems']]",[],[],[],[],[],[],sentiment_analysis,20,17
model,Our first model is designed for addressing the problem of messagelevel sentiment analysis .,"[('designed for addressing', (4, 7))]","[('first model', (1, 3)), ('problem of messagelevel sentiment analysis', (8, 13))]","[['first model', 'designed for addressing', 'problem of messagelevel sentiment analysis']]",[],[],"[['Model', 'has', 'first model']]",[],[],[],[],[],sentiment_analysis,20,18
model,"We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .","[('employ', (1, 2)), ('equipped with', (9, 11))]","[('2 - layer Bidirectional LSTM', (3, 8)), ('attention mechanism', (12, 14))]","[['2 - layer Bidirectional LSTM', 'equipped with', 'attention mechanism']]",[],[],[],[],"[['first model', 'employ', '2 - layer Bidirectional LSTM']]",[],[],[],sentiment_analysis,20,19
model,"For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .","[('For', (0, 1)), ('propose', (10, 11)), ('with', (15, 16))]","[('topic - based sentiment analysis tasks', (2, 8)), ('Siamese Bidirectional LSTM', (12, 15)), ('contextaware attention mechanism', (17, 20))]","[['topic - based sentiment analysis tasks', 'propose', 'Siamese Bidirectional LSTM'], ['Siamese Bidirectional LSTM', 'with', 'contextaware attention mechanism']]",[],"[['Model', 'For', 'topic - based sentiment analysis tasks']]",[],[],[],[],[],[],sentiment_analysis,20,20
hyperparameters,MSA Model ( message - level ),[],"[('MSA Model', (0, 2))]",[],[],[],"[['Hyperparameters', 'has', 'MSA Model']]",[],[],[],[],"[['MSA Model', 'has', 'LSTM layers 150']]",sentiment_analysis,20,79
hyperparameters,TSA Model ( topic - based ),[],"[('TSA Model', (0, 2))]",[],[],[],"[['Hyperparameters', 'has', 'TSA Model']]",[],[],[],[],[],sentiment_analysis,20,103
hyperparameters,"The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .","[('size of', (1, 3)), ('is', (6, 7))]","[('embedding layer', (4, 6)), ('300', (7, 8)), ('LSTM layers 150', (11, 14))]","[['embedding layer', 'is', '300'], ['embedding layer', 'is', '300']]",[],[],[],[],"[['MSA Model', 'size of', 'embedding layer']]",[],[],[],sentiment_analysis,20,156
hyperparameters,"We add Gaussian noise with ? = 0.2 and dropout of 0.3 at the embedding layer , dropout of 0.5 at the LSTM layers and dropout of 0.25 at the recurrent connections of the LSTM .",[],[],"[['Gaussian noise', 'with', '? = 0.2'], ['dropout', 'of', '0.3'], ['0.3', 'at', 'embedding layer'], ['dropout', 'of', '0.5'], ['0.5', 'at', 'LSTM layers'], ['dropout', 'of', '0.25'], ['0.25', 'at', 'recurrent connections'], ['recurrent connections', 'of', 'LSTM'], ['dropout', 'of', '0.3'], ['Gaussian noise', 'with', '? = 0.2']]",[],[],[],[],"[['MSA Model', 'add', 'Gaussian noise'], ['MSA Model', 'add', 'dropout']]",[],[],[],sentiment_analysis,20,157
hyperparameters,"Finally , we add L 2 regularization of 0.0001 at the loss function .","[('of', (7, 8)), ('at', (9, 10))]","[('L 2 regularization', (4, 7)), ('0.0001', (8, 9)), ('loss function', (11, 13))]","[['L 2 regularization', 'of', '0.0001'], ['0.0001', 'at', 'loss function']]",[],[],[],[],[],[],"[['MSA Model', 'add', 'L 2 regularization']]",[],sentiment_analysis,20,158
hyperparameters,"The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .",[],[],[],"[['LSTM layers', 'has', '64']]",[],[],[],"[['TSA Model', 'size of', 'LSTM layers'], ['TSA Model', 'size of', 'embedding layer']]",[],[],[],sentiment_analysis,20,160
hyperparameters,"We insert Gaussian noise with ? = 0.2 at the embedding layer of both inputs and dropout of 0.3 at the embedding layer of the message , dropout of 0.2 at the LSTM layer and the recurrent connection of the LSTM layer and dropout of 0.3 at the attention layer and the Maxout layer .",[],[],"[['Gaussian noise', 'with', 'dropout'], ['dropout', 'of', '0.2'], ['0.2', 'at', 'LSTM layer'], ['0.2', 'at', 'recurrent connection'], ['recurrent connection', 'of', 'LSTM layer'], ['0.3', 'at', 'attention layer and the Maxout layer'], ['? = 0.2', 'at', 'embedding layer'], ['embedding layer', 'of', 'both inputs']]",[],[],[],[],"[['TSA Model', 'insert', 'Gaussian noise']]",[],[],[],sentiment_analysis,20,161
hyperparameters,"Finally , we add L 2 regularization of 0.001 at the loss function .","[('add', (3, 4)), ('of', (7, 8)), ('at', (9, 10))]","[('L 2 regularization', (4, 7)), ('0.001', (8, 9)), ('loss function', (11, 13))]","[['L 2 regularization', 'of', '0.001'], ['0.001', 'at', 'loss function']]",[],[],[],[],"[['TSA Model', 'add', 'L 2 regularization']]",[],[],[],sentiment_analysis,20,162
results,"Our official ranking is 1/38 ( tie ) in Subtask A , 2/24 in Subtask B , 2/16 in Subtask C , 2/16 in Subtask D and 11/12 in Subtask E.",[],[],"[['Our official ranking', 'is', '1/38 ( tie )'], ['1/38 ( tie )', 'in', 'Subtask A'], ['Our official ranking', 'is', '2/16'], ['2/16', 'in', 'Subtask C'], ['2/16', 'in', 'Subtask D'], ['Our official ranking', 'is', '2/24'], ['2/24', 'in', 'Subtask B'], ['Our official ranking', 'is', '11/12']]",[],[],"[['Results', 'has', 'Our official ranking']]",[],[],"[['11/12', 'in', 'Subtask E']]",[],[],sentiment_analysis,20,165
research-problem,Sentiment Classification using Document Embeddings trained with Cosine Similarity,[],"[('Sentiment Classification', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment Classification']]",[],[],[],[],sentiment_analysis,21,2
research-problem,"In document - level sentiment classification , each document must be mapped to a fixed length vector .",[],"[('document - level sentiment classification', (1, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'document - level sentiment classification']]",[],[],[],[],sentiment_analysis,21,4
research-problem,"In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .",[],"[('sentiment classification', (6, 8)), ('binary sentiment classification of long movie reviews', (15, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment classification'], ['Contribution', 'has research problem', 'binary sentiment classification of long movie reviews']]",[],[],[],[],sentiment_analysis,21,10
model,This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .,"[('aims to improve', (2, 5)), ('by training', (9, 11)), ('using', (13, 14))]","[('document embedding models', (6, 9)), ('document embeddings', (11, 13)), ('cosine similarity', (14, 16))]","[['document embedding models', 'by training', 'document embeddings'], ['document embeddings', 'using', 'cosine similarity']]",[],"[['Model', 'aims to improve', 'document embedding models']]",[],[],[],[],[],[],sentiment_analysis,21,13
model,"For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .",[],[],"[['predict', 'given', 'document'], ['words / n - grams', 'in', 'document']]","[['maximize', 'has', 'cosine similarity'], ['predict', 'has', 'words / n - grams']]","[['Model', 'trying to', 'maximize'], ['Model', 'trying to', 'predict']]",[],[],[],[],[],[],sentiment_analysis,21,14
code,Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,"[('Code', (0, 1))]",[],[],[],[],[],[],[],[],[],"[['Contribution', 'Code', 'https://github.com/tanthongtan/dv-cosine']]",sentiment_analysis,21,92
hyperparameters,Grid search was performed using 20 % of the training data as a validation set in order to determine the optimal hyperparameters as well as whether to use a constant learning rate or learning rate annealing .,"[('performed using', (3, 5)), ('as', (11, 12)), ('to determine', (17, 19)), ('to use', (26, 28))]","[('Grid search', (0, 2)), ('20 % of the training data', (5, 11)), ('validation set', (13, 15)), ('optimal hyperparameters', (20, 22)), ('constant learning rate or learning rate annealing', (29, 36))]","[['Grid search', 'performed using', '20 % of the training data'], ['20 % of the training data', 'as', 'validation set'], ['validation set', 'to determine', 'optimal hyperparameters'], ['validation set', 'to use', 'constant learning rate or learning rate annealing']]",[],[],"[['Hyperparameters', 'has', 'Grid search']]",[],[],[],[],[],sentiment_analysis,21,94
hyperparameters,"We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .","[('tune', (3, 4)), ('from', (8, 9))]","[('number of iterations', (5, 8)), ('learning rate', (10, 12)), ('[ 0.25 , 0.025 , 0.0025 , 0.001 ]', (13, 22))]","[['learning rate', 'from', '[ 0.25 , 0.025 , 0.0025 , 0.001 ]']]",[],"[['Hyperparameters', 'tune', 'number of iterations']]","[['Hyperparameters', 'has', 'learning rate']]",[],[],[],[],[],sentiment_analysis,21,97
hyperparameters,"In the case of using L2 regularized dot product , ? ( regularization strength ) was chosen from [ 1 , 0.1 , 0.01 ] .","[('using', (4, 5)), ('chosen from', (16, 18))]","[('L2 regularized dot product', (5, 9)), ('[ 1 , 0.1 , 0.01 ]', (18, 25))]","[['L2 regularized dot product', 'chosen from', '[ 1 , 0.1 , 0.01 ]']]",[],"[['Hyperparameters', 'using', 'L2 regularized dot product']]",[],[],[],[],[],[],sentiment_analysis,21,100
hyperparameters,"The optimal learning rate in the case of cosine similarity is extremely small , suggesting a chaotic error surface .","[('in the case of', (4, 8)), ('is', (10, 11)), ('suggesting', (14, 15))]","[('optimal learning rate', (1, 4)), ('cosine similarity', (8, 10)), ('extremely small', (11, 13)), ('chaotic error surface', (16, 19))]","[['optimal learning rate', 'in the case of', 'cosine similarity'], ['cosine similarity', 'is', 'extremely small'], ['extremely small', 'suggesting', 'chaotic error surface']]",[],[],"[['Hyperparameters', 'has', 'optimal learning rate']]",[],[],[],[],[],sentiment_analysis,21,101
hyperparameters,The model in turn requires a larger number of epochs for convergence .,"[('requires', (4, 5)), ('for', (10, 11))]","[('larger number of epochs', (6, 10)), ('convergence', (11, 12))]","[['larger number of epochs', 'for', 'convergence']]",[],"[['Hyperparameters', 'requires', 'larger number of epochs']]",[],[],[],[],[],[],sentiment_analysis,21,103
hyperparameters,"For the distribution for sampling negative words , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .","[('For', (0, 1)), ('for', (3, 4)), ('used', (9, 10)), ('raised to', (13, 15))]","[('distribution', (2, 3)), ('sampling negative words', (4, 7)), ('n-gram distribution', (11, 13)), ('3 / 4 th power', (16, 21))]","[['distribution', 'for', 'sampling negative words'], ['sampling negative words', 'used', 'n-gram distribution'], ['n-gram distribution', 'raised to', '3 / 4 th power']]",[],"[['Hyperparameters', 'For', 'distribution']]",[],[],[],[],[],[],sentiment_analysis,21,104
hyperparameters,"The weights of the networks were initialized from a uniform distribution in the range of [ - 0.001 , 0.001 ] .",[],[],"[['weights', 'of', 'networks'], ['networks', 'were', 'initialized'], ['initialized', 'from', 'uniform distribution'], ['uniform distribution', 'in', 'range'], ['range', 'of', '[ - 0.001 , 0.001 ]']]",[],[],"[['Hyperparameters', 'has', 'weights']]",[],[],[],[],[],sentiment_analysis,21,105
results,From here we see that using cosine similarity instead of dot product improves accuracy across the board .,"[('see that using', (3, 6)), ('instead of', (8, 10)), ('improves', (12, 13))]","[('cosine similarity', (6, 8)), ('dot product', (10, 12)), ('accuracy', (13, 14))]","[['cosine similarity', 'instead of', 'dot product'], ['dot product', 'improves', 'accuracy']]",[],"[['Results', 'see that using', 'cosine similarity']]",[],[],[],[],[],[],sentiment_analysis,21,109
results,However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .,"[('to', (4, 5)), ('suggest that', (5, 7)), ('from', (8, 9)), ('improves', (15, 16))]","[('switching', (7, 8)), ('dot product', (9, 11)), ('cosine similarity', (12, 14)), ('accuracy', (16, 17))]","[['switching', 'improves', 'accuracy'], ['switching', 'from', 'dot product'], ['dot product', 'to', 'cosine similarity']]",[],"[['Results', 'suggest that', 'switching']]",[],[],[],[],[],[],sentiment_analysis,21,111
results,"As seen during grid search , whenever the initial learning rate was 0.25 , accuracy was always poor .","[('during', (2, 3)), ('was', (11, 12))]","[('grid search', (3, 5)), ('initial learning rate', (8, 11)), ('0.25', (12, 13))]","[['initial learning rate', 'was', '0.25']]","[['grid search', 'has', 'initial learning rate']]","[['Results', 'during', 'grid search']]",[],[],[],[],[],[],sentiment_analysis,21,115
results,"Introducing L2 regularization to dot product improves accuracy for all cases except a depreciation in the case of using unigrams only , lucikily cosine similarity does not suffer from this same depreciation .","[('Introducing', (0, 1)), ('to', (3, 4)), ('improves', (6, 7)), ('for', (8, 9)), ('except', (11, 12)), ('in the case of using', (14, 19))]","[('L2 regularization', (1, 3)), ('dot product', (4, 6)), ('accuracy', (7, 8)), ('all cases', (9, 11)), ('depreciation', (13, 14)), ('unigrams', (19, 20))]","[['L2 regularization', 'improves', 'accuracy'], ['accuracy', 'for', 'all cases'], ['all cases', 'except', 'depreciation'], ['depreciation', 'in the case of using', 'unigrams'], ['L2 regularization', 'to', 'dot product']]",[],"[['Results', 'Introducing', 'L2 regularization']]",[],[],[],[],[],[],sentiment_analysis,21,116
research-problem,Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,[],"[('Aspect-level Sentiment Analysis', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect-level Sentiment Analysis']]",[],[],[],[],sentiment_analysis,22,2
research-problem,"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",[],"[('sentiment analysis', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,22,12
model,"Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .","[('propose', (11, 12)), ('for', (21, 22))]","[('hierarchical attention based positionaware network ( HAPN )', (13, 21)), ('aspect - level sentiment classification', (22, 27))]","[['hierarchical attention based positionaware network ( HAPN )', 'for', 'aspect - level sentiment classification']]",[],"[['Model', 'propose', 'hierarchical attention based positionaware network ( HAPN )']]",[],[],[],[],[],[],sentiment_analysis,22,37
model,A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .,"[('for modelling', (8, 10)), ('to achieve', (12, 14)), ('of', (20, 21))]","[('position - aware encoding layer', (1, 6)), ('sentence', (11, 12)), ('position - aware abstract representation', (15, 20)), ('each word', (21, 23))]","[['position - aware encoding layer', 'for modelling', 'sentence'], ['position - aware encoding layer', 'to achieve', 'position - aware abstract representation'], ['position - aware abstract representation', 'of', 'each word']]",[],[],"[['Model', 'has', 'position - aware encoding layer']]",[],[],[],[],[],sentiment_analysis,22,38
model,"On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .","[('proposed to', (10, 12)), ('information of', (14, 16)), ('achieving', (21, 22))]","[('succinct fusion mechanism', (5, 8)), ('fuse', (12, 13)), ('aspects and the contexts', (16, 20)), ('final sentence representation', (23, 26))]","[['succinct fusion mechanism', 'proposed to', 'fuse'], ['fuse', 'information of', 'aspects and the contexts'], ['aspects and the contexts', 'achieving', 'final sentence representation']]",[],[],"[['Model', 'has', 'succinct fusion mechanism']]",[],[],[],[],[],sentiment_analysis,22,39
model,"Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .","[('feed', (3, 4)), ('into', (8, 9)), ('to predict', (12, 14))]","[('achieved sentence representation', (5, 8)), ('softmax layer', (10, 12)), ('sentiment polarity', (15, 17))]","[['achieved sentence representation', 'into', 'softmax layer'], ['softmax layer', 'to predict', 'sentiment polarity']]",[],"[['Model', 'feed', 'achieved sentence representation']]",[],[],[],[],[],[],sentiment_analysis,22,40
code,We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,22,43
hyperparameters,"We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M ) for our experiments , as previous works did .","[('use', (1, 2)), ('pre-trained by', (7, 9))]","[('300 - dimension word vectors', (2, 7)), ('GloVe', (9, 10))]","[['300 - dimension word vectors', 'pre-trained by', 'GloVe']]",[],"[['Hyperparameters', 'use', '300 - dimension word vectors']]",[],[],[],[],[],[],sentiment_analysis,22,128
hyperparameters,"All out - of - vocabulary words are initialized as zero vectors , and all biases are set to zero .","[('initialized as', (8, 10)), ('set to', (17, 19))]","[('All out - of - vocabulary words', (0, 7)), ('zero vectors', (10, 12)), ('all biases', (14, 16)), ('zero', (19, 20))]","[['All out - of - vocabulary words', 'initialized as', 'zero vectors'], ['all biases', 'set to', 'zero']]",[],[],"[['Hyperparameters', 'has', 'All out - of - vocabulary words'], ['Hyperparameters', 'has', 'all biases']]",[],[],[],[],[],sentiment_analysis,22,129
hyperparameters,The dimensions of hidden states and fused embeddings are set to 300 .,"[('of', (2, 3)), ('set to', (9, 11))]","[('dimensions', (1, 2)), ('hidden states and fused embeddings', (3, 8)), ('300', (11, 12))]","[['dimensions', 'set to', '300'], ['300', 'of', 'hidden states and fused embeddings']]",[],[],"[['Hyperparameters', 'has', 'dimensions']]",[],[],[],[],[],sentiment_analysis,22,130
hyperparameters,The dimension of position embeddings is set to 50 .,"[('of', (2, 3)), ('set to', (6, 8))]","[('dimension', (1, 2)), ('position embeddings', (3, 5)), ('50', (8, 9))]","[['dimension', 'of', 'position embeddings'], ['position embeddings', 'set to', '50']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,22,131
hyperparameters,Keras is used for implementing our neural network model .,"[('for implementing', (3, 5))]","[('Keras', (0, 1)), ('neural network model', (6, 9))]","[['Keras', 'for implementing', 'neural network model']]",[],[],"[['Hyperparameters', 'has', 'Keras']]",[],[],[],[],[],sentiment_analysis,22,132
hyperparameters,"In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .",[],[],"[['model training', 'set', 'learning rate'], ['learning rate', 'to', '0.001'], ['model training', 'set', 'batch size'], ['batch size', 'to', '64'], ['model training', 'set', 'dropout rate'], ['dropout rate', 'to', '0.5']]",[],"[['Hyperparameters', 'In', 'model training']]",[],[],[],[],[],[],sentiment_analysis,22,133
hyperparameters,The paired t- test is used for the significance testing .,"[('used for', (5, 7))]","[('paired t- test', (1, 4)), ('significance testing', (8, 10))]","[['paired t- test', 'used for', 'significance testing']]",[],[],"[['Hyperparameters', 'has', 'paired t- test']]",[],[],[],[],[],sentiment_analysis,22,134
baselines,Majority assigns the sentiment polarity with most frequent occurrences in the training set to each sample in test set .,[],[],"[['Majority', 'assigns', 'sentiment polarity'], ['sentiment polarity', 'with', 'most frequent occurrences'], ['most frequent occurrences', 'in', 'training set'], ['training set', 'to', 'each sample'], ['each sample', 'in', 'test set']]",[],[],"[['Baselines', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,22,137
baselines,Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for prediction respectively .,"[('adopt', (7, 8)), ('to model', (18, 20)), ('use', (23, 24)), ('of', (27, 28)), ('for', (31, 32))]","[('Bi - LSTM and Bi - GRU', (0, 7)), ('Bi - LSTM and a Bi - GRU network', (9, 18)), ('sentence', (21, 22)), ('hidden state', (25, 27)), ('final word', (29, 31)), ('prediction', (32, 33))]","[['Bi - LSTM and Bi - GRU', 'adopt', 'Bi - LSTM and a Bi - GRU network'], ['Bi - LSTM and a Bi - GRU network', 'use', 'hidden state'], ['hidden state', 'of', 'final word'], ['final word', 'for', 'prediction'], ['Bi - LSTM and a Bi - GRU network', 'to model', 'sentence']]",[],[],"[['Baselines', 'has', 'Bi - LSTM and Bi - GRU']]",[],[],[],[],[],sentiment_analysis,22,139
baselines,TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .,[],[],"[['TD - LSTM', 'adopts', 'two LSTMs'], ['two LSTMs', 'to model', 'right context'], ['right context', 'with', 'target'], ['two LSTMs', 'to model', 'left context'], ['left context', 'with', 'target'], ['TD - LSTM', 'takes', 'hidden states'], ['hidden states', 'of', 'LSTM'], ['LSTM', 'at', 'last time - step'], ['last time - step', 'to represent', 'sentence'], ['sentence', 'for', 'prediction']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,22,141
baselines,"MemNet applies attention multiple times on the word embeddings , and the output of last attention is fed to softmax for prediction .","[('applies', (1, 2)), ('on', (5, 6)), ('of', (13, 14)), ('fed to', (17, 19)), ('for', (20, 21))]","[('MemNet', (0, 1)), ('attention', (2, 3)), ('multiple times', (3, 5)), ('word embeddings', (7, 9)), ('output', (12, 13)), ('last attention', (14, 16)), ('softmax', (19, 20)), ('prediction', (21, 22))]","[['MemNet', 'applies', 'attention'], ['multiple times', 'on', 'word embeddings'], ['output', 'of', 'last attention'], ['output', 'fed to', 'softmax'], ['softmax', 'for', 'prediction']]","[['attention', 'has', 'multiple times'], ['MemNet', 'has', 'output']]",[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,22,143
baselines,"IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .","[('learns', (2, 3)), ('in', (4, 5)), ('generates', (11, 12)), ('for', (14, 15))]","[('IAN', (0, 1)), ('attentions', (3, 4)), ('contexts and targets', (6, 9)), ('representations', (13, 14)), ('targets and contexts', (15, 18))]","[['IAN', 'generates', 'representations'], ['representations', 'for', 'targets and contexts'], ['IAN', 'learns', 'attentions'], ['attentions', 'in', 'contexts and targets']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,22,145
baselines,RAM ) is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"[('is', (2, 3)), ('where', (6, 7)), ('consists of', (9, 11)), ('of', (15, 16)), ('to learn', (22, 24))]","[('RAM', (0, 1)), ('multilayer architecture', (4, 6)), ('each layer', (7, 9)), ('attention - based aggregation', (11, 15)), ('word features', (16, 18)), ('GRU cell', (20, 22)), ('sentence representation', (25, 27))]","[['RAM', 'is', 'multilayer architecture'], ['multilayer architecture', 'where', 'each layer'], ['each layer', 'consists of', 'GRU cell'], ['GRU cell', 'to learn', 'sentence representation'], ['each layer', 'consists of', 'attention - based aggregation'], ['attention - based aggregation', 'of', 'word features']]",[],[],"[['Baselines', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,22,147
baselines,"LCR - Rot employs three Bi- LSTMs to model the left context , the target and the right context .","[('employs', (3, 4)), ('to model', (7, 9))]","[('LCR - Rot', (0, 3)), ('three Bi- LSTMs', (4, 7)), ('left context', (10, 12)), ('target', (14, 15)), ('right context', (17, 19))]","[['LCR - Rot', 'employs', 'three Bi- LSTMs'], ['three Bi- LSTMs', 'to model', 'left context'], ['three Bi- LSTMs', 'to model', 'target'], ['three Bi- LSTMs', 'to model', 'right context']]",[],[],"[['Baselines', 'has', 'LCR - Rot']]",[],[],[],[],[],sentiment_analysis,22,149
baselines,AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .,"[('introduces', (3, 4)), ('to model', (14, 16)), ('in', (19, 20)), ('explicitly capture', (24, 26)), ('between', (28, 29))]","[('AOA - LSTM', (0, 3)), ('attention - over- attention ( AOA ) based network', (5, 14)), ('aspects and sentences', (16, 19)), ('joint way', (21, 23)), ('interaction', (27, 28)), ('aspects and context sentences', (29, 33))]","[['AOA - LSTM', 'introduces', 'attention - over- attention ( AOA ) based network'], ['attention - over- attention ( AOA ) based network', 'explicitly capture', 'interaction'], ['interaction', 'between', 'aspects and context sentences'], ['attention - over- attention ( AOA ) based network', 'to model', 'aspects and sentences'], ['aspects and sentences', 'in', 'joint way']]",[],[],"[['Baselines', 'has', 'AOA - LSTM']]",[],[],[],[],[],sentiment_analysis,22,152
results,"( 2 ) The TD - LSTM model , which has been shown to be better than LSTM , gets the worst performance of all RNN based models and the accuracy achieved by TD - LSTM is 2.94 % and 2.4 % lower than those by Bi - LSTM on the two datasets respectively .","[('shown to be better than', (12, 17)), ('gets', (19, 20)), ('of', (23, 24)), ('achieved by', (31, 33)), ('is', (36, 37)), ('lower than', (42, 44))]","[('TD - LSTM model', (4, 8)), ('LSTM', (17, 18)), ('worst performance', (21, 23)), ('all RNN based models', (24, 28)), ('accuracy', (30, 31)), ('TD - LSTM', (33, 36)), ('2.94 % and 2.4 %', (37, 42)), ('Bi - LSTM', (46, 49))]","[['accuracy', 'achieved by', 'TD - LSTM'], ['TD - LSTM', 'is', '2.94 % and 2.4 %'], ['2.94 % and 2.4 %', 'lower than', 'Bi - LSTM'], ['TD - LSTM model', 'gets', 'worst performance'], ['worst performance', 'of', 'all RNN based models'], ['TD - LSTM model', 'shown to be better than', 'LSTM']]","[['TD - LSTM model', 'has', 'accuracy']]",[],"[['Results', 'has', 'TD - LSTM model']]",[],[],[],[],[],sentiment_analysis,22,155
results,"( 3 ) Compared with the state - of - the - art methods , our model achieves the best performance , which illustrates the effectiveness of the proposed approach .","[('Compared with', (3, 5)), ('achieves', (17, 18))]","[('state - of - the - art methods', (6, 14)), ('our model', (15, 17)), ('best performance', (19, 21))]","[['our model', 'achieves', 'best performance']]","[['state - of - the - art methods', 'has', 'our model']]","[['Results', 'Compared with', 'state - of - the - art methods']]",[],[],[],[],[],[],sentiment_analysis,22,159
results,"Our method achieves accuracies of 82.23 % as well as 77 . 27 % on the Restaurant and Laptop dataset respectively , which are 0.89 % and 2.03 % higher than the current best method .","[('achieves', (2, 3)), ('of', (4, 5)), ('as well as', (7, 10)), ('on', (14, 15))]","[('Our method', (0, 2)), ('accuracies', (3, 4)), ('82.23 %', (5, 7)), ('77 . 27 %', (10, 14)), ('Restaurant and Laptop dataset', (16, 20))]","[['Our method', 'achieves', 'accuracies'], ['accuracies', 'of', '82.23 %'], ['82.23 %', 'as well as', '77 . 27 %'], ['77 . 27 %', 'on', 'Restaurant and Laptop dataset']]",[],[],"[['Results', 'has', 'Our method']]",[],[],[],[],[],sentiment_analysis,22,160
results,"After introducing the position embeddings , the accuracy has an increase of 0.62 % and 2.67 % on two datasets .","[('introducing', (1, 2)), ('has an', (8, 10)), ('of', (11, 12)), ('on', (17, 18))]","[('position embeddings', (3, 5)), ('accuracy', (7, 8)), ('increase', (10, 11)), ('0.62 % and 2.67 %', (12, 17)), ('two datasets', (18, 20))]","[['accuracy', 'has an', 'increase'], ['increase', 'of', '0.62 % and 2.67 %'], ['0.62 % and 2.67 %', 'on', 'two datasets']]","[['position embeddings', 'has', 'accuracy']]","[['Results', 'introducing', 'position embeddings']]",[],[],[],[],[],[],sentiment_analysis,22,170
results,"In addition , another observation is that Bi - GRU - PW performs even worse than Bi - GRU .","[('performs', (12, 13)), ('than', (15, 16))]","[('Bi - GRU - PW', (7, 12)), ('even worse', (13, 15)), ('Bi - GRU', (16, 19))]","[['Bi - GRU - PW', 'performs', 'even worse'], ['even worse', 'than', 'Bi - GRU']]",[],[],"[['Results', 'has', 'Bi - GRU - PW']]",[],[],[],[],[],sentiment_analysis,22,172
results,The accuracy achieved by Bi - GRU - PW is 0.72 % as well as 1.41 % lower than that by Bi - GRU on the Restaurant and Laptop dataset respectively .,"[('achieved by', (2, 4)), ('is', (9, 10)), ('as well as', (12, 15)), ('lower than', (17, 19)), ('on', (24, 25))]","[('accuracy', (1, 2)), ('Bi - GRU - PW', (4, 9)), ('0.72 %', (10, 12)), ('1.41 %', (15, 17)), ('Bi - GRU', (21, 24)), ('Restaurant and Laptop dataset', (26, 30))]","[['accuracy', 'achieved by', 'Bi - GRU - PW'], ['Bi - GRU - PW', 'is', '0.72 %'], ['0.72 %', 'as well as', '1.41 %'], ['1.41 %', 'lower than', 'Bi - GRU'], ['Bi - GRU', 'on', 'Restaurant and Laptop dataset']]",[],[],"[['Results', 'has', 'accuracy']]",[],[],[],[],[],sentiment_analysis,22,173
results,HAPN achieves improvement of 0.35 % and 0.78 % on accuracy respectively on the two dataset .,"[('achieves', (1, 2)), ('of', (3, 4)), ('on', (9, 10))]","[('HAPN', (0, 1)), ('improvement', (2, 3)), ('0.35 % and 0.78 %', (4, 9)), ('accuracy', (10, 11))]","[['HAPN', 'achieves', 'improvement'], ['improvement', 'of', '0.35 % and 0.78 %'], ['0.35 % and 0.78 %', 'on', 'accuracy']]",[],[],"[['Results', 'has', 'HAPN']]",[],[],[],[],[],sentiment_analysis,22,180
results,( 1 ) The information fusion operation is only used to calculate the Source2context attention value .,"[('used to', (9, 11))]","[('information fusion operation', (4, 7)), ('calculate', (11, 12)), ('Source2context attention value', (13, 16))]","[['information fusion operation', 'used to', 'calculate']]","[['calculate', 'has', 'Source2context attention value']]",[],"[['Results', 'has', 'information fusion operation']]",[],[],[],[],[],sentiment_analysis,22,192
results,The output of Source2aspect attention is only used for information fusion .,"[('of', (2, 3)), ('used for', (7, 9))]","[('output', (1, 2)), ('Source2aspect attention', (3, 5)), ('information fusion', (9, 11))]","[['output', 'of', 'Source2aspect attention'], ['Source2aspect attention', 'used for', 'information fusion']]",[],[],"[['Results', 'has', 'output']]",[],[],[],[],[],sentiment_analysis,22,193
results,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .","[('achieving', (16, 17)), ('of', (19, 20)), ('on', (25, 26))]","[('Bi - GRU - PE', (6, 11)), ('accuracies', (18, 19)), ('80.89 % and 76.02 %', (20, 25)), ('two datasets', (27, 29))]","[['Bi - GRU - PE', 'achieving', 'accuracies'], ['accuracies', 'of', '80.89 % and 76.02 %'], ['80.89 % and 76.02 %', 'on', 'two datasets']]",[],[],"[['Results', 'has', 'Bi - GRU - PE']]",[],[],[],[],[],sentiment_analysis,22,195
research-problem,Discriminative Neural Sentence Modeling by Tree - Based Convolution,[],"[('Discriminative Neural Sentence Modeling', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Discriminative Neural Sentence Modeling']]",[],[],[],[],sentiment_analysis,23,2
model,"In this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .","[('propose', (5, 6)), ('for', (10, 11)), ('called', (15, 16))]","[('novel neural architecture', (7, 10)), ('discriminative sentence modeling', (11, 14)), ('Tree - Based Convolutional Neural Network ( TBCNN )', (17, 26))]","[['novel neural architecture', 'for', 'discriminative sentence modeling']]",[],"[['Model', 'called', 'Tree - Based Convolutional Neural Network ( TBCNN )'], ['Model', 'propose', 'novel neural architecture']]",[],[],[],[],[],[],sentiment_analysis,23,26
model,"Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .","[('leverage', (3, 4))]","[('different sentence parsing trees', (4, 8))]",[],[],"[['Model', 'leverage', 'different sentence parsing trees']]",[],[],[],[],[],[],sentiment_analysis,23,27
model,"The model variants are denoted as c- TBCNN and d - TBCNN , respectively .","[('denoted as', (4, 6))]","[('variants', (2, 3)), ('c- TBCNN', (6, 8)), ('d - TBCNN', (9, 12))]","[['variants', 'denoted as', 'c- TBCNN'], ['variants', 'denoted as', 'd - TBCNN']]",[],[],"[['Model', 'has', 'variants']]",[],[],[],[],[],sentiment_analysis,23,28
model,"The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .","[('of', (2, 3)), ('apply', (9, 10)), ('sliding over', (17, 19)), ('pooling', (28, 29)), ('taking', (35, 36)), ('in', (39, 40))]","[('tree - based convolution', (3, 7)), ('set of subtree feature detectors', (11, 16)), ('entire parsing tree', (20, 23)), ('sentence', (25, 26)), ('aggregates these extracted feature vectors', (29, 34)), ('maximum value', (37, 39)), ('each dimension', (40, 42))]","[['tree - based convolution', 'apply', 'set of subtree feature detectors'], ['set of subtree feature detectors', 'sliding over', 'entire parsing tree'], ['entire parsing tree', 'of', 'sentence'], ['set of subtree feature detectors', 'pooling', 'aggregates these extracted feature vectors'], ['aggregates these extracted feature vectors', 'taking', 'maximum value'], ['maximum value', 'in', 'each dimension']]",[],[],"[['Model', 'has', 'tree - based convolution']]",[],[],[],[],[],sentiment_analysis,23,29
hyperparameters,"In our d-TBCNN model , the number of units is 300 for convolution and 200 for the last hidden layer .",[],[],"[['number of units', 'is', '200'], ['200', 'for', 'last hidden layer'], ['number of units', 'is', '300'], ['300', 'for', 'convolution']]","[['our d-TBCNN model', 'has', 'number of units']]","[['Hyperparameters', 'In', 'our d-TBCNN model']]",[],[],[],[],[],[],sentiment_analysis,23,193
hyperparameters,"Word embeddings are 300 dimensional , pretrained ourselves using word2vec To train our model , we compute gradient by back - propagation and apply stochastic gradient descent with mini-batch 200 .","[('are', (2, 3)), ('pretrained ourselves using', (6, 9)), ('To train', (10, 12)), ('compute', (16, 17)), ('by', (18, 19)), ('apply', (23, 24)), ('with', (27, 28))]","[('Word embeddings', (0, 2)), ('300 dimensional', (3, 5)), ('word2vec', (9, 10)), ('our model', (12, 14)), ('gradient', (17, 18)), ('back - propagation', (19, 22)), ('stochastic gradient descent', (24, 27)), ('mini-batch 200', (28, 30))]","[['our model', 'compute', 'gradient'], ['gradient', 'by', 'back - propagation'], ['our model', 'apply', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'mini-batch 200'], ['Word embeddings', 'are', '300 dimensional'], ['Word embeddings', 'pretrained ourselves using', 'word2vec']]",[],"[['Hyperparameters', 'To train', 'our model']]","[['Hyperparameters', 'has', 'Word embeddings']]",[],[],[],[],[],sentiment_analysis,23,194
hyperparameters,We use ReLU as the activation function .,"[('use', (1, 2)), ('as', (3, 4))]","[('ReLU', (2, 3)), ('activation function', (5, 7))]","[['ReLU', 'as', 'activation function']]",[],"[['Hyperparameters', 'use', 'ReLU']]",[],[],[],[],[],[],sentiment_analysis,23,195
hyperparameters,"For regularization , we add 2 penalty for weights with a coefficient of 10 ?5 .","[('For', (0, 1)), ('add', (4, 5)), ('for', (7, 8)), ('with', (9, 10)), ('of', (12, 13))]","[('regularization', (1, 2)), ('2 penalty', (5, 7)), ('weights', (8, 9)), ('coefficient', (11, 12)), ('10 ?5', (13, 15))]","[['regularization', 'add', '2 penalty'], ['2 penalty', 'for', 'weights'], ['weights', 'with', 'coefficient'], ['coefficient', 'of', '10 ?5']]",[],"[['Hyperparameters', 'For', 'regularization']]",[],[],[],[],[],[],sentiment_analysis,23,196
hyperparameters,Dropout is further applied to both weights and embeddings .,"[('applied to', (3, 5))]","[('Dropout', (0, 1)), ('both weights and embeddings', (5, 9))]","[['Dropout', 'applied to', 'both weights and embeddings']]",[],[],"[['Hyperparameters', 'has', 'Dropout']]",[],[],[],[],[],sentiment_analysis,23,197
hyperparameters,"All hidden layers are dropped out by 50 % , and embeddings 40 % .","[('dropped out by', (4, 7))]","[('All hidden layers', (0, 3)), ('50 %', (7, 9)), ('embeddings', (11, 12)), ('40 %', (12, 14))]","[['All hidden layers', 'dropped out by', '50 %']]","[['embeddings', 'has', '40 %']]",[],"[['Hyperparameters', 'has', 'embeddings'], ['Hyperparameters', 'has', 'All hidden layers']]",[],[],[],[],[],sentiment_analysis,23,198
results,"Nonetheless , our d-TBCNN model achieves","[('achieves', (5, 6))]","[('d-TBCNN model', (3, 5))]",[],[],[],"[['Results', 'has', 'd-TBCNN model']]",[],[],"[['d-TBCNN model', 'achieves', '87.9 % accuracy']]",[],[],sentiment_analysis,23,208
results,"87.9 % accuracy , ranking third in the list .",[],"[('87.9 % accuracy', (0, 3))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,23,209
results,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .","[('In', (0, 1)), ('with', (6, 7)), ('consistently outperform', (27, 29)), ('to', (30, 31)), ('by more than', (52, 55))]","[('more controlled comparison', (2, 5)), ('shallow architectures', (7, 9)), ('basic interaction ( linearly transformed and non-linearly squashed )', (11, 20)), ('TBCNNs', (21, 22)), ('RNNs', (29, 30)), ('large extent ( 50.4 - 51.4 % versus 43.2 % )', (32, 43)), ('"" flat "" CNNs', (48, 52)), ('10 %', (55, 57))]","[['more controlled comparison', 'with', 'shallow architectures'], ['more controlled comparison', 'with', 'basic interaction ( linearly transformed and non-linearly squashed )'], ['TBCNNs', 'consistently outperform', 'RNNs'], ['RNNs', 'to', 'large extent ( 50.4 - 51.4 % versus 43.2 % )'], ['TBCNNs', 'consistently outperform', '"" flat "" CNNs'], ['"" flat "" CNNs', 'by more than', '10 %']]","[['more controlled comparison', 'has', 'TBCNNs']]","[['Results', 'In', 'more controlled comparison']]",[],[],[],[],[],[],sentiment_analysis,23,210
results,We also observe d- TBCNN achieves higher performance than c - TBCNN .,"[('observe', (2, 3)), ('achieves', (5, 6)), ('than', (8, 9))]","[('d- TBCNN', (3, 5)), ('higher performance', (6, 8)), ('c - TBCNN', (9, 12))]","[['d- TBCNN', 'achieves', 'higher performance'], ['higher performance', 'than', 'c - TBCNN']]",[],"[['Results', 'observe', 'd- TBCNN']]",[],[],[],[],[],[],sentiment_analysis,23,212
research-problem,An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis,[],"[('Aspect - Based Sentiment Analysis', (13, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,24,2
research-problem,Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,[],"[('Aspect - based sentiment analysis ( ABSA )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis ( ABSA )']]",[],[],[],[],sentiment_analysis,24,11
research-problem,"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .",[],"[('aspect term extraction ( AE )', (12, 18)), ('aspect - level sentiment classification ( AS )', (33, 41))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect term extraction ( AE )'], ['Contribution', 'has research problem', 'aspect - level sentiment classification ( AS )']]",[],[],[],[],sentiment_analysis,24,12
model,"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .","[('propose', (5, 6))]","[('interactive multitask learning network ( IMN )', (7, 14))]",[],[],"[['Model', 'propose', 'interactive multitask learning network ( IMN )']]",[],[],[],[],[],[],sentiment_analysis,24,20
model,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .","[('allows', (3, 4)), ('to be trained together with', (7, 12)), ('exploiting', (18, 19)), ('from', (21, 22))]","[('IMN', (2, 3)), ('AE and AS', (4, 7)), ('related document - level tasks', (12, 17)), ('knowledge', (20, 21)), ('larger document - level corpora', (22, 27))]","[['IMN', 'allows', 'AE and AS'], ['AE and AS', 'to be trained together with', 'related document - level tasks'], ['related document - level tasks', 'exploiting', 'knowledge'], ['knowledge', 'from', 'larger document - level corpora']]",[],[],"[['Model', 'has', 'IMN']]",[],[],[],[],[],sentiment_analysis,24,21
model,IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,"[('introduces', (1, 2)), ('allows', (8, 9)), ('between', (11, 12))]","[('novel message passing mechanism', (3, 7)), ('informative interactions', (9, 11)), ('tasks', (12, 13))]","[['novel message passing mechanism', 'allows', 'informative interactions'], ['informative interactions', 'between', 'tasks']]",[],"[['Model', 'introduces', 'novel message passing mechanism']]",[],[],[],[],[],[],sentiment_analysis,24,22
model,"Specifically , it sends useful information from different tasks back to a shared latent representation .","[('sends', (3, 4)), ('from', (6, 7)), ('back to', (9, 11))]","[('useful information', (4, 6)), ('different tasks', (7, 9)), ('shared latent representation', (12, 15))]","[['useful information', 'from', 'different tasks'], ['different tasks', 'back to', 'shared latent representation']]",[],"[['Model', 'sends', 'useful information']]",[],[],[],[],[],[],sentiment_analysis,24,23
model,The information is then combined with the shared latent representation and made available to all tasks for further processing .,"[('combined with', (4, 6)), ('made available to', (11, 14)), ('for', (16, 17))]","[('information', (1, 2)), ('shared latent representation', (7, 10)), ('all tasks', (14, 16)), ('further processing', (17, 19))]","[['information', 'combined with', 'shared latent representation'], ['information', 'made available to', 'all tasks'], ['all tasks', 'for', 'further processing']]",[],[],"[['Model', 'has', 'information']]",[],[],[],[],[],sentiment_analysis,24,24
model,"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .","[('through', (10, 11)), ('explicitly models', (26, 28)), ('between', (30, 31)), ('allowing', (38, 39)), ('to better influence', (41, 44))]","[('interactions', (29, 30)), ('tasks', (31, 32)), ('message passing mechanism', (34, 37)), ('different tasks', (39, 41)), ('each other', (44, 46))]","[['interactions', 'through', 'message passing mechanism'], ['message passing mechanism', 'allowing', 'different tasks'], ['different tasks', 'to better influence', 'each other'], ['interactions', 'between', 'tasks']]",[],[],[],[],"[['IMN', 'explicitly models', 'interactions']]",[],[],[],sentiment_analysis,24,26
model,"In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .","[('to be trained together with', (11, 16))]","[('fined - grained tokenlevel classification tasks', (5, 11)), ('document - level classification tasks', (16, 21))]","[['fined - grained tokenlevel classification tasks', 'to be trained together with', 'document - level classification tasks']]",[],[],[],[],[],[],"[['IMN', 'allows', 'fined - grained tokenlevel classification tasks']]",[],sentiment_analysis,24,27
model,"We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .","[('incorporated', (1, 2)), ('to be jointly trained with', (21, 26)), ('allowing', (30, 31)), ('to benefit from', (36, 39))]","[('two document - level classification tasks', (2, 8)), ('sentiment classification ( DS )', (9, 14)), ('domain classification ( DD )', (15, 20)), ('AE and AS', (26, 29)), ('aspect - level tasks', (32, 36)), ('document - level information', (39, 43))]","[['two document - level classification tasks', 'to be jointly trained with', 'AE and AS'], ['AE and AS', 'allowing', 'aspect - level tasks'], ['aspect - level tasks', 'to benefit from', 'document - level information']]","[['two document - level classification tasks', 'name', 'sentiment classification ( DS )'], ['two document - level classification tasks', 'name', 'domain classification ( DD )']]","[['Model', 'incorporated', 'two document - level classification tasks']]",[],[],[],[],[],[],sentiment_analysis,24,28
hyperparameters,We adopt the multi - layer - CNN structure from as the CNN - based encoders in our proposed network .,"[('adopt', (1, 2)), ('from', (9, 10))]","[('multi - layer - CNN structure', (3, 9)), ('CNN - based encoders', (12, 16))]","[['multi - layer - CNN structure', 'from', 'CNN - based encoders']]",[],"[['Hyperparameters', 'adopt', 'multi - layer - CNN structure']]",[],[],[],[],[],[],sentiment_analysis,24,157
hyperparameters,"For word embedding initialization , we concatenate a general - purpose embedding matrix and a domain - specific embedding matrix 7 following .","[('For', (0, 1)), ('concatenate', (6, 7))]","[('word embedding initialization', (1, 4)), ('general - purpose embedding matrix', (8, 13)), ('domain - specific embedding matrix', (15, 20))]","[['word embedding initialization', 'concatenate', 'general - purpose embedding matrix'], ['word embedding initialization', 'concatenate', 'domain - specific embedding matrix']]",[],"[['Hyperparameters', 'For', 'word embedding initialization']]",[],[],[],[],[],[],sentiment_analysis,24,160
hyperparameters,"We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions , which are trained on a large domain - specific corpus using fast Text .","[('for', (6, 7)), ('with', (11, 12)), ('trained on', (17, 19)), ('using', (25, 26))]","[('released domainspecific embeddings', (3, 6)), ('restaurant and laptop domains', (7, 11)), ('100 dimensions', (12, 14)), ('large domain - specific corpus', (20, 25)), ('fast Text', (26, 28))]","[['released domainspecific embeddings', 'for', 'restaurant and laptop domains'], ['restaurant and laptop domains', 'with', '100 dimensions'], ['released domainspecific embeddings', 'trained on', 'large domain - specific corpus'], ['large domain - specific corpus', 'using', 'fast Text']]",[],[],"[['Hyperparameters', 'adopt', 'released domainspecific embeddings']]",[],[],[],[],[],sentiment_analysis,24,161
hyperparameters,The general - purpose embeddings are pre-trained Glove vectors with 300 dimensions .,"[('are', (5, 6)), ('with', (9, 10))]","[('general - purpose embeddings', (1, 5)), ('pre-trained Glove vectors', (6, 9)), ('300 dimensions', (10, 12))]","[['general - purpose embeddings', 'are', 'pre-trained Glove vectors'], ['pre-trained Glove vectors', 'with', '300 dimensions']]",[],[],"[['Hyperparameters', 'has', 'general - purpose embeddings']]",[],[],[],[],[],sentiment_analysis,24,162
hyperparameters,We tune the maximum number of iterations T in the message passing mechanism by training IMN ?d via cross validation on D1 .,"[('tune', (1, 2)), ('in', (8, 9)), ('by training', (13, 15)), ('via', (17, 18))]","[('maximum number of iterations T', (3, 8)), ('message passing mechanism', (10, 13)), ('IMN ?d', (15, 17)), ('cross validation', (18, 20))]","[['maximum number of iterations T', 'in', 'message passing mechanism'], ['message passing mechanism', 'by training', 'IMN ?d'], ['IMN ?d', 'via', 'cross validation']]",[],"[['Hyperparameters', 'tune', 'maximum number of iterations T']]",[],[],[],[],[],[],sentiment_analysis,24,171
hyperparameters,It is set to 2 .,"[('set to', (2, 4))]","[('2', (4, 5))]",[],[],[],[],[],"[['maximum number of iterations T', 'set to', '2']]",[],[],[],sentiment_analysis,24,172
hyperparameters,"We use Adam optimizer with learning rate set to 10 ? 4 , and we set batch size to 32 .","[('use', (1, 2)), ('with', (4, 5)), ('set to', (7, 9)), ('set', (15, 16)), ('to', (18, 19))]","[('Adam optimizer', (2, 4)), ('learning rate', (5, 7)), ('10 ? 4', (9, 12)), ('batch size', (16, 18)), ('32', (19, 20))]","[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'set to', '10 ? 4'], ['Adam optimizer', 'set', 'batch size'], ['batch size', 'to', '32']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],[],[],[],[],[],sentiment_analysis,24,175
hyperparameters,Learning rate and batch size are set to conventional values without specific tuning for our task .,"[('set to', (6, 8)), ('without specific tuning for', (10, 14))]","[('Learning rate and batch size', (0, 5)), ('conventional values', (8, 10)), ('our task', (14, 16))]","[['Learning rate and batch size', 'set to', 'conventional values'], ['conventional values', 'without specific tuning for', 'our task']]",[],[],"[['Hyperparameters', 'has', 'Learning rate and batch size']]",[],[],[],[],[],sentiment_analysis,24,176
hyperparameters,"At training phase , we randomly sample 20 % of the training data from the aspect - level dataset as the development set and only use the remaining 80 % for training .","[('At', (0, 1)), ('randomly sample', (5, 7)), ('of', (9, 10)), ('from', (13, 14)), ('as', (19, 20)), ('use', (25, 26)), ('for', (30, 31))]","[('training phase', (1, 3)), ('20 %', (7, 9)), ('training data', (11, 13)), ('aspect - level dataset', (15, 19)), ('development set', (21, 23)), ('remaining 80 %', (27, 30)), ('training', (31, 32))]","[['training phase', 'randomly sample', '20 %'], ['20 %', 'as', 'development set'], ['20 %', 'of', 'training data'], ['training data', 'from', 'aspect - level dataset'], ['training phase', 'use', 'remaining 80 %'], ['remaining 80 %', 'for', 'training']]",[],"[['Hyperparameters', 'At', 'training phase']]",[],[],[],[],[],[],sentiment_analysis,24,177
results,"From , we observe that IMN ?d is able to significantly outperform other baselines on F1 - I .","[('observe that', (3, 5)), ('able to', (8, 10)), ('other', (12, 13)), ('on', (14, 15))]","[('IMN ?d', (5, 7)), ('significantly outperform', (10, 12)), ('baselines', (13, 14)), ('F1', (15, 16))]","[['IMN ?d', 'able to', 'significantly outperform'], ['significantly outperform', 'other', 'baselines'], ['significantly outperform', 'on', 'F1']]",[],"[['Results', 'observe that', 'IMN ?d']]",[],[],[],[],[],[],sentiment_analysis,24,220
results,"IMN further boosts the performance and outperforms the best F1 - I results from the baselines by 2.29 % , 1.77 % , and 2.61 % on D1 , D2 , and D3 .","[('boosts', (2, 3)), ('outperforms', (6, 7)), ('from', (13, 14)), ('by', (16, 17)), ('on', (26, 27))]","[('IMN', (0, 1)), ('performance', (4, 5)), ('best F1', (8, 10)), ('baselines', (15, 16)), ('2.29 % , 1.77 % , and 2.61 %', (17, 26)), ('D1 , D2 , and D3', (27, 33))]","[['IMN', 'boosts', 'performance'], ['IMN', 'outperforms', 'best F1'], ['best F1', 'from', 'baselines'], ['baselines', 'by', '2.29 % , 1.77 % , and 2.61 %'], ['2.29 % , 1.77 % , and 2.61 %', 'on', 'D1 , D2 , and D3']]",[],[],"[['Results', 'has', 'IMN']]",[],[],[],[],[],sentiment_analysis,24,221
results,"Specifically , for AE ( F1 - a and F1 - o ) , IMN ?d performs the best in most cases .","[('for', (2, 3)), ('performs', (16, 17)), ('in', (19, 20))]","[('AE ( F1 - a and F1 - o )', (3, 13)), ('IMN ?d', (14, 16)), ('best', (18, 19)), ('most cases', (20, 22))]","[['IMN ?d', 'performs', 'best'], ['best', 'in', 'most cases']]","[['AE ( F1 - a and F1 - o )', 'has', 'IMN ?d']]","[['Results', 'for', 'AE ( F1 - a and F1 - o )']]",[],[],[],[],[],[],sentiment_analysis,24,222
results,"For AS ( acc - s and F1 - s ) , IMN outperforms other methods by large margins .","[('For', (0, 1)), ('outperforms', (13, 14)), ('by', (16, 17))]","[('AS ( acc - s and F1 - s )', (1, 11)), ('IMN', (12, 13)), ('other methods', (14, 16)), ('large margins', (17, 19))]","[['IMN', 'outperforms', 'other methods'], ['other methods', 'by', 'large margins']]","[['AS ( acc - s and F1 - s )', 'has', 'IMN']]","[['Results', 'For', 'AS ( acc - s and F1 - s )']]",[],[],[],[],[],[],sentiment_analysis,24,223
results,IMN wo DE performs only marginally below IMN .,"[('performs', (3, 4)), ('below', (6, 7))]","[('IMN wo DE', (0, 3)), ('only marginally', (4, 6)), ('IMN', (7, 8))]","[['IMN wo DE', 'performs', 'only marginally'], ['only marginally', 'below', 'IMN']]",[],[],"[['Results', 'has', 'IMN wo DE']]",[],[],[],[],[],sentiment_analysis,24,229
results,"IMN ?d is more affected without domain - specific embeddings , while it still outperforms all other baselines except DECNN - d Trans .","[('more affected without', (3, 6)), ('outperforms', (14, 15)), ('except', (18, 19))]","[('IMN ?d', (0, 2)), ('domain - specific embeddings', (6, 10)), ('all other baselines', (15, 18)), ('DECNN - d Trans', (19, 23))]","[['IMN ?d', 'more affected without', 'domain - specific embeddings'], ['IMN ?d', 'outperforms', 'all other baselines'], ['all other baselines', 'except', 'DECNN - d Trans']]",[],[],"[['Results', 'has', 'IMN ?d']]",[],[],[],[],[],sentiment_analysis,24,231
results,DECNN - dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks .,"[('is', (3, 4)), ('exploits', (10, 11)), ('from', (13, 14)), ('for', (16, 17))]","[('DECNN - dTrans', (0, 3)), ('very strong baseline', (5, 8)), ('additional knowledge', (11, 13)), ('larger corpora', (14, 16)), ('both tasks', (17, 19))]","[['DECNN - dTrans', 'is', 'very strong baseline'], ['very strong baseline', 'exploits', 'additional knowledge'], ['additional knowledge', 'from', 'larger corpora'], ['larger corpora', 'for', 'both tasks']]",[],[],"[['Results', 'has', 'DECNN - dTrans']]",[],[],[],[],[],sentiment_analysis,24,232
results,"IMN ?d wo DE is competitive with DECNN - dTrans even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .","[('competitive with', (5, 7))]","[('IMN ?d wo DE', (0, 4)), ('DECNN - dTrans', (7, 10))]","[['IMN ?d wo DE', 'competitive with', 'DECNN - dTrans']]",[],[],"[['Results', 'has', 'IMN ?d wo DE']]",[],[],[],[],[],sentiment_analysis,24,233
ablation-analysis,"We observe that + Message passing - a and + Message passing - d contribute to the performance gains the most , which demonstrates the effectiveness of the proposed message passing mechanism .","[('observe', (1, 2)), ('contribute to', (14, 16)), ('demonstrates', (23, 24)), ('of', (26, 27))]","[('+ Message passing - a and + Message passing - d', (3, 14)), ('performance gains', (17, 19)), ('effectiveness', (25, 26)), ('proposed message passing mechanism', (28, 32))]","[['+ Message passing - a and + Message passing - d', 'contribute to', 'performance gains'], ['+ Message passing - a and + Message passing - d', 'demonstrates', 'effectiveness'], ['effectiveness', 'of', 'proposed message passing mechanism']]",[],"[['Ablation analysis', 'observe', '+ Message passing - a and + Message passing - d']]",[],[],[],[],[],[],sentiment_analysis,24,242
ablation-analysis,We also observe that simply adding documentlevel tasks ( + DS / DD ) with parameter sharing only marginally improves the performance of IMN ?d .,"[('with', (14, 15)), ('marginally improves', (18, 20)), ('of', (22, 23))]","[('adding documentlevel tasks ( + DS / DD )', (5, 14)), ('parameter sharing', (15, 17)), ('performance', (21, 22)), ('IMN ?d', (23, 25))]","[['adding documentlevel tasks ( + DS / DD )', 'with', 'parameter sharing'], ['parameter sharing', 'marginally improves', 'performance'], ['performance', 'of', 'IMN ?d']]",[],[],"[['Ablation analysis', 'observe', 'adding documentlevel tasks ( + DS / DD )']]",[],[],[],[],[],sentiment_analysis,24,243
ablation-analysis,"However , + Message passing -d is still helpful with considerable performance gains , showing that aspect - level tasks can benefit from knowing predictions of the relevant document - level tasks .","[('still helpful with', (7, 10))]","[('Message passing -d', (3, 6)), ('considerable performance gains', (10, 13))]","[['Message passing -d', 'still helpful with', 'considerable performance gains']]",[],[],"[['Ablation analysis', 'has', 'Message passing -d']]",[],[],[],[],[],sentiment_analysis,24,245
research-problem,Aspect Based Sentiment Analysis with Gated Convolutional Networks,[],"[('Aspect Based Sentiment Analysis', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,25,2
research-problem,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",[],"[('Aspect based sentiment analysis ( ABSA )', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect based sentiment analysis ( ABSA )']]",[],[],[],[],sentiment_analysis,25,4
research-problem,We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,[],"[('aspect - category sentiment analysis ( ACSA )', (8, 16)), ('aspect - term sentiment analysis ( ATSA )', (17, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect - category sentiment analysis ( ACSA )'], ['Contribution', 'has research problem', 'aspect - term sentiment analysis ( ATSA )']]",[],[],[],[],sentiment_analysis,25,5
research-problem,"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .",[],"[('ABSA', (8, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'ABSA']]",[],[],[],[],sentiment_analysis,25,18
research-problem,"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .",[],"[('ACSA', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'ACSA']]",[],[],[],[],sentiment_analysis,25,19
research-problem,"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .",[],"[('ATSA', (8, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'ATSA']]",[],[],[],[],sentiment_analysis,25,20
model,"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .","[('propose', (5, 6)), ('for', (12, 13)), ('based on', (16, 18))]","[('fast and effective neural network', (7, 12)), ('ACSA and ATSA', (13, 16)), ('convolutions and gating mechanisms', (18, 22))]","[['fast and effective neural network', 'for', 'ACSA and ATSA'], ['ACSA and ATSA', 'based on', 'convolutions and gating mechanisms']]",[],"[['Model', 'propose', 'fast and effective neural network']]",[],[],[],[],[],[],sentiment_analysis,25,33
model,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .","[('For', (0, 1)), ('on the top of', (11, 15)), ('whose', (19, 20)), ('are combined by', (21, 24))]","[('ACSA task', (1, 3)), ('our model', (4, 6)), ('two separate convolutional layers', (7, 11)), ('embedding layer', (16, 18)), ('outputs', (20, 21)), ('novel gating units', (24, 27))]","[['two separate convolutional layers', 'on the top of', 'embedding layer'], ['embedding layer', 'whose', 'outputs'], ['outputs', 'are combined by', 'novel gating units']]","[['ACSA task', 'has', 'our model'], ['our model', 'has', 'two separate convolutional layers']]","[['Model', 'For', 'ACSA task']]",[],[],[],[],[],[],sentiment_analysis,25,34
model,"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .","[('proposed', (1, 2)), ('have', (4, 5)), ('connected to', (13, 15))]","[('gating units', (2, 4)), ('two nonlinear gates', (5, 8)), ('one convolutional layer', (15, 18))]","[['gating units', 'have', 'two nonlinear gates'], ['two nonlinear gates', 'connected to', 'one convolutional layer']]",[],"[['Model', 'proposed', 'gating units']]",[],[],[],[],[],[],sentiment_analysis,25,36
model,"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .","[('extend', (14, 15)), ('to include', (17, 19)), ('for', (22, 23))]","[('ATSA task', (1, 3)), ('our model', (15, 17)), ('another convolutional layer', (19, 22)), ('target expressions', (24, 26))]","[['ATSA task', 'extend', 'our model'], ['our model', 'to include', 'another convolutional layer'], ['another convolutional layer', 'for', 'target expressions']]",[],[],"[['Model', 'For', 'ATSA task']]",[],[],[],[],[],sentiment_analysis,25,40
hyperparameters,"In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .","[('initialized with', (8, 10)), ('pre-trained on', (17, 19)), ('of', (21, 22))]","[('word embedding vectors', (4, 7)), ('300 - dimension GloVe vectors', (10, 15)), ('unlabeled data', (19, 21)), ('840 billion tokens', (22, 25))]","[['word embedding vectors', 'initialized with', '300 - dimension GloVe vectors'], ['300 - dimension GloVe vectors', 'pre-trained on', 'unlabeled data'], ['unlabeled data', 'of', '840 billion tokens']]",[],[],"[['Hyperparameters', 'has', 'word embedding vectors']]",[],[],[],[],[],sentiment_analysis,25,148
hyperparameters,"Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .","[('of', (5, 6)), ('are', (8, 9)), ('with', (11, 12))]","[('Words out of the vocabulary', (0, 5)), ('Glo Ve', (6, 8)), ('randomly initialized', (9, 11)), ('uniform distribution U ( ? 0.25 , 0.25 )', (13, 22))]","[['Words out of the vocabulary', 'of', 'Glo Ve'], ['Glo Ve', 'are', 'randomly initialized'], ['randomly initialized', 'with', 'uniform distribution U ( ? 0.25 , 0.25 )']]",[],[],"[['Hyperparameters', 'has', 'Words out of the vocabulary']]",[],[],[],[],[],sentiment_analysis,25,149
hyperparameters,"We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .",[],[],"[['Adagrad', 'with', 'batch size'], ['batch size', 'of', '32 instances'], ['Adagrad', 'with', 'default learning rate'], ['default learning rate', 'of', '1 e ? 2'], ['Adagrad', 'with', 'maximal epochs'], ['maximal epochs', 'of', '30']]",[],"[['Hyperparameters', 'use', 'Adagrad']]",[],[],[],[],[],[],sentiment_analysis,25,150
hyperparameters,We only fine tune early stopping with 5 - fold cross validation on training datasets .,"[('fine tune', (2, 4)), ('with', (6, 7)), ('on', (12, 13))]","[('early stopping', (4, 6)), ('5 - fold cross validation', (7, 12)), ('training datasets', (13, 15))]","[['early stopping', 'with', '5 - fold cross validation'], ['5 - fold cross validation', 'on', 'training datasets']]",[],"[['Hyperparameters', 'fine tune', 'early stopping']]",[],[],[],[],[],[],sentiment_analysis,25,151
hyperparameters,All neural models are implemented in PyTorch .,"[('implemented in', (4, 6))]","[('neural models', (1, 3)), ('PyTorch', (6, 7))]","[['neural models', 'implemented in', 'PyTorch']]",[],[],"[['Hyperparameters', 'has', 'neural models']]",[],[],[],[],[],sentiment_analysis,25,152
baselines,NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .,"[('is', (3, 4)), ('in', (7, 8)), ('for', (12, 13))]","[('NRC - Canada', (0, 3)), ('top method', (5, 7)), ('SemEval 2014 Task 4', (8, 12)), ('ACSA and ATSA task', (13, 17))]","[['NRC - Canada', 'is', 'top method'], ['top method', 'in', 'SemEval 2014 Task 4'], ['top method', 'for', 'ACSA and ATSA task']]",[],[],"[['Baselines', 'has', 'NRC - Canada']]",[],[],[],[],[],sentiment_analysis,25,155
baselines,CNN is widely used on text classification task .,"[('widely used on', (2, 5))]","[('CNN', (0, 1)), ('text classification task', (5, 8))]","[['CNN', 'widely used on', 'text classification task']]",[],[],"[['Baselines', 'has', 'CNN']]",[],[],[],[],[],sentiment_analysis,25,158
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,"[('uses', (3, 4)), ('to model', (7, 9)), ('of', (14, 15)), ('to generate', (17, 19)), ('for', (23, 24))]","[('TD - LSTM', (0, 3)), ('two LSTM networks', (4, 7)), ('preceding and following contexts', (10, 14)), ('target', (16, 17)), ('target - dependent representation', (19, 23)), ('sentiment prediction', (24, 26))]","[['TD - LSTM', 'uses', 'two LSTM networks'], ['two LSTM networks', 'to generate', 'target - dependent representation'], ['target - dependent representation', 'for', 'sentiment prediction'], ['two LSTM networks', 'to model', 'preceding and following contexts'], ['preceding and following contexts', 'of', 'target']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,25,161
baselines,ATAE - LSTM is an attention - based LSTM for ACSA task .,"[('is', (3, 4)), ('for', (9, 10))]","[('ATAE - LSTM', (0, 3)), ('attention - based LSTM', (5, 9)), ('ACSA task', (10, 12))]","[['ATAE - LSTM', 'is', 'attention - based LSTM'], ['attention - based LSTM', 'for', 'ACSA task']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,25,162
baselines,"IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .","[('stands for', (1, 3)), ('for', (6, 7)), ('based on', (13, 15))]","[('IAN', (0, 1)), ('interactive attention network', (3, 6)), ('ATSA task', (7, 9)), ('LSTM and attention mechanisms', (15, 19))]","[['IAN', 'based on', 'LSTM and attention mechanisms'], ['IAN', 'stands for', 'interactive attention network'], ['interactive attention network', 'for', 'ATSA task']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,25,164
baselines,"RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .","[('is', (1, 2)), ('for', (6, 7)), ('uses', (11, 12))]","[('RAM', (0, 1)), ('recurrent attention network', (3, 6)), ('ATSA task', (7, 9)), ('LSTM and multiple attention mechanisms', (12, 17))]","[['RAM', 'is', 'recurrent attention network'], ['recurrent attention network', 'for', 'ATSA task'], ['RAM', 'uses', 'LSTM and multiple attention mechanisms']]",[],[],"[['Baselines', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,25,165
baselines,"GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .","[('stands for', (1, 3)), ('in which', (8, 10)), ('does not have', (11, 14)), ('as', (17, 18))]","[('GCN', (0, 1)), ('gated convolutional neural network', (3, 7)), ('GTRU', (10, 11)), ('aspect embedding', (15, 17)), ('additional input', (19, 21))]","[['GCN', 'in which', 'GTRU'], ['GTRU', 'does not have', 'aspect embedding'], ['aspect embedding', 'as', 'additional input'], ['GCN', 'stands for', 'gated convolutional neural network']]",[],[],"[['Baselines', 'has', 'GCN']]",[],[],[],[],[],sentiment_analysis,25,166
results,LSTM based model ATAE - LSTM has the worst performance of all neural networks .,"[('of', (10, 11))]","[('LSTM based model ATAE - LSTM', (0, 6)), ('worst performance', (8, 10)), ('all neural networks', (11, 14))]","[['worst performance', 'of', 'all neural networks']]","[['LSTM based model ATAE - LSTM', 'has', 'worst performance']]",[],[],[],[],[],"[['ACSA', 'has', 'LSTM based model ATAE - LSTM']]",[],sentiment_analysis,25,172
results,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,"[('improves', (1, 2)), ('by', (4, 5)), ('compared with', (10, 12))]","[('GCAE', (0, 1)), ('performance', (3, 4)), ('1.1 % to 2.5 %', (5, 10)), ('ATAE - LSTM', (12, 15))]","[['GCAE', 'improves', 'performance'], ['performance', 'by', '1.1 % to 2.5 %'], ['1.1 % to 2.5 %', 'compared with', 'ATAE - LSTM']]",[],[],[],[],[],[],"[['ACSA', 'has', 'GCAE']]",[],sentiment_analysis,25,177
results,"Without the large amount of sentiment lexicons , SVM perform worse than neural methods .","[('Without the large amount of', (0, 5)), ('perform', (9, 10)), ('than', (11, 12))]","[('sentiment lexicons', (5, 7)), ('SVM', (8, 9)), ('worse', (10, 11)), ('neural methods', (12, 14))]","[['SVM', 'perform', 'worse'], ['worse', 'than', 'neural methods']]","[['sentiment lexicons', 'has', 'SVM']]",[],[],[],"[['ACSA', 'Without the large amount of', 'sentiment lexicons']]",[],[],[],sentiment_analysis,25,184
results,"With multiple sentiment lexicons , the performance is increased by 7.6 % .","[('performance', (6, 7)), ('by', (9, 10))]","[('increased', (8, 9)), ('7.6 %', (10, 12))]","[['increased', 'by', '7.6 %']]",[],[],[],[],"[['SVM', 'performance', 'increased']]",[],[],[],sentiment_analysis,25,185
results,GCAE achieves 4 % higher accuracy than ATAE - LSTM on Restaurant - Large and 5 % higher on SemEval - 2014 on ACSA task .,[],[],"[['GCAE', 'achieves', '5 % higher'], ['5 % higher', 'on', 'SemEval - 2014 on ACSA task'], ['GCAE', 'achieves', '4 % higher accuracy'], ['4 % higher accuracy', 'than', 'ATAE - LSTM'], ['ATAE - LSTM', 'on', 'Restaurant - Large']]",[],[],[],[],[],[],[],[],sentiment_analysis,25,189
results,"However , GCN , which does not have aspect modeling part , has higher score than GCAE on the original restaurant dataset .","[('than', (15, 16)), ('on', (17, 18))]","[('GCN', (2, 3)), ('higher score', (13, 15)), ('GCAE', (16, 17)), ('original restaurant dataset', (19, 22))]","[['higher score', 'than', 'GCAE'], ['GCAE', 'on', 'original restaurant dataset']]","[['GCN', 'has', 'higher score']]",[],[],[],[],[],"[['ACSA', 'has', 'GCN']]",[],sentiment_analysis,25,190
results,ATSA,[],"[('ATSA', (0, 1))]",[],[],[],"[['Results', 'has', 'ATSA']]",[],[],[],[],"[['ATSA', 'has', 'IAN']]",sentiment_analysis,25,192
results,"IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .","[('than', (4, 5))]","[('IAN', (0, 1)), ('better performance', (2, 4)), ('TD - LSTM and ATAE - LSTM', (5, 12))]","[['better performance', 'than', 'TD - LSTM and ATAE - LSTM']]","[['IAN', 'has', 'better performance']]",[],[],[],[],[],[],[],sentiment_analysis,25,197
results,"RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .","[('achieves', (2, 3)), ('by combining', (5, 7)), ('with', (9, 10))]","[('RAM', (0, 1)), ('good accuracy', (3, 5)), ('multiple attentions', (7, 9)), ('recurrent neural network', (11, 14))]","[['RAM', 'achieves', 'good accuracy'], ['good accuracy', 'by combining', 'multiple attentions'], ['multiple attentions', 'with', 'recurrent neural network']]",[],[],[],[],[],[],"[['ATSA', 'has', 'RAM']]",[],sentiment_analysis,25,198
results,"On the hard test dataset , GCAE has 1 % higher accuracy than RAM on restaurant data and 1.7 % higher on laptop data .",[],[],"[['1.7 % higher', 'on', 'laptop data'], ['1 % higher accuracy', 'than', 'RAM'], ['1 % higher accuracy', 'on', 'restaurant data']]","[['hard test dataset', 'has', 'GCAE'], ['GCAE', 'has', '1.7 % higher'], ['GCAE', 'has', '1 % higher accuracy']]",[],[],[],"[['ATSA', 'On', 'hard test dataset']]",[],[],[],sentiment_analysis,25,199
results,"Because of the gating mechanisms and the convolutional layer over aspect terms , GCAE outperforms other neural models and basic SVM .","[('outperforms', (14, 15))]","[('GCAE', (13, 14)), ('other neural models and basic SVM', (15, 21))]","[['GCAE', 'outperforms', 'other neural models and basic SVM']]",[],[],[],[],[],[],"[['ATSA', 'has', 'GCAE']]",[],sentiment_analysis,25,201
research-problem,A Helping Hand : Transfer Learning for Deep Sentiment Analysis,[],"[('Deep Sentiment Analysis', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Deep Sentiment Analysis']]",[],[],[],[],sentiment_analysis,26,2
research-problem,"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .",[],"[('sentiment analysis', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,26,9
research-problem,"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .",[],"[('supervised sentiment polarity classification', (21, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'supervised sentiment polarity classification']]",[],[],[],[],sentiment_analysis,26,12
model,"In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .","[('investigate', (5, 6)), ('incorporated into', (11, 13)), ('for', (16, 17))]","[('extrinsic signals', (7, 9)), ('deep neural networks', (13, 16)), ('sentiment analysis', (17, 19))]","[['extrinsic signals', 'incorporated into', 'deep neural networks'], ['deep neural networks', 'for', 'sentiment analysis']]",[],"[['Model', 'investigate', 'extrinsic signals']]",[],[],[],[],[],[],sentiment_analysis,26,16
model,"In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .","[('consider', (6, 7)), ('specialized for', (10, 12)), ('lead to', (22, 24)), ('obtained using', (37, 39))]","[('word embeddings', (7, 9)), ('sentiment analysis', (15, 17)), ('stronger and more consistent gains', (24, 29)), ('out - of - domain data', (39, 45))]","[['word embeddings', 'lead to', 'stronger and more consistent gains'], ['stronger and more consistent gains', 'obtained using', 'out - of - domain data'], ['word embeddings', 'specialized for', 'sentiment analysis']]",[],"[['Model', 'consider', 'word embeddings']]",[],[],[],[],[],[],sentiment_analysis,26,18
model,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,"[('propose', (2, 3)), ('with', (9, 10)), ('dedicated to', (14, 16))]","[('bespoke convolutional neural network architecture', (4, 9)), ('separate memory module', (11, 14)), ('sentiment embeddings', (17, 19))]","[['bespoke convolutional neural network architecture', 'with', 'separate memory module'], ['separate memory module', 'dedicated to', 'sentiment embeddings']]",[],"[['Model', 'propose', 'bespoke convolutional neural network architecture']]",[],[],[],[],[],[],sentiment_analysis,26,20
experimental-setup,Embeddings .,[],"[('Embeddings', (0, 1))]",[],[],[],[],[],[],[],"[['Experimental Setup', 'has', 'Embeddings']]","[['Embeddings', 'has', 'standard pre-trained word vectors']]",sentiment_analysis,26,125
experimental-setup,"The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .","[('for', (6, 7)), ('are', (8, 9)), ('trained on', (12, 14)), ('of', (17, 18)), ('rely on', (29, 31)), ('as', (36, 37))]","[('standard pre-trained word vectors', (1, 5)), ('English', (7, 8)), ('GloVe ones', (10, 12)), ('840 billion tokens', (14, 17)), ('Common Crawl data', (18, 21)), ('other languages', (25, 27)), ('Facebook fastText Wikipedia embeddings', (32, 36)), ('input representations', (37, 39))]","[['standard pre-trained word vectors', 'for', 'English'], ['English', 'are', 'GloVe ones'], ['GloVe ones', 'trained on', '840 billion tokens'], ['840 billion tokens', 'of', 'Common Crawl data'], ['standard pre-trained word vectors', 'for', 'other languages'], ['other languages', 'rely on', 'Facebook fastText Wikipedia embeddings'], ['Facebook fastText Wikipedia embeddings', 'as', 'input representations']]",[],[],[],[],[],"[['standard pre-trained word vectors', 'are', '300 - dimensional']]",[],[],sentiment_analysis,26,126
experimental-setup,All of these are 300 - dimensional .,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,26,127
experimental-setup,"The vectors are either fed to the CNN , or to the convolutional module of the DM - MCNN during initialization , while unknown words are initialized with zeros .","[('fed to', (4, 6)), ('of', (14, 15)), ('during', (19, 20)), ('initialized with', (26, 28))]","[('vectors', (1, 2)), ('CNN', (7, 8)), ('convolutional module', (12, 14)), ('DM - MCNN', (16, 19)), ('initialization', (20, 21)), ('unknown words', (23, 25)), ('zeros', (28, 29))]","[['vectors', 'fed to', 'CNN'], ['vectors', 'fed to', 'convolutional module'], ['convolutional module', 'of', 'DM - MCNN'], ['DM - MCNN', 'during', 'initialization'], ['unknown words', 'initialized with', 'zeros']]",[],[],[],[],[],[],"[['Embeddings', 'has', 'vectors'], ['Embeddings', 'has', 'unknown words']]",[],sentiment_analysis,26,128
experimental-setup,"All words , including the unknown ones , are fine - tuned during the training process .","[('including', (3, 4)), ('are', (8, 9)), ('during', (12, 13))]","[('All words', (0, 2)), ('unknown ones', (5, 7)), ('fine - tuned', (9, 12)), ('training process', (14, 16))]","[['All words', 'including', 'unknown ones'], ['All words', 'are', 'fine - tuned'], ['fine - tuned', 'during', 'training process']]",[],[],[],[],[],[],"[['Embeddings', 'has', 'All words']]",[],sentiment_analysis,26,129
experimental-setup,"For our transfer learning approach , our experiments rely on the multi-domain sentiment dataset by , collected from Amazon customers reviews .","[('For', (0, 1)), ('rely on', (8, 10)), ('collected from', (16, 18))]","[('transfer learning approach', (2, 5)), ('multi-domain sentiment dataset', (11, 14)), ('Amazon customers reviews', (18, 21))]","[['transfer learning approach', 'rely on', 'multi-domain sentiment dataset'], ['multi-domain sentiment dataset', 'collected from', 'Amazon customers reviews']]",[],[],[],[],"[['Embeddings', 'For', 'transfer learning approach']]",[],[],[],sentiment_analysis,26,130
experimental-setup,"Specifically , we train linear SVMs using scikit - learn to extract word coefficients in each domain and also for the union of all domains together , yielding a 26 - dimensional sentiment embedding .","[('train', (3, 4)), ('using', (6, 7)), ('to extract', (10, 12)), ('in', (14, 15)), ('also for', (18, 20)), ('yielding', (27, 28))]","[('linear SVMs', (4, 6)), ('scikit - learn', (7, 10)), ('word coefficients', (12, 14)), ('each domain', (15, 17)), ('union of all domains', (21, 25)), ('26 - dimensional sentiment embedding', (29, 34))]","[['linear SVMs', 'using', 'scikit - learn'], ['scikit - learn', 'to extract', 'word coefficients'], ['word coefficients', 'yielding', '26 - dimensional sentiment embedding'], ['word coefficients', 'in', 'each domain'], ['word coefficients', 'also for', 'union of all domains']]",[],[],[],[],"[['Embeddings', 'train', 'linear SVMs']]",[],[],[],sentiment_analysis,26,132
experimental-setup,"For comparison and analysis , we also consider several alternative forms of infusing external cues .","[('consider', (7, 8)), ('of infusing', (11, 13))]","[('several alternative forms', (8, 11)), ('external cues', (13, 15))]","[['several alternative forms', 'of infusing', 'external cues']]",[],[],[],[],"[['Embeddings', 'consider', 'several alternative forms']]",[],[],[],sentiment_analysis,26,133
experimental-setup,We consider a recent sentiment lexicon called VADER .,"[('called', (6, 7))]","[('sentiment lexicon', (4, 6)), ('VADER', (7, 8))]","[['sentiment lexicon', 'called', 'VADER']]",[],[],[],[],[],[],"[['Embeddings', 'consider', 'sentiment lexicon']]",[],sentiment_analysis,26,135
experimental-setup,"These contain separate domain - specific scores for 250 different Reddit communities , and hence result in 250 - dimensional embeddings .","[('contain', (1, 2)), ('for', (7, 8)), ('result in', (15, 17))]","[('separate domain - specific scores', (2, 7)), ('250 different Reddit communities', (8, 12)), ('250 - dimensional embeddings', (17, 21))]","[['separate domain - specific scores', 'for', '250 different Reddit communities'], ['250 different Reddit communities', 'result in', '250 - dimensional embeddings']]",[],[],[],[],"[['VADER', 'contain', 'separate domain - specific scores']]",[],[],[],sentiment_analysis,26,137
experimental-setup,"For cross - lingual projection , we extract links between words from a 2017 dump of the English edition of Wiktionary .","[('extract', (7, 8)), ('between', (9, 10)), ('from', (11, 12))]","[('cross - lingual projection', (1, 5)), ('links', (8, 9)), ('words', (10, 11)), ('2017 dump of the English edition of Wiktionary', (13, 21))]","[['cross - lingual projection', 'extract', 'links'], ['links', 'between', 'words'], ['words', 'from', '2017 dump of the English edition of Wiktionary']]",[],[],[],[],[],[],"[['Embeddings', 'For', 'cross - lingual projection']]",[],sentiment_analysis,26,138
experimental-setup,"We restrict the vocabulary link set to include the languages in , mining corresponding translation , synonymy , derivation , and etymological links from Wiktionary .","[('restrict', (1, 2)), ('set to include', (5, 8)), ('mining', (12, 13)), ('corresponding', (13, 14))]","[('vocabulary link', (3, 5)), ('languages', (9, 10)), ('translation', (14, 15)), ('synonymy', (16, 17)), ('derivation', (18, 19)), ('etymological links', (21, 23)), ('Wiktionary', (24, 25))]","[['vocabulary link', 'set to include', 'languages'], ['vocabulary link', 'mining', 'Wiktionary'], ['Wiktionary', 'corresponding', 'translation'], ['Wiktionary', 'corresponding', 'synonymy'], ['Wiktionary', 'corresponding', 'derivation'], ['Wiktionary', 'corresponding', 'etymological links']]",[],[],[],[],"[['Embeddings', 'restrict', 'vocabulary link']]",[],[],[],sentiment_analysis,26,139
experimental-setup,"For CNNs , we make use of the well - known CNN - non-static architecture and hyperparameters proposed by , with a learning rate of 0.0006 , obtained by tuning on the validation data .","[('For', (0, 1)), ('make use of', (4, 7)), ('with', (20, 21)), ('of', (24, 25)), ('obtained by', (27, 29)), ('on', (30, 31))]","[('CNNs', (1, 2)), ('well - known CNN - non-static architecture and hyperparameters', (8, 17)), ('learning rate', (22, 24)), ('0.0006', (25, 26)), ('tuning', (29, 30)), ('validation data', (32, 34))]","[['CNNs', 'make use of', 'well - known CNN - non-static architecture and hyperparameters'], ['well - known CNN - non-static architecture and hyperparameters', 'with', 'learning rate'], ['learning rate', 'of', '0.0006'], ['0.0006', 'obtained by', 'tuning'], ['tuning', 'on', 'validation data']]",[],[],[],[],"[['Experimental Setup', 'For', 'CNNs']]",[],[],[],sentiment_analysis,26,141
experimental-setup,"For our DM - MCNN models , the configuration of the convolutional module is the same as for CNNs , and the remaining hyperparameter values were as well tuned on the validation sets .","[('of', (9, 10)), ('same as for', (15, 18)), ('tuned on', (28, 30))]","[('DM - MCNN models', (2, 6)), ('configuration', (8, 9)), ('convolutional module', (11, 13)), ('CNNs', (18, 19)), ('remaining hyperparameter values', (22, 25)), ('validation sets', (31, 33))]","[['configuration', 'of', 'convolutional module'], ['convolutional module', 'same as for', 'CNNs'], ['remaining hyperparameter values', 'tuned on', 'validation sets']]","[['DM - MCNN models', 'has', 'configuration'], ['DM - MCNN models', 'has', 'remaining hyperparameter values']]",[],[],[],[],[],"[['Experimental Setup', 'For', 'DM - MCNN models']]",[],sentiment_analysis,26,142
experimental-setup,"For greater efficiency and better convergence properties , the training relies on mini-batches .","[('relies on', (10, 12))]","[('greater efficiency and better convergence properties', (1, 7)), ('training', (9, 10)), ('mini-batches', (12, 13))]","[['training', 'relies on', 'mini-batches']]","[['greater efficiency and better convergence properties', 'has', 'training']]",[],[],[],[],[],"[['Experimental Setup', 'For', 'greater efficiency and better convergence properties']]",[],sentiment_analysis,26,144
experimental-setup,"Our implementation considers the maximal sentence length in each mini-batch and zero - pads all other sentences to this length under convolutional module , thus enabling uniform and fast processing of each mini-batch .",[],[],"[['implementation', 'considers', 'maximal sentence length'], ['maximal sentence length', 'in', 'each mini-batch'], ['implementation', 'zero - pads', 'all other sentences'], ['all other sentences', 'to', 'this length'], ['this length', 'under', 'convolutional module'], ['convolutional module', 'enabling', 'uniform and fast processing'], ['uniform and fast processing', 'of', 'each mini-batch']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'implementation']]",[],sentiment_analysis,26,145
experimental-setup,All neural network architectures are implemented using the PyTorch framework 2 .,"[('implemented using', (5, 7))]","[('neural network architectures', (1, 4)), ('PyTorch', (8, 9))]","[['neural network architectures', 'implemented using', 'PyTorch']]",[],[],[],[],[],[],"[['Experimental Setup', 'has', 'neural network architectures']]",[],sentiment_analysis,26,146
results,"Comparing this to CNNs with GloVe / fastText embeddings , where Glo Ve is used for English , and fastText is used for all other languages , we observe substantial improvements across all datasets .",[],[],"[['CNNs', 'with', 'GloVe / fastText embeddings'], ['GloVe / fastText embeddings', 'where', 'fastText'], ['fastText', 'used for', 'all other languages'], ['GloVe / fastText embeddings', 'where', 'Glo Ve'], ['Glo Ve', 'used for', 'English'], ['CNNs', 'observe', 'substantial improvements'], ['substantial improvements', 'across all', 'datasets']]",[],"[['Results', 'Comparing this to', 'CNNs']]",[],[],[],[],[],[],sentiment_analysis,26,152
results,This shows that word vectors do tend to convey pertinent word semantics signals that enable models to generalize better .,"[('shows', (1, 2)), ('tend to convey', (6, 9)), ('that enable', (13, 15)), ('to generalize', (16, 18))]","[('word vectors', (3, 5)), ('pertinent word semantics signals', (9, 13)), ('models', (15, 16)), ('better', (18, 19))]","[['word vectors', 'tend to convey', 'pertinent word semantics signals'], ['pertinent word semantics signals', 'that enable', 'models'], ['models', 'to generalize', 'better']]",[],"[['Results', 'shows', 'word vectors']]",[],[],[],[],[],[],sentiment_analysis,26,153
results,Note also that the accuracy using GloVe on the English movies review dataset is consistent with numbers reported in previous work .,"[('Note', (0, 1)), ('using', (5, 6)), ('on', (7, 8)), ('consistent with', (14, 16)), ('reported in', (17, 19))]","[('accuracy', (4, 5)), ('GloVe', (6, 7)), ('English movies review dataset', (9, 13)), ('numbers', (16, 17)), ('previous work', (19, 21))]","[['accuracy', 'using', 'GloVe'], ['GloVe', 'on', 'English movies review dataset'], ['English movies review dataset', 'consistent with', 'numbers'], ['numbers', 'reported in', 'previous work']]",[],"[['Results', 'Note', 'accuracy']]",[],[],[],[],[],[],sentiment_analysis,26,154
results,"Next , we consider our DM - MCNNs with their dual - module mechanism to take advantage of transfer learning .","[('consider', (3, 4)), ('with', (8, 9))]","[('our DM - MCNNs', (4, 8)), ('dual - module mechanism', (10, 14))]","[['our DM - MCNNs', 'with', 'dual - module mechanism']]",[],"[['Results', 'consider', 'our DM - MCNNs']]",[],[],[],[],[],[],sentiment_analysis,26,156
results,We observe fairly consistent and sometimes quite substan - tial gains over CNNs with just the GloVe / fastText vectors .,"[('observe', (1, 2)), ('over', (11, 12)), ('with just', (13, 15))]","[('fairly consistent and sometimes quite substan - tial gains', (2, 11)), ('CNNs', (12, 13)), ('GloVe / fastText vectors', (16, 20))]","[['fairly consistent and sometimes quite substan - tial gains', 'over', 'CNNs'], ['CNNs', 'with just', 'GloVe / fastText vectors']]",[],"[['Results', 'observe', 'fairly consistent and sometimes quite substan - tial gains']]",[],[],[],[],[],[],sentiment_analysis,26,157
results,"Although the automatically projected cross - lingual embeddings are very noisy and limited in their coverage , particularly with respect to inflected forms , our model succeeds in exploiting them to obtain substantial gains in several different languages and domains .","[('are', (8, 9)), ('particularly with respect to', (17, 21)), ('succeeds in exploiting them to obtain', (26, 32)), ('in', (34, 35))]","[('automatically projected cross - lingual embeddings', (2, 8)), ('very noisy and limited in their coverage', (9, 16)), ('inflected forms', (21, 23)), ('our model', (24, 26)), ('substantial gains', (32, 34)), ('several different languages and domains', (35, 40))]","[['automatically projected cross - lingual embeddings', 'are', 'very noisy and limited in their coverage'], ['very noisy and limited in their coverage', 'particularly with respect to', 'inflected forms'], ['our model', 'succeeds in exploiting them to obtain', 'substantial gains'], ['substantial gains', 'in', 'several different languages and domains']]","[['automatically projected cross - lingual embeddings', 'has', 'our model']]",[],"[['Results', 'has', 'automatically projected cross - lingual embeddings']]",[],[],[],[],[],sentiment_analysis,26,171
research-problem,Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification,[],"[('Aspect - level Sentiment Classification', (8, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level Sentiment Classification']]",[],[],[],[],sentiment_analysis,27,2
research-problem,"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .",[],"[('sentiment analysis', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,27,14
model,"In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .","[('propose', (5, 6)), ('to model', (9, 11)), ('for', (20, 21))]","[('novel method', (7, 9)), ('Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )', (11, 20)), ('aspect - level sentiment classification', (21, 26))]","[['novel method', 'to model', 'Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )'], ['Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )', 'for', 'aspect - level sentiment classification']]",[],"[['Model', 'propose', 'novel method']]",[],[],[],[],[],[],sentiment_analysis,27,39
model,"GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .","[('is', (1, 2)), ('operating on', (9, 11)), ('can catch', (14, 16)), ('from', (18, 19))]","[('GCN', (0, 1)), ('simple and effective convolutional neural network', (3, 9)), ('graphs', (11, 12)), ('inter-dependent information', (16, 18)), ('rich relational data', (19, 22))]","[['GCN', 'is', 'simple and effective convolutional neural network'], ['simple and effective convolutional neural network', 'operating on', 'graphs'], ['graphs', 'can catch', 'inter-dependent information'], ['inter-dependent information', 'from', 'rich relational data']]",[],[],"[['Model', 'has', 'GCN']]",[],[],[],[],[],sentiment_analysis,27,40
model,"For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .","[('For', (0, 1)), ('in', (3, 4)), ('encodes', (7, 8)), ('about', (10, 11)), ('as', (13, 14))]","[('every node', (1, 3)), ('graph', (4, 5)), ('GCN', (6, 7)), ('relevant information', (8, 10)), ('neighborhoods', (12, 13)), ('new feature representation vector', (15, 19))]","[['every node', 'in', 'graph'], ['GCN', 'encodes', 'relevant information'], ['relevant information', 'as', 'new feature representation vector'], ['relevant information', 'about', 'neighborhoods']]","[['every node', 'has', 'GCN']]","[['Model', 'For', 'every node']]",[],[],[],[],[],[],sentiment_analysis,27,41
model,"In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .","[('treated as', (7, 9)), ('represents', (15, 16)), ('of', (20, 21))]","[('aspect', (5, 6)), ('node', (10, 11)), ('edge', (14, 15)), ('sentiment dependency relation', (17, 20)), ('two nodes', (21, 23))]","[['edge', 'represents', 'sentiment dependency relation'], ['sentiment dependency relation', 'of', 'two nodes'], ['aspect', 'treated as', 'node']]",[],[],"[['Model', 'has', 'edge'], ['Model', 'has', 'aspect']]",[],[],[],[],[],sentiment_analysis,27,42
model,Our model learns the sentiment dependencies of aspects via this graph structure .,"[('learns', (2, 3)), ('of', (6, 7)), ('via', (8, 9))]","[('sentiment dependencies', (4, 6)), ('aspects', (7, 8)), ('graph structure', (10, 12))]","[['sentiment dependencies', 'of', 'aspects'], ['aspects', 'via', 'graph structure']]",[],"[['Model', 'learns', 'sentiment dependencies']]",[],[],[],[],[],[],sentiment_analysis,27,43
model,"As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .","[('first to consider', (10, 13)), ('between', (16, 17)), ('in', (18, 19)), ('for', (21, 22))]","[('sentiment dependencies', (14, 16)), ('aspects', (17, 18)), ('one sentence', (19, 21)), ('aspect - level sentiment classification task', (22, 28))]","[['sentiment dependencies', 'between', 'aspects'], ['aspects', 'in', 'one sentence'], ['one sentence', 'for', 'aspect - level sentiment classification task']]",[],"[['Model', 'first to consider', 'sentiment dependencies']]",[],[],[],[],[],[],sentiment_analysis,27,44
model,"Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .","[('applies', (14, 15)), ('with', (18, 19)), ('before', (21, 22))]","[('bidirectional attention mechanism', (15, 18)), ('position encoding', (19, 21)), ('GCN', (22, 23))]","[['bidirectional attention mechanism', 'with', 'position encoding'], ['position encoding', 'before', 'GCN']]",[],"[['Model', 'applies', 'bidirectional attention mechanism']]",[],[],[],[],[],[],sentiment_analysis,27,45
hyperparameters,"In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .","[('use', (6, 7)), ('to initialize', (21, 23))]","[('GloVe 3 word vector', (8, 12)), ('pre-trained language model word representation BERT', (14, 20)), ('word embeddings', (24, 26))]","[['word embeddings', 'use', 'GloVe 3 word vector'], ['word embeddings', 'use', 'pre-trained language model word representation BERT']]",[],"[['Hyperparameters', 'to initialize', 'word embeddings']]",[],[],[],[],[],[],sentiment_analysis,27,184
hyperparameters,The dimension of each word vector is 300 for GloVe and 768 for BERT .,[],[],"[['dimension', 'of', 'each word vector'], ['each word vector', 'is', '300'], ['300', 'for', 'GloVe'], ['each word vector', 'is', '768'], ['768', 'for', 'BERT']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,27,185
hyperparameters,"The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .",[],[],"[['number', 'of', 'LSTM hidden units'], ['LSTM hidden units', 'set to', '300'], ['output dimension', 'of', 'GCN layer'], ['GCN layer', 'set to', '600']]",[],[],"[['Hyperparameters', 'has', 'number'], ['Hyperparameters', 'has', 'output dimension']]",[],[],[],[],[],sentiment_analysis,27,186
hyperparameters,"The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .","[('of', (3, 4)), ('randomly initialized by', (9, 12))]","[('weight matrix', (1, 3)), ('last fully connect layer', (4, 8)), ('normal distribution N ( 0 , 1 )', (13, 21))]","[['weight matrix', 'of', 'last fully connect layer'], ['last fully connect layer', 'randomly initialized by', 'normal distribution N ( 0 , 1 )']]",[],[],"[['Hyperparameters', 'has', 'weight matrix']]",[],[],[],[],[],sentiment_analysis,27,187
hyperparameters,"Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .","[('Besides', (0, 1)), ('randomly initialized by', (12, 15))]","[('last fully connect layer', (2, 6)), ('all the weight matrices', (7, 11)), ('uniform distribution U ( ? 0.01 , 0.01 )', (16, 25))]","[['all the weight matrices', 'randomly initialized by', 'uniform distribution U ( ? 0.01 , 0.01 )'], ['all the weight matrices', 'Besides', 'last fully connect layer']]",[],[],"[['Hyperparameters', 'has', 'all the weight matrices']]",[],[],[],[],[],sentiment_analysis,27,188
hyperparameters,"In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .","[('add', (4, 5)), ('to', (6, 7)), ('with', (12, 13)), ('of', (15, 16))]","[('L2-regularization', (5, 6)), ('last fully connect layer', (8, 12)), ('weight', (14, 15)), ('0.01', (16, 17))]","[['L2-regularization', 'to', 'last fully connect layer'], ['last fully connect layer', 'with', 'weight'], ['weight', 'of', '0.01']]",[],"[['Hyperparameters', 'add', 'L2-regularization']]",[],[],[],[],[],[],sentiment_analysis,27,189
hyperparameters,"During training , we set dropout to 0.5 , the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001 .","[('During', (0, 1)), ('set', (4, 5)), ('to', (6, 7)), ('is', (12, 13)), ('set to', (13, 15)), ('with', (22, 23)), ('of', (26, 27))]","[('training', (1, 2)), ('dropout', (5, 6)), ('0.5', (7, 8)), ('batch size', (10, 12)), ('32', (15, 16)), ('optimizer', (18, 19)), ('Adam Optimizer', (20, 22)), ('learning rate', (24, 26)), ('0.001', (27, 28))]","[['training', 'set', 'dropout'], ['dropout', 'to', '0.5'], ['optimizer', 'is', 'Adam Optimizer'], ['Adam Optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.001'], ['batch size', 'set to', '32']]","[['training', 'has', 'optimizer'], ['training', 'has', 'batch size']]","[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],sentiment_analysis,27,190
hyperparameters,We implement our proposed model using Tensorflow 5 .,"[('implement', (1, 2)), ('using', (5, 6))]","[('proposed model', (3, 5)), ('Tensorflow', (6, 7))]","[['proposed model', 'using', 'Tensorflow']]",[],"[['Hyperparameters', 'implement', 'proposed model']]",[],[],[],[],[],[],sentiment_analysis,27,191
baselines,"TD - LSTM constructs aspect-specific representation by the left context with aspect and the right context with aspect , then employs two LSTMs to model them respectively .",[],[],"[['TD - LSTM', 'employs', 'two LSTMs'], ['TD - LSTM', 'constructs', 'aspect-specific representation'], ['aspect-specific representation', 'by', 'right context'], ['right context', 'with', 'aspect'], ['aspect-specific representation', 'by', 'left context'], ['left context', 'with', 'aspect']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,27,196
baselines,The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .,[],[],"[['last hidden states', 'of', 'two LSTMs'], ['finally concatenated', 'for predicting', 'sentiment polarity'], ['sentiment polarity', 'of', 'aspect']]","[['last hidden states', 'has', 'finally concatenated']]",[],[],[],[],[],"[['TD - LSTM', 'has', 'last hidden states']]",[],sentiment_analysis,27,197
baselines,"ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .","[('attaches', (4, 5)), ('to', (8, 9)), ('to capture', (12, 14)), ('employs', (21, 22)), ('to get', (24, 26)), ('for', (29, 30))]","[('ATAE - LSTM', (0, 3)), ('aspect embedding', (6, 8)), ('each word embedding', (9, 12)), ('aspect - dependent information', (14, 18)), ('attention mechanism', (22, 24)), ('sentence representation', (27, 29)), ('final classification', (30, 32))]","[['ATAE - LSTM', 'employs', 'attention mechanism'], ['attention mechanism', 'to get', 'sentence representation'], ['sentence representation', 'for', 'final classification'], ['ATAE - LSTM', 'attaches', 'aspect embedding'], ['aspect embedding', 'to', 'each word embedding'], ['each word embedding', 'to capture', 'aspect - dependent information']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,27,198
baselines,Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,"[('uses', (2, 3)), ('on', (7, 8)), ('for', (12, 13)), ('to capture', (15, 17)), ('between', (19, 20))]","[('Mem Net', (0, 2)), ('deep memory network', (4, 7)), ('context word embeddings', (9, 12)), ('sentence representation', (13, 15)), ('relevance', (18, 19)), ('each context word and the aspect', (20, 26))]","[['Mem Net', 'uses', 'deep memory network'], ['deep memory network', 'on', 'context word embeddings'], ['context word embeddings', 'for', 'sentence representation'], ['sentence representation', 'to capture', 'relevance'], ['relevance', 'between', 'each context word and the aspect'], ['relevance', 'between', 'each context word and the aspect']]",[],[],"[['Baselines', 'has', 'Mem Net']]",[],[],[],[],[],sentiment_analysis,27,199
baselines,IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .,"[('generates', (1, 2)), ('for', (4, 5)), ('with', (9, 10))]","[('IAN', (0, 1)), ('representations', (3, 4)), ('aspect terms and contexts', (5, 9)), ('two attention - based LSTM network', (10, 16))]","[['IAN', 'generates', 'representations'], ['representations', 'for', 'aspect terms and contexts'], ['aspect terms and contexts', 'with', 'two attention - based LSTM network']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,27,201
baselines,"RAM [ 10 ] employs a gated recurrent unit network to model a multiple attention mechanism , and captures the relevance between each context word and the aspect .",[],[],"[['RAM', 'captures', 'relevance'], ['RAM', 'employs', 'gated recurrent unit network'], ['gated recurrent unit network', 'to model', 'multiple attention mechanism']]",[],[],"[['Baselines', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,27,203
baselines,PBAN appends the position embedding into each word embedding .,"[('appends', (1, 2)), ('into', (5, 6))]","[('PBAN', (0, 1)), ('position embedding', (3, 5)), ('each word embedding', (6, 9))]","[['PBAN', 'appends', 'position embedding'], ['position embedding', 'into', 'each word embedding']]",[],[],"[['Baselines', 'has', 'PBAN']]",[],[],[],[],[],sentiment_analysis,27,205
baselines,TSN is a two - stage framework for aspect - level sentiment analysis .,"[('is', (1, 2)), ('for', (7, 8))]","[('TSN', (0, 1)), ('two - stage framework', (3, 7)), ('aspect - level sentiment analysis', (8, 13))]","[['TSN', 'is', 'two - stage framework'], ['two - stage framework', 'for', 'aspect - level sentiment analysis']]",[],[],"[['Baselines', 'has', 'TSN']]",[],[],[],[],[],sentiment_analysis,27,207
baselines,"AEN mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an output layer .","[('consists of', (2, 4))]","[('AEN', (0, 1)), ('embedding layer', (5, 7)), ('attentional encoder layer', (9, 12)), ('aspect - specific attention layer', (14, 19)), ('output layer', (22, 24))]","[['AEN', 'consists of', 'embedding layer'], ['AEN', 'consists of', 'attentional encoder layer'], ['AEN', 'consists of', 'aspect - specific attention layer'], ['AEN', 'consists of', 'output layer']]",[],[],"[['Baselines', 'has', 'AEN']]",[],[],[],[],[],sentiment_analysis,27,210
baselines,AEN - BERT is AEN with BERT embedding .,"[('is', (3, 4)), ('with', (5, 6))]","[('AEN - BERT', (0, 3)), ('AEN', (4, 5)), ('BERT embedding', (6, 8))]","[['AEN - BERT', 'is', 'AEN'], ['AEN', 'with', 'BERT embedding']]",[],[],"[['Baselines', 'has', 'AEN - BERT']]",[],[],[],[],[],sentiment_analysis,27,212
results,"Among all the GloVe - based methods , the TD - LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way .","[('Among', (0, 1)), ('performs', (13, 14))]","[('all the GloVe - based methods', (1, 7)), ('TD - LSTM approach', (9, 13)), ('worst', (14, 15))]","[['TD - LSTM approach', 'performs', 'worst']]","[['all the GloVe - based methods', 'has', 'TD - LSTM approach']]","[['Results', 'Among', 'all the GloVe - based methods']]",[],[],[],[],[],[],sentiment_analysis,27,218
results,"After taking the importance of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .","[('After taking', (0, 2)), ('of', (4, 5)), ('into', (7, 8)), ('with', (9, 10)), ('achieve', (14, 15)), ('comparing to', (18, 20))]","[('importance', (3, 4)), ('aspect', (6, 7)), ('account', (8, 9)), ('attention mechanism', (10, 12)), ('stable improvement', (16, 18)), ('TD - LSTM', (21, 24))]","[['importance', 'with', 'attention mechanism'], ['importance', 'achieve', 'stable improvement'], ['stable improvement', 'comparing to', 'TD - LSTM'], ['importance', 'of', 'aspect'], ['aspect', 'into', 'account']]",[],"[['Results', 'After taking', 'importance']]",[],[],[],[],[],[],sentiment_analysis,27,220
results,"RAM achieves a better performance than other basic attention - based models , because it combines multiple attentions with a recurrent neural network to capture aspect - specific representations .","[('achieves', (1, 2)), ('than', (5, 6))]","[('RAM', (0, 1)), ('better performance', (3, 5)), ('other basic attention - based models', (6, 12))]","[['RAM', 'achieves', 'better performance'], ['better performance', 'than', 'other basic attention - based models']]",[],[],"[['Results', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,27,221
results,PBAN achieves a similar performance as RAM by employing a position embedding .,"[('achieves', (1, 2)), ('as', (5, 6)), ('by employing', (7, 9))]","[('PBAN', (0, 1)), ('similar performance', (3, 5)), ('RAM', (6, 7)), ('position embedding', (10, 12))]","[['PBAN', 'achieves', 'similar performance'], ['similar performance', 'as', 'RAM'], ['RAM', 'by employing', 'position embedding']]",[],[],"[['Results', 'has', 'PBAN']]",[],[],[],[],[],sentiment_analysis,27,222
results,"To be specific , PBAN is better than RAM on Restaurant dataset , but worse than RAN on Laptop dataset .",[],[],"[['RAN', 'on', 'Laptop dataset'], ['RAM', 'on', 'Restaurant dataset']]",[],[],[],[],"[['PBAN', 'worse than', 'RAN'], ['PBAN', 'better than', 'RAM']]",[],[],[],sentiment_analysis,27,223
results,"Compared with RAM and PBAN , the over all performance of TSN is not perform well on both Restaurant dataset and Laptop dataset , which might because the framework of TSN is too simple to model the representations of context and aspect effectively .","[('Compared with', (0, 2)), ('of', (10, 11)), ('on both', (16, 18))]","[('RAM and PBAN', (2, 5)), ('over all performance', (7, 10)), ('TSN', (11, 12)), ('not perform well', (13, 16)), ('Restaurant dataset and Laptop dataset', (18, 23))]","[['over all performance', 'of', 'TSN'], ['not perform well', 'on both', 'Restaurant dataset and Laptop dataset']]","[['RAM and PBAN', 'has', 'over all performance'], ['over all performance', 'has', 'not perform well']]","[['Results', 'Compared with', 'RAM and PBAN']]",[],[],[],[],[],[],sentiment_analysis,27,224
results,"AEN is slightly better than TSN , but still worse than RAM and PBAN .","[('slightly better than', (2, 5)), ('still worse than', (8, 11))]","[('AEN', (0, 1)), ('TSN', (5, 6)), ('RAM and PBAN', (11, 14))]","[['AEN', 'still worse than', 'RAM and PBAN'], ['AEN', 'slightly better than', 'TSN']]",[],[],"[['Results', 'has', 'AEN']]",[],[],[],[],[],sentiment_analysis,27,225
results,"Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .","[('Comparing', (0, 1)), ('of', (3, 4)), ('observe', (27, 28)), ('built with', (31, 33)), ('slightly higher than', (37, 40)), ('in both', (45, 47))]","[('results', (2, 3)), ('SDGCN - A w/o position and SDGCN - G w/o position', (4, 15)), ('SDGCN - A and SDGCN - G', (16, 23)), ('GCN', (30, 31)), ('global - relation', (33, 36)), ('built with adjacent - relation', (40, 45)), ('accuracy and Macro - F1 measure', (47, 53))]","[['results', 'of', 'SDGCN - A w/o position and SDGCN - G w/o position'], ['results', 'of', 'SDGCN - A and SDGCN - G'], ['results', 'observe', 'GCN'], ['GCN', 'slightly higher than', 'built with adjacent - relation'], ['built with adjacent - relation', 'in both', 'accuracy and Macro - F1 measure'], ['GCN', 'built with', 'global - relation']]",[],"[['Results', 'Comparing', 'results']]",[],[],[],[],[],[],sentiment_analysis,27,227
results,"Moreover , the two models ( SDGCN - A and SDGCN - G ) with position information gain a significant improvement compared to the two models without position information .",[],[],"[['two models ( SDGCN - A and SDGCN - G )', 'with', 'position information'], ['position information', 'gain', 'significant improvement'], ['significant improvement', 'compared to', 'two models'], ['two models', 'without', 'position information']]",[],[],"[['Results', 'has', 'two models ( SDGCN - A and SDGCN - G )']]",[],[],[],[],[],sentiment_analysis,27,229
results,"Benefits from the power of pre-trained BERT , BERT - based models have shown huge superiority over GloVe - based models .","[('Benefits from', (0, 2)), ('of', (4, 5)), ('shown', (13, 14)), ('over', (16, 17))]","[('power', (3, 4)), ('pre-trained BERT', (5, 7)), ('BERT - based models', (8, 12)), ('huge superiority', (14, 16)), ('GloVe - based models', (17, 21))]","[['power', 'of', 'pre-trained BERT'], ['BERT - based models', 'shown', 'huge superiority'], ['huge superiority', 'over', 'GloVe - based models']]",[],"[['Results', 'Benefits from', 'power'], ['Results', 'Benefits from', 'BERT - based models']]",[],[],[],[],[],[],sentiment_analysis,27,231
results,"Furthermore , compared with AEN - BERT , on the Restaurant dataset , SDGCN - BERT achieves absolute increases of 1.09 % and 1.86 % in accuracy and Macro - F1 measure respectively , and gains absolute increases of 1.42 % and 2.03 % in accuracy and Macro - F1 measure respectively on the Laptop dataset .",[],[],"[['AEN - BERT', 'on', 'Restaurant dataset'], ['SDGCN - BERT', 'achieves', 'absolute increases'], ['absolute increases', 'in', 'accuracy and Macro - F1 measure'], ['absolute increases', 'of', '1.09 % and 1.86 %'], ['SDGCN - BERT', 'gains', 'absolute increases'], ['absolute increases', 'in', 'accuracy and Macro - F1 measure'], ['absolute increases', 'of', '1.42 % and 2.03 %'], ['absolute increases', 'on', 'Laptop dataset']]",[],"[['Results', 'compared with', 'AEN - BERT'], ['Results', 'compared with', 'SDGCN - BERT']]",[],[],[],[],[],[],sentiment_analysis,27,232
research-problem,Attentional Encoder Network for Targeted Sentiment Classification,[],"[('Targeted Sentiment Classification', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Targeted Sentiment Classification']]",[],[],[],[],sentiment_analysis,28,2
research-problem,"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence .",[],"[('fine - grained sentiment analysis', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'fine - grained sentiment analysis']]",[],[],[],[],sentiment_analysis,28,12
research-problem,"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .",[],"[('fine - grained targeted sentiment classification', (14, 20))]",[],[],[],[],"[['Contribution', 'has research problem', 'fine - grained targeted sentiment classification']]",[],[],[],[],sentiment_analysis,28,16
model,This paper propose an attention based model to solve the problems above .,"[('propose', (2, 3))]","[('attention based model', (4, 7))]",[],[],"[['Model', 'propose', 'attention based model']]",[],[],[],[],[],[],sentiment_analysis,28,26
model,"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .","[('eschews', (4, 5)), ('employs', (7, 8)), ('as', (9, 10)), ('to draw', (13, 15)), ('between', (20, 21))]","[('our model', (2, 4)), ('recurrence', (5, 6)), ('attention', (8, 9)), ('competitive alternative', (11, 13)), ('introspective and interactive semantics', (16, 20)), ('target and context words', (21, 25))]","[['our model', 'employs', 'attention'], ['attention', 'as', 'competitive alternative'], ['competitive alternative', 'to draw', 'introspective and interactive semantics'], ['introspective and interactive semantics', 'between', 'target and context words'], ['our model', 'eschews', 'recurrence']]",[],[],"[['Model', 'has', 'our model']]",[],[],[],[],[],sentiment_analysis,28,27
model,"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .","[('To deal with', (0, 3)), ('employ', (9, 10)), ('to encourage', (14, 16)), ('to be', (18, 20)), ('with', (22, 23))]","[('label unreliability issue', (4, 7)), ('label smoothing regularization', (11, 14)), ('model', (17, 18)), ('less confident', (20, 22)), ('fuzzy labels', (23, 25))]","[['label unreliability issue', 'employ', 'label smoothing regularization'], ['label smoothing regularization', 'to encourage', 'model'], ['model', 'with', 'fuzzy labels'], ['model', 'to be', 'less confident']]",[],"[['Model', 'To deal with', 'label unreliability issue']]",[],[],[],[],[],[],sentiment_analysis,28,28
model,We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,"[('apply', (2, 3))]","[('pre-trained BERT', (3, 5))]",[],[],"[['Model', 'apply', 'pre-trained BERT']]",[],[],[],[],[],[],sentiment_analysis,28,29
hyperparameters,shows the number of training and test instances in each category .,"[('shows', (0, 1)), ('in', (8, 9))]","[('number of training and test instances', (2, 8)), ('each category', (9, 11))]","[['number of training and test instances', 'in', 'each category']]",[],"[['Hyperparameters', 'shows', 'number of training and test instances']]",[],[],[],[],[],[],sentiment_analysis,28,126
hyperparameters,"Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .",[],[],"[['Word embeddings', 'in', 'AEN - Glo Ve'], ['AEN - Glo Ve', 'do not get updated in', 'learning process'], ['Word embeddings', 'fine - tune', 'pre-trained BERT'], ['pre-trained BERT', 'in', 'AEN - BERT']]",[],[],"[['Hyperparameters', 'has', 'Word embeddings']]",[],[],[],[],[],sentiment_analysis,28,127
hyperparameters,Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .,[],[],"[['Embedding dimension d dim', 'is', '300'], ['300', 'for', 'GloVe'], ['Embedding dimension d dim', 'is', '768'], ['768', 'for', 'pretrained BERT']]",[],[],"[['Hyperparameters', 'has', 'Embedding dimension d dim']]",[],[],[],[],[],sentiment_analysis,28,128
hyperparameters,Dimension of hidden states d hid is set to 300 .,"[('of', (1, 2)), ('set to', (7, 9))]","[('Dimension', (0, 1)), ('hidden states d hid', (2, 6)), ('300', (9, 10))]","[['Dimension', 'of', 'hidden states d hid'], ['hidden states d hid', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'Dimension']]",[],[],[],[],[],sentiment_analysis,28,129
hyperparameters,The weights of our model are initialized with Glorot initialization .,"[('of', (2, 3)), ('initialized with', (6, 8))]","[('weights', (1, 2)), ('our model', (3, 5)), ('Glorot initialization', (8, 10))]","[['weights', 'of', 'our model'], ['weights', 'initialized with', 'Glorot initialization']]",[],[],"[['Hyperparameters', 'has', 'weights']]",[],[],[],[],[],sentiment_analysis,28,130
hyperparameters,"During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .",[],[],"[['training', 'set', 'label smoothing parameter'], ['label smoothing parameter', 'to', '0.2'], ['training', 'set', 'dropout rate'], ['dropout rate', 'is', '0.1'], ['training', 'set', 'coefficient ? of L 2 regularization item'], ['coefficient ? of L 2 regularization item', 'is', '10 ? 5']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],sentiment_analysis,28,131
hyperparameters,"Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .","[('applied to', (10, 12))]","[('Adam optimizer ( Kingma and Ba , 2014 )', (0, 9)), ('update', (12, 13)), ('all the parameters', (13, 16))]","[['Adam optimizer ( Kingma and Ba , 2014 )', 'applied to', 'update']]","[['update', 'has', 'all the parameters']]",[],"[['Hyperparameters', 'has', 'Adam optimizer ( Kingma and Ba , 2014 )']]",[],[],[],[],[],sentiment_analysis,28,132
baselines,We also design a basic BERT - based model to evaluate the performance of AEN - BERT .,"[('design', (2, 3)), ('to evaluate', (9, 11)), ('of', (13, 14))]","[('basic BERT - based model', (4, 9)), ('performance', (12, 13)), ('AEN - BERT', (14, 17))]","[['basic BERT - based model', 'to evaluate', 'performance'], ['performance', 'of', 'AEN - BERT']]",[],"[['Baselines', 'design', 'basic BERT - based model']]",[],[],[],[],[],[],sentiment_analysis,28,136
baselines,Non - RNN based baselines :,[],"[('Non - RNN based baselines', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Non - RNN based baselines']]",[],[],[],[],"[['Non - RNN based baselines', 'has', 'Feature - based SVM']]",sentiment_analysis,28,137
baselines,Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .,"[('is', (4, 5)), ('with', (12, 13))]","[('Feature - based SVM', (0, 4)), ('traditional support vector machine based model', (6, 12)), ('extensive feature engineering', (13, 16))]","[['Feature - based SVM', 'is', 'traditional support vector machine based model'], ['traditional support vector machine based model', 'with', 'extensive feature engineering']]",[],[],[],[],[],[],[],[],sentiment_analysis,28,138
baselines,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .","[('uses', (4, 5)), ('to transform', (6, 8)), ('put', (12, 13)), ('at', (16, 17)), ('learns', (22, 23)), ('toward', (26, 27)), ('via', (28, 29)), ('using', (31, 32))]","[('Rec - NN', (0, 3)), ('rules', (5, 6)), ('dependency tree', (9, 11)), ('opinion target', (14, 16)), ('root', (18, 19)), ('sentence representation', (24, 26)), ('target', (27, 28)), ('semantic composition', (29, 31)), ('Recursive NNs', (32, 34))]","[['Rec - NN', 'learns', 'sentence representation'], ['sentence representation', 'toward', 'target'], ['sentence representation', 'via', 'semantic composition'], ['semantic composition', 'using', 'Recursive NNs'], ['Rec - NN', 'uses', 'rules'], ['rules', 'to transform', 'dependency tree'], ['Rec - NN', 'put', 'opinion target'], ['opinion target', 'at', 'root']]",[],[],[],[],[],[],"[['Non - RNN based baselines', 'has', 'Rec - NN']]",[],sentiment_analysis,28,139
baselines,MemNet uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word .,"[('uses', (1, 2)), ('on', (6, 7)), ('for', (11, 12)), ('to explicitly captures', (14, 17)), ('of', (19, 20))]","[('MemNet', (0, 1)), ('multi-hops of attention layers', (2, 6)), ('context word embeddings', (8, 11)), ('sentence representation', (12, 14)), ('importance', (18, 19)), ('each context word', (20, 23))]","[['MemNet', 'uses', 'multi-hops of attention layers'], ['multi-hops of attention layers', 'to explicitly captures', 'importance'], ['importance', 'of', 'each context word'], ['multi-hops of attention layers', 'on', 'context word embeddings'], ['context word embeddings', 'for', 'sentence representation']]",[],[],[],[],[],[],"[['Non - RNN based baselines', 'has', 'MemNet']]",[],sentiment_analysis,28,140
baselines,RNN based baselines :,[],"[('RNN based baselines', (0, 3))]",[],[],[],"[['Baselines', 'has', 'RNN based baselines']]",[],[],[],[],"[['RNN based baselines', 'has', 'TD - LSTM']]",sentiment_analysis,28,141
baselines,TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .,[],[],"[['TD - LSTM', 'extends', 'LSTM'], ['LSTM', 'by using', 'two LSTM networks'], ['LSTM', 'to model', 'right context'], ['right context', 'with', 'target'], ['LSTM', 'to model', 'left context'], ['left context', 'with', 'target']]",[],[],[],[],[],[],[],[],sentiment_analysis,28,142
baselines,ATAE - LSTM,[],"[('ATAE - LSTM', (0, 3))]",[],[],[],[],[],[],[],"[['RNN based baselines', 'has', 'ATAE - LSTM']]",[],sentiment_analysis,28,144
baselines,"( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .",[],[],"[['LSTM', 'with', 'attention'], ['attention', 'to get', 'final representation'], ['final representation', 'for', 'classification'], ['effect of target embeddings', 'which appends', 'target embeddings'], ['target embeddings', 'with', 'each word embeddings']]",[],[],[],[],"[['ATAE - LSTM', 'use', 'LSTM'], ['ATAE - LSTM', 'strengthens', 'effect of target embeddings']]",[],[],[],sentiment_analysis,28,145
baselines,"IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .",[],[],"[['IAN', 'learns', 'representations'], ['representations', 'of', 'target and context'], ['target and context', 'with', 'two LSTMs and attentions'], ['target and context', 'which generates', 'representations'], ['representations', 'for', 'targets and contexts'], ['targets and contexts', 'with respect to', 'each other']]",[],[],[],[],[],[],"[['RNN based baselines', 'has', 'IAN']]",[],sentiment_analysis,28,146
baselines,RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .,"[('strengthens', (1, 2)), ('by representing', (5, 7)), ('with', (8, 9)), ('using', (12, 13)), ('to combine', (18, 20)), ('for', (24, 25))]","[('RAM', (0, 1)), ('Mem - Net', (2, 5)), ('memory', (7, 8)), ('bidirectional LSTM', (9, 11)), ('gated recurrent unit network', (14, 18)), ('multiple attention outputs', (21, 24)), ('sentence representation', (25, 27))]","[['RAM', 'using', 'gated recurrent unit network'], ['gated recurrent unit network', 'to combine', 'multiple attention outputs'], ['multiple attention outputs', 'for', 'sentence representation'], ['RAM', 'strengthens', 'Mem - Net'], ['Mem - Net', 'by representing', 'memory'], ['memory', 'with', 'bidirectional LSTM']]",[],[],[],[],[],[],"[['RNN based baselines', 'has', 'RAM']]",[],sentiment_analysis,28,147
baselines,AEN - Glo Ve ablations :,[],"[('AEN - Glo Ve ablations', (0, 5))]",[],[],[],"[['Baselines', 'has', 'AEN - Glo Ve ablations']]",[],[],[],[],"[['AEN - Glo Ve ablations', 'has', 'AEN - GloVe w/ o PCT']]",sentiment_analysis,28,148
baselines,AEN - GloVe w/ o PCT ablates PCT module .,"[('ablates', (6, 7))]","[('AEN - GloVe w/ o PCT', (0, 6)), ('PCT module', (7, 9))]","[['AEN - GloVe w/ o PCT', 'ablates', 'PCT module']]",[],[],[],[],[],[],[],[],sentiment_analysis,28,149
baselines,AEN - GloVe w/ o MHA ablates MHA module .,"[('ablates', (6, 7))]","[('AEN - GloVe w/ o MHA', (0, 6)), ('MHA module', (7, 9))]","[['AEN - GloVe w/ o MHA', 'ablates', 'MHA module']]",[],[],[],[],[],[],"[['AEN - Glo Ve ablations', 'has', 'AEN - GloVe w/ o MHA']]",[],sentiment_analysis,28,150
baselines,AEN - GloVe w/ o LSR ablates label smoothing regularization .,"[('ablates', (6, 7))]","[('AEN - GloVe w/ o LSR', (0, 6)), ('label smoothing regularization', (7, 10))]","[['AEN - GloVe w/ o LSR', 'ablates', 'label smoothing regularization']]",[],[],[],[],[],[],"[['AEN - Glo Ve ablations', 'has', 'AEN - GloVe w/ o LSR']]",[],sentiment_analysis,28,151
baselines,AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM .,"[('replaces', (1, 2)), ('with', (6, 7))]","[('AEN-GloVe-BiLSTM', (0, 1)), ('attentional encoder layer', (3, 6)), ('two bidirectional LSTM', (7, 10))]","[['AEN-GloVe-BiLSTM', 'replaces', 'attentional encoder layer'], ['attentional encoder layer', 'with', 'two bidirectional LSTM']]",[],[],[],[],[],[],"[['AEN - Glo Ve ablations', 'has', 'AEN-GloVe-BiLSTM']]",[],sentiment_analysis,28,152
baselines,Basic BERT - based model :,[],"[('Basic BERT - based model', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Basic BERT - based model']]",[],[],[],[],"[['Basic BERT - based model', 'has', 'BERT - SPC']]",sentiment_analysis,28,153
baselines,"BERT - SPC feeds sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] "" into the basic BERT model for sentence pair classification task .","[('feeds', (3, 4)), ('into', (22, 23)), ('for', (27, 28))]","[('BERT - SPC', (0, 3)), ('sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""', (4, 22)), ('basic BERT model', (24, 27)), ('sentence pair classification task', (28, 32))]","[['BERT - SPC', 'feeds', 'sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""'], ['sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""', 'into', 'basic BERT model'], ['sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""', 'for', 'sentence pair classification task']]",[],[],[],[],[],[],[],[],sentiment_analysis,28,154
results,The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .,[],[],"[['over all performance', 'of', 'TD - LSTM'], ['TD - LSTM', 'makes', 'rough treatment'], ['rough treatment', 'of', 'target words'], ['TD - LSTM', 'is', 'not good']]",[],[],"[['Results', 'has', 'over all performance']]",[],[],[],[],[],sentiment_analysis,28,160
results,"ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .","[('are', (7, 8)), ('stably exceed', (13, 15)), ('on', (20, 21))]","[('ATAE - LSTM , IAN and RAM', (0, 7)), ('attention based models', (8, 11)), ('TD - LSTM method', (16, 20)), ('Restaurant and Laptop datasets', (21, 25))]","[['ATAE - LSTM , IAN and RAM', 'stably exceed', 'TD - LSTM method'], ['TD - LSTM method', 'on', 'Restaurant and Laptop datasets'], ['ATAE - LSTM , IAN and RAM', 'are', 'attention based models']]",[],[],"[['Results', 'has', 'ATAE - LSTM , IAN and RAM']]",[],[],[],[],[],sentiment_analysis,28,161
results,"RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .","[('better than', (2, 4)), ('does not perform', (11, 14)), ('on', (15, 16))]","[('RAM', (0, 1)), ('other RNN based models', (4, 8)), ('well', (14, 15)), ('Twitter dataset', (16, 18))]","[['RAM', 'does not perform', 'well'], ['well', 'on', 'Twitter dataset'], ['RAM', 'better than', 'other RNN based models']]",[],[],"[['Results', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,28,162
results,"Feature - based SVM is still a competitive baseline , but relying on manually - designed features .","[('still', (5, 6)), ('relying on', (11, 13))]","[('Feature - based SVM', (0, 4)), ('competitive baseline', (7, 9)), ('manually - designed features', (13, 17))]","[['Feature - based SVM', 'still', 'competitive baseline'], ['Feature - based SVM', 'relying on', 'manually - designed features']]",[],[],"[['Results', 'has', 'Feature - based SVM']]",[],[],[],[],[],sentiment_analysis,28,163
results,Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .,"[('gets', (3, 4)), ('among', (7, 8))]","[('Rec - NN', (0, 3)), ('worst performances', (5, 7)), ('all neural network baselines', (8, 12))]","[['Rec - NN', 'gets', 'worst performances'], ['worst performances', 'among', 'all neural network baselines']]",[],[],"[['Results', 'has', 'Rec - NN']]",[],[],[],[],[],sentiment_analysis,28,164
results,"Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .","[('Like', (0, 1)), ('eschews', (6, 7)), ('is', (14, 15))]","[('AEN', (1, 2)), ('Mem Net', (3, 5)), ('recurrence', (7, 8)), ('over all performance', (11, 14)), ('not good', (15, 17))]","[['Mem Net', 'eschews', 'recurrence'], ['over all performance', 'is', 'not good']]","[['AEN', 'has', 'Mem Net'], ['Mem Net', 'has', 'over all performance']]","[['Results', 'Like', 'AEN']]",[],[],[],[],[],[],sentiment_analysis,28,165
ablation-analysis,"Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .","[('Comparing the results of', (0, 4)), ('observe', (18, 19)), ('of', (22, 23)), ('drops', (31, 32)), ('on', (33, 34))]","[('AEN - GloVe and AEN - Glo Ve w / o LSR', (4, 16)), ('accuracy', (21, 22)), ('AEN - Glo Ve w / o LSR', (23, 31)), ('significantly', (32, 33)), ('all three datasets', (34, 37))]","[['AEN - GloVe and AEN - Glo Ve w / o LSR', 'observe', 'accuracy'], ['accuracy', 'drops', 'significantly'], ['significantly', 'on', 'all three datasets'], ['accuracy', 'of', 'AEN - Glo Ve w / o LSR']]",[],"[['Ablation analysis', 'Comparing the results of', 'AEN - GloVe and AEN - Glo Ve w / o LSR']]",[],[],[],[],[],[],sentiment_analysis,28,169
ablation-analysis,"The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .","[('of', (4, 5)), ('is', (15, 16)), ('performs', (23, 24)), ('on', (25, 26))]","[('over all performance', (1, 4)), ('AEN - GloVe and AEN - Glo Ve - BiLSTM', (5, 15)), ('relatively close', (16, 18)), ('AEN - Glo Ve', (19, 23)), ('better', (24, 25)), ('Restaurant dataset', (27, 29))]","[['over all performance', 'of', 'AEN - Glo Ve'], ['AEN - Glo Ve', 'performs', 'better'], ['better', 'on', 'Restaurant dataset'], ['over all performance', 'of', 'AEN - GloVe and AEN - Glo Ve - BiLSTM'], ['AEN - GloVe and AEN - Glo Ve - BiLSTM', 'is', 'relatively close']]",[],[],"[['Ablation analysis', 'has', 'over all performance']]",[],[],[],[],[],sentiment_analysis,28,171
ablation-analysis,"More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .","[('easier to', (12, 14))]","[('AEN - Glo Ve', (3, 7)), ('fewer parameters', (8, 10)), ('parallelize', (14, 15))]","[['AEN - Glo Ve', 'easier to', 'parallelize']]","[['AEN - Glo Ve', 'has', 'fewer parameters']]",[],"[['Ablation analysis', 'has', 'AEN - Glo Ve']]",[],[],[],[],[],sentiment_analysis,28,172
ablation-analysis,"AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .","[('ranks', (7, 8))]","[(""AEN - Glo Ve 's lightweight level"", (0, 7)), ('second', (8, 9))]","[[""AEN - Glo Ve 's lightweight level"", 'ranks', 'second']]",[],[],"[['Ablation analysis', 'has', ""AEN - Glo Ve 's lightweight level""]]",[],[],[],[],[],sentiment_analysis,28,179
ablation-analysis,"As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .",[],[],"[['model size', 'of', 'AEN - Glo Ve - BiLSTM'], ['AEN - Glo Ve - BiLSTM', 'more than', 'twice'], ['twice', 'of', 'AEN - GloVe'], ['AEN - Glo Ve - BiLSTM', 'does not bring', 'any performance improvements']]",[],[],"[['Ablation analysis', 'has', 'model size']]",[],[],[],[],[],sentiment_analysis,28,180
research-problem,Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks,[],"[('Aspect Level Sentiment Classification', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,29,2
research-problem,Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,[],"[('Aspect - level sentiment classification', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level sentiment classification']]",[],[],[],[],sentiment_analysis,29,4
approach,"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .","[('based on', (15, 17))]","[('long short - term memory ( LSTM ) neural networks', (17, 27))]",[],[],"[['Approach', 'based on', 'long short - term memory ( LSTM ) neural networks']]",[],[],[],[],[],[],sentiment_analysis,29,24
approach,"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .","[('models', (15, 16)), ('using', (20, 21))]","[('aspects and texts simultaneously', (16, 20)), ('LSTMs', (21, 22))]","[['aspects and texts simultaneously', 'using', 'LSTMs']]",[],"[['Approach', 'models', 'aspects and texts simultaneously']]",[],[],[],[],[],[],sentiment_analysis,29,25
approach,"Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .","[('generated from', (8, 10)), ('interact with', (11, 13)), ('by', (15, 16))]","[('target representation and text representation', (3, 8)), ('LSTMs', (10, 11)), ('each other', (13, 15)), ('attention - over - attention ( AOA ) module', (17, 26))]","[['target representation and text representation', 'generated from', 'LSTMs'], ['target representation and text representation', 'interact with', 'each other'], ['each other', 'by', 'attention - over - attention ( AOA ) module']]",[],[],"[['Approach', 'has', 'target representation and text representation']]",[],[],[],[],[],sentiment_analysis,29,26
approach,AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .,"[('automatically generates', (1, 3)), ('not only from', (5, 8)), ('but also', (13, 15))]","[('AOA', (0, 1)), ('mutual attentions', (3, 5)), ('aspect - to - text', (8, 13)), ('text - to - aspect', (15, 20))]","[['AOA', 'automatically generates', 'mutual attentions'], ['mutual attentions', 'but also', 'text - to - aspect'], ['mutual attentions', 'not only from', 'aspect - to - text']]",[],[],"[['Approach', 'has', 'AOA']]",[],[],[],[],[],sentiment_analysis,29,27
approach,That is why we choose AOA to attend to the most important parts in both aspect and sentence .,"[('choose', (4, 5)), ('to attend', (6, 8)), ('in both', (13, 15))]","[('AOA', (5, 6)), ('most important parts', (10, 13)), ('aspect and sentence', (15, 18))]","[['AOA', 'to attend', 'most important parts'], ['most important parts', 'in both', 'aspect and sentence']]",[],"[['Approach', 'choose', 'AOA']]",[],[],[],[],[],[],sentiment_analysis,29,33
hyperparameters,"In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .","[('randomly select', (5, 7)), ('of', (9, 10)), ('as', (12, 13)), ('to tune', (15, 17))]","[('20 %', (7, 9)), ('training data', (10, 12)), ('validation set', (13, 15)), ('hyperparameters', (18, 19))]","[['20 %', 'of', 'training data'], ['training data', 'as', 'validation set'], ['training data', 'to tune', 'hyperparameters']]",[],"[['Hyperparameters', 'randomly select', '20 %']]",[],[],[],[],[],[],sentiment_analysis,29,120
hyperparameters,"All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .","[('randomly initialized from', (4, 7)), ('set to', (22, 24))]","[('weight matrices', (1, 3)), ('uniform distribution U ( ?10 ?4 , 10 ?4 )', (7, 17)), ('bias terms', (19, 21)), ('zero', (24, 25))]","[['weight matrices', 'randomly initialized from', 'uniform distribution U ( ?10 ?4 , 10 ?4 )'], ['bias terms', 'set to', 'zero']]",[],[],"[['Hyperparameters', 'has', 'weight matrices'], ['Hyperparameters', 'has', 'bias terms']]",[],[],[],[],[],sentiment_analysis,29,121
hyperparameters,The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,[],[],"[['L 2 regularization coefficient', 'set to', '10 ? 4'], ['dropout keep rate', 'set to', '0.2']]",[],[],"[['Hyperparameters', 'has', 'L 2 regularization coefficient'], ['Hyperparameters', 'has', 'dropout keep rate']]",[],[],[],[],[],sentiment_analysis,29,122
hyperparameters,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"[('initialized with', (4, 6)), ('fixed during', (13, 15))]","[('word embeddings', (1, 3)), ('300 - dimensional Glove vectors', (6, 11)), ('training', (15, 16))]","[['word embeddings', 'fixed during', 'training'], ['word embeddings', 'initialized with', '300 - dimensional Glove vectors']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],sentiment_analysis,29,123
hyperparameters,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","[('For', (0, 1)), ('initialize them', (7, 9)), ('from', (10, 11))]","[('out of vocabulary words', (2, 6)), ('randomly', (9, 10)), ('uniform distribution U ( ? 0.01 , 0.01 )', (11, 20))]","[['out of vocabulary words', 'initialize them', 'randomly'], ['randomly', 'from', 'uniform distribution U ( ? 0.01 , 0.01 )']]",[],"[['Hyperparameters', 'For', 'out of vocabulary words']]",[],[],[],[],[],[],sentiment_analysis,29,124
hyperparameters,The dimension of LSTM hidden states is set to 150 .,"[('of', (2, 3)), ('set to', (7, 9))]","[('dimension', (1, 2)), ('LSTM hidden states', (3, 6)), ('150', (9, 10))]","[['dimension', 'of', 'LSTM hidden states'], ['LSTM hidden states', 'set to', '150']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,29,125
hyperparameters,The initial learning rate is 0.01 for the Adam optimizer .,"[('is', (4, 5)), ('for', (6, 7))]","[('initial learning rate', (1, 4)), ('0.01', (5, 6)), ('Adam optimizer', (8, 10))]","[['initial learning rate', 'is', '0.01'], ['0.01', 'for', 'Adam optimizer']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],[],[],[],[],sentiment_analysis,29,126
hyperparameters,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","[('does not', (4, 6)), ('after', (7, 8)), ('decrease', (13, 14)), ('by', (17, 18))]","[('training loss', (2, 4)), ('drop', (6, 7)), ('every three epochs', (8, 11)), ('learning rate', (15, 17)), ('half', (18, 19))]","[['training loss', 'does not', 'drop'], ['drop', 'after', 'every three epochs'], ['drop', 'decrease', 'learning rate'], ['learning rate', 'by', 'half']]",[],[],"[['Hyperparameters', 'has', 'training loss']]",[],[],[],[],[],sentiment_analysis,29,127
hyperparameters,The batch size is set as 25 .,"[('set as', (4, 6))]","[('batch size', (1, 3)), ('25', (6, 7))]","[['batch size', 'set as', '25']]",[],[],"[['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],sentiment_analysis,29,128
baselines,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .",[],[],"[['Majority', 'is', 'basic baseline method'], ['Majority', 'assigns', 'largest sentiment polarity'], ['largest sentiment polarity', 'in', 'training set'], ['training set', 'to', 'each sample'], ['each sample', 'in', 'test set']]",[],[],"[['Baselines', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,29,134
baselines,"LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .","[('uses', (1, 2)), ('to model', (5, 7)), ('used as', (16, 18)), ('for', (21, 22))]","[('LSTM', (0, 1)), ('one LSTM network', (2, 5)), ('sentence', (8, 9)), ('last hidden state', (12, 15)), ('sentence representation', (19, 21)), ('final classification', (23, 25))]","[['LSTM', 'uses', 'one LSTM network'], ['one LSTM network', 'to model', 'sentence'], ['last hidden state', 'used as', 'sentence representation'], ['sentence representation', 'for', 'final classification']]","[['LSTM', 'has', 'last hidden state']]",[],"[['Baselines', 'has', 'LSTM']]",[],[],[],[],[],sentiment_analysis,29,135
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"[('uses', (3, 4)), ('to model', (7, 9)), ('surrounding', (14, 15))]","[('TD - LSTM', (0, 3)), ('two LSTM networks', (4, 7)), ('preceding and following contexts', (10, 14)), ('aspect term', (16, 18))]","[['TD - LSTM', 'uses', 'two LSTM networks'], ['two LSTM networks', 'to model', 'preceding and following contexts'], ['preceding and following contexts', 'surrounding', 'aspect term']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,29,136
baselines,AT - LSTM first models the sentence via a LSTM model .,"[('first models', (3, 5)), ('via', (7, 8))]","[('AT - LSTM', (0, 3)), ('sentence', (6, 7)), ('LSTM model', (9, 11))]","[['AT - LSTM', 'first models', 'sentence'], ['sentence', 'via', 'LSTM model']]",[],[],"[['Baselines', 'has', 'AT - LSTM']]",[],[],[],[],[],sentiment_analysis,29,138
baselines,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"[('further extends', (3, 5)), ('by appending', (8, 10)), ('into', (13, 14))]","[('ATAE - LSTM', (0, 3)), ('AT - LSTM', (5, 8)), ('aspect embedding', (11, 13)), ('each word vector', (14, 17))]","[['ATAE - LSTM', 'further extends', 'AT - LSTM'], ['AT - LSTM', 'by appending', 'aspect embedding'], ['aspect embedding', 'into', 'each word vector']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,29,141
baselines,IAN uses two LSTM networks to model the sentence and aspect term respectively .,"[('uses', (1, 2)), ('to model', (5, 7))]","[('IAN', (0, 1)), ('two LSTM networks', (2, 5)), ('sentence and aspect term', (8, 12))]","[['IAN', 'uses', 'two LSTM networks'], ['two LSTM networks', 'to model', 'sentence and aspect term']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,29,142
results,"In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .","[('found that', (5, 7)), ('with', (10, 11))]","[('performance fluctuates', (8, 10)), ('different random initialization', (11, 14))]","[['performance fluctuates', 'with', 'different random initialization']]",[],"[['Results', 'found that', 'performance fluctuates']]",[],[],[],[],[],[],sentiment_analysis,29,150
results,"On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .","[('On average', (0, 2)), ('better than', (6, 8)), ('outperforms them in', (16, 19))]","[('our algorithm', (3, 5)), ('baseline methods', (9, 11)), ('our best trained model', (12, 16)), ('large margin', (20, 22))]","[['our best trained model', 'outperforms them in', 'large margin'], ['our algorithm', 'better than', 'baseline methods']]",[],"[['Results', 'On average', 'our best trained model'], ['Results', 'On average', 'our algorithm']]",[],[],[],[],[],[],sentiment_analysis,29,153
research-problem,Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations,[],"[('Emotion Detection in Textual Conversations', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion Detection in Textual Conversations']]",[],[],[],[],sentiment_analysis,3,2
research-problem,The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .,[],"[('detecting emotions in textual conversations', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'detecting emotions in textual conversations']]",[],[],[],[],sentiment_analysis,3,5
research-problem,"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .",[],"[('detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations', (6, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations']]",[],[],[],[],sentiment_analysis,3,14
model,"To this end , we propose a Knowledge - Enriched Transformer ( KET ) to effectively incorporate contextual information and external knowledge bases to address the aforementioned challenges .","[('propose', (5, 6)), ('incorporate', (16, 17))]","[('Knowledge - Enriched Transformer ( KET )', (7, 14)), ('contextual information and external knowledge bases', (17, 23))]","[['Knowledge - Enriched Transformer ( KET )', 'incorporate', 'contextual information and external knowledge bases']]",[],"[['Model', 'propose', 'Knowledge - Enriched Transformer ( KET )']]",[],[],[],[],[],[],sentiment_analysis,3,29
,"The self - attention and cross-attention modules in the Transformer capture the intra-sentence and inter-sentence correlations , respectively .",[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,3,31
,The shorter path of information flow in these two modules compared to gated RNNs and CNNs allows KET to model contextual information more efficiently .,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,3,32
model,"In addition , we propose a hierarchical self - attention mechanism allowing KET to model the hierarchical structure of conversations .","[('allowing', (11, 12)), ('to model', (13, 15))]","[('hierarchical self - attention mechanism', (6, 11)), ('KET', (12, 13)), ('hierarchical structure of conversations', (16, 20))]","[['hierarchical self - attention mechanism', 'allowing', 'KET'], ['KET', 'to model', 'hierarchical structure of conversations']]",[],[],"[['Model', 'propose', 'hierarchical self - attention mechanism']]",[],[],[],[],[],sentiment_analysis,3,33
model,"Our model separates context and response into the encoder and decoder , respectively , which is different from other Transformer - based models , e.g. , BERT , which directly concatenate context and response , and then train language models using only the encoder part .","[('separates', (2, 3)), ('into', (6, 7))]","[('context and response', (3, 6)), ('encoder and decoder', (8, 11))]","[['context and response', 'into', 'encoder and decoder']]",[],"[['Model', 'separates', 'context and response']]",[],[],[],[],[],[],sentiment_analysis,3,34
model,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to related knowledge entities .","[('exploit', (3, 4)), ('leverage', (8, 9)), ('to facilitate', (12, 14)), ('in', (19, 20)), ('by referring to', (22, 25))]","[('commonsense knowledge', (4, 6)), ('external knowledge bases', (9, 12)), ('understanding of each word', (15, 19)), ('utterances', (21, 22)), ('related knowledge entities', (25, 28))]","[['commonsense knowledge', 'leverage', 'external knowledge bases'], ['external knowledge bases', 'to facilitate', 'understanding of each word'], ['understanding of each word', 'in', 'utterances'], ['understanding of each word', 'by referring to', 'related knowledge entities']]",[],"[['Model', 'exploit', 'commonsense knowledge']]",[],[],[],[],[],[],sentiment_analysis,3,35
model,The referring process is dynamic and balances between relatedness and affectiveness of the retrieved knowledge entities using a context - aware affective graph attention mechanism .,"[('is', (3, 4)), ('balances between', (6, 8)), ('of', (11, 12)), ('using', (16, 17))]","[('referring process', (1, 3)), ('dynamic', (4, 5)), ('relatedness and affectiveness', (8, 11)), ('retrieved knowledge entities', (13, 16)), ('context - aware affective graph attention mechanism', (18, 25))]","[['referring process', 'balances between', 'relatedness and affectiveness'], ['relatedness and affectiveness', 'of', 'retrieved knowledge entities'], ['retrieved knowledge entities', 'using', 'context - aware affective graph attention mechanism'], ['referring process', 'is', 'dynamic']]",[],[],"[['Model', 'has', 'referring process']]",[],[],[],[],[],sentiment_analysis,3,36
,c LSTM : A contextual LSTM model .,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,3,199
baselines,An utterance - level bidirectional LSTM is used to encode each utterance .,"[('to encode', (8, 10))]","[('utterance - level bidirectional LSTM', (1, 6)), ('each utterance', (10, 12))]","[['utterance - level bidirectional LSTM', 'to encode', 'each utterance']]",[],[],[],[],[],[],"[['c LSTM', 'has', 'utterance - level bidirectional LSTM']]",[],sentiment_analysis,3,200
baselines,A context - level unidirectional LSTM is used to encode the context .,"[('to encode', (8, 10))]","[('context - level unidirectional LSTM', (1, 6)), ('context', (11, 12))]","[['context - level unidirectional LSTM', 'to encode', 'context']]",[],[],[],[],[],[],"[['c LSTM', 'has', 'context - level unidirectional LSTM']]",[],sentiment_analysis,3,201
baselines,CNN+cLSTM : An CNN is used to extract utterance features .,"[('to extract', (6, 8))]","[('CNN+cLSTM', (0, 1)), ('CNN', (3, 4)), ('utterance features', (8, 10))]","[['CNN', 'to extract', 'utterance features']]","[['CNN+cLSTM', 'has', 'CNN']]",[],"[['Baselines', 'has', 'CNN+cLSTM']]",[],[],[],[],"[['CNN+cLSTM', 'has', 'c LSTM']]",sentiment_analysis,3,204
baselines,An c LSTM is then applied to learn context representations .,"[('applied to learn', (5, 8))]","[('c LSTM', (1, 3)), ('context representations', (8, 10))]","[['c LSTM', 'applied to learn', 'context representations']]",[],[],[],[],[],[],[],[],sentiment_analysis,3,205
baselines,BERT BASE :,[],"[('BERT BASE', (0, 2))]",[],[],[],"[['Baselines', 'has', 'BERT BASE']]",[],[],[],[],[],sentiment_analysis,3,206
baselines,We treat each utterance with its context as a single document .,"[('treat', (1, 2)), ('with', (4, 5)), ('as', (7, 8))]","[('each utterance', (2, 4)), ('context', (6, 7)), ('single document', (9, 11))]","[['each utterance', 'with', 'context'], ['context', 'as', 'single document']]",[],[],[],[],"[['BERT BASE', 'treat', 'each utterance']]",[],[],[],sentiment_analysis,3,208
baselines,We limit the document length to the last 100 tokens to allow larger batch size .,"[('limit', (1, 2)), ('to', (5, 6)), ('to allow', (10, 12))]","[('document length', (3, 5)), ('last 100 tokens', (7, 10)), ('larger batch size', (12, 15))]","[['document length', 'to', 'last 100 tokens'], ['last 100 tokens', 'to allow', 'larger batch size']]",[],[],[],[],"[['BERT BASE', 'limit', 'document length']]",[],[],[],sentiment_analysis,3,209
baselines,DialogueRNN : The stateof - the - art model for emotion detection in textual conversations .,"[('for', (9, 10)), ('in', (12, 13))]","[('DialogueRNN', (0, 1)), ('stateof - the - art model', (3, 9)), ('emotion detection', (10, 12)), ('textual conversations', (13, 15))]","[['stateof - the - art model', 'for', 'emotion detection'], ['emotion detection', 'in', 'textual conversations']]","[['DialogueRNN', 'has', 'stateof - the - art model']]",[],"[['Baselines', 'has', 'DialogueRNN']]",[],[],[],[],[],sentiment_analysis,3,211
baselines,It models both context and speakers information .,"[('models both', (1, 3))]","[('context and speakers information', (3, 7))]",[],[],[],[],[],"[['DialogueRNN', 'models both', 'context and speakers information']]",[],[],[],sentiment_analysis,3,212
baselines,KET SingleSelfAttn :,[],"[('KET SingleSelfAttn', (0, 2))]",[],[],[],"[['Baselines', 'has', 'KET SingleSelfAttn']]",[],[],[],[],[],sentiment_analysis,3,216
baselines,We replace the hierarchical self - attention by a single self - attention layer to learn context representations .,"[('replace', (1, 2)), ('by', (7, 8)), ('to learn', (14, 16))]","[('hierarchical self - attention', (3, 7)), ('single self - attention layer', (9, 14)), ('context representations', (16, 18))]","[['hierarchical self - attention', 'by', 'single self - attention layer'], ['single self - attention layer', 'to learn', 'context representations']]",[],[],[],[],"[['KET SingleSelfAttn', 'replace', 'hierarchical self - attention']]",[],[],[],sentiment_analysis,3,217
baselines,Contextual utterances are concatenated together prior to the single self - attention layer .,"[('are', (2, 3)), ('prior to', (5, 7))]","[('Contextual utterances', (0, 2)), ('concatenated together', (3, 5)), ('single self - attention layer', (8, 13))]","[['Contextual utterances', 'are', 'concatenated together'], ['concatenated together', 'prior to', 'single self - attention layer']]",[],[],[],[],[],[],"[['KET SingleSelfAttn', 'has', 'Contextual utterances']]",[],sentiment_analysis,3,218
baselines,KET StdAttn :,[],"[('KET StdAttn', (0, 2))]",[],[],[],"[['Baselines', 'has', 'KET StdAttn']]",[],[],[],[],[],sentiment_analysis,3,219
baselines,We replace the dynamic contextaware affective graph attention by the standard graph attention .,"[('replace', (1, 2)), ('by', (8, 9))]","[('dynamic contextaware affective graph attention', (3, 8)), ('standard graph attention', (10, 13))]","[['dynamic contextaware affective graph attention', 'by', 'standard graph attention']]",[],[],[],[],"[['KET StdAttn', 'replace', 'dynamic contextaware affective graph attention']]",[],[],[],sentiment_analysis,3,220
experimental-setup,We preprocessed all datasets by lower - casing and tokenization using Spacy 2 .,"[('preprocessed', (1, 2)), ('by', (4, 5)), ('using', (10, 11))]","[('all datasets', (2, 4)), ('lower - casing', (5, 8)), ('tokenization', (9, 10)), ('Spacy', (11, 12))]","[['all datasets', 'by', 'lower - casing'], ['all datasets', 'by', 'tokenization'], ['tokenization', 'using', 'Spacy']]",[],"[['Experimental setup', 'preprocessed', 'all datasets']]",[],[],[],[],[],[],sentiment_analysis,3,222
experimental-setup,We use the released code for BERT BASE and DialogueRNN .,"[('use', (1, 2)), ('for', (5, 6))]","[('released code', (3, 5)), ('BERT BASE and DialogueRNN', (6, 10))]","[['released code', 'for', 'BERT BASE and DialogueRNN']]",[],"[['Experimental setup', 'use', 'released code']]",[],[],[],[],[],[],sentiment_analysis,3,224
experimental-setup,"For each dataset , all models are fine - tuned based on their performance on the validation set .","[('For', (0, 1)), ('are', (6, 7)), ('based on', (10, 12)), ('on', (14, 15))]","[('each dataset', (1, 3)), ('all models', (4, 6)), ('fine - tuned', (7, 10)), ('performance', (13, 14)), ('validation set', (16, 18))]","[['all models', 'are', 'fine - tuned'], ['fine - tuned', 'based on', 'performance'], ['performance', 'on', 'validation set']]","[['each dataset', 'has', 'all models']]","[['Experimental setup', 'For', 'each dataset']]",[],[],[],[],[],[],sentiment_analysis,3,225
experimental-setup,"For our model in all datasets , we use Adam optimization ( Kingma and Ba , 2014 ) with a batch size of 64 and learning rate of 0.0001 throughout the training process .",[],[],"[['our model', 'in', 'all datasets'], ['all datasets', 'use', 'Adam optimization ( Kingma and Ba , 2014 )'], ['Adam optimization ( Kingma and Ba , 2014 )', 'with', 'learning rate'], ['learning rate', 'of', '0.0001'], ['Adam optimization ( Kingma and Ba , 2014 )', 'with', 'batch size'], ['batch size', 'of', '64']]",[],[],"[['Experimental setup', 'For', 'our model']]",[],[],[],[],[],sentiment_analysis,3,226
experimental-setup,We use Glo Ve embedding for initialization in the word and concept embedding layers,"[('for', (5, 6)), ('in', (7, 8))]","[('Glo Ve embedding', (2, 5)), ('initialization', (6, 7)), ('word and concept embedding layers', (9, 14))]","[['Glo Ve embedding', 'for', 'initialization'], ['initialization', 'in', 'word and concept embedding layers']]",[],[],"[['Experimental setup', 'use', 'Glo Ve embedding']]",[],[],[],[],[],sentiment_analysis,3,227
experimental-setup,"For the class weights in cross - entropy loss for each dataset , we set them as the ratio of the class distribution in the validation set to the class distribution in the training set .",[],[],"[['class weights', 'set them as', 'ratio'], ['ratio', 'of', 'class distribution'], ['class distribution', 'in', 'validation set'], ['class distribution', 'to', 'class distribution'], ['class distribution', 'in', 'training set'], ['class weights', 'in', 'cross - entropy loss'], ['cross - entropy loss', 'for', 'each dataset']]",[],[],"[['Experimental setup', 'For', 'class weights']]",[],[],[],[],[],sentiment_analysis,3,228
results,"c LSTM performs reasonably well on short conversations ( i.e. , EC and DailyDialog ) , but the worst on long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ) .",[],[],"[['c LSTM', 'performs', 'worst'], ['worst', 'on', 'long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP )'], ['c LSTM', 'performs', 'reasonably well'], ['reasonably well', 'on', 'short conversations ( i.e. , EC and DailyDialog )']]",[],[],"[['Results', 'has', 'c LSTM']]",[],[],[],[],[],sentiment_analysis,3,237
results,"In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .",[],[],"[['utterance - level LSTM', 'in', 'c LSTM'], ['c LSTM', 'replaced by', 'features'], ['features', 'extracted by', 'CNN'], ['model', 'performs', 'significantly better'], ['significantly better', 'than', 'c LSTM'], ['c LSTM', 'on', 'long conversations']]","[['features', 'has', 'model']]","[['Results', 'when', 'utterance - level LSTM']]",[],[],[],[],[],[],sentiment_analysis,3,239
results,BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .,"[('achieves', (2, 3)), ('on', (6, 7)), ('except', (9, 10)), ('due to', (11, 13)), ('via', (17, 18)), ('using', (21, 22))]","[('BERT BASE', (0, 2)), ('very competitive performance', (3, 6)), ('all datasets', (7, 9)), ('EC', (10, 11)), ('strong representational power', (14, 17)), ('bi-directional context modelling', (18, 21)), ('Transformer', (23, 24))]","[['BERT BASE', 'achieves', 'very competitive performance'], ['very competitive performance', 'on', 'all datasets'], ['all datasets', 'except', 'EC'], ['EC', 'due to', 'strong representational power'], ['strong representational power', 'via', 'bi-directional context modelling'], ['bi-directional context modelling', 'using', 'Transformer'], ['all datasets', 'except', 'EC']]",[],[],"[['Results', 'has', 'BERT BASE']]",[],[],[],[],[],sentiment_analysis,3,240
results,"In particular , DialogueRNN performs better than our model on IEMOCAP , which maybe attributed to its detailed speaker information for modelling the emotion dynamics in each speaker as the conversation flows .","[('performs better than', (4, 7)), ('on', (9, 10))]","[('DialogueRNN', (3, 4)), ('our model', (7, 9)), ('IEMOCAP', (10, 11))]","[['DialogueRNN', 'performs better than', 'our model'], ['our model', 'on', 'IEMOCAP']]",[],[],"[['Results', 'has', 'DialogueRNN']]",[],[],[],[],[],sentiment_analysis,3,243
results,"This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .","[('indicates that', (2, 4)), ('is', (6, 7)), ('across', (8, 9)), ('with', (10, 11))]","[('our model', (4, 6)), ('robust', (7, 8)), ('datasets', (9, 10)), ('varying training sizes , context lengths and domains', (11, 19))]","[['our model', 'is', 'robust'], ['robust', 'across', 'datasets'], ['datasets', 'with', 'varying training sizes , context lengths and domains']]",[],"[['Results', 'indicates that', 'our model']]",[],[],[],[],[],[],sentiment_analysis,3,245
results,Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .,"[('perform', (8, 9)), ('with', (10, 11)), ('on', (14, 15)), ('except', (17, 18))]","[('KET variants', (1, 3)), ('KET SingleSelfAttn and KET StdAttn', (3, 8)), ('comparably', (9, 10)), ('best baselines', (12, 14)), ('all datasets', (15, 17)), ('IEMOCAP', (18, 19))]","[['KET variants', 'perform', 'comparably'], ['comparably', 'with', 'best baselines'], ['best baselines', 'on', 'all datasets'], ['all datasets', 'except', 'IEMOCAP']]","[['KET variants', 'name', 'KET SingleSelfAttn and KET StdAttn']]",[],"[['Results', 'has', 'KET variants']]",[],[],[],[],[],sentiment_analysis,3,246
results,"However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .",[],[],"[['noticeably worse', 'than', 'KET'], ['KET', 'on', 'all datasets']]",[],[],[],[],[],[],"[['KET variants', 'perform', 'noticeably worse']]",[],sentiment_analysis,3,247
results,One observation worth mentioning is that these two variants perform on a par with the KET model on EC .,"[('with', (13, 14)), ('on', (17, 18))]","[('on a par', (10, 13)), ('KET model', (15, 17)), ('EC', (18, 19))]","[['on a par', 'with', 'KET model'], ['KET model', 'on', 'EC']]",[],[],[],[],[],[],"[['KET variants', 'perform', 'on a par']]",[],sentiment_analysis,3,248
ablation-analysis,It is clear that both context and knowledge are essential to the strong performance of KET on all datasets .,"[('essential to', (9, 11)), ('of', (14, 15)), ('on', (16, 17))]","[('both context and knowledge', (4, 8)), ('strong performance', (12, 14)), ('KET', (15, 16)), ('all datasets', (17, 19))]","[['both context and knowledge', 'essential to', 'strong performance'], ['strong performance', 'of', 'KET'], ['KET', 'on', 'all datasets']]",[],[],"[['Ablation analysis', 'has', 'both context and knowledge']]",[],[],[],[],[],sentiment_analysis,3,274
ablation-analysis,"Note that removing context has a greater impact on long conversations than short conversations , which is expected because more contextual information is lost in long conversations .","[('removing', (2, 3)), ('on', (8, 9)), ('than', (11, 12))]","[('context', (3, 4)), ('greater impact', (6, 8)), ('long conversations', (9, 11)), ('short conversations', (12, 14))]","[['greater impact', 'on', 'long conversations'], ['long conversations', 'than', 'short conversations']]","[['context', 'has', 'greater impact']]","[['Ablation analysis', 'removing', 'context']]",[],[],[],[],[],[],sentiment_analysis,3,275
research-problem,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,[],"[('Targeted Aspect - based Sentiment Analysis', (8, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Targeted Aspect - based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,30,2
research-problem,"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .",[],"[('targeted aspect - based sentiment analysis ( TABSA )', (17, 26))]",[],[],[],[],"[['Contribution', 'has research problem', 'targeted aspect - based sentiment analysis ( TABSA )']]",[],[],[],[],sentiment_analysis,30,4
model,"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .","[('propose', (5, 6)), ('for', (10, 11)), ('augmented with', (13, 15)), ('equipped with', (22, 24)), ('to keep track of', (30, 34))]","[('novel model architecture', (7, 10)), ('TABSA', (11, 12)), ('multiple "" memory chains ""', (15, 20)), ('delayed memory update mechanism', (25, 29)), ('numerous entities independently', (34, 37))]","[['novel model architecture', 'augmented with', 'multiple "" memory chains ""'], ['novel model architecture', 'for', 'TABSA'], ['delayed memory update mechanism', 'to keep track of', 'numerous entities independently']]",[],"[['Model', 'propose', 'novel model architecture'], ['Model', 'equipped with', 'delayed memory update mechanism']]",[],[],[],[],[],[],sentiment_analysis,30,26
hyperparameters,"We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .",[],[],"[['corpus', 'with', 'tokenisation'], ['tokenisation', 'using', 'NLTK'], ['tokenisation', 'using', 'case folding'], ['our model', 'with', 'GloVe'], ['GloVe', 'not updated during', 'training'], ['GloVe', 'trained on', '42B tokens , 1.9 M vocab']]","[['GloVe', 'has', '300 - D']]","[['Hyperparameters', 'pre-process', 'corpus'], ['Hyperparameters', 'initialise', 'our model']]",[],[],[],[],[],[],sentiment_analysis,30,90
hyperparameters,Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,[],[],"[['Training', 'with', 'FTRL optimiser'], ['Training', 'carried out over', '800 epochs'], ['learning rate', 'of', '0.05'], ['batch size', 'of', '128']]",[],[],"[['Hyperparameters', 'has', 'Training'], ['Hyperparameters', 'has', 'learning rate'], ['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],sentiment_analysis,30,91
hyperparameters,"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .","[('use', (1, 2)), ('for', (7, 8)), ('in', (10, 11)), ('of', (28, 29)), ('is', (46, 47))]","[('following hyper - parameters', (3, 7)), ('weight matrices', (8, 10)), ('both directions', (11, 13)), ('300', (36, 37)), ('hidden size', (39, 41)), ('GRU', (43, 44))]","[['following hyper - parameters', 'for', 'weight matrices'], ['weight matrices', 'in', 'both directions'], ['following hyper - parameters', 'for', 'hidden size'], ['hidden size', 'of', 'GRU'], ['GRU', 'is', '300']]",[],"[['Hyperparameters', 'use', 'following hyper - parameters']]",[],[],[],[],[],[],sentiment_analysis,30,92
hyperparameters,Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,"[('applied to', (2, 4)), ('of', (6, 7)), ('in', (8, 9)), ('with', (15, 16))]","[('Dropout', (0, 1)), ('output', (5, 6)), ('final classifier', (10, 12)), ('rate', (17, 18)), ('0.2', (19, 20))]","[['Dropout', 'with', 'rate'], ['rate', 'of', '0.2'], ['Dropout', 'applied to', 'output'], ['output', 'in', 'final classifier']]",[],[],"[['Hyperparameters', 'has', 'Dropout']]",[],[],[],[],[],sentiment_analysis,30,93
hyperparameters,"Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?","[('to curb', (2, 4)), ('regularise', (7, 8)), ('with', (14, 15)), ('on', (19, 20))]","[('overfitting', (4, 5)), ('last layer', (9, 11)), ('L 2 penalty', (16, 19)), ('weights', (21, 22))]","[['overfitting', 'regularise', 'last layer'], ['last layer', 'with', 'L 2 penalty'], ['L 2 penalty', 'on', 'weights']]",[],"[['Hyperparameters', 'to curb', 'overfitting']]",[],[],[],[],[],[],sentiment_analysis,30,95
hyperparameters,"We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .","[('empirically set', (1, 3)), ('to', (8, 9)), ('with', (11, 12)), ('set to', (18, 20)), ('as', (23, 24))]","[('number of memory chains', (4, 8)), ('6', (9, 10)), ('keys of two of them', (13, 18)), ('same embeddings', (21, 23)), ('target words LOC1 and LOC2', (25, 30))]","[['number of memory chains', 'with', 'keys of two of them'], ['keys of two of them', 'set to', 'same embeddings'], ['same embeddings', 'as', 'target words LOC1 and LOC2'], ['number of memory chains', 'to', '6']]",[],"[['Hyperparameters', 'empirically set', 'number of memory chains']]",[],[],[],[],[],[],sentiment_analysis,30,97
results,Our model achieves state - of - the - art results for both aspect detection and sentiment classification .,"[('achieves', (2, 3)), ('for both', (11, 13))]","[('Our model', (0, 2)), ('state - of - the - art results', (3, 11)), ('aspect detection', (13, 15)), ('sentiment classification', (16, 18))]","[['Our model', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'for both', 'aspect detection'], ['state - of - the - art results', 'for both', 'sentiment classification']]",[],[],"[['Results', 'has', 'Our model']]",[],[],[],[],[],sentiment_analysis,30,114
results,"It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .","[('equipped only with', (8, 11)), ('outperforms', (18, 19)), ('heavily reliant on', (24, 27))]","[('proposed model', (5, 7)), ('domainindependent general - purpose GloVe embeddings', (11, 17)), ('Sentic LSTM', (19, 21)), ('external knowledge bases and domainspecific embeddings', (27, 33))]","[['proposed model', 'equipped only with', 'domainindependent general - purpose GloVe embeddings'], ['proposed model', 'outperforms', 'Sentic LSTM'], ['Sentic LSTM', 'heavily reliant on', 'external knowledge bases and domainspecific embeddings']]",[],[],"[['Results', 'has', 'proposed model']]",[],[],[],[],[],sentiment_analysis,30,115
results,Ent Net vs. our model .,[],"[('Ent Net vs. our model', (0, 5))]",[],[],[],"[['Results', 'has', 'Ent Net vs. our model']]",[],[],[],[],[],sentiment_analysis,30,116
results,"We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .","[('see', (1, 2)), ('for', (5, 6)), ('in both', (8, 10)), ('compared to', (16, 18))]","[('consistent performance gains', (2, 5)), ('our model', (6, 8)), ('aspect detection and sentiment classification', (10, 15)), ('EntNet', (18, 19))]","[['consistent performance gains', 'for', 'our model'], ['our model', 'in both', 'aspect detection and sentiment classification'], ['our model', 'compared to', 'EntNet']]",[],[],[],[],"[['Ent Net vs. our model', 'see', 'consistent performance gains']]",[],[],[],sentiment_analysis,30,117
research-problem,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,[],"[('Aspect Level Sentiment Classification', (5, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,31,2
research-problem,Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .,[],"[('sentiment classification', (14, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment classification']]",[],[],[],[],sentiment_analysis,31,8
research-problem,"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .",[],"[('general sentiment classification', (2, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'general sentiment classification']]",[],[],[],[],sentiment_analysis,31,12
model,"In the present work , we propose two simple yet effective convolutional neural networks with aspect information incorporated .","[('propose', (6, 7)), ('with', (14, 15))]","[('two simple yet effective convolutional neural networks', (7, 14)), ('aspect information', (15, 17))]","[['two simple yet effective convolutional neural networks', 'with', 'aspect information']]",[],"[['Model', 'propose', 'two simple yet effective convolutional neural networks']]",[],[],[],[],[],[],sentiment_analysis,31,20
model,"Specifically , we design two novel neural units that take target aspects into account .","[('design', (3, 4)), ('take', (9, 10)), ('into', (12, 13))]","[('two novel neural units', (4, 8)), ('target aspects', (10, 12)), ('account', (13, 14))]","[['two novel neural units', 'take', 'target aspects'], ['target aspects', 'into', 'account']]",[],"[['Model', 'design', 'two novel neural units']]",[],[],[],[],[],[],sentiment_analysis,31,22
model,"One is parameterized filter , the other is parameterized gate .","[('One is', (0, 2)), ('other is', (6, 8))]","[('parameterized filter', (2, 4)), ('parameterized gate', (8, 10))]",[],[],[],[],[],"[['two novel neural units', 'other is', 'parameterized gate'], ['two novel neural units', 'One is', 'parameterized filter']]",[],[],[],sentiment_analysis,31,23
model,These units both are generated from aspect - specific features and are further applied on the sentence .,"[('generated from', (4, 6))]","[('units', (1, 2)), ('aspect - specific features', (6, 10))]","[['units', 'generated from', 'aspect - specific features']]",[],[],"[['Model', 'has', 'units']]",[],[],[],[],[],sentiment_analysis,31,24
hyperparameters,"We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .",[],[],"[['minibatch size', 'of', '25'], ['rectifier', 'as', 'non-linear function f'], ['non-linear function f', 'in', 'CNN g , CNN t'], ['rectifier', 'as', 'sigmoid'], ['sigmoid', 'in', 'CNN s'], ['l 2 regularization term', 'of', '0.001'], ['filter window sizes', 'of', '1 , 2 , 3 , 4 with 100 feature maps each']]",[],"[['Hyperparameters', 'use', 'minibatch size'], ['Hyperparameters', 'use', 'rectifier'], ['Hyperparameters', 'use', 'l 2 regularization term'], ['Hyperparameters', 'use', 'filter window sizes']]",[],[],[],[],[],[],sentiment_analysis,31,112
hyperparameters,Parameterized filters and gates have the same size and number as normal filters .,"[('have', (4, 5)), ('as', (10, 11))]","[('Parameterized filters and gates', (0, 4)), ('same size and number', (6, 10)), ('normal filters', (11, 13))]","[['Parameterized filters and gates', 'have', 'same size and number'], ['same size and number', 'as', 'normal filters']]",[],[],"[['Hyperparameters', 'has', 'Parameterized filters and gates']]",[],[],[],[],[],sentiment_analysis,31,113
hyperparameters,"They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .","[('generated uniformly by', (2, 5)), ('with', (6, 7)), ('of', (9, 10))]","[('CNN', (5, 6)), ('window sizes', (7, 9)), ('1 , 2 , 3 , 4', (10, 17))]","[['CNN', 'with', 'window sizes'], ['window sizes', 'of', '1 , 2 , 3 , 4']]",[],[],[],[],"[['Parameterized filters and gates', 'generated uniformly by', 'CNN']]",[],[],[],sentiment_analysis,31,114
hyperparameters,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"[('initialized with', (4, 6)), ('fixed during', (13, 15))]","[('word embeddings', (1, 3)), ('300 - dimensional Glove vectors', (6, 11)), ('training', (15, 16))]","[['word embeddings', 'initialized with', '300 - dimensional Glove vectors'], ['300 - dimensional Glove vectors', 'fixed during', 'training']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],sentiment_analysis,31,115
hyperparameters,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","[('For', (0, 1)), ('initialize them', (7, 9)), ('from', (10, 11))]","[('out of vocabulary words', (2, 6)), ('randomly', (9, 10)), ('uniform distribution U ( ? 0.01 , 0.01 )', (11, 20))]","[['out of vocabulary words', 'initialize them', 'randomly'], ['out of vocabulary words', 'from', 'uniform distribution U ( ? 0.01 , 0.01 )']]",[],"[['Hyperparameters', 'For', 'out of vocabulary words']]",[],[],[],[],[],[],sentiment_analysis,31,116
hyperparameters,We apply dropout on the final classification features of PG - CNN .,"[('apply', (1, 2)), ('on', (3, 4)), ('of', (8, 9))]","[('dropout', (2, 3)), ('final classification features', (5, 8)), ('PG - CNN', (9, 12))]","[['dropout', 'on', 'final classification features'], ['final classification features', 'of', 'PG - CNN']]",[],"[['Hyperparameters', 'apply', 'dropout']]",[],[],[],[],[],[],sentiment_analysis,31,117
hyperparameters,The dropout rate is chosen as 0.3 .,"[('chosen as', (4, 6))]","[('dropout rate', (1, 3)), ('0.3', (6, 7))]","[['dropout rate', 'chosen as', '0.3']]",[],[],"[['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],sentiment_analysis,31,118
hyperparameters,Training is done through mini-batch stochastic gradient descent with Adam update rule .,"[('done through', (2, 4)), ('with', (8, 9))]","[('Training', (0, 1)), ('mini-batch stochastic gradient descent', (4, 8)), ('Adam update rule', (9, 12))]","[['Training', 'done through', 'mini-batch stochastic gradient descent'], ['mini-batch stochastic gradient descent', 'with', 'Adam update rule']]",[],[],"[['Hyperparameters', 'has', 'Training']]",[],[],[],[],[],sentiment_analysis,31,119
hyperparameters,The initial learning rate is 0.001 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.001', (5, 6))]","[['initial learning rate', 'is', '0.001']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate']]",[],[],[],[],[],sentiment_analysis,31,120
hyperparameters,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","[('does not', (4, 6)), ('after', (7, 8)), ('decrease', (13, 14)), ('by', (17, 18))]","[('training loss', (2, 4)), ('drop', (6, 7)), ('every three epochs', (8, 11)), ('learning rate', (15, 17)), ('half', (18, 19))]","[['training loss', 'does not', 'drop'], ['drop', 'after', 'every three epochs'], ['every three epochs', 'decrease', 'learning rate'], ['learning rate', 'by', 'half']]",[],[],"[['Hyperparameters', 'has', 'training loss']]",[],[],[],[],[],sentiment_analysis,31,121
hyperparameters,We adopt early stopping based on the validation loss on development sets .,"[('adopt', (1, 2)), ('based on', (4, 6)), ('on', (9, 10))]","[('early stopping', (2, 4)), ('validation loss', (7, 9)), ('development sets', (10, 12))]","[['early stopping', 'based on', 'validation loss'], ['validation loss', 'on', 'development sets']]",[],"[['Hyperparameters', 'adopt', 'early stopping']]",[],[],[],[],[],[],sentiment_analysis,31,122
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"[('uses', (3, 4)), ('to model', (7, 9)), ('surrounding', (14, 15))]","[('TD - LSTM', (0, 3)), ('two LSTM networks', (4, 7)), ('preceding and following contexts', (10, 14)), ('aspect term', (16, 18))]","[['TD - LSTM', 'uses', 'two LSTM networks'], ['two LSTM networks', 'to model', 'preceding and following contexts'], ['preceding and following contexts', 'surrounding', 'aspect term']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,31,127
baselines,AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .,"[('combines', (3, 4)), ('from', (8, 9)), ('with', (11, 12)), ('to generate', (16, 18))]","[('AT - LSTM', (0, 3)), ('sentence hidden states', (5, 8)), ('LSTM', (10, 11)), ('aspect term embedding', (13, 16)), ('attention vector', (19, 21))]","[['AT - LSTM', 'combines', 'sentence hidden states'], ['sentence hidden states', 'with', 'aspect term embedding'], ['aspect term embedding', 'to generate', 'attention vector'], ['sentence hidden states', 'from', 'LSTM']]",[],[],"[['Baselines', 'has', 'AT - LSTM']]",[],[],[],[],[],sentiment_analysis,31,129
baselines,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"[('extends', (4, 5)), ('by appending', (8, 10)), ('into', (13, 14))]","[('ATAE - LSTM', (0, 3)), ('AT - LSTM', (5, 8)), ('aspect embedding', (11, 13)), ('each word vector', (14, 17))]","[['ATAE - LSTM', 'extends', 'AT - LSTM'], ['AT - LSTM', 'by appending', 'aspect embedding'], ['aspect embedding', 'into', 'each word vector']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,31,131
baselines,AF - LSTM introduces a word - aspect fusion attention to learn associative relationships between aspect and context words .,"[('introduces', (3, 4)), ('to learn', (10, 12)), ('between', (14, 15))]","[('AF - LSTM', (0, 3)), ('word - aspect fusion attention', (5, 10)), ('associative relationships', (12, 14)), ('aspect and context words', (15, 19))]","[['AF - LSTM', 'introduces', 'word - aspect fusion attention'], ['word - aspect fusion attention', 'to learn', 'associative relationships'], ['associative relationships', 'between', 'aspect and context words']]",[],[],"[['Baselines', 'has', 'AF - LSTM']]",[],[],[],[],[],sentiment_analysis,31,132
baselines,CNN uses the architecture proposed in without explicitly considering aspect .,"[('uses', (1, 2)), ('without explicitly considering', (6, 9))]","[('CNN', (0, 1)), ('architecture', (3, 4)), ('aspect', (9, 10))]","[['CNN', 'uses', 'architecture'], ['architecture', 'without explicitly considering', 'aspect']]",[],[],"[['Baselines', 'has', 'CNN']]",[],[],[],[],[],sentiment_analysis,31,133
results,"Our two models achieve the best performance when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .","[('achieve', (3, 4)), ('compared to', (8, 10))]","[('Our two models', (0, 3)), ('best performance', (5, 7)), ('baselines', (11, 12))]","[['Our two models', 'achieve', 'best performance'], ['best performance', 'compared to', 'baselines']]",[],[],"[['Results', 'has', 'Our two models']]",[],[],[],[],[],sentiment_analysis,31,137
results,"Compared to one recently proposed model AF - LSTM , our method achieve 2 % - 5 % improvements .","[('Compared to', (0, 2)), ('achieve', (12, 13))]","[('recently proposed model AF - LSTM', (3, 9)), ('our method', (10, 12)), ('2 % - 5 % improvements', (13, 19))]","[['our method', 'achieve', '2 % - 5 % improvements']]","[['recently proposed model AF - LSTM', 'has', 'our method']]","[['Results', 'Compared to', 'recently proposed model AF - LSTM']]",[],[],[],[],[],[],sentiment_analysis,31,138
results,"Surprisingly , a vanilla CNN works quite well on this problem .","[('works', (5, 6))]","[('vanilla CNN', (3, 5)), ('quite well', (6, 8))]","[['vanilla CNN', 'works', 'quite well']]",[],[],"[['Results', 'has', 'vanilla CNN']]",[],[],[],[],[],sentiment_analysis,31,139
results,"It even beats these welldesigned LSTM models , which further proves that using CNN - based methods is a direction worth exploring .","[('beats', (2, 3))]","[('welldesigned LSTM models', (4, 7))]",[],[],[],[],[],"[['vanilla CNN', 'beats', 'welldesigned LSTM models']]",[],[],[],sentiment_analysis,31,140
research-problem,Interactive Attention Networks for Aspect - Level Sentiment Classification,[],"[('Aspect - Level Sentiment Classification', (4, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,32,2
research-problem,Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,[],"[('sentiment classification', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment classification']]",[],[],[],[],sentiment_analysis,32,5
model,"Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .","[('propose', (9, 10)), ('based on', (20, 22))]","[('interactive attention network ( IAN ) model', (11, 18)), ('long - short term memory networks ( LSTM )', (22, 31)), ('attention mechanism', (32, 34))]","[['interactive attention network ( IAN ) model', 'based on', 'long - short term memory networks ( LSTM )'], ['interactive attention network ( IAN ) model', 'based on', 'attention mechanism']]",[],"[['Model', 'propose', 'interactive attention network ( IAN ) model']]",[],[],[],[],[],[],sentiment_analysis,32,42
model,IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .,"[('utilizes', (1, 2)), ('associated with', (5, 7)), ('to get', (9, 11)), ('from', (13, 14)), ('compute', (17, 18)), ('for', (20, 21))]","[('IAN', (0, 1)), ('attention mechanism', (3, 5)), ('target', (8, 9)), ('important information', (11, 13)), ('context', (15, 16)), ('context representation', (18, 20)), ('sentiment classification', (21, 23))]","[['IAN', 'compute', 'context representation'], ['context representation', 'for', 'sentiment classification'], ['IAN', 'utilizes', 'attention mechanism'], ['attention mechanism', 'associated with', 'target'], ['attention mechanism', 'to get', 'important information'], ['important information', 'from', 'context']]",[],[],"[['Model', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,32,43
model,"Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .","[('makes use of', (3, 6)), ('from', (9, 10)), ('to supervise', (11, 13)), ('of', (15, 16))]","[('interactive information', (7, 9)), ('context', (10, 11)), ('modeling', (14, 15)), ('target', (17, 18))]","[['interactive information', 'to supervise', 'modeling'], ['modeling', 'of', 'target'], ['interactive information', 'from', 'context']]",[],[],[],[],"[['IAN', 'makes use of', 'interactive information']]",[],[],[],sentiment_analysis,32,44
model,"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .","[('with', (2, 3)), ('predicts', (12, 13)), ('for', (16, 17)), ('within', (19, 20))]","[('both target representation and context representation concatenated', (3, 10)), ('sentiment polarity', (14, 16)), ('target', (18, 19)), ('its context', (20, 22))]","[['sentiment polarity', 'with', 'both target representation and context representation concatenated'], ['sentiment polarity', 'for', 'target'], ['target', 'within', 'its context']]",[],[],[],[],"[['IAN', 'predicts', 'sentiment polarity']]",[],[],[],sentiment_analysis,32,45
hyperparameters,"In our experiments , all word embeddings from context and target are initialized by GloVe 2 , and all out - of - vocabulary words are initialized by sampling from the uniform distribution U ( ?0.1 , 0.1 ) .",[],[],"[['all word embeddings', 'from', 'context and target'], ['all word embeddings', 'initialized by', 'GloVe'], ['all out - of - vocabulary words', 'initialized by', 'sampling'], ['sampling', 'from', 'uniform distribution U ( ?0.1 , 0.1 )']]",[],[],"[['Hyperparameters', 'has', 'all word embeddings'], ['Hyperparameters', 'has', 'all out - of - vocabulary words']]",[],[],[],[],[],sentiment_analysis,32,125
hyperparameters,"All weight matrices are given their initial values by sampling from uniform distribution U ( ?0.1 , 0.1 ) , and all biases are set to zeros .","[('given their', (4, 6)), ('sampling from', (9, 11)), ('set to', (24, 26))]","[('weight matrices', (1, 3)), ('initial values', (6, 8)), ('uniform distribution U ( ?0.1 , 0.1 )', (11, 19)), ('biases', (22, 23)), ('zeros', (26, 27))]","[['biases', 'set to', 'zeros'], ['weight matrices', 'given their', 'initial values'], ['initial values', 'sampling from', 'uniform distribution U ( ?0.1 , 0.1 )']]",[],[],"[['Hyperparameters', 'has', 'biases'], ['Hyperparameters', 'has', 'weight matrices']]",[],[],[],[],[],sentiment_analysis,32,126
hyperparameters,"The dimensions of word embeddings , attention vectors and LSTM hidden states are set to 300 as in .","[('of', (2, 3)), ('set to', (13, 15))]","[('dimensions', (1, 2)), ('word embeddings , attention vectors and LSTM hidden states', (3, 12)), ('300', (15, 16))]","[['dimensions', 'of', 'word embeddings , attention vectors and LSTM hidden states'], ['word embeddings , attention vectors and LSTM hidden states', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'dimensions']]",[],[],[],[],[],sentiment_analysis,32,127
hyperparameters,"To train the parameters of IAN , we employ the Momentum , which adds a fraction ? of the update vector in the prior step to the current update vector .",[],[],"[['parameters', 'employ', 'Momentum'], ['Momentum', 'adds', 'fraction'], ['fraction', 'of', 'update vector'], ['update vector', 'in', 'prior step'], ['update vector', 'to', 'current update vector'], ['parameters', 'of', 'IAN']]",[],"[['Hyperparameters', 'train', 'parameters']]",[],[],[],[],[],[],sentiment_analysis,32,128
hyperparameters,"The coefficient of L 2 normalization in the objective function is set to 10 ?5 , and the dropout rate is set to 0.5 .",[],[],"[['coefficient', 'of', 'L 2 normalization'], ['L 2 normalization', 'in', 'objective function'], ['L 2 normalization', 'set to', '10 ?5'], ['dropout rate', 'set to', '0.5']]",[],[],"[['Hyperparameters', 'has', 'coefficient'], ['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],sentiment_analysis,32,129
baselines,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .",[],[],"[['Majority', 'is', 'basic baseline method'], ['Majority', 'assigns', 'largest sentiment polarity'], ['largest sentiment polarity', 'in', 'training set'], ['training set', 'to', 'each sample'], ['each sample', 'in', 'test set']]",[],[],"[['Baselines', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,32,133
baselines,LSTM only uses one LSTM network to model the context and get the hidden state of each word .,"[('uses', (2, 3)), ('to model', (6, 8)), ('get', (11, 12)), ('of', (15, 16))]","[('LSTM', (0, 1)), ('one LSTM network', (3, 6)), ('context', (9, 10)), ('hidden state', (13, 15)), ('each word', (16, 18))]","[['LSTM', 'get', 'hidden state'], ['hidden state', 'of', 'each word'], ['LSTM', 'uses', 'one LSTM network'], ['one LSTM network', 'to model', 'context']]",[],[],"[['Baselines', 'has', 'LSTM']]",[],[],[],[],[],sentiment_analysis,32,134
baselines,TD - LSTM adopts two long short - term memory ( LSTM ) networks to model the left context with target and the right context with target respectively .,[],[],"[['TD - LSTM', 'adopts', 'two long short - term memory ( LSTM ) networks'], ['two long short - term memory ( LSTM ) networks', 'to model', 'right context'], ['right context', 'with', 'target'], ['two long short - term memory ( LSTM ) networks', 'to model', 'left context'], ['left context', 'with', 'target']]",[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,32,136
baselines,AE - LSTM represents targets with aspect embeddings .,"[('represents', (3, 4)), ('with', (5, 6))]","[('AE - LSTM', (0, 3)), ('targets', (4, 5)), ('aspect embeddings', (6, 8))]","[['AE - LSTM', 'represents', 'targets'], ['targets', 'with', 'aspect embeddings']]",[],[],"[['Baselines', 'has', 'AE - LSTM']]",[],[],[],[],[],sentiment_analysis,32,138
baselines,ATAE - LSTM is developed based on AE - LSTM .,"[('based on', (5, 7))]","[('ATAE - LSTM', (0, 3)), ('AE - LSTM', (7, 10))]","[['ATAE - LSTM', 'based on', 'AE - LSTM']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,32,140
results,"All the other methods are based on LSTM models and better than the Majority method , showing that LSTM has potentials in automatically generating representations and can all bring performance improvement for sentiment classification .","[('based on', (5, 7)), ('better than', (10, 12))]","[('All the other methods', (0, 4)), ('LSTM models', (7, 9)), ('Majority method', (13, 15))]","[['All the other methods', 'based on', 'LSTM models'], ['LSTM models', 'better than', 'Majority method']]",[],[],"[['Results', 'has', 'All the other methods']]",[],[],[],[],[],sentiment_analysis,32,143
results,"The LSTM method gets the worst performance of all the neural network baseline methods , because it treats targets equally with other context words and does not make full use of the target information .","[('gets', (3, 4)), ('of', (7, 8))]","[('LSTM method', (1, 3)), ('worst performance', (5, 7)), ('all the neural network baseline methods', (8, 14))]","[['LSTM method', 'gets', 'worst performance'], ['worst performance', 'of', 'all the neural network baseline methods']]",[],[],"[['Results', 'has', 'LSTM method']]",[],[],[],[],[],sentiment_analysis,32,144
results,"TD - LSTM outperforms LSTM over 1 percent and 2 percent on the Restaurant and Laptop category respectively , since it develops from the standard LSTM and processes the left and right contexts with targets .","[('outperforms', (3, 4)), ('over', (5, 6)), ('on', (11, 12))]","[('TD - LSTM', (0, 3)), ('LSTM', (4, 5)), ('1 percent and 2 percent', (6, 11)), ('Restaurant and Laptop category', (13, 17))]","[['TD - LSTM', 'outperforms', 'LSTM'], ['LSTM', 'over', '1 percent and 2 percent'], ['1 percent and 2 percent', 'on', 'Restaurant and Laptop category']]",[],[],"[['Results', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,32,146
results,"Further , both AE - LSTM and ATAE - LSTM stably exceed the TD - LSTM method because of the introduction of attention mechanism .","[('stably exceed', (10, 12))]","[('both AE - LSTM and ATAE - LSTM', (2, 10)), ('TD - LSTM method', (13, 17))]","[['both AE - LSTM and ATAE - LSTM', 'stably exceed', 'TD - LSTM method']]",[],[],"[['Results', 'has', 'both AE - LSTM and ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,32,148
results,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .","[('For', (0, 1)), ('capture', (10, 11)), ('in', (13, 14)), ('with', (16, 17)), ('of', (19, 20)), ('generate', (22, 23)), ('for', (26, 27))]","[('AE - LSTM and ATAE - LSTM', (1, 8)), ('important information', (11, 13)), ('context', (15, 16)), ('supervision', (18, 19)), ('target', (20, 21)), ('more reasonable representations', (23, 26)), ('aspect - level sentiment classification', (27, 32))]","[['AE - LSTM and ATAE - LSTM', 'capture', 'important information'], ['important information', 'in', 'context'], ['context', 'with', 'supervision'], ['supervision', 'of', 'target'], ['AE - LSTM and ATAE - LSTM', 'generate', 'more reasonable representations'], ['more reasonable representations', 'for', 'aspect - level sentiment classification']]",[],"[['Results', 'For', 'AE - LSTM and ATAE - LSTM']]",[],[],[],[],[],[],sentiment_analysis,32,149
results,"We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of targets via the addition of the aspect embedding , which is also the reason of performance improvement .","[('see', (3, 4)), ('emphasize', (13, 14)), ('of', (16, 17)), ('via', (18, 19))]","[('AE - LSTM and ATAE - LSTM', (5, 12)), ('modeling', (15, 16)), ('targets', (17, 18)), ('addition of the aspect embedding', (20, 25))]","[['AE - LSTM and ATAE - LSTM', 'emphasize', 'modeling'], ['modeling', 'of', 'targets'], ['modeling', 'via', 'addition of the aspect embedding']]",[],"[['Results', 'see', 'AE - LSTM and ATAE - LSTM']]",[],[],[],[],[],[],sentiment_analysis,32,150
results,"Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .",[],[],"[['ATAE - LSTM', 'especially enhance', 'interaction'], ['interaction', 'between', 'context words and target'], ['better performance', 'than', 'AE - LSTM'], ['ATAE - LSTM', 'Compared with', 'AE - LSTM']]","[['ATAE - LSTM', 'has', 'better performance']]",[],"[['Results', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,32,151
results,We can see that IAN achieves the best performance among all baselines .,"[('achieves', (5, 6)), ('among', (9, 10))]","[('IAN', (4, 5)), ('best performance', (7, 9)), ('all baselines', (10, 12))]","[['IAN', 'achieves', 'best performance'], ['best performance', 'among', 'all baselines']]",[],[],"[['Results', 'see', 'IAN']]",[],[],[],[],[],sentiment_analysis,32,153
results,"Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .","[('Compared with', (0, 2)), ('improves', (8, 9)), ('about', (11, 12)), ('on', (17, 18))]","[('ATAE - LSTM model', (2, 6)), ('IAN', (7, 8)), ('performance', (10, 11)), ('1.4 % and 3.2 %', (12, 17)), ('Restaurant and Laptop categories', (19, 23))]","[['IAN', 'improves', 'performance'], ['performance', 'about', '1.4 % and 3.2 %'], ['1.4 % and 3.2 %', 'on', 'Restaurant and Laptop categories']]","[['ATAE - LSTM model', 'has', 'IAN']]","[['Results', 'Compared with', 'ATAE - LSTM model']]",[],[],[],[],[],[],sentiment_analysis,32,154
results,"The more attentions are paid to targets , the higher accuracy the system achieves .","[('paid to', (4, 6)), ('achieves', (13, 14))]","[('more attentions', (1, 3)), ('targets', (6, 7)), ('higher accuracy', (9, 11))]","[['more attentions', 'achieves', 'higher accuracy'], ['more attentions', 'paid to', 'targets']]",[],[],"[['Results', 'has', 'more attentions']]",[],[],[],[],[],sentiment_analysis,32,159
research-problem,Convolutional Neural Networks with Recurrent Neural Filters,[],"[('Convolutional Neural Networks with Recurrent Neural Filters', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Convolutional Neural Networks with Recurrent Neural Filters']]",[],[],[],[],sentiment_analysis,33,2
research-problem,"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .",[],"[('model convolution filters with RNNs', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'model convolution filters with RNNs']]",[],[],[],[],sentiment_analysis,33,7
model,"To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .","[('propose to employ', (5, 8)), ('as', (14, 15)), ('of', (17, 18))]","[('recurrent neural networks ( RNNs )', (8, 14)), ('convolution filters', (15, 17)), ('CNN systems', (18, 20))]","[['recurrent neural networks ( RNNs )', 'as', 'convolution filters'], ['convolution filters', 'of', 'CNN systems']]",[],"[['Model', 'propose to employ', 'recurrent neural networks ( RNNs )']]",[],[],[],[],[],[],sentiment_analysis,33,18
model,"Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .","[('deal with', (9, 11)), ('with', (13, 14)), ('that models', (17, 19)), ('to implicitly model', (27, 30))]","[('recurrent neural filters ( RNFs )', (1, 7)), ('language compositionality', (11, 13)), ('recurrent function', (15, 17)), ('word relations', (19, 21)), ('long - term dependencies', (30, 34))]","[['recurrent neural filters ( RNFs )', 'deal with', 'language compositionality'], ['language compositionality', 'with', 'recurrent function'], ['recurrent function', 'that models', 'word relations'], ['recurrent neural filters ( RNFs )', 'to implicitly model', 'long - term dependencies']]",[],[],"[['Model', 'has', 'recurrent neural filters ( RNFs )']]",[],[],[],[],[],sentiment_analysis,33,19
model,"RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .",[],[],"[['RNFs', 'alleviates', 'some well - known drawbacks'], ['some well - known drawbacks', 'including', 'vulnerability'], ['vulnerability', 'to', 'gradient vanishing and exploding problems'], ['some well - known drawbacks', 'of', 'RNNs'], ['RNFs', 'applied to', 'word sequences'], ['word sequences', 'of', 'moderate lengths']]",[],[],"[['Model', 'has', 'RNFs']]",[],[],[],[],[],sentiment_analysis,33,20
model,"As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .","[('As in', (0, 2)), ('computation of', (6, 8)), ('with', (11, 12)), ('can be', (13, 15))]","[('conventional CNNs', (2, 4)), ('convolution operation', (9, 11)), ('RNFs', (12, 13)), ('parallelized', (16, 17))]","[['convolution operation', 'with', 'RNFs'], ['RNFs', 'can be', 'parallelized'], ['parallelized', 'As in', 'conventional CNNs']]",[],"[['Model', 'computation of', 'convolution operation']]",[],[],[],[],[],[],sentiment_analysis,33,21
model,"As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .","[('can be', (9, 11)), ('than', (16, 17))]","[('RNF - based CNN models', (4, 9)), ('3 - 8 x faster', (11, 16)), ('RNN counterparts', (18, 20))]","[['RNF - based CNN models', 'can be', '3 - 8 x faster'], ['3 - 8 x faster', 'than', 'RNN counterparts']]",[],[],"[['Model', 'has', 'RNF - based CNN models']]",[],[],[],[],[],sentiment_analysis,33,22
model,We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .,"[('present', (1, 2)), ('for', (8, 9))]","[('two RNF - based CNN architectures', (2, 8)), ('sentence classification and answer sentence selection problems', (9, 16))]","[['two RNF - based CNN architectures', 'for', 'sentence classification and answer sentence selection problems']]",[],"[['Model', 'present', 'two RNF - based CNN architectures']]",[],[],[],[],[],[],sentiment_analysis,33,23
baselines,We consider CNN variants with linear filters and RNFs.,"[('consider', (1, 2)), ('with', (4, 5))]","[('CNN variants', (2, 4))]",[],[],"[['Baselines', 'consider', 'CNN variants']]",[],[],[],"[['CNN variants', 'with', 'linear filters and RNFs']]",[],[],sentiment_analysis,33,75
baselines,"For RNFs , we adopt two implementations based on GRUs and LSTMs respectively .","[('For', (0, 1)), ('adopt', (4, 5)), ('based on', (7, 9))]","[('RNFs', (1, 2)), ('two implementations', (5, 7)), ('GRUs and LSTMs', (9, 12))]","[['RNFs', 'adopt', 'two implementations'], ['two implementations', 'based on', 'GRUs and LSTMs']]",[],"[['Baselines', 'For', 'RNFs']]",[],[],[],[],[],[],sentiment_analysis,33,76
baselines,"We also compare against the following RNN variants : GRU , LSTM , GRU with max pooling , and LSTM with max pooling .",[],[],"[['GRU', 'with', 'max pooling'], ['LSTM', 'with', 'max pooling']]","[['RNN variants', 'name', 'GRU'], ['RNN variants', 'name', 'LSTM'], ['RNN variants', 'name', 'GRU'], ['RNN variants', 'name', 'LSTM']]","[['Baselines', 'compare against', 'RNN variants']]",[],[],[],[],[],[],sentiment_analysis,33,77
results,"In particular , CNN - RNF - LSTM achieves 53.4 % and 90.0 % accuracies on the fine - grained and binary sentiment classification tasks respectively , which match the state - of the - art results on the Stanford Sentiment Treebank .",[],[],"[['CNN - RNF - LSTM', 'achieves', '53.4 % and 90.0 % accuracies'], ['53.4 % and 90.0 % accuracies', 'match', 'state - of the - art results'], ['state - of the - art results', 'on', 'Stanford Sentiment Treebank'], ['53.4 % and 90.0 % accuracies', 'on', 'fine - grained and binary sentiment classification tasks']]",[],[],"[['Results', 'has', 'CNN - RNF - LSTM']]",[],[],[],[],[],sentiment_analysis,33,81
results,"CNN - RNF - LSTM also obtains competitive results on answer sentence selection datasets , despite the simple model architecture compared to state - of - the - art systems .","[('obtains', (6, 7)), ('on', (9, 10)), ('despite', (15, 16)), ('compared to', (20, 22))]","[('competitive results', (7, 9)), ('answer sentence selection datasets', (10, 14)), ('simple model architecture', (17, 20)), ('state - of - the - art systems', (22, 30))]","[['competitive results', 'despite', 'simple model architecture'], ['simple model architecture', 'compared to', 'state - of - the - art systems'], ['competitive results', 'on', 'answer sentence selection datasets']]",[],[],[],[],"[['CNN - RNF - LSTM', 'obtains', 'competitive results']]",[],[],[],sentiment_analysis,33,82
results,"Conventional RNN models clearly benefit from max pooling , especially on the task of answer sentence selection .","[('benefit from', (4, 6)), ('especially on', (9, 11))]","[('Conventional RNN models', (0, 3)), ('max pooling', (6, 8)), ('answer sentence selection', (14, 17))]","[['Conventional RNN models', 'benefit from', 'max pooling'], ['max pooling', 'especially on', 'answer sentence selection']]",[],[],"[['Results', 'has', 'Conventional RNN models']]",[],[],[],[],[],sentiment_analysis,33,83
results,"As a result , RNF - based CNN models perform consistently better than max - pooled RNN models .","[('perform', (9, 10)), ('than', (12, 13))]","[('RNF - based CNN models', (4, 9)), ('consistently better', (10, 12)), ('max - pooled RNN models', (13, 18))]","[['RNF - based CNN models', 'perform', 'consistently better'], ['consistently better', 'than', 'max - pooled RNN models']]",[],[],"[['Results', 'has', 'RNF - based CNN models']]",[],[],[],[],[],sentiment_analysis,33,87
research-problem,Mazajak : An Online Arabic Sentiment Analyser,[],"[('Arabic Sentiment Analyser', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Arabic Sentiment Analyser']]",[],[],[],[],sentiment_analysis,34,2
research-problem,Sentiment analysis ( SA ) is one of the most useful natural language processing applications .,[],"[('Sentiment analysis ( SA )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis ( SA )']]",[],[],[],[],sentiment_analysis,34,4
research-problem,Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .,[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,34,14
research-problem,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",[],"[('SA', (2, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'SA']]",[],[],[],[],sentiment_analysis,34,17
model,"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .","[('present', (5, 6)), ('utilises', (16, 17))]","[('Mazajak', (6, 7)), ('Online Arabic sentiment analysis system', (10, 15)), ('deep learning', (17, 19)), ('massive Arabic word embeddings', (20, 24))]","[['Mazajak', 'utilises', 'deep learning'], ['Mazajak', 'utilises', 'massive Arabic word embeddings']]","[['Mazajak', 'has', 'Online Arabic sentiment analysis system']]","[['Model', 'present', 'Mazajak']]",[],[],[],[],[],[],sentiment_analysis,34,27
model,The system is available as an online API that can be used by other researchers .,"[('available as', (3, 5)), ('used by', (11, 13))]","[('online API', (6, 8)), ('other researchers', (13, 15))]","[['online API', 'used by', 'other researchers']]",[],"[['Model', 'available as', 'online API']]",[],[],[],[],[],[],sentiment_analysis,34,28
baselines,The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61 .,"[('in', (4, 5))]","[('best performing system', (1, 4)), ('SemEval 2017 task', (6, 9))]","[['best performing system', 'in', 'SemEval 2017 task']]",[],[],"[['Baselines', 'has', 'best performing system']]",[],[],[],[],[],sentiment_analysis,34,113
baselines,"For the ASTD , the best reported results are by who used an ensemble system combining output of CNN and Bi - LSTM architectures , which achieved an F P N of 0.71 .","[('For', (0, 1)), ('by', (9, 10)), ('combining output of', (15, 18))]","[('ASTD', (2, 3)), ('best reported results', (5, 8)), ('ensemble system', (13, 15)), ('CNN and Bi - LSTM architectures', (18, 24))]","[['best reported results', 'by', 'ensemble system'], ['ensemble system', 'combining output of', 'CNN and Bi - LSTM architectures']]","[['ASTD', 'has', 'best reported results']]","[['Baselines', 'For', 'ASTD']]",[],[],[],[],[],[],sentiment_analysis,34,114
results,"As shown in the table , Mazajak model outperformed the current state - of - the - art models on the SemEval and ASTD datasets .","[('outperformed', (8, 9)), ('on', (19, 20))]","[('Mazajak model', (6, 8)), ('current state - of - the - art models', (10, 19)), ('SemEval and ASTD datasets', (21, 25))]","[['Mazajak model', 'outperformed', 'current state - of - the - art models'], ['current state - of - the - art models', 'on', 'SemEval and ASTD datasets']]",[],[],"[['Results', 'has', 'Mazajak model']]",[],[],[],[],[],sentiment_analysis,34,118
results,"In addition , it achieved a high performance on the ArSAS dataset .","[('achieved', (4, 5)), ('on', (8, 9))]","[('high performance', (6, 8)), ('ArSAS dataset', (10, 12))]","[['high performance', 'on', 'ArSAS dataset']]",[],[],[],[],"[['Mazajak model', 'achieved', 'high performance']]",[],[],[],sentiment_analysis,34,119
results,"Our reported scores are higher than current top systems for all the evaluation scores , including average recall , F P N , and accuracy .","[('higher than', (4, 6)), ('for', (9, 10)), ('including', (15, 16))]","[('reported scores', (1, 3)), ('current top systems', (6, 9)), ('all the evaluation scores', (10, 14)), ('average recall , F P N , and accuracy', (16, 25))]","[['reported scores', 'higher than', 'current top systems'], ['reported scores', 'for', 'all the evaluation scores'], ['all the evaluation scores', 'including', 'average recall , F P N , and accuracy']]",[],[],"[['Results', 'has', 'reported scores']]",[],[],[],[],[],sentiment_analysis,34,120
results,These results confirm that our model choice for our tool represents the current state - of - the - art for Arabic SA .,[],[],"[['our tool', 'represents', 'current state - of - the - art'], ['current state - of - the - art', 'for', 'Arabic SA']]",[],"[['Results', 'for', 'our tool']]",[],[],[],[],[],[],sentiment_analysis,34,121
research-problem,Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification,[],"[('Aspect - level Sentiment Classification', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level Sentiment Classification']]",[],[],[],[],sentiment_analysis,35,2
research-problem,"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .",[],"[('Aspect - level sentiment classification ( ASC )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level sentiment classification ( ASC )']]",[],[],[],[],sentiment_analysis,35,4
research-problem,"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",[],"[('aspect - oriented sentiment analysis', (2, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect - oriented sentiment analysis']]",[],[],[],[],sentiment_analysis,35,22
model,"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .","[('propose', (6, 7)), ('simultaneously align', (20, 22)), ('across', (29, 30))]","[('novel framework named Multi - Granularity Alignment Network ( MGAN )', (8, 19)), ('aspect granularity and aspect- specific feature representations', (22, 29)), ('domains', (30, 31))]","[['novel framework named Multi - Granularity Alignment Network ( MGAN )', 'simultaneously align', 'aspect granularity and aspect- specific feature representations'], ['aspect granularity and aspect- specific feature representations', 'across', 'domains']]",[],"[['Model', 'propose', 'novel framework named Multi - Granularity Alignment Network ( MGAN )']]",[],[],[],[],[],[],sentiment_analysis,35,38
model,"Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .","[('consists of', (4, 6)), ('for learning', (8, 10)), ('for', (14, 15))]","[('MGAN', (3, 4)), ('two networks', (6, 8)), ('aspect - specific representations', (10, 14)), ('two domains', (16, 18))]","[['MGAN', 'consists of', 'two networks'], ['two networks', 'for learning', 'aspect - specific representations'], ['aspect - specific representations', 'for', 'two domains']]",[],[],"[['Model', 'has', 'MGAN']]",[],[],[],[],[],sentiment_analysis,35,39
model,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .","[('to reduce', (2, 4)), ('between', (7, 8)), ('modeling', (12, 13)), ('at', (16, 17)), ('propose', (25, 26)), ('to help', (35, 37)), ('automatically capture', (40, 42)), ('in', (46, 47)), ('towards', (49, 50))]","[('task discrepancy', (5, 7)), ('domains', (8, 9)), ('two tasks', (14, 16)), ('same fine - grained level', (18, 23)), ('novel Coarse2 Fine ( C2F ) attention module', (27, 35)), ('source task', (38, 40)), ('corresponding aspect term', (43, 46)), ('context', (48, 49)), ('given aspect category', (51, 54))]","[['task discrepancy', 'modeling', 'two tasks'], ['two tasks', 'at', 'same fine - grained level'], ['task discrepancy', 'propose', 'novel Coarse2 Fine ( C2F ) attention module'], ['novel Coarse2 Fine ( C2F ) attention module', 'to help', 'source task'], ['source task', 'automatically capture', 'corresponding aspect term'], ['corresponding aspect term', 'in', 'context'], ['corresponding aspect term', 'towards', 'given aspect category'], ['task discrepancy', 'between', 'domains']]",[],"[['Model', 'to reduce', 'task discrepancy']]",[],[],[],[],[],[],sentiment_analysis,35,40
model,"To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .","[('To prevent', (0, 2)), ('adopt', (6, 7)), ('to semantically align', (20, 23))]","[('false alignment', (2, 4)), ('Contrastive Feature Alignment ( CFA )', (8, 14)), ('aspect - specific representations', (23, 27))]","[['false alignment', 'adopt', 'Contrastive Feature Alignment ( CFA )'], ['Contrastive Feature Alignment ( CFA )', 'to semantically align', 'aspect - specific representations']]",[],"[['Model', 'To prevent', 'false alignment']]",[],[],[],[],[],[],sentiment_analysis,35,45
model,"The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .","[('considers', (2, 3)), ('by maximally ensuring', (6, 9)), ('from', (12, 13)), ('same', (17, 18)), ('by guaranteeing', (23, 25)), ('from both', (26, 28)), ('to be', (32, 34))]","[('CFA', (1, 2)), ('semantic alignment', (4, 6)), ('equivalent distributions', (10, 12)), ('different domains', (13, 15)), ('class', (18, 19)), ('semantic separation', (21, 23)), ('distributions', (25, 26)), ('different classes and domains', (28, 32)), ('as dissimilar as possible', (34, 38))]","[['CFA', 'considers', 'semantic separation'], ['semantic separation', 'by guaranteeing', 'distributions'], ['distributions', 'from both', 'different classes and domains'], ['distributions', 'to be', 'as dissimilar as possible'], ['CFA', 'considers', 'semantic alignment'], ['semantic alignment', 'by maximally ensuring', 'equivalent distributions'], ['equivalent distributions', 'from', 'different domains'], ['different domains', 'same', 'class']]",[],[],"[['Model', 'has', 'CFA']]",[],[],[],[],[],sentiment_analysis,35,46
hyperparameters,The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .,"[('initialized with', (4, 6)), ('fine - tuned during', (12, 16))]","[('word embeddings', (1, 3)), ('200 - dimension GloVE vectors', (6, 11)), ('training', (17, 18))]","[['word embeddings', 'fine - tuned during', 'training'], ['word embeddings', 'initialized with', '200 - dimension GloVE vectors']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],sentiment_analysis,35,184
hyperparameters,The fc layer size is 300 .,"[('is', (4, 5))]","[('fc layer size', (1, 4)), ('300', (5, 6))]","[['fc layer size', 'is', '300']]",[],[],"[['Hyperparameters', 'has', 'fc layer size']]",[],[],[],[],[],sentiment_analysis,35,186
hyperparameters,The Adam ( Kingma and Ba 2014 ) is used as the optimizer with the initial learning rate 10 ? 4 .,"[('used as', (9, 11)), ('with', (13, 14))]","[('Adam ( Kingma and Ba 2014 )', (1, 8)), ('optimizer', (12, 13)), ('initial learning rate 10 ? 4', (15, 21))]","[['Adam ( Kingma and Ba 2014 )', 'used as', 'optimizer'], ['optimizer', 'with', 'initial learning rate 10 ? 4']]",[],[],"[['Hyperparameters', 'has', 'Adam ( Kingma and Ba 2014 )']]",[],[],[],[],[],sentiment_analysis,35,187
hyperparameters,Gradients with the 2 norm larger than 40 are normalized to be 40 .,[],[],"[['Gradients', 'with', '2 norm'], ['2 norm', 'larger than', '40'], ['Gradients', 'are', 'normalized'], ['normalized', 'to be', '40']]",[],[],"[['Hyperparameters', 'has', 'Gradients']]",[],[],[],[],[],sentiment_analysis,35,188
hyperparameters,"All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .","[('in', (2, 3)), ('randomly initialized from', (5, 8))]","[('All weights', (0, 2)), ('networks', (3, 4)), ('uniform distribution U ( ? 0.01 , 0.01 )', (9, 18))]","[['All weights', 'randomly initialized from', 'uniform distribution U ( ? 0.01 , 0.01 )'], ['All weights', 'in', 'networks']]",[],[],"[['Hyperparameters', 'has', 'All weights']]",[],[],[],[],[],sentiment_analysis,35,189
hyperparameters,"The batch sizes are 64 and 32 for source and target domains , respectively .","[('are', (3, 4)), ('for', (7, 8))]","[('batch sizes', (1, 3)), ('64 and 32', (4, 7)), ('source and target domains', (8, 12))]","[['batch sizes', 'are', '64 and 32'], ['64 and 32', 'for', 'source and target domains']]",[],[],"[['Hyperparameters', 'has', 'batch sizes']]",[],[],[],[],[],sentiment_analysis,35,190
hyperparameters,"To alleviate overfitting , we apply dropout on the word embeddings of the context with dropout rate 0.5 .","[('To alleviate', (0, 2)), ('apply', (5, 6)), ('on', (7, 8)), ('of', (11, 12)), ('with', (14, 15))]","[('overfitting', (2, 3)), ('dropout', (6, 7)), ('word embeddings', (9, 11)), ('context', (13, 14)), ('dropout rate 0.5', (15, 18))]","[['overfitting', 'apply', 'dropout'], ['dropout', 'on', 'word embeddings'], ['word embeddings', 'with', 'dropout rate 0.5'], ['word embeddings', 'of', 'context']]",[],"[['Hyperparameters', 'To alleviate', 'overfitting']]",[],[],[],[],[],[],sentiment_analysis,35,193
hyperparameters,We also perform early stopping on the validation set during the training process .,"[('perform', (2, 3)), ('on', (5, 6)), ('during', (9, 10))]","[('early stopping', (3, 5)), ('validation set', (7, 9)), ('training process', (11, 13))]","[['early stopping', 'on', 'validation set'], ['validation set', 'during', 'training process']]",[],"[['Hyperparameters', 'perform', 'early stopping']]",[],[],[],[],[],[],sentiment_analysis,35,194
hyperparameters,The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in R1?L task and are fixed to be used in all transfer pairs .,"[('tuned on', (3, 5)), ('of', (13, 14)), ('in', (17, 18)), ('fixed to be used in all', (22, 28))]","[('10 % randomly held - out training data', (5, 13)), ('target domain', (15, 17)), ('R1?L task', (18, 20)), ('transfer pairs', (28, 30))]","[['10 % randomly held - out training data', 'fixed to be used in all', 'transfer pairs'], ['10 % randomly held - out training data', 'of', 'target domain'], ['target domain', 'in', 'R1?L task']]",[],"[['Hyperparameters', 'tuned on', '10 % randomly held - out training data']]",[],[],[],[],[],[],sentiment_analysis,35,195
baselines,Non-Transfer,[],"[('Non-Transfer', (0, 1))]",[],[],[],"[['Baselines', 'has', 'Non-Transfer']]",[],[],[],[],"[['Non-Transfer', 'has', 'Target Network ( TN )']]",sentiment_analysis,35,198
baselines,"To demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :",[],"[('Target Network ( TN )', (29, 34))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,35,199
baselines,It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .,"[('is', (1, 2)), ('trained on', (13, 15)), ('for', (17, 18))]","[('proposed base model ( BiLSTM + C2A + Pas )', (3, 13)), ('D t', (15, 17)), ('target task', (19, 21))]","[['proposed base model ( BiLSTM + C2A + Pas )', 'trained on', 'D t'], ['D t', 'for', 'target task']]",[],[],[],[],"[['Target Network ( TN )', 'is', 'proposed base model ( BiLSTM + C2A + Pas )']]",[],[],[],sentiment_analysis,35,200
baselines,Transfer,[],"[('Transfer', (0, 1))]",[],[],[],"[['Baselines', 'has', 'Transfer']]",[],[],[],[],"[['Transfer', 'has', 'Source- only ( SO )']]",sentiment_analysis,35,202
baselines,Source- only ( SO ) :,[],"[('Source- only ( SO )', (0, 5))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,35,204
baselines,It uses a source network trained on D s to initialize a target network and then tests it on D t .,"[('uses', (1, 2)), ('trained on', (5, 7)), ('to initialize', (9, 11)), ('tests it on', (16, 19))]","[('source network', (3, 5)), ('D s', (7, 9)), ('target network', (12, 14)), ('D t', (19, 21))]","[['source network', 'tests it on', 'D t'], ['source network', 'trained on', 'D s'], ['D s', 'to initialize', 'target network']]",[],[],[],[],"[['Source- only ( SO )', 'uses', 'source network']]",[],[],[],sentiment_analysis,35,205
baselines,Fine-tuning ( FT ) : It advances SO with further finetuning the target network on D t .,"[('advances', (6, 7)), ('with further finetuning', (8, 11)), ('on', (14, 15))]","[('Fine-tuning ( FT )', (0, 4)), ('SO', (7, 8)), ('target network', (12, 14)), ('D t', (15, 17))]","[['Fine-tuning ( FT )', 'advances', 'SO'], ['SO', 'with further finetuning', 'target network'], ['target network', 'on', 'D t']]",[],[],[],[],[],[],"[['Transfer', 'has', 'Fine-tuning ( FT )']]",[],sentiment_analysis,35,206
baselines,M- DAN : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,"[('is', (4, 5)), ('of', (8, 9)), ('based on', (16, 18))]","[('M- DAN', (0, 2)), ('multi-adversarial version', (6, 8)), ('Domain Adversarial Network ( DAN )', (9, 15)), ('multiple domain discriminators', (18, 21))]","[['M- DAN', 'is', 'multi-adversarial version'], ['multi-adversarial version', 'based on', 'multiple domain discriminators'], ['multi-adversarial version', 'of', 'Domain Adversarial Network ( DAN )']]",[],[],[],[],[],[],"[['Transfer', 'has', 'M- DAN']]",[],sentiment_analysis,35,207
baselines,"M - MMD : Similar with M - DAN , M - MMD aligns different class distributions between domains based on multiple Maximum Mean Discrepancy ( MMD ) ) .","[('aligns', (13, 14)), ('between', (17, 18)), ('based on', (19, 21))]","[('M - MMD', (0, 3)), ('different class distributions', (14, 17)), ('domains', (18, 19)), ('multiple Maximum Mean Discrepancy ( MMD )', (21, 28))]","[['M - MMD', 'aligns', 'different class distributions'], ['different class distributions', 'between', 'domains'], ['domains', 'based on', 'multiple Maximum Mean Discrepancy ( MMD )']]",[],[],[],[],[],[],"[['Transfer', 'has', 'M - MMD']]",[],sentiment_analysis,35,209
results,Comparison with Non - Transfer,[],"[('Comparison with Non - Transfer', (0, 5))]",[],[],[],"[['Results', 'has', 'Comparison with Non - Transfer']]",[],[],[],[],"[['Comparison with Non - Transfer', 'has', 'MGAN']]",sentiment_analysis,35,214
results,"( 2 ) MGAN consistently outperforms the MGAN w / o C2 F , where C2F module of the source network is removed and the source position information is missed ( we set all p s i to 1 ) , by 1.41 % , 1.03 % , 1.09 % for accuracy and 1.79 % , 3.62 % and 1.16 % for Macro - F1 on average .",[],[],"[['MGAN', 'consistently outperforms', 'MGAN w / o C2 F'], ['MGAN w / o C2 F', 'where', 'source position information'], ['source position information', 'is', 'missed'], ['missed', 'by', '1.41 % , 1.03 % , 1.09 %'], ['1.41 % , 1.03 % , 1.09 %', 'for', 'accuracy'], ['missed', 'by', '1.79 % , 3.62 % and 1.16 %'], ['1.79 % , 3.62 % and 1.16 %', 'for', 'Macro - F1'], ['MGAN w / o C2 F', 'where', 'C2F module'], ['C2F module', 'of', 'source network'], ['C2F module', 'is', 'removed']]",[],[],[],[],[],[],[],[],sentiment_analysis,35,220
results,"The MGAN w / o PI , which does not utilize the position information , performs very poorly .","[('does not utilize', (8, 11)), ('performs', (15, 16))]","[('MGAN w / o PI', (1, 6)), ('position information', (12, 14)), ('very poorly', (16, 18))]","[['MGAN w / o PI', 'performs', 'very poorly'], ['MGAN w / o PI', 'does not utilize', 'position information']]",[],[],[],[],[],[],"[['Comparison with Non - Transfer', 'has', 'MGAN w / o PI']]",[],sentiment_analysis,35,223
results,Comparison with Transfer,[],"[('Comparison with Transfer', (0, 3))]",[],[],[],"[['Results', 'has', 'Comparison with Transfer']]",[],[],[],[],"[['Comparison with Transfer', 'has', 'SO']]",sentiment_analysis,35,224
results,SO performs poorly due to no adaptation applied .,"[('performs', (1, 2))]","[('SO', (0, 1)), ('poorly', (2, 3))]","[['SO', 'performs', 'poorly']]",[],[],[],[],[],[],[],[],sentiment_analysis,35,227
results,The popular technique FT can not achieve satisfactory results since fine - tuning may cause the oblivion of useful knowledge from the source task .,"[('can not achieve', (4, 7))]","[('popular technique FT', (1, 4)), ('satisfactory results', (7, 9))]","[['popular technique FT', 'can not achieve', 'satisfactory results']]",[],[],[],[],[],[],"[['Comparison with Transfer', 'has', 'popular technique FT']]",[],sentiment_analysis,35,228
results,"The full model MGAN outperforms M - DAN and M - MMD by 1.80 % and 1.33 % for accuracy and 1.90 % and 1.66 % for Marco - F1 on average , respectively .",[],[],"[['full model MGAN', 'outperforms', 'M - DAN and M - MMD'], ['M - DAN and M - MMD', 'by', '1.90 % and 1.66 %'], ['1.90 % and 1.66 %', 'for', 'Marco - F1 on average'], ['M - DAN and M - MMD', 'by', '1.80 % and 1.33 %'], ['1.80 % and 1.33 %', 'for', 'accuracy']]",[],[],[],[],[],[],"[['Comparison with Transfer', 'has', 'full model MGAN']]",[],sentiment_analysis,35,229
results,"Remarkably , MGAN considers both of them in a point - wise surrogate , which altogether improves the performance of our method .","[('considers both of them in', (3, 8)), ('improves', (16, 17)), ('of', (19, 20))]","[('MGAN', (2, 3)), ('point - wise surrogate', (9, 13)), ('performance', (18, 19)), ('our method', (20, 22))]","[['MGAN', 'considers both of them in', 'point - wise surrogate'], ['point - wise surrogate', 'improves', 'performance'], ['performance', 'of', 'our method']]",[],[],[],[],[],[],"[['Comparison with Transfer', 'has', 'MGAN']]",[],sentiment_analysis,35,231
results,"Besides , MGAN outperforms its ablation MGAN w/ o SS removing the semantic separation loss of the CFA by 0.81 % for accuracy and 1.00 % for Macro - F1 on average , which implies that the semantic separation plays an important role in alleviating false alignment .",[],[],"[['ablation MGAN w/ o SS', 'removing', 'semantic separation loss'], ['semantic separation loss', 'of', 'CFA'], ['CFA', 'by', '0.81 %'], ['0.81 %', 'for', 'accuracy'], ['CFA', 'by', '1.00 %'], ['1.00 %', 'for', 'Macro - F1 on average']]",[],[],[],[],"[['MGAN', 'outperforms', 'ablation MGAN w/ o SS']]",[],[],[],sentiment_analysis,35,232
results,Effect of C2F Attention Module,[],"[('Effect of C2F Attention Module', (0, 5))]",[],[],[],"[['Results', 'has', 'Effect of C2F Attention Module']]",[],[],[],[],"[['Effect of C2F Attention Module', 'has', 'MGAN']]",sentiment_analysis,35,233
results,"Then , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as "" shells "" to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .",[],[],"[['MGAN', 'uses', 'C2F'], ['C2F', 'to capture', 'more specific aspect terms'], ['more specific aspect terms', 'from', 'context'], ['context', 'towards', 'aspect category'], ['more specific aspect terms', 'helps', 'source task'], ['source task', 'capture', 'more fine - grained semantics'], ['more fine - grained semantics', 'of', 'aspect category'], ['source task', 'capture', 'detailed position information'], ['detailed position information', 'like', 'target task']]",[],[],[],[],[],"[['MGAN', 'compared with', 'MGAN w / o C2F']]",[],[],sentiment_analysis,35,237
results,While MGAN w / o C2F locates wrong sentiment contexts and fails in ( c ) .,"[('locates', (6, 7))]","[('MGAN w / o C2F', (1, 6)), ('wrong sentiment contexts', (7, 10))]","[['MGAN w / o C2F', 'locates', 'wrong sentiment contexts']]",[],[],[],[],[],[],[],[],sentiment_analysis,35,240
results,"As such , benefited from distilled knowledge from the source task , MGAN can better model the complicated relatedness between the context and aspect term for the target domain L , but MGAN w / o C2F performs poorly though it make true predictions in ( d ) and ( e ) .","[('benefited from', (3, 5)), ('from', (7, 8)), ('can better model', (13, 16)), ('between', (19, 20)), ('for', (25, 26)), ('performs', (37, 38))]","[('distilled knowledge', (5, 7)), ('source task', (9, 11)), ('MGAN', (12, 13)), ('complicated relatedness', (17, 19)), ('context and aspect term', (21, 25)), ('target domain L', (27, 30)), ('MGAN w / o C2F', (32, 37)), ('poorly', (38, 39))]","[['distilled knowledge', 'from', 'source task'], ['MGAN', 'can better model', 'complicated relatedness'], ['complicated relatedness', 'between', 'context and aspect term'], ['context and aspect term', 'for', 'target domain L'], ['MGAN w / o C2F', 'performs', 'poorly']]","[['distilled knowledge', 'has', 'MGAN'], ['distilled knowledge', 'has', 'MGAN w / o C2F']]",[],[],[],"[['Effect of C2F Attention Module', 'benefited from', 'distilled knowledge']]",[],[],[],sentiment_analysis,35,241
research-problem,Transformation Networks for Target - Oriented Sentiment Classification,[],"[('Target - Oriented Sentiment Classification', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Target - Oriented Sentiment Classification']]",[],[],[],[],sentiment_analysis,36,2
model,"We propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .","[('propose', (1, 2)), ('named', (6, 7))]","[('new architecture', (3, 5)), ('Target - Specific Transformation Networks ( TNet )', (7, 15))]","[['new architecture', 'named', 'Target - Specific Transformation Networks ( TNet )']]",[],"[['Model', 'propose', 'new architecture']]",[],[],[],[],[],[],sentiment_analysis,36,29
model,TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .,"[('firstly encodes', (1, 3)), ('into', (6, 7)), ('generates', (10, 11)), ('with', (15, 16))]","[('TNet', (0, 1)), ('context information', (4, 6)), ('word embeddings', (7, 9)), ('contextualized word representations', (12, 15)), ('LSTMs', (16, 17))]","[['TNet', 'generates', 'contextualized word representations'], ['contextualized word representations', 'with', 'LSTMs'], ['TNet', 'firstly encodes', 'context information'], ['context information', 'into', 'word embeddings']]",[],[],"[['Model', 'has', 'TNet']]",[],[],[],[],[],sentiment_analysis,36,30
model,"To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .","[('To integrate', (0, 2)), ('into', (5, 6)), ('introduces', (11, 12)), ('for generating', (22, 24))]","[('target information', (3, 5)), ('word representations', (7, 9)), ('novel Target - Specific Transformation ( TST ) component', (13, 22)), ('target - specific word representations', (25, 30))]","[['novel Target - Specific Transformation ( TST ) component', 'for generating', 'target - specific word representations'], ['novel Target - Specific Transformation ( TST ) component', 'To integrate', 'target information'], ['target information', 'into', 'word representations']]",[],[],[],[],"[['TNet', 'introduces', 'novel Target - Specific Transformation ( TST ) component']]",[],[],[],sentiment_analysis,36,31
model,"Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .","[('of', (19, 20)), ('generates', (26, 27)), ('conditioned on', (32, 34)), ('consolidates', (40, 41)), ('with', (44, 45)), ('to obtain', (51, 53))]","[('target', (12, 13)), ('individual context words', (20, 23)), ('TST', (24, 25)), ('different representations', (27, 29)), ('each context word', (41, 44)), ('tailor - made target representation', (46, 51)), ('transformed word representation', (54, 57))]","[['TST', 'consolidates', 'each context word'], ['each context word', 'with', 'tailor - made target representation'], ['tailor - made target representation', 'to obtain', 'transformed word representation'], ['TST', 'generates', 'different representations'], ['different representations', 'of', 'target'], ['target', 'conditioned on', 'individual context words']]",[],[],"[['Model', 'has', 'TST']]",[],[],[],[],[],sentiment_analysis,36,32
model,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .","[('design', (21, 22)), ('to contextualize', (25, 27))]","[('contextpreserving mechanism', (23, 25)), ('generated target - specific word representations', (28, 34))]","[['contextpreserving mechanism', 'to contextualize', 'generated target - specific word representations']]",[],"[['Model', 'design', 'contextpreserving mechanism']]",[],[],[],[],[],[],sentiment_analysis,36,37
model,"To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .","[('To help', (0, 2)), ('locate', (6, 7)), ('adopt', (13, 14)), ('to scale', (17, 19)), ('of', (21, 22)), ('with', (24, 25)), ('between', (27, 28))]","[('CNN feature extractor', (3, 6)), ('sentiment indicators', (7, 9)), ('more accurately', (9, 11)), ('proximity strategy', (15, 17)), ('input', (20, 21)), ('convolutional layer', (22, 24)), ('positional relevance', (25, 27)), ('a word and the target', (28, 33))]","[['proximity strategy', 'To help', 'CNN feature extractor'], ['CNN feature extractor', 'locate', 'sentiment indicators'], ['proximity strategy', 'to scale', 'input'], ['input', 'of', 'convolutional layer'], ['convolutional layer', 'with', 'positional relevance'], ['positional relevance', 'between', 'a word and the target']]","[['sentiment indicators', 'has', 'more accurately']]","[['Model', 'adopt', 'proximity strategy']]",[],[],[],[],[],[],sentiment_analysis,36,39
baselines,SVM : It is a traditional support vector machine based model with extensive feature engineering ;,"[('is', (3, 4)), ('with', (11, 12))]","[('SVM', (0, 1)), ('traditional support vector machine based model', (5, 11)), ('extensive feature engineering', (12, 15))]","[['SVM', 'is', 'traditional support vector machine based model'], ['traditional support vector machine based model', 'with', 'extensive feature engineering']]",[],[],"[['Baselines', 'has', 'SVM']]",[],[],[],[],[],sentiment_analysis,36,150
baselines,AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ;,"[('learns', (3, 4)), ('toward', (7, 8)), ('for', (9, 10)), ('via', (12, 13)), ('over', (15, 16))]","[('AdaRNN', (0, 1)), ('sentence representation', (5, 7)), ('target', (8, 9)), ('sentiment prediction', (10, 12)), ('semantic composition', (13, 15)), ('dependency tree', (16, 18))]","[['AdaRNN', 'learns', 'sentence representation'], ['sentence representation', 'toward', 'target'], ['sentence representation', 'for', 'sentiment prediction'], ['sentiment prediction', 'via', 'semantic composition'], ['semantic composition', 'over', 'dependency tree']]",[],[],"[['Baselines', 'has', 'AdaRNN']]",[],[],[],[],[],sentiment_analysis,36,151
baselines,"AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ;",[],[],"[['AE - LSTM', 'is', 'simple LSTM model'], ['simple LSTM model', 'incorporating', 'target embedding'], ['target embedding', 'as', 'input'], ['ATAE - LSTM', 'extends', 'AE - LSTM'], ['AE - LSTM', 'with', 'attention']]",[],[],"[['Baselines', 'has', 'AE - LSTM'], ['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,36,152
baselines,IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ;,"[('employs', (3, 4)), ('to learn', (6, 8)), ('of', (10, 11))]","[('IAN', (0, 1)), ('two LSTMs', (4, 6)), ('representations', (9, 10)), ('context and the target phrase', (12, 17))]","[['IAN', 'employs', 'two LSTMs'], ['two LSTMs', 'to learn', 'representations'], ['representations', 'of', 'context and the target phrase']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,36,153
baselines,CNN - ASP :,[],"[('CNN - ASP', (0, 3))]",[],[],[],"[['Baselines', 'has', 'CNN - ASP']]",[],[],[],[],[],sentiment_analysis,36,154
baselines,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ;,"[('is', (1, 2)), ('which directly concatenates', (10, 13)), ('to', (15, 16))]","[('CNN - based model', (3, 7)), ('target representation', (13, 15)), ('each word embedding', (16, 19))]","[['CNN - based model', 'which directly concatenates', 'target representation'], ['target representation', 'to', 'each word embedding']]",[],[],[],[],"[['CNN - ASP', 'is', 'CNN - based model']]",[],[],[],sentiment_analysis,36,155
baselines,TD - LSTM :,[],"[('TD - LSTM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,36,156
baselines,"It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;","[('employs', (1, 2)), ('to model', (4, 6)), ('of', (11, 12)), ('performs', (17, 18)), ('based on', (19, 21))]","[('two LSTMs', (2, 4)), ('left and right contexts', (7, 11)), ('target', (13, 14)), ('predictions', (18, 19)), ('concatenated context representations', (21, 24))]","[['predictions', 'based on', 'concatenated context representations'], ['two LSTMs', 'to model', 'left and right contexts'], ['left and right contexts', 'of', 'target']]",[],[],[],[],"[['TD - LSTM', 'performs', 'predictions'], ['TD - LSTM', 'employs', 'two LSTMs']]",[],[],[],sentiment_analysis,36,157
baselines,MemNet : It applies attention mechanism over the word embeddings multiple times and predicts sentiments based on the top - most sentence representations ;,"[('applies', (3, 4)), ('over', (6, 7)), ('predicts', (13, 14)), ('based on', (15, 17))]","[('MemNet', (0, 1)), ('attention mechanism', (4, 6)), ('word embeddings multiple times', (8, 12)), ('sentiments', (14, 15)), ('top - most sentence representations', (18, 23))]","[['MemNet', 'applies', 'attention mechanism'], ['attention mechanism', 'over', 'word embeddings multiple times'], ['MemNet', 'predicts', 'sentiments'], ['sentiments', 'based on', 'top - most sentence representations']]",[],[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,36,158
baselines,"BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and introduces gates to measure the importance of left context , right context , and the entire sentence for the prediction ;","[('models', (6, 7)), ('using', (11, 12)), ('introduces', (18, 19)), ('to measure', (20, 22)), ('for', (35, 36))]","[('BILSTM - ATT -G', (0, 4)), ('left and right contexts', (7, 11)), ('two attention - based LSTMs', (12, 17)), ('gates', (19, 20)), ('importance of left context , right context , and the entire sentence', (23, 35)), ('prediction', (37, 38))]","[['BILSTM - ATT -G', 'models', 'left and right contexts'], ['left and right contexts', 'using', 'two attention - based LSTMs'], ['BILSTM - ATT -G', 'introduces', 'gates'], ['gates', 'to measure', 'importance of left context , right context , and the entire sentence'], ['importance of left context , right context , and the entire sentence', 'for', 'prediction']]",[],[],"[['Baselines', 'has', 'BILSTM - ATT -G']]",[],[],[],[],[],sentiment_analysis,36,159
baselines,RAM : RAM is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"[('is', (3, 4)), ('where', (7, 8)), ('consists of', (10, 12)), ('to learn', (23, 25))]","[('RAM', (0, 1)), ('multilayer architecture', (5, 7)), ('each layer', (8, 10)), ('attention - based aggregation of word features', (12, 19)), ('GRU cell', (21, 23)), ('sentence representation', (26, 28))]","[['RAM', 'is', 'multilayer architecture'], ['multilayer architecture', 'where', 'each layer'], ['each layer', 'consists of', 'attention - based aggregation of word features'], ['each layer', 'consists of', 'GRU cell'], ['GRU cell', 'to learn', 'sentence representation']]",[],[],"[['Baselines', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,36,160
results,LSTM - based models relying on sequential information can perform well for formal sentences by capturing more useful context features ;,"[('relying on', (4, 6)), ('can perform', (8, 10)), ('for', (11, 12)), ('by capturing', (14, 16))]","[('LSTM - based models', (0, 4)), ('sequential information', (6, 8)), ('well', (10, 11)), ('formal sentences', (12, 14)), ('more useful context features', (16, 20))]","[['LSTM - based models', 'relying on', 'sequential information'], ['LSTM - based models', 'can perform', 'well'], ['well', 'for', 'formal sentences'], ['well', 'by capturing', 'more useful context features']]",[],[],"[['Results', 'has', 'LSTM - based models']]",[],[],[],[],[],sentiment_analysis,36,188
results,"For ungrammatical text , CNN - based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns .","[('For', (0, 1)), ('may have', (8, 10))]","[('ungrammatical text', (1, 3)), ('CNN - based models', (4, 8)), ('some advantages', (10, 12))]","[['CNN - based models', 'may have', 'some advantages']]","[['ungrammatical text', 'has', 'CNN - based models']]","[['Results', 'For', 'ungrammatical text']]",[],[],[],[],[],[],sentiment_analysis,36,189
ablation-analysis,"After removing the deep transformation ( i.e. , the techniques introduced in Section 2.2 ) , both TNet - LF and TNet - AS are reduced to TNet w/o transformation ( where position relevance is kept ) , and their results in both accuracy and F 1 measure are incomparable with those of TNet .","[('removing', (1, 2)), ('reduced to', (25, 27)), ('in both', (41, 43)), ('incomparable with', (49, 51))]","[('deep transformation', (3, 5)), ('TNet - LF and TNet - AS', (17, 24)), ('TNet w/o transformation', (27, 30)), ('results', (40, 41)), ('accuracy and F 1 measure', (43, 48)), ('TNet', (53, 54))]","[['TNet - LF and TNet - AS', 'reduced to', 'TNet w/o transformation'], ['results', 'incomparable with', 'TNet'], ['results', 'in both', 'accuracy and F 1 measure']]","[['deep transformation', 'has', 'TNet - LF and TNet - AS'], ['TNet - LF and TNet - AS', 'has', 'results']]","[['Ablation analysis', 'removing', 'deep transformation']]",[],[],[],[],[],[],sentiment_analysis,36,192
ablation-analysis,It shows that the integration of target information into the word - level representations is crucial for good performance .,"[('shows that', (1, 3)), ('of', (5, 6)), ('into', (8, 9)), ('crucial for', (15, 17))]","[('integration', (4, 5)), ('target information', (6, 8)), ('word - level representations', (10, 14)), ('good performance', (17, 19))]","[['integration', 'of', 'target information'], ['target information', 'into', 'word - level representations'], ['target information', 'crucial for', 'good performance']]",[],[],[],[],"[['deep transformation', 'shows that', 'integration']]",[],[],[],sentiment_analysis,36,193
ablation-analysis,"Comparing the results of TNet and TNet w/o context ( where TST and position relevance are kept ) , we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST 7 , while on TWITTER , TNet w/o context performs very competitive ( p- values with TNet - LF and TNet - AS are 0.066 and 0.053 respectively for Accuracy ) .",[],[],"[['results', 'of', 'TNet and TNet w/o context'], ['TNet and TNet w/o context', 'observe', 'performance'], ['performance', 'drops', 'significantly'], ['significantly', 'on', 'LAPTOP and REST'], ['performance', 'of', 'TNet w/o context'], ['TNet and TNet w/o context', 'on', 'TWITTER'], ['TNet w/o context', 'performs', 'very competitive']]","[['TWITTER', 'has', 'TNet w/o context']]","[['Ablation analysis', 'Comparing', 'results']]",[],[],[],[],[],[],sentiment_analysis,36,194
ablation-analysis,"TNet w/o context performs consistently better than TNet w/o transformation , which verifies the efficacy of the target specific transformation ( TST ) , before applying context - preserving .","[('performs', (3, 4)), ('than', (6, 7))]","[('TNet w/o context', (0, 3)), ('consistently better', (4, 6)), ('TNet w/o transformation', (7, 10))]","[['TNet w/o context', 'performs', 'consistently better'], ['consistently better', 'than', 'TNet w/o transformation']]",[],[],"[['Ablation analysis', 'has', 'TNet w/o context']]",[],[],[],[],[],sentiment_analysis,36,196
ablation-analysis,"All of the produced p-values are less than 0.05 , suggesting that the improvements brought in by position information are significant .","[('are less than', (5, 8)), ('suggesting that', (10, 12)), ('brought in by', (14, 17)), ('are', (19, 20))]","[('produced p-values', (3, 5)), ('0.05', (8, 9)), ('improvements', (13, 14)), ('position information', (17, 19)), ('significant', (20, 21))]","[['produced p-values', 'are less than', '0.05'], ['0.05', 'suggesting that', 'improvements'], ['improvements', 'are', 'significant'], ['improvements', 'brought in by', 'position information']]",[],[],"[['Ablation analysis', 'has', 'produced p-values']]",[],[],[],[],[],sentiment_analysis,36,198
research-problem,IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis,[],"[('Aspect - Based Sentiment Analysis', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,37,2
research-problem,Sentiment analysis has immense implications in modern businesses through user-feedback mining .,[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,37,4
research-problem,Aspect - based sentiment analysis ( ABSA ) caters to these needs .,[],"[('Aspect - based sentiment analysis ( ABSA )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis ( ABSA )']]",[],[],[],[],sentiment_analysis,37,13
research-problem,The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .,[],"[('ABSA', (4, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'ABSA']]",[],[],[],[],sentiment_analysis,37,26
model,"To model these scenarios , firstly , following , we independently generate aspect - aware sentence representations for all the aspects using gated recurrent unit ( GRU ) and attention mechanism .","[('independently generate', (10, 12)), ('for', (17, 18)), ('using', (21, 22))]","[('aspect - aware sentence representations', (12, 17)), ('all the aspects', (18, 21)), ('gated recurrent unit ( GRU )', (22, 28)), ('attention mechanism', (29, 31))]","[['aspect - aware sentence representations', 'using', 'gated recurrent unit ( GRU )'], ['aspect - aware sentence representations', 'using', 'attention mechanism'], ['aspect - aware sentence representations', 'for', 'all the aspects']]",[],"[['Model', 'independently generate', 'aspect - aware sentence representations']]",[],[],[],[],[],[],sentiment_analysis,37,33
model,"Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .","[('employ', (3, 4)), ('to repeatedly match', (6, 9)), ('with', (13, 14)), ('to generate', (17, 19)), ('of', (22, 23))]","[('memory networks', (4, 6)), ('target aspect representation', (10, 13)), ('other aspects', (15, 17)), ('more accurate representation', (19, 22)), ('target aspect', (24, 26))]","[['memory networks', 'to repeatedly match', 'target aspect representation'], ['target aspect representation', 'with', 'other aspects'], ['target aspect representation', 'to generate', 'more accurate representation'], ['more accurate representation', 'of', 'target aspect']]",[],"[['Model', 'employ', 'memory networks']]",[],[],[],[],[],"[['more accurate representation', 'name', 'refined representation']]",sentiment_analysis,37,34
model,This refined representation is fed to a softmax classifier for final classification .,"[('fed to', (4, 6)), ('for', (9, 10))]","[('refined representation', (1, 3)), ('softmax classifier', (7, 9)), ('final classification', (10, 12))]","[['refined representation', 'fed to', 'softmax classifier'], ['softmax classifier', 'for', 'final classification']]",[],[],[],[],[],[],[],[],sentiment_analysis,37,35
,LSTM,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,37,162
baselines,"Following , the sentence is fed to along short - term memory ( LSTM ) network to propagate context among the constituent words .","[('fed to', (5, 7)), ('to propagate', (16, 18)), ('among', (19, 20))]","[('sentence', (3, 4)), ('short - term memory ( LSTM ) network', (8, 16)), ('context', (18, 19)), ('constituent words', (21, 23))]","[['sentence', 'fed to', 'short - term memory ( LSTM ) network'], ['short - term memory ( LSTM ) network', 'to propagate', 'context'], ['context', 'among', 'constituent words']]",[],[],[],[],[],[],"[['LSTM', 'has', 'sentence']]",[],sentiment_analysis,37,163
baselines,TD- LSTM,[],"[('TD- LSTM', (0, 2))]",[],[],[],"[['Baselines', 'has', 'TD- LSTM']]",[],[],[],[],"[['TD- LSTM', 'has', 'sequence']]",sentiment_analysis,37,166
baselines,"Following , sequence of words preceding ( left context ) and succeeding ( right context ) target aspect term are fed to two different LSTMs .","[('of', (3, 4)), ('fed to', (20, 22))]","[('sequence', (2, 3)), ('words preceding ( left context ) and succeeding ( right context ) target aspect term', (4, 19)), ('two different LSTMs', (22, 25))]","[['sequence', 'of', 'words preceding ( left context ) and succeeding ( right context ) target aspect term'], ['words preceding ( left context ) and succeeding ( right context ) target aspect term', 'fed to', 'two different LSTMs']]",[],[],[],[],[],[],[],[],sentiment_analysis,37,167
baselines,"This representation is fed to softmax classifier. , ATAE - LSTM is identical to AE - LSTM , except the LSTM is fed with the concatenation of aspect - term representation and word representation . , target - aspect and its context are sent to two distinct LSTMs and the means of the hidden outputs are taken as intermediate aspect representation and context representation respectively .","[('identical to', (12, 14)), ('except', (18, 19)), ('fed with', (22, 24)), ('of', (26, 27))]","[('ATAE - LSTM', (8, 11)), ('AE - LSTM', (14, 17)), ('LSTM', (20, 21)), ('concatenation', (25, 26)), ('aspect - term representation and word representation', (27, 34))]","[['ATAE - LSTM', 'identical to', 'AE - LSTM'], ['AE - LSTM', 'except', 'LSTM'], ['LSTM', 'fed with', 'concatenation'], ['concatenation', 'of', 'aspect - term representation and word representation']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,37,172
results,"It is evident from the results that our IARM model outperforms all the baseline models , including the state of the art , in both of the domains .","[('evident', (2, 3)), ('outperforms', (10, 11)), ('including', (16, 17)), ('in both of', (23, 26))]","[('our IARM model', (7, 10)), ('all the baseline models', (11, 15)), ('state of the art', (18, 22)), ('domains', (27, 28))]","[['our IARM model', 'outperforms', 'all the baseline models'], ['all the baseline models', 'in both of', 'domains'], ['all the baseline models', 'including', 'state of the art']]",[],"[['Results', 'evident', 'our IARM model']]",[],[],[],[],[],[],sentiment_analysis,37,196
results,"We obtained bigger improvement in laptop domain , of 1.7 % , compared to restaurant domain , of 1.4 % .",[],[],"[['bigger improvement', 'in', 'laptop domain'], ['laptop domain', 'of', '1.7 %'], ['laptop domain', 'compared to', 'restaurant domain'], ['restaurant domain', 'of', '1.4 %']]",[],"[['Results', 'obtained', 'bigger improvement']]",[],[],[],[],[],[],sentiment_analysis,37,197
research-problem,Representation learning ) plays a critical role in many modern machine learning systems .,[],"[('Representation learning', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Representation learning']]",[],[],[],[],sentiment_analysis,38,13
research-problem,We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,"[('attempt to learn', (10, 13)), ('accurately contains', (17, 19))]","[('sentiment analysis', (7, 9)), ('unsupervised representation', (14, 16))]","[['unsupervised representation', 'accurately contains', 'sentiment analysis']]",[],"[['Approach', 'attempt to learn', 'unsupervised representation']]",[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,38,49
approach,"As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .","[('consider', (5, 6)), ('due to', (18, 20))]","[('research benchmark of byte ( character ) level language modelling', (8, 18)), ('its further simplicity and generality', (20, 25))]","[['research benchmark of byte ( character ) level language modelling', 'due to', 'its further simplicity and generality']]",[],"[['Approach', 'consider', 'research benchmark of byte ( character ) level language modelling']]",[],[],[],[],[],[],sentiment_analysis,38,51
approach,We train on a very large corpus picked to have a similar distribution as our task of interest .,"[('train on', (1, 3)), ('picked to have', (7, 10)), ('as', (13, 14))]","[('very large corpus', (4, 7)), ('similar distribution', (11, 13)), ('our task of interest', (14, 18))]","[['very large corpus', 'picked to have', 'similar distribution'], ['similar distribution', 'as', 'our task of interest']]",[],"[['Approach', 'train on', 'very large corpus']]",[],[],[],[],[],[],sentiment_analysis,38,53
results,Review Sentiment Analysis,[],"[('Review Sentiment Analysis', (0, 3))]",[],[],[],"[['Results', 'has', 'Review Sentiment Analysis']]",[],[],[],[],"[['Review Sentiment Analysis', 'has', 'The representation learned by our model']]",sentiment_analysis,38,99
results,The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .,"[('achieves', (6, 7)), ('significantly outperforming', (9, 11)), ('of', (16, 17)), ('by', (19, 20))]","[('The representation learned by our model', (0, 6)), ('91.8 %', (7, 9)), ('state of the art', (12, 16)), ('90.2 %', (17, 19)), ('30 model ensemble', (21, 24))]","[['The representation learned by our model', 'achieves', '91.8 %'], ['91.8 %', 'significantly outperforming', 'state of the art'], ['state of the art', 'of', '90.2 %'], ['state of the art', 'by', '30 model ensemble']]",[],[],[],[],[],[],[],[],sentiment_analysis,38,105
results,It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples .,"[('matches', (1, 2)), ('of', (4, 5)), ('using', (6, 7)), ('outperforms', (15, 16)), ('with', (19, 20))]","[('performance', (3, 4)), ('baselines', (5, 6)), ('dozen labeled examples', (11, 14)), ('all previous results', (16, 19)), ('few hundred labeled examples', (22, 26))]","[['performance', 'using', 'dozen labeled examples'], ['performance', 'of', 'baselines'], ['all previous results', 'with', 'few hundred labeled examples']]",[],[],[],[],"[['The representation learned by our model', 'matches', 'performance'], ['The representation learned by our model', 'outperforms', 'all previous results']]",[],[],[],sentiment_analysis,38,107
results,"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the fine - grained subtask , achieving 52.9 % .","[('on', (9, 10)), ('does not reach', (15, 18)), ('of', (23, 24)), ('achieving', (33, 34))]","[('state of the art', (19, 23)), ('53.6 %', (24, 26)), ('fine - grained subtask', (28, 32)), ('52.9 %', (34, 36))]","[['state of the art', 'achieving', '52.9 %'], ['state of the art', 'of', '53.6 %'], ['state of the art', 'on', 'fine - grained subtask']]",[],[],[],[],"[['The representation learned by our model', 'does not reach', 'state of the art']]",[],[],[],sentiment_analysis,38,109
results,L1 regularization is known to reduce sample complexity when there are many irrelevant features .,[],"[('L1 regularization', (0, 2))]",[],[],[],[],[],[],[],"[['Review Sentiment Analysis', 'has', 'L1 regularization']]",[],sentiment_analysis,38,112
results,"Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .",[],[],"[['threshold', 'achieves', 'test accuracy'], ['test accuracy', 'of', '92.30 %'], ['test accuracy', 'outperforms', 'strong supervised results'], ['91.87 %', 'of', 'NB - SVM trigram'], ['91.87 %', 'still below', 'semi-supervised state of the art'], ['semi-supervised state of the art', 'of', '94.09 %']]","[['threshold', 'has', '91.87 %']]",[],[],[],"[['L1 regularization', 'Fitting', 'threshold']]",[],[],[],sentiment_analysis,38,117
results,Using the full 4096 unit representation achieves 92.88 % .,"[('Using', (0, 1)), ('achieves', (6, 7))]","[('full 4096 unit representation', (2, 6)), ('92.88 %', (7, 9))]","[['full 4096 unit representation', 'achieves', '92.88 %']]",[],[],[],[],"[['Review Sentiment Analysis', 'Using', 'full 4096 unit representation']]",[],[],[],sentiment_analysis,38,118
results,Capacity Ceiling,[],"[('Capacity Ceiling', (0, 2))]",[],[],[],"[['Results', 'has', 'Capacity Ceiling']]",[],[],[],[],[],sentiment_analysis,38,122
results,We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .,"[('try', (1, 2)), ('on', (4, 5)), ('of', (8, 9))]","[('approach', (3, 4)), ('binary version', (6, 8)), ('Yelp Dataset Challenge in 2015', (10, 15))]","[['approach', 'on', 'binary version'], ['binary version', 'of', 'Yelp Dataset Challenge in 2015']]",[],[],[],[],"[['Capacity Ceiling', 'try', 'approach']]",[],[],[],sentiment_analysis,38,124
results,"Using the full dataset , we achieve 95 . 22 % test accuracy .","[('Using', (0, 1)), ('achieve', (6, 7))]","[('full dataset', (2, 4)), ('95 . 22 % test accuracy', (7, 13))]","[['full dataset', 'achieve', '95 . 22 % test accuracy']]",[],[],[],[],"[['Capacity Ceiling', 'Using', 'full dataset']]",[],[],[],sentiment_analysis,38,127
results,The observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations .,"[('observed', (1, 2)), ('is', (4, 5)), ('for', (11, 12))]","[('capacity ceiling', (2, 4)), ('interesting phenomena and stumbling point', (6, 11)), ('scaling our unsupervised representations', (12, 16))]","[['capacity ceiling', 'is', 'interesting phenomena and stumbling point'], ['interesting phenomena and stumbling point', 'for', 'scaling our unsupervised representations']]",[],[],[],[],"[['Capacity Ceiling', 'observed', 'capacity ceiling']]",[],[],[],sentiment_analysis,38,129
results,"Additionally , there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets .","[('there is', (2, 4)), ('in', (7, 8)), ('of', (11, 12)), ('transitioning from', (14, 16))]","[('notable drop', (5, 7)), ('relative performance', (9, 11)), ('our approach', (12, 14)), ('sentence to document datasets', (16, 20))]","[['notable drop', 'in', 'relative performance'], ['relative performance', 'of', 'our approach'], ['our approach', 'transitioning from', 'sentence to document datasets']]",[],[],[],[],"[['Capacity Ceiling', 'there is', 'notable drop']]",[],[],[],sentiment_analysis,38,134
results,"Finally , as the amount of labeled data increases , the performance of the simple linear model we train on top of our static representation will eventually saturate .","[('of', (5, 6)), ('train on top of', (18, 22)), ('will eventually', (25, 27))]","[('labeled data', (6, 8)), ('increases', (8, 9)), ('performance', (11, 12)), ('simple linear model', (14, 17)), ('static representation', (23, 25)), ('saturate', (27, 28))]","[['performance', 'of', 'simple linear model'], ['simple linear model', 'train on top of', 'static representation'], ['performance', 'will eventually', 'saturate']]","[['labeled data', 'has', 'increases'], ['increases', 'has', 'performance']]",[],[],[],[],[],"[['Capacity Ceiling', 'has', 'labeled data']]",[],sentiment_analysis,38,136
research-problem,SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,[],"[('Targeted Aspect Based Sentiment Analysis', (2, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Targeted Aspect Based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,39,2
research-problem,"In this paper , we introduce the task of targeted aspect - based sentiment analysis .",[],"[('targeted aspect - based sentiment analysis', (9, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'targeted aspect - based sentiment analysis']]",[],[],[],[],sentiment_analysis,39,4
research-problem,Sentiment analysis is an important task in natural language processing .,[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,39,16
research-problem,"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .",[],"[('Aspect - based sentiment analysis ( ABSA )', (0, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis ( ABSA )']]",[],[],[],[],sentiment_analysis,39,20
research-problem,Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .,[],"[('Targeted sentiment analysis', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Targeted sentiment analysis']]",[],[],[],[],sentiment_analysis,39,24
dataset,SentiHood currently contains annotated sentences containing one or two location entity mentions .,"[('contains', (2, 3)), ('containing', (5, 6))]","[('SentiHood', (0, 1)), ('annotated sentences', (3, 5)), ('one or two location entity mentions', (6, 12))]","[['SentiHood', 'contains', 'annotated sentences'], ['annotated sentences', 'containing', 'one or two location entity mentions']]",[],[],"[['Dataset', 'name', 'SentiHood']]",[],[],[],[],[],sentiment_analysis,39,128
dataset,2 Sen-tiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple ( two ) locations .,[],[],"[['5215 sentences', 'with', '3862 sentences'], ['3862 sentences', 'containing', 'single location'], ['5215 sentences', 'with', '1353 sentences'], ['1353 sentences', 'containing', 'multiple ( two ) locations']]",[],[],[],[],[],[],"[['SentiHood', 'contains', '5215 sentences']]",[],sentiment_analysis,39,129
dataset,""" Positive "" sentiment is dominant for aspects such as dining and shopping .","[('dominant for', (5, 7)), ('such as', (8, 10))]","[('"" Positive "" sentiment', (0, 4)), ('aspects', (7, 8)), ('dining and shopping', (10, 13))]","[['"" Positive "" sentiment', 'dominant for', 'aspects'], ['aspects', 'such as', 'dining and shopping']]",[],[],"[['Dataset', 'has', '"" Positive "" sentiment']]",[],[],[],[],[],sentiment_analysis,39,131
dataset,The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .,"[('is', (3, 4)), ('with', (8, 9)), ('occurred in', (16, 18))]","[('general aspect', (1, 3)), ('most frequent aspect', (5, 8)), ('over 2000 sentences', (9, 12)), ('aspect touristy', (13, 15)), ('less than 100 sentences', (18, 22))]","[['aspect touristy', 'occurred in', 'less than 100 sentences'], ['general aspect', 'is', 'most frequent aspect'], ['most frequent aspect', 'with', 'over 2000 sentences']]",[],[],"[['Dataset', 'has', 'aspect touristy'], ['Dataset', 'has', 'general aspect']]",[],[],[],[],[],sentiment_analysis,39,133
dataset,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .","[('since', (2, 3)), ('can contain', (5, 7)), ('in', (20, 21)), ('higher than', (24, 26))]","[('each sentence', (3, 5)), ('one or more opinions', (7, 11)), ('total number of opinions ( 5920 )', (13, 20)), ('dataset', (22, 23)), ('number of sentences', (27, 30))]","[['total number of opinions ( 5920 )', 'higher than', 'number of sentences'], ['number of sentences', 'since', 'each sentence'], ['each sentence', 'can contain', 'one or more opinions'], ['total number of opinions ( 5920 )', 'in', 'dataset']]",[],[],"[['Dataset', 'has', 'total number of opinions ( 5920 )']]",[],[],[],[],[],sentiment_analysis,39,134
dataset,"Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .","[('masked by', (4, 6)), ('in', (10, 11))]","[('Location entity names', (0, 3)), ('location1 and location 2', (6, 10)), ('whole dataset', (12, 14))]","[['Location entity names', 'masked by', 'location1 and location 2'], ['location1 and location 2', 'in', 'whole dataset']]",[],[],"[['Dataset', 'has', 'Location entity names']]",[],[],[],[],[],sentiment_analysis,39,135
baselines,Logistic Regression,[],"[('Logistic Regression', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Logistic Regression']]",[],[],[],[],[],sentiment_analysis,39,161
baselines,"Many existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .","[('such as', (17, 19)), ('based on', (24, 26))]","[('linguistic features', (26, 28)), ('n-grams', (30, 31)), ('POS information', (32, 34)), ('more hand - engineered features', (35, 40))]","[['linguistic features', 'such as', 'n-grams'], ['linguistic features', 'such as', 'POS information'], ['linguistic features', 'such as', 'more hand - engineered features']]",[],[],[],[],"[['Logistic Regression', 'based on', 'linguistic features']]",[],[],[],sentiment_analysis,39,162
baselines,"More concretely , we define the following sparse representations of locations :","[('define', (4, 5))]","[('sparse representations of locations', (7, 11))]",[],[],[],[],[],"[['Logistic Regression', 'define', 'sparse representations of locations']]",[],[],"[['sparse representations of locations', 'name', 'Mask target entity n-grams'], ['sparse representations of locations', 'name', 'Left - right n- grams'], ['sparse representations of locations', 'name', 'Left right pooling']]",sentiment_analysis,39,164
baselines,Mask target entity n-grams :,[],"[('Mask target entity n-grams', (0, 4))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,39,165
baselines,Left - right n- grams : we create an n-gram representation for both the right and the left context around each location mention .,[],"[('Left - right n- grams', (0, 5))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,39,168
baselines,Left right pooling :,[],"[('Left right pooling', (0, 3))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,39,170
baselines,Long Short - Term Memory ( LSTM ),[],"[('Long Short - Term Memory ( LSTM )', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Long Short - Term Memory ( LSTM )']]",[],[],[],[],[],sentiment_analysis,39,177
baselines,"Inspired by the recent success of applying deep neural networks on language tasks , we use a bidirectional LSTM to learn a classifier for each of the aspects .","[('use', (15, 16)), ('to learn', (19, 21)), ('for', (23, 24))]","[('bidirectional LSTM', (17, 19)), ('classifier', (22, 23)), ('each of the aspects', (24, 28))]","[['bidirectional LSTM', 'to learn', 'classifier'], ['classifier', 'for', 'each of the aspects']]",[],[],[],[],"[['Long Short - Term Memory ( LSTM )', 'use', 'bidirectional LSTM']]",[],[],[],sentiment_analysis,39,178
baselines,Representations for a location ( e l ) are obtained using one of the following two approaches :,"[('for', (1, 2)), ('using', (10, 11))]","[('Representations', (0, 1)), ('location ( e l )', (3, 8))]","[['Representations', 'for', 'location ( e l )']]",[],[],[],[],[],"[['location ( e l )', 'using', 'Final output state ( LSTM - Final )'], ['location ( e l )', 'using', 'Location output state ( LSTM - Location )']]","[['Long Short - Term Memory ( LSTM )', 'has', 'Representations']]",[],sentiment_analysis,39,179
baselines,Final output state ( LSTM - Final ) : e l is the output embedding of the bidirectional LSTM .,[],"[('Final output state ( LSTM - Final )', (0, 8))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,39,180
baselines,Location output state ( LSTM - Location ) :,[],"[('Location output state ( LSTM - Location )', (0, 8))]",[],[],[],[],[],[],[],[],[],sentiment_analysis,39,181
results,"As we can see , the n-gram representation with location masking achieves slightly better results over the left - right context .","[('see', (3, 4)), ('with', (8, 9)), ('achieves', (11, 12)), ('over', (15, 16))]","[('n-gram representation', (6, 8)), ('location masking', (9, 11)), ('slightly better results', (12, 15)), ('left - right context', (17, 21))]","[['n-gram representation', 'with', 'location masking'], ['n-gram representation', 'achieves', 'slightly better results'], ['slightly better results', 'over', 'left - right context']]",[],"[['Results', 'see', 'n-gram representation']]",[],[],[],[],[],[],sentiment_analysis,39,211
results,"Also , by adding POS information , we gain an increase in the performance .","[('by adding', (2, 4)), ('gain', (8, 9)), ('in', (11, 12))]","[('POS information', (4, 6)), ('increase', (10, 11)), ('performance', (13, 14))]","[['POS information', 'gain', 'increase'], ['increase', 'in', 'performance']]",[],"[['Results', 'by adding', 'POS information']]",[],[],[],[],[],[],sentiment_analysis,39,213
results,"Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .","[('Separating', (0, 1)), ('for', (14, 15)), ('does not improve', (18, 21))]","[('left and the right context ( LR - Left - Right )', (2, 14)), ('BoW representation', (15, 17)), ('performance', (22, 23))]","[['left and the right context ( LR - Left - Right )', 'does not improve', 'performance'], ['left and the right context ( LR - Left - Right )', 'for', 'BoW representation']]",[],"[['Results', 'Separating', 'left and the right context ( LR - Left - Right )']]",[],[],[],[],[],[],sentiment_analysis,39,215
results,"Amongst the two variations of LSTM , the model with final state embeddings does slightly better than the model where we use the embeddings at the location index , however they are not significantly different ( with a p valueless than 0.01 ) .",[],[],"[['model', 'with', 'final state embeddings'], ['final state embeddings', 'does', 'slightly better'], ['slightly better', 'than', 'model'], ['model', 'use', 'embeddings at the location index']]","[['two variations of LSTM', 'has', 'model']]","[['Results', 'Amongst', 'two variations of LSTM']]",[],[],[],[],[],[],sentiment_analysis,39,217
results,"It is interesting to note that the best LSTM model is not superior to logistic regression model , especially in terms of AUC .","[('interesting to note', (2, 5)), ('not superior to', (11, 14)), ('in terms of', (19, 22))]","[('best LSTM model', (7, 10)), ('logistic regression model', (14, 17)), ('AUC', (22, 23))]","[['best LSTM model', 'not superior to', 'logistic regression model'], ['best LSTM model', 'in terms of', 'AUC']]",[],"[['Results', 'interesting to note', 'best LSTM model']]",[],[],[],[],[],[],sentiment_analysis,39,218
results,Another interesting observation is that the F 1 measure for logistic regression model with n-grams and POS information is very low while this model 's performance is superior to other models in terms of AUC .,[],[],"[['logistic regression model', 'with', 'n-grams and POS information'], ['logistic regression model', 'interesting observation is that', 'performance'], ['performance', 'is', 'superior'], ['superior', 'to', 'other models'], ['other models', 'in terms of', 'AUC'], ['logistic regression model', 'interesting observation is that', 'F 1 measure'], ['F 1 measure', 'is', 'very low']]",[],"[['Results', 'for', 'logistic regression model']]",[],[],[],[],[],[],sentiment_analysis,39,221
research-problem,ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection,[],"[('Multimodal Emotion Detection', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multimodal Emotion Detection']]",[],[],[],[],sentiment_analysis,4,2
research-problem,Emotion recognition in conversations is crucial for building empathetic machines .,[],"[('Emotion recognition in conversations', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion recognition in conversations']]",[],[],[],[],sentiment_analysis,4,4
research-problem,"Analyzing emotional dynamics in conversations , however , poses complex challenges .",[],"[('Analyzing emotional dynamics in conversations', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Analyzing emotional dynamics in conversations']]",[],[],[],[],sentiment_analysis,4,16
model,"We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .","[('propose', (1, 2)), ('for identifying', (13, 15)), ('in', (16, 17))]","[('Interactive COnversational memory Network ( ICON )', (2, 9)), ('multimodal network', (11, 13)), ('emotions', (15, 16)), ('utterance - videos', (17, 20))]","[['multimodal network', 'for identifying', 'emotions'], ['emotions', 'in', 'utterance - videos']]","[['Interactive COnversational memory Network ( ICON )', 'has', 'multimodal network']]","[['Model', 'propose', 'Interactive COnversational memory Network ( ICON )']]",[],[],[],[],[],[],sentiment_analysis,4,20
model,"First , it extracts multimodal features from all utterancevideos .","[('extracts', (3, 4)), ('from', (6, 7))]","[('multimodal features', (4, 6)), ('all utterancevideos', (7, 9))]","[['multimodal features', 'from', 'all utterancevideos']]",[],"[['Model', 'extracts', 'multimodal features']]",[],[],[],[],[],[],sentiment_analysis,4,28
model,"Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .","[('given', (2, 3)), ('to be', (6, 8)), ('considers', (11, 12)), ('of', (15, 16)), ('falling within', (18, 20)), ('models', (25, 26)), ('using', (31, 32))]","[('test utterance', (4, 6)), ('classified', (8, 9)), ('ICON', (10, 11)), ('preceding utterances', (13, 15)), ('both speakers', (16, 18)), ('context - window', (21, 24)), ('self - emotional influences', (27, 31)), ('local gated recurrent units', (32, 36))]","[['ICON', 'considers', 'preceding utterances'], ['preceding utterances', 'of', 'both speakers'], ['both speakers', 'falling within', 'context - window'], ['ICON', 'models', 'self - emotional influences'], ['self - emotional influences', 'using', 'local gated recurrent units'], ['test utterance', 'to be', 'classified']]","[['test utterance', 'has', 'ICON']]","[['Model', 'given', 'test utterance']]",[],[],[],[],[],[],sentiment_analysis,4,29
model,"Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .","[('to incorporate', (2, 4)), ('is', (11, 12)), ('using', (13, 14)), ('intakes', (17, 18)), ('of', (19, 20))]","[('inter -speaker influences', (4, 7)), ('global representation', (9, 11)), ('generated', (12, 13)), ('GRU', (15, 16)), ('output', (18, 19)), ('local GRUs', (21, 23))]","[['global representation', 'is', 'generated'], ['generated', 'using', 'GRU'], ['GRU', 'intakes', 'output'], ['output', 'of', 'local GRUs']]","[['inter -speaker influences', 'has', 'global representation']]","[['Model', 'to incorporate', 'inter -speaker influences']]",[],[],[],[],[],[],sentiment_analysis,4,30
model,"For each instance in the context - window , the output of this global GRU is stored as a memory cell .","[('For', (0, 1)), ('in', (3, 4)), ('of', (11, 12)), ('stored as', (16, 18))]","[('each instance', (1, 3)), ('context - window', (5, 8)), ('output', (10, 11)), ('global GRU', (13, 15)), ('memory cell', (19, 21))]","[['each instance', 'in', 'context - window'], ['output', 'of', 'global GRU'], ['global GRU', 'stored as', 'memory cell']]","[['context - window', 'has', 'output']]","[['Model', 'For', 'each instance']]",[],[],[],[],[],[],sentiment_analysis,4,31
model,These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .,"[('subjected to', (4, 6)), ('that include', (11, 13)), ('for generating', (15, 17)), ('of', (19, 20))]","[('memories', (1, 2)), ('multiple read / write cycles', (6, 11)), ('attention mechanism', (13, 15)), ('contextual summaries', (17, 19)), ('conversational history', (21, 23))]","[['memories', 'subjected to', 'multiple read / write cycles'], ['multiple read / write cycles', 'that include', 'attention mechanism'], ['attention mechanism', 'for generating', 'contextual summaries'], ['contextual summaries', 'of', 'conversational history']]",[],[],"[['Model', 'has', 'memories']]",[],[],[],[],[],sentiment_analysis,4,32
model,"At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .","[('At', (0, 1)), ('improved with', (11, 13)), ('used for', (18, 20))]","[('each iteration', (1, 3)), ('representation of the test utterance', (5, 10)), ('summary representation', (14, 16)), ('prediction', (20, 21))]","[['representation of the test utterance', 'improved with', 'summary representation'], ['representation of the test utterance', 'used for', 'prediction']]","[['each iteration', 'has', 'representation of the test utterance']]","[['Model', 'At', 'each iteration']]",[],[],[],[],[],[],sentiment_analysis,4,33
experimental-setup,20 % of the training set is used as validation set for hyper - parameter tuning .,"[('used as', (7, 9)), ('for', (11, 12))]","[('20 % of the training set', (0, 6)), ('validation set', (9, 11)), ('hyper - parameter tuning', (12, 16))]","[['20 % of the training set', 'used as', 'validation set'], ['validation set', 'for', 'hyper - parameter tuning']]",[],[],"[['Experimental setup', 'has', '20 % of the training set']]",[],[],[],[],[],sentiment_analysis,4,219
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .","[('use', (1, 2)), ('for', (12, 13)), ('starting with', (16, 18)), ('of', (22, 23))]","[('Adam optimizer ( Kingma and Ba , 2014 )', (3, 12)), ('training', (13, 14)), ('parameters', (15, 16)), ('initial learning rate', (19, 22)), ('0.001', (23, 24))]","[['Adam optimizer ( Kingma and Ba , 2014 )', 'for', 'training'], ['parameters', 'starting with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]","[['training', 'has', 'parameters']]","[['Experimental setup', 'use', 'Adam optimizer ( Kingma and Ba , 2014 )']]",[],[],[],[],[],[],sentiment_analysis,4,220
experimental-setup,Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .,[],[],"[['Termination', 'of', 'training - phase'], ['training - phase', 'decided by', 'early - stopping'], ['early - stopping', 'with', 'patience'], ['patience', 'of', '10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs']]",[],[],"[['Experimental setup', 'has', 'Termination']]",[],[],[],[],[],sentiment_analysis,4,221
experimental-setup,The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .,"[('subjected to', (3, 5)), ('in the form of', (6, 10)), ('for', (15, 16)), ('of', (18, 19))]","[('network', (1, 2)), ('regularization', (5, 6)), ('Dropout and Gradient - clipping', (10, 15)), ('norm', (17, 18)), ('40', (19, 20))]","[['network', 'subjected to', 'regularization'], ['regularization', 'for', 'norm'], ['norm', 'of', '40'], ['regularization', 'in the form of', 'Dropout and Gradient - clipping']]",[],[],"[['Experimental setup', 'has', 'network']]",[],[],[],[],[],sentiment_analysis,4,222
experimental-setup,"Finally , the best hyper - parameters are decided using a gridsearch .","[('decided using', (8, 10))]","[('best hyper - parameters', (3, 7)), ('gridsearch', (11, 12))]","[['best hyper - parameters', 'decided using', 'gridsearch']]",[],[],"[['Experimental setup', 'has', 'best hyper - parameters']]",[],[],[],[],[],sentiment_analysis,4,223
experimental-setup,"For multimodal feature extraction , we explore different designs for the employed CNNs .","[('For', (0, 1)), ('explore', (6, 7)), ('for', (9, 10))]","[('multimodal feature extraction', (1, 4)), ('different designs', (7, 9)), ('employed CNNs', (11, 13))]","[['multimodal feature extraction', 'explore', 'different designs'], ['different designs', 'for', 'employed CNNs']]",[],"[['Experimental setup', 'For', 'multimodal feature extraction']]",[],[],[],[],[],[],sentiment_analysis,4,225
experimental-setup,"For text , we find the single layer CNN to perform at par with deeper variants .","[('find', (4, 5)), ('to perform at par with', (9, 14))]","[('text', (1, 2)), ('single layer CNN', (6, 9)), ('deeper variants', (14, 16))]","[['text', 'find', 'single layer CNN'], ['single layer CNN', 'to perform at par with', 'deeper variants']]",[],[],"[['Experimental setup', 'For', 'text']]",[],[],[],[],[],sentiment_analysis,4,226
experimental-setup,"For visual features , however , a deeper CNN provides better representations .","[('provides', (9, 10))]","[('visual features', (1, 3)), ('deeper CNN', (7, 9)), ('better representations', (10, 12))]","[['deeper CNN', 'provides', 'better representations']]","[['visual features', 'has', 'deeper CNN']]",[],"[['Experimental setup', 'For', 'visual features']]",[],[],[],[],[],sentiment_analysis,4,227
experimental-setup,We also find that contextually conditioned features perform better than context - less features .,"[('find', (2, 3)), ('perform better than', (7, 10))]","[('contextually conditioned features', (4, 7)), ('context - less features', (10, 14))]","[['contextually conditioned features', 'perform better than', 'context - less features']]",[],"[['Experimental setup', 'find', 'contextually conditioned features']]",[],[],[],[],[],[],sentiment_analysis,4,228
baselines,memnet is an end - toend memory network .,"[('is', (1, 2))]","[('memnet', (0, 1)), ('end - toend memory network', (3, 8))]","[['memnet', 'is', 'end - toend memory network']]",[],[],"[['Baselines', 'has', 'memnet']]",[],[],[],[],[],sentiment_analysis,4,233
baselines,cLSTM 4 classifies utterances using neighboring utterances ( of same speaker ) as context .,"[('classifies', (2, 3)), ('using', (4, 5)), ('as', (12, 13))]","[('cLSTM', (0, 1)), ('utterances', (3, 4)), ('neighboring utterances ( of same speaker )', (5, 12)), ('context', (13, 14))]","[['cLSTM', 'classifies', 'utterances'], ['utterances', 'using', 'neighboring utterances ( of same speaker )'], ['neighboring utterances ( of same speaker )', 'as', 'context']]",[],[],"[['Baselines', 'has', 'cLSTM']]",[],[],[],[],[],sentiment_analysis,4,235
baselines,"TFN 5 models intra-and intermodality dynamics by explicitly aggregating uni - , bi- and trimodal interactions .","[('models', (2, 3)), ('by explicitly aggregating', (6, 9))]","[('TFN', (0, 1)), ('intra-and intermodality dynamics', (3, 6)), ('uni - , bi- and trimodal interactions', (9, 16))]","[['TFN', 'models', 'intra-and intermodality dynamics'], ['intra-and intermodality dynamics', 'by explicitly aggregating', 'uni - , bi- and trimodal interactions']]",[],[],"[['Baselines', 'has', 'TFN']]",[],[],[],[],[],sentiment_analysis,4,237
baselines,"MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .","[('performs', (1, 2)), ('by using', (4, 6)), ('to learn', (15, 17))]","[('MFN', (0, 1)), ('multi-view learning', (2, 4)), ('Delta - memory Attention Network', (6, 11)), ('fusion mechanism', (13, 15)), ('cross - view interactions', (17, 21))]","[['MFN', 'performs', 'multi-view learning'], ['multi-view learning', 'by using', 'Delta - memory Attention Network'], ['fusion mechanism', 'to learn', 'cross - view interactions']]","[['Delta - memory Attention Network', 'has', 'fusion mechanism']]",[],"[['Baselines', 'has', 'MFN']]",[],[],[],[],[],sentiment_analysis,4,239
baselines,CMN models separate contexts for both speaker and listener to an utterance .,"[('separate', (2, 3)), ('for', (4, 5)), ('to an', (9, 11))]","[('CMN models', (0, 2)), ('contexts', (3, 4)), ('speaker and listener', (6, 9)), ('utterance', (11, 12))]","[['CMN models', 'separate', 'contexts'], ['contexts', 'to an', 'utterance'], ['contexts', 'for', 'speaker and listener']]",[],[],"[['Baselines', 'has', 'CMN models']]",[],[],[],[],[],sentiment_analysis,4,241
results,ICON performs better than the compared models with significant performance increase in emotions ( ? 2.1 % acc. ) .,"[('performs better than', (1, 4)), ('with', (7, 8)), ('in', (11, 12))]","[('ICON', (0, 1)), ('compared models', (5, 7)), ('significant performance increase', (8, 11)), ('emotions ( ? 2.1 % acc. )', (12, 19))]","[['ICON', 'performs better than', 'compared models'], ['compared models', 'with', 'significant performance increase'], ['significant performance increase', 'in', 'emotions ( ? 2.1 % acc. )']]",[],[],"[['Results', 'has', 'ICON']]",[],[],[],[],[],sentiment_analysis,4,246
results,"For each emotion , ICON outperforms all the compared models except for happiness emotion .","[('For', (0, 1)), ('outperforms', (5, 6)), ('except for', (10, 12))]","[('each emotion', (1, 3)), ('ICON', (4, 5)), ('all the compared models', (6, 10)), ('happiness emotion', (12, 14))]","[['ICON', 'outperforms', 'all the compared models'], ['all the compared models', 'except for', 'happiness emotion']]","[['each emotion', 'has', 'ICON']]","[['Results', 'For', 'each emotion']]",[],[],[],[],[],"[['ICON', 'has', 'performance']]",sentiment_analysis,4,247
results,"However , its performance is still at par with c LSTM without a significant gap .","[('at par with', (6, 9)), ('without', (11, 12))]","[('performance', (3, 4)), ('c LSTM', (9, 11)), ('significant gap', (13, 15))]","[['performance', 'at par with', 'c LSTM'], ['performance', 'without', 'significant gap']]",[],[],[],[],[],[],[],[],sentiment_analysis,4,248
results,"Also , ICON manages to correctly identify the relatively similar excitement emotion by a large margin .","[('manages to', (3, 5)), ('correctly', (5, 6)), ('by', (12, 13))]","[('identify', (6, 7)), ('relatively similar excitement emotion', (8, 12)), ('large margin', (14, 16))]","[['identify', 'correctly', 'relatively similar excitement emotion'], ['relatively similar excitement emotion', 'by', 'large margin']]",[],[],[],[],"[['ICON', 'manages to', 'identify']]",[],[],[],sentiment_analysis,4,249
results,"In all the labels , ICON attains improved performance over its counterparts , suggesting the efficacy of its context - modeling scheme .","[('In', (0, 1)), ('attains', (6, 7)), ('over', (9, 10))]","[('labels', (3, 4)), ('ICON', (5, 6)), ('improved performance', (7, 9)), ('counterparts', (11, 12))]","[['ICON', 'attains', 'improved performance'], ['improved performance', 'over', 'counterparts']]","[['labels', 'has', 'ICON']]","[['Results', 'In', 'labels']]",[],[],[],[],[],[],sentiment_analysis,4,251
results,presents the results for different combinations of modes used by ICON on IEMOCAP .,"[('presents', (0, 1)), ('used by', (8, 10)), ('on', (11, 12))]","[('different combinations of modes', (4, 8)), ('ICON', (10, 11)), ('IEMOCAP', (12, 13))]","[['different combinations of modes', 'used by', 'ICON'], ['ICON', 'on', 'IEMOCAP']]",[],"[['Results', 'presents', 'different combinations of modes']]",[],[],[],[],[],[],sentiment_analysis,4,263
results,"As seen , the trimodal network provides the best performance which is preceded by the bimodal variants .","[('provides', (6, 7)), ('preceded by', (12, 14))]","[('trimodal network', (4, 6)), ('best performance', (8, 10)), ('bimodal variants', (15, 17))]","[['trimodal network', 'provides', 'best performance'], ['best performance', 'preceded by', 'bimodal variants']]",[],[],"[['Results', 'has', 'trimodal network']]",[],[],[],[],[],sentiment_analysis,4,264
results,"Among unimodals , language modality performs the best , reaffirming its significance in multimodal systems .","[('Among', (0, 1)), ('performs', (5, 6))]","[('unimodals', (1, 2)), ('language modality', (3, 5)), ('best', (7, 8))]","[['language modality', 'performs', 'best']]","[['unimodals', 'has', 'language modality']]","[['Results', 'Among', 'unimodals']]",[],[],[],[],[],[],sentiment_analysis,4,265
results,"Interestingly , the audio and visual modality , on their own , do not provide good performance , but when used with text , complementary data is shared to improve over all performance .","[('when used with', (19, 22)), ('shared', (27, 28)), ('to improve', (28, 30))]","[('audio and visual modality', (3, 7)), ('text', (22, 23)), ('complementary data', (24, 26)), ('over all performance', (30, 33))]","[['audio and visual modality', 'when used with', 'text'], ['text', 'shared', 'complementary data'], ['complementary data', 'to improve', 'over all performance']]",[],[],"[['Results', 'has', 'audio and visual modality']]",[],[],[],[],[],sentiment_analysis,4,266
ablation-analysis,Self vs Dual History :,[],"[('Self vs Dual History', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Self vs Dual History']]",[],[],[],[],[],sentiment_analysis,4,272
ablation-analysis,"Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , these models provide lesser performance .","[('Compared to', (0, 2)), ('provide', (19, 20))]","[('dual - history variants ( variants 3 , 5 , and 7 )', (3, 16)), ('these models', (17, 19)), ('lesser performance', (20, 22))]","[['these models', 'provide', 'lesser performance']]","[['dual - history variants ( variants 3 , 5 , and 7 )', 'has', 'these models']]",[],[],[],"[['Self vs Dual History', 'Compared to', 'dual - history variants ( variants 3 , 5 , and 7 )']]",[],[],[],sentiment_analysis,4,274
ablation-analysis,DGIM prevents the storage of dynamic influences between speakers at each historical time step and leads to performance deterioration .,"[('prevents', (1, 2)), ('between', (7, 8)), ('at each', (9, 11)), ('leads to', (15, 17))]","[('DGIM', (0, 1)), ('storage of dynamic influences', (3, 7)), ('speakers', (8, 9)), ('historical time step', (11, 14)), ('performance deterioration', (17, 19))]","[['DGIM', 'prevents', 'storage of dynamic influences'], ['storage of dynamic influences', 'between', 'speakers'], ['speakers', 'leads to', 'performance deterioration'], ['speakers', 'at each', 'historical time step']]",[],[],"[['Ablation analysis', 'has', 'DGIM']]",[],[],[],[],[],sentiment_analysis,4,277
ablation-analysis,"Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .","[('than', (30, 31))]","[('Multi - hop vs No - hop', (0, 7)), ('multi-hop', (15, 16))]",[],"[['Multi - hop vs No - hop', 'removal of', 'multi-hop']]",[],"[['Ablation analysis', 'has', 'Multi - hop vs No - hop']]",[],[],[],[],[],sentiment_analysis,4,278
ablation-analysis,"Also , removal of multi-hop leads to worse performance than the removal of DGIM .","[('removal of', (2, 4)), ('leads to', (5, 7))]","[('worse performance', (7, 9)), ('removal of DGIM', (11, 14))]",[],"[['worse performance', 'than', 'removal of DGIM']]",[],[],[],"[['multi-hop', 'leads to', 'worse performance']]",[],[],[],sentiment_analysis,4,279
ablation-analysis,"However , best performance is achieved by variant 6 which contains all the proposed modules in its pipeline .","[('achieved by', (5, 7)), ('which contains', (9, 11)), ('in', (15, 16))]","[('best performance', (2, 4)), ('variant 6', (7, 9)), ('all the proposed modules', (11, 15)), ('its pipeline', (16, 18))]","[['best performance', 'achieved by', 'variant 6'], ['variant 6', 'which contains', 'all the proposed modules'], ['all the proposed modules', 'in', 'its pipeline']]",[],[],[],[],[],[],"[['Multi - hop vs No - hop', 'has', 'best performance']]",[],sentiment_analysis,4,281
research-problem,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,[],"[('Aspect Sentiment Analysis', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Sentiment Analysis']]",[],[],[],[],sentiment_analysis,40,2
model,"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .","[('propose', (5, 6)), ('to solve', (9, 11)), ('in', (14, 15))]","[('novel framework', (7, 9)), ('problems', (13, 14)), ('target sentiment analysis', (15, 18))]","[['novel framework', 'to solve', 'problems'], ['problems', 'in', 'target sentiment analysis']]",[],"[['Model', 'propose', 'novel framework']]",[],[],[],[],[],[],sentiment_analysis,40,30
model,"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .","[('first adopts', (4, 6)), ('to produce', (12, 14)), ('from', (27, 28))]","[('framework', (3, 4)), ('bidirectional LSTM ( BLSTM )', (7, 12)), ('memory ( i.e. the states of time steps generated by LSTM )', (15, 27)), ('input', (29, 30))]","[['framework', 'first adopts', 'bidirectional LSTM ( BLSTM )'], ['bidirectional LSTM ( BLSTM )', 'to produce', 'memory ( i.e. the states of time steps generated by LSTM )'], ['memory ( i.e. the states of time steps generated by LSTM )', 'from', 'input']]",[],[],"[['Model', 'has', 'framework']]",[],[],[],[],[],sentiment_analysis,40,31
model,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .","[('weighted according to', (5, 8)), ('to', (11, 12))]","[('memory slices', (1, 3)), ('relative positions', (9, 11)), ('target', (13, 14))]","[['memory slices', 'weighted according to', 'relative positions'], ['relative positions', 'to', 'target']]",[],[],"[['Model', 'has', 'memory slices']]",[],[],[],[],[],sentiment_analysis,40,32
model,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .","[('pay', (4, 5)), ('on', (7, 8)), ('nonlinearly combine', (14, 16)), ('with', (19, 20))]","[('multiple attentions', (5, 7)), ('position - weighted memory', (9, 13)), ('attention results', (17, 19)), ('recurrent network , i.e. GRUs', (21, 26))]","[['attention results', 'with', 'recurrent network , i.e. GRUs'], ['multiple attentions', 'on', 'position - weighted memory']]",[],"[['Model', 'nonlinearly combine', 'attention results'], ['Model', 'pay', 'multiple attentions']]",[],[],[],[],[],[],sentiment_analysis,40,33
model,"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .",[],[],"[['softmax', 'to predict', 'sentiment'], ['sentiment', 'on', 'target'], ['softmax', 'on', 'output'], ['output', 'of', 'GRU network']]",[],"[['Model', 'apply', 'softmax']]",[],[],[],[],[],[],sentiment_analysis,40,34
model,Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,"[('introduces', (2, 3)), ('of applying', (6, 8)), ('to synthesize', (12, 14)), ('in', (16, 17))]","[('Our framework', (0, 2)), ('novel way', (4, 6)), ('multiple - attention mechanism', (8, 12)), ('important features', (14, 16)), ('difficult sentence structures', (17, 20))]","[['Our framework', 'introduces', 'novel way'], ['novel way', 'of applying', 'multiple - attention mechanism'], ['multiple - attention mechanism', 'to synthesize', 'important features'], ['important features', 'in', 'difficult sentence structures']]",[],[],"[['Model', 'has', 'Our framework']]",[],[],[],[],[],sentiment_analysis,40,35
baselines,Average Context :,[],"[('Average Context', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Average Context']]",[],[],[],[],"[['Average Context', 'has', 'first one'], ['Average Context', 'has', 'second one']]",sentiment_analysis,40,149
baselines,There are two versions of this method .,"[('of', (4, 5))]",[],[],[],[],[],[],[],[],[],"[['word vectors', 'of', 'full context']]",sentiment_analysis,40,150
baselines,"The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .",[],[],"[['first one', 'named', 'AC - S'], ['first one', 'averages', 'word vectors'], ['word vectors', 'before', 'target'], ['word vectors', 'after', 'target']]",[],[],[],[],"[['second one', 'averages', 'word vectors']]",[],[],[],sentiment_analysis,40,151
baselines,"The second one , named AC , averages the word vectors of the full context .",[],[],"[['second one', 'named', 'AC']]",[],[],[],[],[],[],[],[],sentiment_analysis,40,152
baselines,"SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .","[('on', (14, 15))]","[('SVM', (0, 1)), ('surface features , lexicon features and parsing features', (15, 23))]","[['SVM', 'on', 'surface features , lexicon features and parsing features']]",[],[],"[['Baselines', 'has', 'SVM']]",[],[],[],[],[],sentiment_analysis,40,153
baselines,"Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .","[('firstly uses', (5, 7)), ('to transform', (8, 10)), ('put', (14, 15)), ('at', (18, 19)), ('performs', (24, 25)), ('with', (27, 28)), ('for', (30, 31))]","[('Rec - NN', (0, 3)), ('rules', (7, 8)), ('dependency tree', (11, 13)), ('opinion target', (16, 18)), ('root', (20, 21)), ('semantic composition', (25, 27)), ('Recursive NNs', (28, 30)), ('sentiment prediction', (31, 33))]","[['Rec - NN', 'performs', 'semantic composition'], ['semantic composition', 'with', 'Recursive NNs'], ['Recursive NNs', 'for', 'sentiment prediction'], ['Rec - NN', 'firstly uses', 'rules'], ['rules', 'to transform', 'dependency tree'], ['Rec - NN', 'put', 'opinion target'], ['opinion target', 'at', 'root']]",[],[],"[['Baselines', 'has', 'Rec - NN']]",[],[],[],[],[],sentiment_analysis,40,154
baselines,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .,"[('uses', (4, 5)), ('to abstract', (12, 14)), ('before and after', (16, 19))]","[('TD- LSTM', (0, 2)), ('forward LSTM and a backward LSTM', (6, 12)), ('information', (15, 16)), ('target', (20, 21))]","[['TD- LSTM', 'uses', 'forward LSTM and a backward LSTM'], ['forward LSTM and a backward LSTM', 'to abstract', 'information'], ['information', 'before and after', 'target']]",[],[],"[['Baselines', 'has', 'TD- LSTM']]",[],[],[],[],[],sentiment_analysis,40,155
baselines,TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vec,"[('developed', (7, 8)), ('have', (14, 15)), ('on', (17, 18))]","[('TD - LSTM - A', (0, 5)), ('TD - LSTM', (8, 11)), ('one attention', (15, 17)), ('outputs', (19, 20))]","[['TD - LSTM - A', 'developed', 'TD - LSTM'], ['TD - LSTM', 'have', 'one attention'], ['one attention', 'on', 'outputs']]",[],[],"[['Baselines', 'has', 'TD - LSTM - A']]",[],[],[],[],[],sentiment_analysis,40,158
baselines,"MemNet : It applies attention multiple times on the word embeddings , and the last attention 's output is fed to softmax for prediction , without combining the results of different attentions .","[('applies', (3, 4)), ('on', (7, 8)), ('fed to', (19, 21)), ('for', (22, 23))]","[('MemNet', (0, 1)), ('attention', (4, 5)), ('multiple times', (5, 7)), ('word embeddings', (9, 11)), (""last attention 's output"", (14, 18)), ('softmax', (21, 22)), ('prediction', (23, 24))]","[['MemNet', 'applies', 'attention'], ['attention', 'on', 'word embeddings'], [""last attention 's output"", 'fed to', 'softmax'], ['softmax', 'for', 'prediction']]","[['attention', 'has', 'multiple times'], ['MemNet', 'has', ""last attention 's output""]]",[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,40,160
results,"As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .","[('consistently outperforms', (9, 11))]","[('our RAM', (7, 9)), ('compared methods', (12, 14))]","[['our RAM', 'consistently outperforms', 'compared methods']]",[],[],"[['Results', 'has', 'our RAM']]",[],[],[],[],[],sentiment_analysis,40,167
results,"AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .","[('perform', (5, 6))]","[('AC and AC - S', (0, 5)), ('poorly', (6, 7))]","[['AC and AC - S', 'perform', 'poorly']]",[],[],"[['Results', 'has', 'AC and AC - S']]",[],[],[],[],[],sentiment_analysis,40,168
results,Rec - NN is better than TD - LSTM but not as good as our method .,"[('better than', (4, 6)), ('not as good as', (10, 14))]","[('Rec - NN', (0, 3)), ('TD - LSTM', (6, 9)), ('our method', (14, 16))]","[['Rec - NN', 'better than', 'TD - LSTM'], ['Rec - NN', 'not as good as', 'our method']]",[],[],"[['Results', 'has', 'Rec - NN']]",[],[],[],[],[],sentiment_analysis,40,169
results,"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .","[('performs', (3, 4)), ('than', (6, 7))]","[('TD - LSTM', (0, 3)), ('less competitive', (4, 6)), ('our method', (7, 9))]","[['TD - LSTM', 'performs', 'less competitive'], ['less competitive', 'than', 'our method']]",[],[],"[['Results', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,40,172
results,"TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .","[('performs', (6, 7)), ('than', (8, 9))]","[('TD - LSTM - A', (0, 5)), ('worse', (7, 8)), ('our method', (9, 11))]","[['TD - LSTM - A', 'performs', 'worse'], ['worse', 'than', 'our method']]",[],[],"[['Results', 'has', 'TD - LSTM - A']]",[],[],[],[],[],sentiment_analysis,40,173
results,"MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .","[('adopts', (1, 2)), ('in order to improve', (4, 8))]","[('MemNet', (0, 1)), ('multiple attentions', (2, 4)), ('attention results', (9, 11))]","[['MemNet', 'adopts', 'multiple attentions'], ['multiple attentions', 'in order to improve', 'attention results']]",[],[],"[['Results', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,40,175
research-problem,Attention - based LSTM for Aspect - level Sentiment Classification,[],"[('Aspect - level Sentiment Classification', (5, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level Sentiment Classification']]",[],[],[],[],sentiment_analysis,41,2
research-problem,Aspect - level sentiment classification is a finegrained task in sentiment analysis .,[],"[('sentiment analysis', (10, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,41,4
research-problem,"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .",[],"[('aspect - level sentiment classification', (7, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect - level sentiment classification']]",[],[],[],[],sentiment_analysis,41,15
model,"In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .","[('propose', (5, 6)), ('to enforce', (9, 11)), ('to attend', (13, 15))]","[('attention mechanism', (7, 9)), ('model', (12, 13)), ('important part of a sentence', (17, 22))]","[['attention mechanism', 'to attend', 'important part of a sentence'], ['attention mechanism', 'to enforce', 'model']]",[],"[['Model', 'propose', 'attention mechanism']]",[],[],[],[],[],[],sentiment_analysis,41,24
model,We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .,"[('design', (1, 2)), ('can concentrate on', (9, 12)), ('of', (15, 16)), ('given', (18, 19))]","[('aspect - tosentence attention mechanism', (3, 8)), ('key part', (13, 15)), ('sentence', (17, 18)), ('aspect', (20, 21))]","[['aspect - tosentence attention mechanism', 'can concentrate on', 'key part'], ['key part', 'given', 'aspect'], ['key part', 'of', 'sentence']]",[],"[['Model', 'design', 'aspect - tosentence attention mechanism']]",[],[],[],[],[],[],sentiment_analysis,41,25
model,We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .,"[('explore', (1, 2)), ('of', (5, 6)), ('in', (10, 11))]","[('potential correlation', (3, 5)), ('aspect and sentiment polarity', (6, 10)), ('aspect - level sentiment classification', (11, 16))]","[['potential correlation', 'of', 'aspect and sentiment polarity'], ['aspect and sentiment polarity', 'in', 'aspect - level sentiment classification']]",[],"[['Model', 'explore', 'potential correlation']]",[],[],[],[],[],[],sentiment_analysis,41,26
model,"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .","[('to capture', (2, 4)), ('in response to', (6, 9))]","[('important information', (4, 6)), ('given aspect', (10, 12)), ('design', (14, 15)), ('attentionbased LSTM', (16, 18))]","[['important information', 'in response to', 'given aspect']]","[['design', 'has', 'attentionbased LSTM']]","[['Model', 'to capture', 'important information'], ['Model', 'to capture', 'design']]",[],[],[],[],[],[],sentiment_analysis,41,27
hyperparameters,We apply the proposed model to aspect - level sentiment classification .,"[('apply', (1, 2)), ('to', (5, 6))]","[('proposed model', (3, 5)), ('aspect - level sentiment classification', (6, 11))]","[['proposed model', 'to', 'aspect - level sentiment classification']]",[],"[['Hyperparameters', 'apply', 'proposed model']]",[],[],[],[],[],[],sentiment_analysis,41,146
hyperparameters,"In our experiments , all word vectors are initialized by Glove 1 .","[('In', (0, 1)), ('are', (7, 8)), ('by', (9, 10))]","[('our experiments', (1, 3)), ('all word vectors', (4, 7)), ('initialized', (8, 9)), ('Glove', (10, 11))]","[['all word vectors', 'are', 'initialized'], ['initialized', 'by', 'Glove']]","[['our experiments', 'has', 'all word vectors']]","[['Hyperparameters', 'In', 'our experiments']]",[],[],[],[],[],[],sentiment_analysis,41,147
hyperparameters,The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion .,"[('pre-trained on', (5, 7)), ('whose', (10, 11)), ('is', (12, 13))]","[('word embedding vectors', (1, 4)), ('unlabeled corpus', (8, 10)), ('size', (11, 12)), ('about 840 billion', (13, 16))]","[['word embedding vectors', 'pre-trained on', 'unlabeled corpus'], ['unlabeled corpus', 'whose', 'size'], ['size', 'is', 'about 840 billion']]",[],[],"[['Hyperparameters', 'has', 'word embedding vectors']]",[],[],[],[],[],sentiment_analysis,41,148
hyperparameters,"The other parameters are initialized by sampling from a uniform distribution U (?? , ? ) .","[('initialized by', (4, 6)), ('from', (7, 8))]","[('other parameters', (1, 3)), ('sampling', (6, 7)), ('uniform distribution U (?? , ? )', (9, 16))]","[['other parameters', 'initialized by', 'sampling'], ['sampling', 'from', 'uniform distribution U (?? , ? )']]",[],[],"[['Hyperparameters', 'has', 'other parameters']]",[],[],[],[],[],sentiment_analysis,41,149
hyperparameters,"The dimension of word vectors , aspect embeddings and the size of hidden layer are 300 .","[('of', (2, 3)), ('are', (14, 15))]","[('dimension', (1, 2)), ('word vectors', (3, 5)), ('aspect embeddings', (6, 8)), ('size of hidden layer', (10, 14)), ('300', (15, 16))]","[['dimension', 'are', '300'], ['300', 'of', 'word vectors'], ['300', 'of', 'aspect embeddings'], ['300', 'of', 'size of hidden layer']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,41,150
hyperparameters,The length of attention weights is the same as the length of sentence .,[],[],"[['length', 'of', 'attention weights'], ['attention weights', 'same as', 'length'], ['length', 'of', 'sentence']]",[],[],"[['Hyperparameters', 'has', 'length']]",[],[],[],[],[],sentiment_analysis,41,151
hyperparameters,Theano is used for implementing our neural network models .,"[('used for', (2, 4))]","[('Theano', (0, 1)), ('implementing', (4, 5)), ('neural network models', (6, 9))]","[['Theano', 'used for', 'implementing']]","[['implementing', 'has', 'neural network models']]",[],"[['Hyperparameters', 'has', 'Theano']]",[],[],[],[],[],sentiment_analysis,41,152
hyperparameters,"We trained all models with a batch size of 25 examples , and a momentum of 0.9 , L 2 - regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad .",[],[],"[['all models', 'with', 'L 2 - regularization weight'], ['L 2 - regularization weight', 'of', '0.001'], ['all models', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.01'], ['0.01', 'for', 'AdaGrad'], ['all models', 'with', 'batch size'], ['batch size', 'of', '25 examples'], ['all models', 'with', 'momentum'], ['momentum', 'of', '0.9']]",[],"[['Hyperparameters', 'trained', 'all models']]",[],[],[],[],[],[],sentiment_analysis,41,153
results,"LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .","[('can not capture', (4, 7)), ('in', (10, 11))]","[('LSTM', (0, 1)), ('Standard LSTM', (2, 4)), ('aspect information', (8, 10)), ('sentence', (11, 12))]","[['Standard LSTM', 'can not capture', 'aspect information'], ['aspect information', 'in', 'sentence']]","[['LSTM', 'has', 'Standard LSTM']]",[],"[['Results', 'has', 'LSTM']]",[],[],[],[],[],sentiment_analysis,41,176
results,"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .","[('can not take advantage of', (2, 7))]","[('aspect information', (8, 10)), ('worst performance', (16, 18))]",[],"[['aspect information', 'has', 'worst performance']]",[],[],[],"[['LSTM', 'can not take advantage of', 'aspect information']]",[],[],[],sentiment_analysis,41,182
results,TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .,"[('can improve', (7, 9)), ('of', (11, 12)), ('by treating', (14, 16)), ('as', (18, 19))]","[('TD - LSTM', (0, 3)), ('performance', (10, 11)), ('sentiment classifier', (12, 14)), ('aspect', (17, 18)), ('target', (20, 21))]","[['TD - LSTM', 'can improve', 'performance'], ['performance', 'of', 'sentiment classifier'], ['performance', 'by treating', 'aspect'], ['aspect', 'as', 'target']]",[],[],"[['Results', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,41,183
results,"Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .","[('there is no', (1, 4)), ('in', (6, 7)), ('can not', (12, 14)), ('which', (17, 18)), ('are', (19, 20)), ('for', (21, 22))]","[('attention mechanism', (4, 6)), ('TD - LSTM', (7, 10)), ('know', (15, 16)), ('words', (18, 19)), ('important', (20, 21)), ('given aspect', (23, 25))]","[['TD - LSTM', 'there is no', 'attention mechanism'], ['attention mechanism', 'in', 'TD - LSTM'], ['TD - LSTM', 'can not', 'know'], ['know', 'which', 'words'], ['words', 'are', 'important'], ['important', 'for', 'given aspect']]",[],[],[],[],[],[],[],[],sentiment_analysis,41,184
results,It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .,"[('performs', (8, 9)), ('than', (10, 11))]","[('TC - LSTM', (5, 8)), ('worse', (9, 10)), ('LSTM and TD - LSTM', (11, 16))]","[['TC - LSTM', 'performs', 'worse'], ['worse', 'than', 'LSTM and TD - LSTM']]",[],[],"[['Results', 'has', 'TC - LSTM']]",[],[],[],[],[],sentiment_analysis,41,186
results,"ATAE - LSTM not only addresses the shortcoming of the unconformity between word vectors and aspect embeddings , but also can capture the most important information in response to a given aspect .","[('addresses', (5, 6)), ('of', (8, 9)), ('between', (11, 12)), ('capture', (21, 22)), ('in response to', (26, 29))]","[('ATAE - LSTM', (0, 3)), ('shortcoming', (7, 8)), ('unconformity', (10, 11)), ('word vectors and aspect embeddings', (12, 17)), ('most important information', (23, 26)), ('given aspect', (30, 32))]","[['ATAE - LSTM', 'addresses', 'shortcoming'], ['shortcoming', 'of', 'unconformity'], ['unconformity', 'between', 'word vectors and aspect embeddings'], ['ATAE - LSTM', 'capture', 'most important information'], ['most important information', 'in response to', 'given aspect']]",[],[],"[['Results', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,41,190
research-problem,Aspect Level Sentiment Classification with Deep Memory Network,[],"[('Aspect Level Sentiment Classification', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,42,2
research-problem,Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .,[],"[('sentiment analysis', (12, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,42,12
approach,"In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .","[('develop', (7, 8)), ('for', (11, 12))]","[('deep memory network', (8, 11)), ('aspect level sentiment classification', (12, 16))]","[['deep memory network', 'for', 'aspect level sentiment classification']]",[],"[['Approach', 'develop', 'deep memory network']]",[],[],[],[],[],[],sentiment_analysis,42,25
approach,"Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .","[('is', (2, 3))]","[('data - driven', (3, 6)), ('computationally efficient', (7, 9))]",[],[],"[['Approach', 'is', 'data - driven'], ['Approach', 'is', 'computationally efficient']]",[],[],[],[],[],[],sentiment_analysis,42,26
approach,The approach consists of multiple computational layers with shared parameters .,"[('consists of', (2, 4)), ('with', (7, 8))]","[('multiple computational layers', (4, 7)), ('shared parameters', (8, 10))]","[['multiple computational layers', 'with', 'shared parameters']]",[],"[['Approach', 'consists of', 'multiple computational layers']]",[],[],[],[],[],[],sentiment_analysis,42,27
approach,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .","[('is', (2, 3)), ('first learns', (14, 16)), ('of', (20, 21)), ('utilizes', (26, 27)), ('to calculate', (29, 31))]","[('Each layer', (0, 2)), ('content - and location - based attention model', (4, 12)), ('importance / weight', (17, 20)), ('each context word', (21, 24)), ('information', (28, 29)), ('continuous text representation', (31, 34))]","[['Each layer', 'is', 'content - and location - based attention model'], ['content - and location - based attention model', 'first learns', 'importance / weight'], ['importance / weight', 'of', 'each context word'], ['content - and location - based attention model', 'utilizes', 'information'], ['information', 'to calculate', 'continuous text representation']]",[],[],"[['Approach', 'has', 'Each layer']]",[],[],[],[],[],sentiment_analysis,42,28
approach,The text representation in the last layer is regarded as the feature for sentiment classification .,"[('in', (3, 4)), ('regarded as', (8, 10)), ('for', (12, 13))]","[('text representation', (1, 3)), ('last layer', (5, 7)), ('feature', (11, 12)), ('sentiment classification', (13, 15))]","[['text representation', 'in', 'last layer'], ['last layer', 'regarded as', 'feature'], ['feature', 'for', 'sentiment classification']]",[],[],"[['Approach', 'has', 'text representation']]",[],[],[],[],[],sentiment_analysis,42,29
approach,"As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .",[],[],"[['every component', 'is', 'differentiable'], ['entire model', 'could be efficiently trained', 'end - toend'], ['end - toend', 'with', 'gradient descent'], ['gradient descent', 'where', 'loss function'], ['loss function', 'is', 'cross - entropy error'], ['cross - entropy error', 'of', 'sentiment classification']]","[['every component', 'has', 'entire model']]",[],"[['Approach', 'has', 'every component']]",[],[],[],[],[],sentiment_analysis,42,30
baselines,"( 1 ) Majority is a basic baseline method , which assigns the majority sentiment label in training set to each instance in the test set .",[],[],"[['Majority', 'is', 'basic baseline method'], ['Majority', 'assigns', 'majority sentiment label'], ['majority sentiment label', 'in', 'training set'], ['training set', 'to each', 'instance'], ['instance', 'in', 'test set']]",[],[],"[['Baselines', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,42,158
baselines,( 2 ) Feature - based SVM performs state - of - the - art on aspect level sentiment classification .,"[('performs', (7, 8)), ('on', (15, 16))]","[('Feature - based SVM', (3, 7)), ('state - of - the - art', (8, 15)), ('aspect level sentiment classification', (16, 20))]","[['Feature - based SVM', 'performs', 'state - of - the - art'], ['state - of - the - art', 'on', 'aspect level sentiment classification']]",[],[],"[['Baselines', 'has', 'Feature - based SVM']]",[],[],[],[],[],sentiment_analysis,42,159
baselines,"( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .",[],"[('three LSTM models', (6, 9))]",[],[],[],"[['Baselines', 'has', 'three LSTM models']]",[],[],[],[],"[['three LSTM models', 'In', 'TDLSTM'], ['three LSTM models', 'In', 'TDLSTM + ATT']]",sentiment_analysis,42,161
baselines,"In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .","[('In', (0, 1)), ('applied from', (9, 11)), ('to', (13, 14)), ('of', (16, 17)), ('used as', (26, 28))]","[('LSTM', (1, 2)), ('LSTM based recurrent model', (4, 8)), ('start', (12, 13)), ('end', (15, 16)), ('sentence', (18, 19)), ('last hidden vector', (22, 25)), ('sentence representation', (29, 31))]","[['LSTM based recurrent model', 'applied from', 'start'], ['start', 'to', 'end'], ['end', 'of', 'sentence'], ['last hidden vector', 'used as', 'sentence representation']]","[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]",[],[],[],"[['three LSTM models', 'In', 'LSTM']]",[],[],[],sentiment_analysis,42,162
baselines,"TDLSTM extends LSTM by taking into account of the aspect , and uses two LSTM networks , a forward one and a backward one , towards the aspect .",[],[],"[['TDLSTM', 'extends', 'LSTM'], ['TDLSTM', 'by taking into account', 'aspect'], ['TDLSTM', 'uses', 'two LSTM networks'], ['two LSTM networks', 'towards', 'aspect']]","[['two LSTM networks', 'has', 'forward one and a backward one']]",[],[],[],[],[],[],[],sentiment_analysis,42,163
baselines,"TDLSTM + ATT extends TDLSTM by incorporating an attention mechanism ( Bahdanau et al. , 2015 ) over the hidden vectors .","[('extends', (3, 4)), ('by incorporating', (5, 7)), ('over', (17, 18))]","[('TDLSTM + ATT', (0, 3)), ('TDLSTM', (4, 5)), ('attention mechanism', (8, 10)), ('hidden vectors', (19, 21))]","[['TDLSTM + ATT', 'by incorporating', 'attention mechanism'], ['attention mechanism', 'over', 'hidden vectors'], ['TDLSTM + ATT', 'extends', 'TDLSTM']]",[],[],[],[],[],[],[],[],sentiment_analysis,42,164
baselines,We use the same Glove word vectors for fair comparison .,"[('use', (1, 2)), ('for', (7, 8))]","[('same Glove word vectors', (3, 7)), ('fair comparison', (8, 10))]","[['same Glove word vectors', 'for', 'fair comparison']]",[],"[['Baselines', 'use', 'same Glove word vectors']]",[],[],[],[],[],[],sentiment_analysis,42,165
baselines,"( 4 ) We also implement ContextAVG , a simplistic version of our approach .","[('implement', (5, 6)), ('of', (11, 12))]","[('ContextAVG', (6, 7)), ('simplistic version', (9, 11)), ('our approach', (12, 14))]","[['simplistic version', 'of', 'our approach']]","[['ContextAVG', 'has', 'simplistic version']]","[['Baselines', 'implement', 'ContextAVG']]",[],[],[],[],[],[],sentiment_analysis,42,166
results,"We can find that feature - based SVM is an extremely strong performer and substantially outperforms other baseline methods , which demonstrates the importance of a powerful feature representation for aspect level sentiment classification .","[('find that', (2, 4)), ('is', (8, 9)), ('substantially outperforms', (14, 16))]","[('feature - based SVM', (4, 8)), ('extremely strong performer', (10, 13)), ('other baseline methods', (16, 19))]","[['feature - based SVM', 'substantially outperforms', 'other baseline methods'], ['feature - based SVM', 'is', 'extremely strong performer']]",[],"[['Results', 'find that', 'feature - based SVM']]",[],[],[],[],[],[],sentiment_analysis,42,171
results,"Among three recurrent models , TDLSTM performs better than LSTM , which indicates that taking into account of the aspect information is helpful .","[('Among', (0, 1)), ('performs', (6, 7)), ('than', (8, 9))]","[('three recurrent models', (1, 4)), ('TDLSTM', (5, 6)), ('better', (7, 8)), ('LSTM', (9, 10))]","[['TDLSTM', 'performs', 'better'], ['better', 'than', 'LSTM']]","[['three recurrent models', 'has', 'TDLSTM']]","[['Results', 'Among', 'three recurrent models']]",[],[],[],[],[],[],sentiment_analysis,42,172
results,We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,[],[],"[['each hidden vector', 'of', 'TDLSTM'], ['each hidden vector', 'encodes', 'semantics'], ['semantics', 'of', 'word sequence'], ['word sequence', 'until', 'current position']]",[],"[['Results', 'consider', 'each hidden vector']]",[],[],[],[],[],[],sentiment_analysis,42,175
results,"We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .","[('of', (7, 8)), ('is', (11, 12))]","[('performance', (6, 7)), ('Contex - tAVG', (8, 11)), ('very poor', (12, 14))]","[['performance', 'of', 'Contex - tAVG'], ['Contex - tAVG', 'is', 'very poor']]",[],[],"[['Results', 'find that', 'performance']]",[],[],[],[],[],sentiment_analysis,42,178
results,"Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .","[('using', (15, 16)), ('lead to', (21, 23)), ('especially when', (26, 28)), ('is', (32, 33))]","[('observe that', (13, 15)), ('more computational layers', (16, 19)), ('better performance', (23, 25)), ('number of hops', (29, 32)), ('less than six', (33, 36))]","[['observe that', 'using', 'more computational layers'], ['more computational layers', 'lead to', 'better performance'], ['better performance', 'especially when', 'number of hops'], ['number of hops', 'is', 'less than six']]",[],[],"[['Results', 'Among', 'observe that']]",[],[],[],[],[],sentiment_analysis,42,179
results,"The best performances are achieved when the model contains seven and nine hops , respectively .","[('achieved when', (4, 6)), ('contains', (8, 9))]","[('best performances', (1, 3)), ('model', (7, 8)), ('seven and nine hops', (9, 13))]","[['best performances', 'achieved when', 'model'], ['model', 'contains', 'seven and nine hops']]",[],[],"[['Results', 'has', 'best performances']]",[],[],[],[],[],sentiment_analysis,42,180
results,"On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .","[('On', (0, 1)), ('could obtain', (7, 9)), ('compared to', (11, 13))]","[('both datasets', (1, 3)), ('proposed approach', (5, 7)), ('comparable accuracy', (9, 11)), ('state - of - art feature - based SVM system', (14, 24))]","[['proposed approach', 'could obtain', 'comparable accuracy'], ['comparable accuracy', 'compared to', 'state - of - art feature - based SVM system']]","[['both datasets', 'has', 'proposed approach']]","[['Results', 'On', 'both datasets']]",[],[],[],[],[],[],sentiment_analysis,42,181
results,We can find that using multiple computational layers could consistently improve the classification accuracy in all these models .,"[('consistently improve', (9, 11)), ('in', (14, 15))]","[('multiple computational layers', (5, 8)), ('classification accuracy', (12, 14)), ('all these models', (15, 18))]","[['multiple computational layers', 'consistently improve', 'classification accuracy'], ['classification accuracy', 'in', 'all these models']]",[],[],"[['Results', 'find that', 'multiple computational layers']]",[],[],[],[],[],sentiment_analysis,42,202
results,All these models perform comparably when the number of hops is larger than five .,"[('perform', (3, 4)), ('when', (5, 6)), ('is', (10, 11))]","[('All these models', (0, 3)), ('comparably', (4, 5)), ('number of hops', (7, 10)), ('larger than five', (11, 14))]","[['All these models', 'perform', 'comparably'], ['comparably', 'when', 'number of hops'], ['number of hops', 'is', 'larger than five']]",[],[],"[['Results', 'has', 'All these models']]",[],[],[],[],[],sentiment_analysis,42,203
research-problem,Multi - grained Attention Network for Aspect - Level Sentiment Classification,[],"[('Aspect - Level Sentiment Classification', (6, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Level Sentiment Classification']]",[],[],[],[],sentiment_analysis,43,2
research-problem,We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,[],"[('aspect level sentiment classification', (12, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect level sentiment classification']]",[],[],[],[],sentiment_analysis,43,4
model,"In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .","[('propose', (5, 6))]","[('multi -grained attention network', (7, 11))]",[],[],"[['Model', 'propose', 'multi -grained attention network']]",[],[],[],[],[],[],sentiment_analysis,43,29
model,"Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .","[('to characterize', (23, 25)), ('between', (30, 31)), ('relieve', (37, 38)), ('occurred in', (41, 43))]","[('fine - grained attention mechanism', (5, 10)), ('word - level interactions', (26, 30)), ('aspect and context words', (31, 35)), ('information loss', (39, 41)), ('coarse - grained attention mechanism', (43, 48))]","[['fine - grained attention mechanism', 'to characterize', 'word - level interactions'], ['word - level interactions', 'between', 'aspect and context words'], ['fine - grained attention mechanism', 'relieve', 'information loss'], ['information loss', 'occurred in', 'coarse - grained attention mechanism']]",[],[],"[['Model', 'propose', 'fine - grained attention mechanism']]",[],[],[],[],[],sentiment_analysis,43,30
model,"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .","[('utilize', (4, 5)), ('combine them with', (19, 22)), ('to compose', (25, 27)), ('for', (31, 32))]","[('bidirectional coarsegrained attention', (6, 9)), ('finegrained attention vectors', (22, 25)), ('multigrained attention network', (28, 31)), ('final sentiment polarity prediction', (33, 37))]","[['bidirectional coarsegrained attention', 'combine them with', 'finegrained attention vectors'], ['finegrained attention vectors', 'to compose', 'multigrained attention network'], ['multigrained attention network', 'for', 'final sentiment polarity prediction']]",[],"[['Model', 'utilize', 'bidirectional coarsegrained attention']]",[],[],[],[],[],[],sentiment_analysis,43,31
model,"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .","[('in', (3, 4)), ('to make use of', (5, 9)), ('design', (18, 19)), ('to enhance', (27, 29)), ('of', (31, 32)), ('towards', (35, 36)), ('which have', (38, 40))]","[('valuable aspect - level interaction information', (10, 16)), ('aspect alignment loss', (20, 23)), ('objective function', (25, 27)), ('difference', (30, 31)), ('attention weights', (33, 35)), ('aspects', (37, 38)), ('same context', (41, 43)), ('different sentiment polarities', (44, 47))]","[['valuable aspect - level interaction information', 'design', 'aspect alignment loss'], ['aspect alignment loss', 'to enhance', 'difference'], ['difference', 'of', 'attention weights'], ['difference', 'towards', 'aspects'], ['aspects', 'which have', 'same context'], ['aspects', 'which have', 'different sentiment polarities'], ['aspect alignment loss', 'in', 'objective function']]",[],"[['Model', 'to make use of', 'valuable aspect - level interaction information']]",[],[],[],[],[],[],sentiment_analysis,43,32
hyperparameters,"In our experiments , word embeddings for both context and aspect words are initialized by Glove .","[('for both', (6, 8)), ('initialized by', (13, 15))]","[('word embeddings', (4, 6)), ('context and aspect words', (8, 12)), ('Glove', (15, 16))]","[['word embeddings', 'for both', 'context and aspect words'], ['context and aspect words', 'initialized by', 'Glove']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",[],[],[],[],[],sentiment_analysis,43,171
hyperparameters,The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,"[('of', (2, 3))]","[('dimension', (1, 2)), ('word embedding d v', (3, 7)), ('hidden stated', (8, 10))]",[],[],[],"[['Hyperparameters', 'has', 'dimension']]",[],"[['300', 'of', 'word embedding d v'], ['300', 'of', 'hidden stated']]",[],[],[],sentiment_analysis,43,172
hyperparameters,set to 300 .,"[('set to', (0, 2))]","[('300', (2, 3))]",[],[],[],[],[],"[['dimension', 'set to', '300']]",[],[],[],sentiment_analysis,43,173
hyperparameters,"The weight matrix and bias are initialized by sampling from a uniform distribution U ( 0.01 , 0.01 ) .","[('are', (5, 6)), ('by', (7, 8)), ('from', (9, 10))]","[('weight matrix and bias', (1, 5)), ('initialized', (6, 7)), ('sampling', (8, 9)), ('uniform distribution U ( 0.01 , 0.01 )', (11, 19))]","[['weight matrix and bias', 'are', 'initialized'], ['initialized', 'by', 'sampling'], ['sampling', 'from', 'uniform distribution U ( 0.01 , 0.01 )']]",[],[],"[['Hyperparameters', 'has', 'weight matrix and bias']]",[],[],[],[],[],sentiment_analysis,43,174
hyperparameters,"The coefficient ? of L 2 regularization item is 10 ? 5 , the parameter ?","[('of', (3, 4)), ('is', (8, 9))]","[('coefficient', (1, 2)), ('L 2 regularization', (4, 7)), ('10 ? 5', (9, 12))]","[['coefficient', 'of', 'L 2 regularization'], ['L 2 regularization', 'is', '10 ? 5']]",[],[],"[['Hyperparameters', 'has', 'coefficient']]",[],[],[],[],[],sentiment_analysis,43,175
hyperparameters,of aspect alignment loss and dropout rate are set to 0.5 .,"[('set to', (8, 10))]","[('dropout rate', (5, 7)), ('0.5', (10, 11))]","[['dropout rate', 'set to', '0.5']]",[],[],"[['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],sentiment_analysis,43,176
baselines,"Majority is the basic baseline , which chooses the largest sentiment polarity in the training set to each instance in the test set .",[],[],"[['Majority', 'is', 'basic baseline'], ['Majority', 'chooses', 'largest sentiment polarity'], ['largest sentiment polarity', 'in', 'training set'], ['training set', 'to each', 'instance'], ['instance', 'in', 'test set']]",[],[],"[['Baselines', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,43,179
baselines,"MemNet applys multi-hop attentions on the word embeddings , learns the attention weights on context word vectors with respect to the averaged query vector .",[],[],"[['MemNet', 'applys', 'multi-hop attentions'], ['multi-hop attentions', 'on', 'word embeddings'], ['MemNet', 'learns', 'attention weights'], ['attention weights', 'on', 'context word vectors'], ['context word vectors', 'with respect to', 'averaged query vector']]",[],[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,43,180
baselines,"IAN interactively learns the coarse - grained attentions between the context and aspect , and concatenate the vectors for prediction .","[('interactively learns', (1, 3)), ('between', (8, 9)), ('concatenate', (15, 16)), ('for', (18, 19))]","[('IAN', (0, 1)), ('coarse - grained attentions', (4, 8)), ('context and aspect', (10, 13)), ('vectors', (17, 18)), ('prediction', (19, 20))]","[['IAN', 'interactively learns', 'coarse - grained attentions'], ['coarse - grained attentions', 'concatenate', 'vectors'], ['vectors', 'for', 'prediction'], ['coarse - grained attentions', 'between', 'context and aspect']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,43,181
baselines,"BILSTM - ATT -G ( Liu and Zhang , 2017 ) models left and right context with two attention - based LSTMs and utilizes gates to control the importance of left context , right context and the entire sentence for prediction .","[('models', (11, 12)), ('with', (16, 17)), ('utilizes', (23, 24)), ('to control', (25, 27)), ('of', (29, 30)), ('for', (39, 40))]","[('BILSTM - ATT -G', (0, 4)), ('left and right context', (12, 16)), ('two attention - based LSTMs', (17, 22)), ('gates', (24, 25)), ('importance', (28, 29)), ('left context', (30, 32)), ('right context', (33, 35)), ('entire sentence', (37, 39)), ('prediction', (40, 41))]","[['BILSTM - ATT -G', 'models', 'left and right context'], ['left and right context', 'with', 'two attention - based LSTMs'], ['BILSTM - ATT -G', 'utilizes', 'gates'], ['gates', 'to control', 'importance'], ['importance', 'of', 'left context'], ['importance', 'of', 'right context'], ['importance', 'of', 'entire sentence'], ['entire sentence', 'for', 'prediction']]",[],[],"[['Baselines', 'has', 'BILSTM - ATT -G']]",[],[],[],[],[],sentiment_analysis,43,182
baselines,"RAM learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words , and proposes to use GRU network to get the aggregated vector from the attentions .","[('learns', (1, 2)), ('on', (4, 5)), ('of', (8, 9)), ('for', (12, 13)), ('proposes to use', (17, 20)), ('to get', (22, 24)), ('from', (27, 28))]","[('RAM', (0, 1)), ('multi-hop attentions', (2, 4)), ('hidden states', (6, 8)), ('bidirectional LSTM networks', (9, 12)), ('context words', (13, 15)), ('GRU network', (20, 22)), ('aggregated vector', (25, 27)), ('attentions', (29, 30))]","[['RAM', 'proposes to use', 'GRU network'], ['GRU network', 'to get', 'aggregated vector'], ['aggregated vector', 'from', 'attentions'], ['RAM', 'learns', 'multi-hop attentions'], ['multi-hop attentions', 'on', 'hidden states'], ['hidden states', 'of', 'bidirectional LSTM networks'], ['bidirectional LSTM networks', 'for', 'context words']]",[],[],"[['Baselines', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,43,183
baselines,"MGAN - C only employs the coarse - grained attentions for prediction , which is similar with IAN .","[('employs', (4, 5)), ('for', (10, 11))]","[('MGAN - C', (0, 3)), ('coarse - grained attentions', (6, 10)), ('prediction', (11, 12))]","[['MGAN - C', 'employs', 'coarse - grained attentions'], ['coarse - grained attentions', 'for', 'prediction']]",[],[],"[['Baselines', 'has', 'MGAN - C']]",[],[],[],[],[],sentiment_analysis,43,186
baselines,MGAN - F only utilizes the proposed fine - grained attentions for prediction .,"[('utilizes', (4, 5)), ('for', (11, 12))]","[('MGAN - F', (0, 3)), ('proposed fine - grained attentions', (6, 11)), ('prediction', (12, 13))]","[['MGAN - F', 'utilizes', 'proposed fine - grained attentions'], ['proposed fine - grained attentions', 'for', 'prediction']]",[],[],"[['Baselines', 'has', 'MGAN - F']]",[],[],[],[],[],sentiment_analysis,43,187
baselines,"MGAN - CF adopts both the coarse - grained and fine - grained attentions , while without applying the aspect alignment loss .","[('adopts both', (3, 5))]","[('MGAN - CF', (0, 3)), ('coarse - grained and fine - grained attentions', (6, 14))]","[['MGAN - CF', 'adopts both', 'coarse - grained and fine - grained attentions']]",[],[],"[['Baselines', 'has', 'MGAN - CF']]",[],[],[],[],[],sentiment_analysis,43,188
baselines,MGAN is the complete multi-grained attention network model .,"[('is', (1, 2))]","[('MGAN', (0, 1)), ('complete multi-grained attention network model', (3, 8))]","[['MGAN', 'is', 'complete multi-grained attention network model']]",[],[],"[['Baselines', 'has', 'MGAN']]",[],[],[],[],[],sentiment_analysis,43,189
results,( 1 ) Majority performs worst since it only utilizes the data distribution information .,"[('performs', (4, 5))]","[('Majority', (3, 4)), ('worst', (5, 6))]","[['Majority', 'performs', 'worst']]",[],[],"[['Results', 'has', 'Majority']]",[],[],[],[],[],sentiment_analysis,43,193
results,Our method MGAN outperforms Majority and Feature + SVM since MGAN could learn the high quality representation for prediction .,"[('outperforms', (3, 4))]","[('MGAN', (2, 3)), ('Majority and Feature + SVM', (4, 9))]","[['MGAN', 'outperforms', 'Majority and Feature + SVM']]",[],[],"[['Results', 'has', 'MGAN']]",[],[],[],[],[],sentiment_analysis,43,195
results,( 2 ) ATAE - LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation .,"[('is', (6, 7)), ('than', (8, 9))]","[('ATAE - LSTM', (3, 6)), ('better', (7, 8)), ('LSTM', (9, 10))]","[['ATAE - LSTM', 'is', 'better'], ['better', 'than', 'LSTM']]",[],[],"[['Results', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,43,196
results,"TD - LSTM performs slightly better than ATAE - LSTM , and it employs two LSTM networks to capture the left and right context of the aspect .","[('performs', (3, 4)), ('than', (6, 7))]","[('TD - LSTM', (0, 3)), ('slightly better', (4, 6)), ('ATAE - LSTM', (7, 10))]","[['TD - LSTM', 'performs', 'slightly better'], ['slightly better', 'than', 'ATAE - LSTM']]",[],[],"[['Results', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,43,197
results,TD - LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context .,"[('than', (5, 6))]","[('worse', (4, 5)), ('our method MGAN', (6, 9))]","[['worse', 'than', 'our method MGAN']]",[],[],[],[],[],[],"[['TD - LSTM', 'performs', 'worse']]",[],sentiment_analysis,43,198
results,"( 3 ) IAN achieves slightly better results with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .","[('achieves', (4, 5)), ('with', (8, 9))]","[('IAN', (3, 4)), ('slightly better results', (5, 8)), ('previous LSTM - based methods', (10, 15))]","[['IAN', 'achieves', 'slightly better results'], ['slightly better results', 'with', 'previous LSTM - based methods']]",[],[],"[['Results', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,43,199
results,Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .,"[('consistently performs', (2, 4)), ('than', (5, 6))]","[('Our method', (0, 2)), ('better', (4, 5)), ('IAN', (6, 7))]","[['Our method', 'consistently performs', 'better'], ['better', 'than', 'IAN']]",[],[],"[['Results', 'has', 'Our method']]",[],[],[],[],[],sentiment_analysis,43,200
results,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .","[('models', (5, 6)), ('using', (11, 12)), ('achieves', (18, 19)), ('than', (21, 22))]","[('BILSTM - ATT - G', (0, 5)), ('left context and right context', (6, 11)), ('attention - based LSTMs', (12, 16)), ('better performance', (19, 21)), ('MemNet', (22, 23))]","[['BILSTM - ATT - G', 'models', 'left context and right context'], ['left context and right context', 'using', 'attention - based LSTMs'], ['BILSTM - ATT - G', 'achieves', 'better performance'], ['better performance', 'than', 'MemNet']]",[],[],"[['Results', 'has', 'BILSTM - ATT - G']]",[],[],[],[],[],sentiment_analysis,43,202
results,RAM performs better than other baselines .,"[('performs', (1, 2)), ('than', (3, 4))]","[('RAM', (0, 1)), ('better', (2, 3)), ('other baselines', (4, 6))]","[['RAM', 'performs', 'better'], ['better', 'than', 'other baselines']]",[],[],"[['Results', 'has', 'RAM']]",[],[],[],[],[],sentiment_analysis,43,203
results,"Our proposed MGAN consistently performs better than MemNet , BILSTM - ATT - G and RAM on all three datasets .","[('consistently performs', (3, 5)), ('than', (6, 7))]","[('Our proposed MGAN', (0, 3)), ('better', (5, 6)), ('MemNet', (7, 8)), ('BILSTM - ATT - G', (9, 14)), ('RAM', (15, 16))]","[['Our proposed MGAN', 'consistently performs', 'better'], ['better', 'than', 'MemNet'], ['better', 'than', 'BILSTM - ATT - G'], ['better', 'than', 'RAM']]",[],[],"[['Results', 'has', 'Our proposed MGAN']]",[],[],[],[],[],sentiment_analysis,43,206
research-problem,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,[],"[('Sentence Level Discourse Parsing and Sentiment Analysis', (5, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentence Level Discourse Parsing and Sentiment Analysis']]",[],[],[],[],sentiment_analysis,44,2
model,Our framework consists of three main sub parts .,"[('consists of', (2, 4))]","[('framework', (1, 2)), ('three main sub parts', (4, 8))]","[['framework', 'consists of', 'three main sub parts']]",[],[],"[['Model', 'has', 'framework']]",[],[],[],[],[],sentiment_analysis,44,107
model,"Given a segmented sentence , the first step is to create meaningful vector representations for all the EDUs .","[('Given', (0, 1)), ('to create', (9, 11)), ('for', (14, 15))]","[('segmented sentence', (2, 4)), ('first step', (6, 8)), ('meaningful vector representations', (11, 14)), ('all the EDUs', (15, 18))]","[['first step', 'to create', 'meaningful vector representations'], ['meaningful vector representations', 'for', 'all the EDUs']]","[['segmented sentence', 'has', 'first step']]","[['Model', 'Given', 'segmented sentence']]",[],[],[],[],[],[],sentiment_analysis,44,108
model,"Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , discourse relation prediction and sentiment analysis .","[('devise', (3, 4)), ('designed for', (12, 14)), ('of', (15, 16))]","[('three different Recursive Neural Net models', (4, 10)), ('one', (14, 15)), ('discourse structure prediction', (16, 19)), ('discourse relation prediction', (20, 23)), ('sentiment analysis', (24, 26))]","[['three different Recursive Neural Net models', 'designed for', 'one'], ['one', 'of', 'discourse structure prediction'], ['one', 'of', 'discourse relation prediction'], ['one', 'of', 'sentiment analysis']]",[],"[['Model', 'devise', 'three different Recursive Neural Net models']]",[],[],[],[],[],[],sentiment_analysis,44,109
model,"Finally , we join these Neural Nets in two different ways : Multitasking and Pre-training .","[('join', (3, 4)), ('in', (7, 8))]","[('Neural Nets', (5, 7)), ('two different ways', (8, 11)), ('Multitasking', (12, 13)), ('Pre-training', (14, 15))]","[['Neural Nets', 'in', 'two different ways']]","[['two different ways', 'name', 'Multitasking'], ['two different ways', 'name', 'Pre-training']]","[['Model', 'join', 'Neural Nets']]",[],[],[],[],[],[],sentiment_analysis,44,110
hyperparameters,All the neural models presented in this paper were implemented using the Tensor Flow python pack - .,"[('implemented using', (9, 11))]","[('Tensor Flow python pack', (12, 16))]",[],[],"[['Hyperparameters', 'implemented using', 'Tensor Flow python pack']]",[],[],[],[],[],[],sentiment_analysis,44,161
hyperparameters,We minimize the crossentropy error using the Adam optimizer and L2regularization on the set of weights .,"[('minimize', (1, 2)), ('using', (5, 6)), ('on', (11, 12))]","[('crossentropy error', (3, 5)), ('Adam optimizer', (7, 9)), ('L2regularization', (10, 11)), ('set of weights', (13, 16))]","[['crossentropy error', 'using', 'Adam optimizer'], ['crossentropy error', 'using', 'L2regularization'], ['L2regularization', 'on', 'set of weights']]",[],"[['Hyperparameters', 'minimize', 'crossentropy error']]",[],[],[],[],[],[],sentiment_analysis,44,162
hyperparameters,"For the individual models ( before joining ) , we use 200 training epochs and a batch size of 100 .","[('For', (0, 1)), ('use', (10, 11)), ('of', (18, 19))]","[('individual models ( before joining )', (2, 8)), ('200 training epochs', (11, 14)), ('batch size', (16, 18)), ('100', (19, 20))]","[['individual models ( before joining )', 'use', '200 training epochs'], ['individual models ( before joining )', 'use', 'batch size'], ['batch size', 'of', '100']]",[],"[['Hyperparameters', 'For', 'individual models ( before joining )']]",[],[],[],[],[],[],sentiment_analysis,44,163
results,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .","[('see', (5, 6)), ('on', (8, 9)), ('when we are using', (12, 16)), ('is', (22, 23)), ('only for', (25, 27))]","[('some improvement', (6, 8)), ('Discourse Structure prediction', (9, 12)), ('joint model', (17, 19)), ('improvement', (21, 22)), ('statistically significant', (23, 25)), ('Nuclearity and Relation predictions', (28, 32))]","[['some improvement', 'on', 'Discourse Structure prediction'], ['Discourse Structure prediction', 'when we are using', 'joint model'], ['improvement', 'is', 'statistically significant'], ['statistically significant', 'only for', 'Nuclearity and Relation predictions']]","[['Discourse Structure prediction', 'has', 'improvement']]","[['Results', 'see', 'some improvement']]",[],[],[],[],[],[],sentiment_analysis,44,169
results,"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .","[('on', (2, 3)), ('mainly on', (7, 9)), ('specifically', (13, 14))]","[('improvements', (1, 2)), ('Relation predictions', (4, 6)), ('Contrastive set', (10, 12)), ('Contrast', (17, 18)), ('Comparison', (19, 20)), ('Cause', (21, 22))]","[['improvements', 'on', 'Relation predictions'], ['Relation predictions', 'mainly on', 'Contrastive set'], ['Contrastive set', 'specifically', 'Contrast'], ['Contrastive set', 'specifically', 'Comparison'], ['Contrastive set', 'specifically', 'Cause']]",[],[],"[['Results', 'has', 'improvements']]",[],[],[],[],[],sentiment_analysis,44,170
results,In the fine grained setting we compute the accuracy of exact match across five classes .,"[('In', (0, 1)), ('compute', (6, 7)), ('of', (9, 10)), ('across', (12, 13))]","[('fine grained setting', (2, 5)), ('accuracy', (8, 9)), ('exact match', (10, 12)), ('five classes', (13, 15))]","[['fine grained setting', 'compute', 'accuracy'], ['accuracy', 'across', 'five classes'], ['accuracy', 'of', 'exact match']]",[],"[['Results', 'In', 'fine grained setting']]",[],[],[],[],[],[],sentiment_analysis,44,178
research-problem,Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence,[],"[('Aspect - Based Sentiment Analysis', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - Based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,45,2
research-problem,"Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .",[],"[('Aspect - based sentiment analysis ( ABSA )', (0, 8)), ('sentiment analysis ( SA )', (28, 33))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis ( ABSA )'], ['Contribution', 'has research problem', 'sentiment analysis ( SA )']]",[],[],[],[],sentiment_analysis,45,4
research-problem,"Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .",[],"[('SA', (1, 2)), ('ABSA', (3, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'SA'], ['Contribution', 'has research problem', 'ABSA']]",[],[],[],[],sentiment_analysis,45,16
research-problem,"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .",[],"[('targeted aspect - based sentiment analysis ( TABSA )', (6, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'targeted aspect - based sentiment analysis ( TABSA )']]",[],[],[],[],sentiment_analysis,45,17
approach,"In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .","[('investigate', (5, 6)), ('of', (8, 9)), ('an', (10, 11)), ('transform', (14, 15)), ('into', (19, 20))]","[('several methods', (6, 8)), ('constructing', (9, 10)), ('auxiliary sentence', (11, 13)), ('( T ) ABSA', (15, 19)), ('sentence - pair classification task', (21, 26))]","[['( T ) ABSA', 'into', 'sentence - pair classification task'], ['several methods', 'of', 'constructing'], ['constructing', 'an', 'auxiliary sentence']]",[],"[['Approach', 'transform', '( T ) ABSA'], ['Approach', 'investigate', 'several methods']]",[],[],[],[],[],[],sentiment_analysis,45,27
approach,We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .,"[('fine - tune', (1, 4)), ('from', (7, 8))]","[('pre-trained model', (5, 7)), ('BERT', (8, 9))]","[['pre-trained model', 'from', 'BERT']]",[],"[['Approach', 'fine - tune', 'pre-trained model']]",[],[],[],[],[],[],sentiment_analysis,45,28
hyperparameters,We use the pre-trained uncased BERT - base model 5 for fine - tuning .,"[('use', (1, 2)), ('for', (10, 11))]","[('pre-trained uncased BERT - base model', (3, 9)), ('fine - tuning', (11, 14))]","[['pre-trained uncased BERT - base model', 'for', 'fine - tuning']]",[],"[['Hyperparameters', 'use', 'pre-trained uncased BERT - base model']]",[],[],[],[],[],[],sentiment_analysis,45,100
hyperparameters,"The number of Transformer blocks is 12 , the hidden layer size is 768 , the number of self - attention heads is 12 , and the total number of parameters for the pretrained model is 110M .",[],[],"[['number of self - attention heads', 'is', '12'], ['total number of parameters', 'for', 'pretrained model'], ['total number of parameters', 'is', '110M'], ['hidden layer size', 'is', '768'], ['number of Transformer blocks', 'is', '12']]",[],[],"[['Hyperparameters', 'has', 'number of self - attention heads'], ['Hyperparameters', 'has', 'total number of parameters'], ['Hyperparameters', 'has', 'hidden layer size'], ['Hyperparameters', 'has', 'number of Transformer blocks']]",[],[],[],[],[],sentiment_analysis,45,101
hyperparameters,"the dropout probability at 0.1 , set the number of epochs to 4 .","[('at', (3, 4)), ('set', (6, 7)), ('to', (11, 12))]","[('dropout probability', (1, 3)), ('0.1', (4, 5)), ('number of epochs', (8, 11)), ('4', (12, 13))]","[['number of epochs', 'to', '4'], ['dropout probability', 'at', '0.1']]",[],"[['Hyperparameters', 'set', 'number of epochs']]","[['Hyperparameters', 'has', 'dropout probability']]",[],[],[],[],[],sentiment_analysis,45,105
hyperparameters,"The initial learning rate is 2 e - 5 , and the batch size is 24 .",[],[],"[['initial learning rate', 'is', '2 e - 5'], ['batch size', 'is', '24']]",[],[],"[['Hyperparameters', 'has', 'initial learning rate'], ['Hyperparameters', 'has', 'batch size']]",[],[],[],[],[],sentiment_analysis,45,106
baselines,LR : a logistic regression classifier with n-gram and pos-tag features .,"[('with', (6, 7))]","[('LR', (0, 1)), ('logistic regression classifier', (3, 6)), ('n-gram and pos-tag features', (7, 11))]","[['logistic regression classifier', 'with', 'n-gram and pos-tag features']]","[['LR', 'has', 'logistic regression classifier']]",[],"[['Baselines', 'has', 'LR']]",[],[],[],[],[],sentiment_analysis,45,109
baselines,LSTM - Final ) : a biLSTM model with the final state as a representation .,"[('with', (8, 9))]","[('LSTM - Final', (0, 3)), ('biLSTM model', (6, 8)), ('final state as a representation', (10, 15))]","[['biLSTM model', 'with', 'final state as a representation']]","[['LSTM - Final', 'has', 'biLSTM model']]",[],"[['Baselines', 'has', 'LSTM - Final']]",[],[],[],[],[],sentiment_analysis,45,110
baselines,LSTM - Loc ) : a biLSTM model with the state associated with the target position as a representation .,"[('with', (8, 9)), ('associated with', (11, 13))]","[('LSTM - Loc', (0, 3)), ('biLSTM model', (6, 8)), ('state', (10, 11)), ('target position as a representation', (14, 19))]","[['biLSTM model', 'with', 'state'], ['state', 'associated with', 'target position as a representation']]","[['LSTM - Loc', 'has', 'biLSTM model']]",[],"[['Baselines', 'has', 'LSTM - Loc']]",[],[],[],[],[],sentiment_analysis,45,111
baselines,LSTM + TA + SA ) : a biLSTM model which introduces complex target - level and sentence - level attention mechanisms .,"[('introduces', (11, 12))]","[('LSTM + TA + SA', (0, 5)), ('biLSTM model', (8, 10)), ('complex target - level and sentence - level attention mechanisms', (12, 22))]","[['biLSTM model', 'introduces', 'complex target - level and sentence - level attention mechanisms']]","[['LSTM + TA + SA', 'has', 'biLSTM model']]",[],"[['Baselines', 'has', 'LSTM + TA + SA']]",[],[],[],[],[],sentiment_analysis,45,112
baselines,SenticLSTM : an upgraded version of the LSTM + TA + SA model which introduces external information from Sentic - Net .,"[('of', (5, 6)), ('introduces', (14, 15))]","[('SenticLSTM', (0, 1)), ('upgraded version', (3, 5)), ('LSTM + TA + SA model', (7, 13)), ('external information from Sentic - Net', (15, 21))]","[['upgraded version', 'of', 'LSTM + TA + SA model'], ['LSTM + TA + SA model', 'introduces', 'external information from Sentic - Net']]","[['SenticLSTM', 'has', 'upgraded version']]",[],"[['Baselines', 'has', 'SenticLSTM']]",[],[],[],[],[],sentiment_analysis,45,113
baselines,"Dmu - Entnet : a bidirectional EntNet with external "" memory chains "" with a delayed memory update mechanism to track entities .",[],[],"[['bidirectional EntNet', 'with', 'external "" memory chains ""'], ['external "" memory chains ""', 'with', 'delayed memory update mechanism'], ['delayed memory update mechanism', 'to track', 'entities']]","[['Dmu - Entnet', 'has', 'bidirectional EntNet']]",[],"[['Baselines', 'has', 'Dmu - Entnet']]",[],[],[],[],[],sentiment_analysis,45,114
results,"We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .",[],[],"[['BERT - single', 'achieved', 'better results'], ['BERT - pair', 'achieved', 'further improvements'], ['further improvements', 'over', 'BERT - single']]",[],[],"[['Results', 'has', 'BERT - single'], ['Results', 'has', 'BERT - pair']]",[],[],[],[],[],sentiment_analysis,45,120
results,The BERT - pair - NLI - B model achieves the best performance for aspect category detection .,"[('achieves', (9, 10)), ('for', (13, 14))]","[('BERT - pair - NLI - B model', (1, 9)), ('best performance', (11, 13)), ('aspect category detection', (14, 17))]","[['BERT - pair - NLI - B model', 'achieves', 'best performance'], ['best performance', 'for', 'aspect category detection']]",[],[],"[['Results', 'has', 'BERT - pair - NLI - B model']]",[],[],[],[],[],sentiment_analysis,45,121
results,"For aspect category polarity , BERTpair - QA - B performs best on all 4 - way , 3 - way , and binary settings .","[('For', (0, 1)), ('performs', (10, 11)), ('on', (12, 13))]","[('aspect category polarity', (1, 4)), ('BERTpair - QA - B', (5, 10)), ('best', (11, 12)), ('all 4 - way , 3 - way , and binary settings', (13, 25))]","[['BERTpair - QA - B', 'performs', 'best'], ['best', 'on', 'all 4 - way , 3 - way , and binary settings']]","[['aspect category polarity', 'has', 'BERTpair - QA - B']]","[['Results', 'For', 'aspect category polarity']]",[],[],[],[],[],[],sentiment_analysis,45,122
research-problem,A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification,[],"[('Sentiment Classification', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment Classification']]",[],[],[],[],sentiment_analysis,46,2
model,"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .","[('propose', (5, 6)), ('for', (15, 16)), ('to integrate', (21, 23)), ('into', (29, 30)), ('via', (33, 34))]","[('Multi- sentimentresource Enhanced Attention Network ( MEAN )', (7, 15)), ('sentence - level sentiment classification', (16, 21)), ('many kinds of sentiment linguistic knowledge', (23, 29)), ('deep neural networks', (30, 33)), ('multi -path attention mechanism', (34, 38))]","[['Multi- sentimentresource Enhanced Attention Network ( MEAN )', 'for', 'sentence - level sentiment classification'], ['Multi- sentimentresource Enhanced Attention Network ( MEAN )', 'to integrate', 'many kinds of sentiment linguistic knowledge'], ['many kinds of sentiment linguistic knowledge', 'into', 'deep neural networks'], ['many kinds of sentiment linguistic knowledge', 'via', 'multi -path attention mechanism']]",[],"[['Model', 'propose', 'Multi- sentimentresource Enhanced Attention Network ( MEAN )']]",[],[],[],[],[],[],sentiment_analysis,46,15
model,"Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .","[('design', (4, 5)), ('to model', (10, 12)), ('from', (15, 16))]","[('coupled word embedding module', (6, 10)), ('word representation', (13, 15)), ('character - level and word - level semantics', (16, 24))]","[['coupled word embedding module', 'to model', 'word representation'], ['word representation', 'from', 'character - level and word - level semantics']]",[],"[['Model', 'design', 'coupled word embedding module']]",[],[],[],[],[],[],sentiment_analysis,46,16
model,"Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .","[('to learn', (10, 12)), ('by using', (21, 23)), ('as', (30, 31)), ('attending to', (33, 35))]","[('multisentiment - resource attention module', (5, 10)), ('more comprehensive and meaningful sentiment - specific sentence representation', (12, 21)), ('three types of sentiment resource words', (24, 30)), ('attention sources', (31, 33)), ('context words', (36, 38))]","[['multisentiment - resource attention module', 'to learn', 'more comprehensive and meaningful sentiment - specific sentence representation'], ['more comprehensive and meaningful sentiment - specific sentence representation', 'by using', 'three types of sentiment resource words'], ['three types of sentiment resource words', 'attending to', 'context words'], ['three types of sentiment resource words', 'as', 'attention sources']]",[],[],"[['Model', 'propose', 'multisentiment - resource attention module']]",[],[],[],[],[],sentiment_analysis,46,18
model,"In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .","[('attend to', (6, 8)), ('from', (11, 12)), ('implied by', (15, 17)), ('capture', (23, 24)), ('of', (28, 29)), ('for', (36, 37))]","[('different sentimentrelevant information', (8, 11)), ('different representation subspaces', (12, 15)), ('different types of sentiment sources', (17, 22)), ('over all semantics', (25, 28)), ('sentiment , negation and intensity words', (30, 36)), ('sentiment prediction', (37, 39))]","[['different sentimentrelevant information', 'from', 'different representation subspaces'], ['different representation subspaces', 'capture', 'over all semantics'], ['over all semantics', 'of', 'sentiment , negation and intensity words'], ['sentiment , negation and intensity words', 'for', 'sentiment prediction'], ['different representation subspaces', 'implied by', 'different types of sentiment sources']]",[],[],[],[],"[['multisentiment - resource attention module', 'attend to', 'different sentimentrelevant information']]",[],[],[],sentiment_analysis,46,19
baselines,RNTN : Recursive Tensor Neural Network ) is used to model correlations between different dimensions of child nodes vectors .,"[('to model', (9, 11)), ('between', (12, 13)), ('of', (15, 16))]","[('RNTN', (0, 1)), ('Recursive Tensor Neural Network', (2, 6)), ('correlations', (11, 12)), ('different dimensions', (13, 15)), ('child nodes vectors', (16, 19))]","[['RNTN', 'to model', 'correlations'], ['correlations', 'between', 'different dimensions'], ['different dimensions', 'of', 'child nodes vectors']]","[['RNTN', 'name', 'Recursive Tensor Neural Network']]",[],"[['Baselines', 'has', 'RNTN']]",[],[],[],[],[],sentiment_analysis,46,90
baselines,LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .,"[('employs', (10, 11)), ('to capture', (20, 22))]","[('LSTM / Bi-LSTM', (0, 3)), ('Long Short - Term Memory', (11, 16)), ('bidirectional variant', (18, 20)), ('sequential information', (22, 24))]","[['LSTM / Bi-LSTM', 'employs', 'Long Short - Term Memory'], ['LSTM / Bi-LSTM', 'employs', 'bidirectional variant'], ['bidirectional variant', 'to capture', 'sequential information']]",[],[],"[['Baselines', 'has', 'LSTM / Bi-LSTM']]",[],[],[],[],[],sentiment_analysis,46,91
baselines,"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .","[('introduced by', (5, 7)), ('into', (17, 18))]","[('Tree-LSTM', (0, 1)), ('Memory cells', (2, 4)), ('Tree - Structured Long Short - Term Memory', (7, 15)), ('gates', (16, 17)), ('tree - structured neural network', (18, 23))]","[['gates', 'into', 'tree - structured neural network'], ['Memory cells', 'introduced by', 'Tree - Structured Long Short - Term Memory']]","[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]",[],"[['Baselines', 'has', 'Tree-LSTM']]",[],[],[],[],[],sentiment_analysis,46,92
baselines,CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .,"[('to generate', (8, 10))]","[('CNN', (0, 1)), ('Convolutional Neural Networks', (2, 5)), ('task - specific sentence representation', (10, 15))]","[['CNN', 'to generate', 'task - specific sentence representation']]","[['CNN', 'name', 'Convolutional Neural Networks']]",[],"[['Baselines', 'has', 'CNN']]",[],[],[],[],[],sentiment_analysis,46,93
baselines,NCSL : designs a Neural Context - Sensitive Lexicon ( NSCL ) to obtain prior sentiment scores of words in the sentence .,"[('to obtain', (12, 14)), ('in', (19, 20))]","[('NCSL', (0, 1)), ('Neural Context - Sensitive Lexicon ( NSCL )', (4, 12)), ('prior sentiment scores of words', (14, 19)), ('sentence', (21, 22))]","[['NCSL', 'to obtain', 'prior sentiment scores of words'], ['prior sentiment scores of words', 'in', 'sentence']]","[['NCSL', 'name', 'Neural Context - Sensitive Lexicon ( NSCL )']]",[],"[['Baselines', 'has', 'NCSL']]",[],[],[],[],[],sentiment_analysis,46,94
baselines,LR - Bi-LSTM : imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence .,"[('imposes', (4, 5)), ('into', (7, 8)), ('by applying', (10, 12)), ('on', (14, 15)), ('with', (17, 18))]","[('LR - Bi-LSTM', (0, 3)), ('linguistic roles', (5, 7)), ('neural networks', (8, 10)), ('linguistic regularization', (12, 14)), ('intermediate outputs', (15, 17)), ('KL divergence', (18, 20))]","[['LR - Bi-LSTM', 'imposes', 'linguistic roles'], ['linguistic roles', 'into', 'neural networks'], ['linguistic roles', 'by applying', 'linguistic regularization'], ['linguistic regularization', 'with', 'KL divergence'], ['linguistic regularization', 'on', 'intermediate outputs']]",[],[],"[['Baselines', 'has', 'LR - Bi-LSTM']]",[],[],[],[],[],sentiment_analysis,46,95
baselines,Self - attention : proposes a selfattention mechanism to learn structured sentence embedding .,"[('proposes', (4, 5)), ('to learn', (8, 10))]","[('Self - attention', (0, 3)), ('selfattention mechanism', (6, 8)), ('structured sentence embedding', (10, 13))]","[['Self - attention', 'proposes', 'selfattention mechanism'], ['selfattention mechanism', 'to learn', 'structured sentence embedding']]",[],[],"[['Baselines', 'has', 'Self - attention']]",[],[],[],[],[],sentiment_analysis,46,96
baselines,ID - LSTM : uses reinforcement learning to learn structured sentence representation for sentiment classification .,"[('uses', (4, 5)), ('to learn', (7, 9)), ('for', (12, 13))]","[('ID - LSTM', (0, 3)), ('reinforcement learning', (5, 7)), ('structured sentence representation', (9, 12)), ('sentiment classification', (13, 15))]","[['ID - LSTM', 'uses', 'reinforcement learning'], ['reinforcement learning', 'to learn', 'structured sentence representation'], ['structured sentence representation', 'for', 'sentiment classification']]",[],[],"[['Baselines', 'has', 'ID - LSTM']]",[],[],[],[],[],sentiment_analysis,46,97
hyperparameters,"In our experiments , the dimensions of characterlevel embedding and word embedding ( Glo Ve ) are both set to 300 .","[('of', (6, 7)), ('set to', (18, 20))]","[('dimensions', (5, 6)), ('characterlevel embedding and word embedding ( Glo Ve )', (7, 16)), ('300', (20, 21))]","[['dimensions', 'of', 'characterlevel embedding and word embedding ( Glo Ve )'], ['characterlevel embedding and word embedding ( Glo Ve )', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'dimensions']]",[],[],[],[],[],sentiment_analysis,46,99
hyperparameters,"Kernel sizes of multi-gram convolution for Char - CNN are set to 2 , 3 , respectively .","[('of', (2, 3)), ('for', (5, 6)), ('set to', (10, 12))]","[('Kernel sizes', (0, 2)), ('multi-gram convolution', (3, 5)), ('Char - CNN', (6, 9)), ('2 , 3', (12, 15))]","[['Kernel sizes', 'of', 'multi-gram convolution'], ['multi-gram convolution', 'for', 'Char - CNN'], ['Kernel sizes', 'set to', '2 , 3']]",[],[],"[['Hyperparameters', 'has', 'Kernel sizes']]",[],[],[],[],[],sentiment_analysis,46,100
hyperparameters,"All the weight matrices are initialized as random orthogonal matrices , and we set all the bias vectors as zero vectors .","[('initialized as', (5, 7)), ('set', (13, 14)), ('as', (18, 19))]","[('weight matrices', (2, 4)), ('random orthogonal matrices', (7, 10)), ('all the bias vectors', (14, 18)), ('zero vectors', (19, 21))]","[['all the bias vectors', 'as', 'zero vectors'], ['weight matrices', 'initialized as', 'random orthogonal matrices']]",[],"[['Hyperparameters', 'set', 'all the bias vectors']]","[['Hyperparameters', 'has', 'weight matrices']]",[],[],[],[],[],sentiment_analysis,46,101
hyperparameters,"We optimize the proposed model with RMSprop algorithm , using mini-batch training .","[('optimize', (1, 2)), ('with', (5, 6)), ('using', (9, 10))]","[('proposed model', (3, 5)), ('RMSprop algorithm', (6, 8)), ('mini-batch training', (10, 12))]","[['proposed model', 'with', 'RMSprop algorithm'], ['proposed model', 'using', 'mini-batch training']]",[],"[['Hyperparameters', 'optimize', 'proposed model']]",[],[],[],[],[],[],sentiment_analysis,46,102
hyperparameters,The size of mini-batch is 60 .,"[('of', (2, 3)), ('is', (4, 5))]","[('size', (1, 2)), ('mini-batch', (3, 4)), ('60', (5, 6))]","[['size', 'of', 'mini-batch'], ['mini-batch', 'is', '60']]",[],[],"[['Hyperparameters', 'has', 'size']]",[],[],[],[],[],sentiment_analysis,46,103
hyperparameters,"The dropout rate is 0.5 , and the coefficient ?","[('is', (3, 4))]","[('dropout rate', (1, 3)), ('0.5', (4, 5)), ('coefficient', (8, 9))]","[['dropout rate', 'is', '0.5']]",[],[],"[['Hyperparameters', 'has', 'coefficient'], ['Hyperparameters', 'has', 'dropout rate']]",[],[],[],[],[],sentiment_analysis,46,104
hyperparameters,of L 2 normalization is set to 10 ?5 . is set to 10 ? 4 . ?,"[('of', (0, 1)), ('set to', (5, 7))]","[('L 2 normalization', (1, 4)), ('10 ?5', (7, 9))]","[['L 2 normalization', 'set to', '10 ?5']]",[],[],[],[],"[['coefficient', 'of', 'L 2 normalization']]",[],[],[],sentiment_analysis,46,105
results,"First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .","[('brings', (4, 5)), ('over', (8, 9)), ('that do not leverage', (11, 15))]","[('our model', (2, 4)), ('substantial improvement', (6, 8)), ('methods', (10, 11)), ('sentiment linguistic knowledge', (15, 18))]","[['our model', 'brings', 'substantial improvement'], ['substantial improvement', 'over', 'methods'], ['methods', 'that do not leverage', 'sentiment linguistic knowledge']]",[],[],"[['Results', 'has', 'our model']]",[],[],[],[],[],sentiment_analysis,46,112
results,"Second , our model also consistently outperforms LR - Bi - LSTM which integrates linguistic roles of sentiment , negation and intensity words into neural networks via the linguistic regularization .","[('consistently outperforms', (5, 7)), ('which integrates', (12, 14)), ('of', (16, 17)), ('into', (23, 24)), ('via', (26, 27))]","[('LR - Bi - LSTM', (7, 12)), ('linguistic roles', (14, 16)), ('sentiment , negation and intensity words', (17, 23)), ('neural networks', (24, 26)), ('linguistic regularization', (28, 30))]","[['LR - Bi - LSTM', 'which integrates', 'linguistic roles'], ['linguistic roles', 'into', 'neural networks'], ['neural networks', 'via', 'linguistic regularization'], ['linguistic roles', 'of', 'sentiment , negation and intensity words']]",[],[],[],[],"[['our model', 'consistently outperforms', 'LR - Bi - LSTM']]",[],[],[],sentiment_analysis,46,114
results,"For example , our model achieves 2.4 % improvements over the MR dataset and 0.8 % improvements over the SST dataset compared to LR - Bi - LSTM .",[],[],"[['2.4 % improvements', 'over', 'MR dataset'], ['0.8 % improvements', 'over', 'SST dataset']]",[],[],[],[],"[['our model', 'achieves', '2.4 % improvements'], ['our model', 'achieves', '0.8 % improvements'], ['our model', 'achieves', 'compared to LR - Bi - LSTM']]",[],[],[],sentiment_analysis,46,115
research-problem,Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention,[],"[('Aspect - based Sentiment Analysis', (9, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based Sentiment Analysis']]",[],[],[],[],sentiment_analysis,47,2
research-problem,"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .",[],"[('sentiment analysis', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,47,16
research-problem,"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .",[],"[('sentiment classification', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment classification']]",[],[],[],[],sentiment_analysis,47,17
model,"With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .","[('propose', (14, 15)), ('with', (24, 25))]","[('left - center - right separated neural network', (16, 24)), ('rotatory attention mechanism ( LCR - Rot )', (25, 33))]","[['left - center - right separated neural network', 'with', 'rotatory attention mechanism ( LCR - Rot )']]",[],"[['Model', 'propose', 'left - center - right separated neural network']]",[],[],[],[],[],[],sentiment_analysis,47,38
model,"Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .","[('design', (3, 4)), ('contains', (13, 14)), ('i.e.', (17, 18)), ('modeling', (30, 31)), ('of', (34, 35))]","[('left - center - right separated LSTMs', (5, 12)), ('three LSTMs', (14, 16)), ('left - , center - and right - LSTM', (19, 28)), ('three parts', (32, 34)), ('review', (36, 37)), ('left context', (38, 40)), ('target phrase', (41, 43)), ('right context', (44, 46))]","[['left - center - right separated LSTMs', 'contains', 'three LSTMs'], ['three LSTMs', 'i.e.', 'left - , center - and right - LSTM'], ['left - center - right separated LSTMs', 'modeling', 'three parts'], ['three parts', 'of', 'review']]","[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","[['Model', 'design', 'left - center - right separated LSTMs']]",[],[],[],[],[],[],sentiment_analysis,47,39
model,"On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .",[],[],"[['rotatory attention mechanism', 'take into account', 'interaction'], ['interaction', 'to better represent', 'targets and contexts'], ['interaction', 'between', 'targets and contexts']]",[],[],"[['Model', 'propose', 'rotatory attention mechanism']]",[],[],[],[],[],sentiment_analysis,47,40
model,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,"[('to capture', (5, 7)), ('in', (12, 13))]","[('target2context attention', (1, 3)), ('most indicative sentiment words', (8, 12)), ('left / right contexts', (13, 17))]","[['target2context attention', 'to capture', 'most indicative sentiment words'], ['most indicative sentiment words', 'in', 'left / right contexts']]",[],[],"[['Model', 'has', 'target2context attention']]",[],[],[],[],[],sentiment_analysis,47,41
model,"Subsequently , the context2target attention is used to capture the most important word in the target .","[('to capture', (7, 9)), ('in', (13, 14))]","[('context2target attention', (3, 5)), ('most important word', (10, 13)), ('target', (15, 16))]","[['context2target attention', 'to capture', 'most important word'], ['most important word', 'in', 'target']]",[],[],"[['Model', 'has', 'context2target attention']]",[],[],[],[],[],sentiment_analysis,47,42
model,This leads to a two - side representation of the target : left - aware target and right - aware target .,"[('leads to', (1, 3)), ('of', (8, 9))]","[('two - side representation', (4, 8)), ('target', (10, 11)), ('left - aware target', (12, 16)), ('right - aware target', (17, 21))]","[['two - side representation', 'of', 'target']]","[['two - side representation', 'name', 'left - aware target'], ['two - side representation', 'name', 'right - aware target']]","[['Model', 'leads to', 'two - side representation']]",[],[],[],[],[],[],sentiment_analysis,47,43
model,"Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .","[('concatenate', (3, 4)), ('as', (7, 8)), ('of', (11, 12)), ('feed it into', (15, 18)), ('to predict', (21, 23))]","[('component representations', (5, 7)), ('final representation', (9, 11)), ('sentence', (13, 14)), ('softmax layer', (19, 21)), ('sentiment polarity', (24, 26))]","[['component representations', 'as', 'final representation'], ['final representation', 'of', 'sentence'], ['component representations', 'feed it into', 'softmax layer'], ['softmax layer', 'to predict', 'sentiment polarity']]",[],"[['Model', 'concatenate', 'component representations']]",[],[],[],[],[],[],sentiment_analysis,47,44
hyperparameters,"In our work , the dimension of word embedding vectors and hidden state vectors is 300 .","[('of', (6, 7)), ('is', (14, 15))]","[('dimension', (5, 6)), ('word embedding vectors', (7, 10)), ('hidden state vectors', (11, 14)), ('300', (15, 16))]","[['dimension', 'of', 'word embedding vectors'], ['dimension', 'of', 'hidden state vectors'], ['dimension', 'is', '300']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",[],[],[],[],[],sentiment_analysis,47,131
hyperparameters,"We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings , the same as .","[('use', (1, 2)), ('with', (5, 6)), ('to initialize', (8, 10))]","[('GloVe 2 vectors', (2, 5)), ('300 dimensions', (6, 8)), ('word embeddings', (11, 13))]","[['GloVe 2 vectors', 'with', '300 dimensions'], ['300 dimensions', 'to initialize', 'word embeddings']]",[],"[['Hyperparameters', 'use', 'GloVe 2 vectors']]",[],[],[],[],[],[],sentiment_analysis,47,132
hyperparameters,"All out - ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U ( - 0.1 , 0.1 ) , and all bias are set to zero .","[('randomly initialized by', (9, 12)), ('set to', (27, 29))]","[('All out - ofvocabulary words and weight matrices', (0, 8)), ('uniform distribution U ( - 0.1 , 0.1 )', (13, 22)), ('all bias', (24, 26)), ('zero', (29, 30))]","[['All out - ofvocabulary words and weight matrices', 'randomly initialized by', 'uniform distribution U ( - 0.1 , 0.1 )'], ['all bias', 'set to', 'zero']]",[],[],"[['Hyperparameters', 'has', 'All out - ofvocabulary words and weight matrices'], ['Hyperparameters', 'has', 'all bias']]",[],[],[],[],[],sentiment_analysis,47,133
hyperparameters,Tensor Flow is used for implementing our neural network model .,"[('for implementing', (4, 6))]","[('Tensor Flow', (0, 2)), ('neural network model', (7, 10))]","[['Tensor Flow', 'for implementing', 'neural network model']]",[],[],"[['Hyperparameters', 'has', 'Tensor Flow']]",[],[],[],[],[],sentiment_analysis,47,134
hyperparameters,"In model training , the learning rate is set to 0.1 , the weight for L 2 - norm regularization is set to 1 e - 5 , and dropout rate is set to 0.5 .",[],[],"[['learning rate', 'set to', '0.1'], ['weight', 'for', 'L 2 - norm regularization'], ['L 2 - norm regularization', 'set to', '1 e - 5'], ['dropout rate', 'set to', '0.5']]","[['model training', 'has', 'learning rate'], ['model training', 'has', 'weight'], ['model training', 'has', 'dropout rate']]","[['Hyperparameters', 'In', 'model training']]",[],[],[],[],[],[],sentiment_analysis,47,135
hyperparameters,We train the model use stochastic gradient descent optimizer with momentum of 0.9 .,"[('train', (1, 2)), ('use', (4, 5)), ('with', (9, 10)), ('of', (11, 12))]","[('model', (3, 4)), ('stochastic gradient descent optimizer', (5, 9)), ('momentum', (10, 11)), ('0.9', (12, 13))]","[['model', 'use', 'stochastic gradient descent optimizer'], ['stochastic gradient descent optimizer', 'with', 'momentum'], ['momentum', 'of', '0.9']]",[],"[['Hyperparameters', 'train', 'model']]",[],[],[],[],[],[],sentiment_analysis,47,136
hyperparameters,The paired t- test is used for the significance testing .,"[('used for', (5, 7))]","[('paired t- test', (1, 4)), ('significance testing', (8, 10))]","[['paired t- test', 'used for', 'significance testing']]",[],[],"[['Hyperparameters', 'has', 'paired t- test']]",[],[],[],[],[],sentiment_analysis,47,137
baselines,"Majority assigns the sentiment polarity that has the largest probability in the training set ; 2 . Simple SVM is a SVM classifier with simple features such as unigrams and bigrams ; 3 . Feature - enhanced SVM is a SVM classifier with a state - of - the - art feature template which contains n-gram features , parse features and lexicon features ; 4 . LSTM represents a standard LSTM for aspect - based sentiment classification task ; 5 . TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; 74.30 66.50 66.50 TD- LSTM 75.60 68.10 70.80 AE - LSTM 76.60 68.90 - ATAE - LSTM 77.20 68.70 - GRNN- G3 79.55 * 71.47 * 70.09 * MemNet 79.98 * 70.33 * 70.52 * IAN 78.60 72.10 - LCR - Rot ( our approach ) 81.34 75.24 72.69 : The performance ( classification accuracy ) of different methods on three datasets .",[],[],"[['Feature - enhanced SVM', 'is', 'SVM classifier'], ['SVM classifier', 'with', 'state - of - the - art feature template'], ['state - of - the - art feature template', 'which contains', 'n-gram features'], ['state - of - the - art feature template', 'which contains', 'parse features'], ['state - of - the - art feature template', 'which contains', 'lexicon features'], ['Simple SVM', 'is', 'SVM classifier'], ['SVM classifier', 'with', 'simple features'], ['simple features', 'such as', 'unigrams and bigrams'], ['Majority', 'assigns', 'sentiment polarity'], ['largest probability', 'in', 'training set'], ['TD - LSTM', 'adopts', 'two LSTMs'], ['TD - LSTM', 'to model', 'right context'], ['right context', 'with', 'target'], ['TD - LSTM', 'to model', 'left context'], ['left context', 'with', 'target']]","[['sentiment polarity', 'has', 'largest probability']]",[],"[['Baselines', 'has', 'Feature - enhanced SVM'], ['Baselines', 'has', 'Simple SVM'], ['Baselines', 'has', 'Majority'], ['Baselines', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,47,141
baselines,6 . AE - LSTM is an upgraded version of LSTM .,"[('is', (5, 6)), ('of', (9, 10))]","[('AE - LSTM', (2, 5)), ('upgraded version', (7, 9)), ('LSTM', (10, 11))]","[['AE - LSTM', 'is', 'upgraded version'], ['upgraded version', 'of', 'LSTM']]",[],[],"[['Baselines', 'has', 'AE - LSTM']]",[],[],[],[],[],sentiment_analysis,47,143
baselines,7 . ATAE - LSTM is developed based on AE - LSTM .,"[('developed based on', (6, 9))]","[('ATAE - LSTM', (2, 5)), ('AE - LSTM', (9, 12))]","[['ATAE - LSTM', 'developed based on', 'AE - LSTM']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,47,146
baselines,8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a three - way structure to leverage contexts .,"[('adopts', (5, 6)), ('to represent', (10, 12)), ('use', (14, 15)), ('to leverage', (20, 22))]","[('GRNN - G3', (2, 5)), ('Gated - RNN', (7, 10)), ('sentence', (12, 13)), ('three - way structure', (16, 20)), ('contexts', (22, 23))]","[['GRNN - G3', 'adopts', 'Gated - RNN'], ['Gated - RNN', 'use', 'three - way structure'], ['three - way structure', 'to leverage', 'contexts'], ['Gated - RNN', 'to represent', 'sentence']]",[],[],"[['Baselines', 'has', 'GRNN - G3']]",[],[],[],[],[],sentiment_analysis,47,148
baselines,MemNet is a deep memory network which considers the content and position of target .,"[('is', (1, 2)), ('considers', (7, 8)), ('of', (12, 13))]","[('MemNet', (0, 1)), ('deep memory network', (3, 6)), ('content and position', (9, 12)), ('target', (13, 14))]","[['MemNet', 'is', 'deep memory network'], ['deep memory network', 'considers', 'content and position'], ['content and position', 'of', 'target']]",[],[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,47,150
baselines,"IAN interactively learns attentions in the contexts and targets , and generate the representations for targets and contexts separately .","[('interactively learns', (1, 3)), ('in', (4, 5)), ('generate', (11, 12)), ('for', (14, 15))]","[('IAN', (0, 1)), ('attentions', (3, 4)), ('contexts and targets', (6, 9)), ('representations', (13, 14)), ('targets and contexts', (15, 18))]","[['IAN', 'generate', 'representations'], ['representations', 'for', 'targets and contexts'], ['IAN', 'interactively learns', 'attentions'], ['attentions', 'in', 'contexts and targets']]",[],[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,47,152
results,"We can find that the Majority method is the worst , which means the majority sentiment polarity occupies 53.50 % , 65.00 % and 50 % of all samples on the Restaurant , Laptop and Twitter testing datasets respectively .","[('find', (2, 3)), ('is', (7, 8)), ('occupies', (17, 18)), ('on', (29, 30))]","[('Majority method', (5, 7)), ('worst', (9, 10)), ('majority sentiment polarity', (14, 17)), ('53.50 % , 65.00 % and 50 %', (18, 26)), ('Restaurant , Laptop and Twitter testing datasets', (31, 38))]","[['Majority method', 'is', 'worst'], ['majority sentiment polarity', 'occupies', '53.50 % , 65.00 % and 50 %'], ['53.50 % , 65.00 % and 50 %', 'on', 'Restaurant , Laptop and Twitter testing datasets']]","[['Majority method', 'has', 'majority sentiment polarity']]","[['Results', 'find', 'Majority method']]",[],[],[],[],[],[],sentiment_analysis,47,155
results,The Simple SVM model performs better than Majority .,"[('performs', (4, 5)), ('than', (6, 7))]","[('Simple SVM model', (1, 4)), ('better', (5, 6)), ('Majority', (7, 8))]","[['Simple SVM model', 'performs', 'better'], ['better', 'than', 'Majority']]",[],[],"[['Results', 'has', 'Simple SVM model']]",[],[],[],[],[],sentiment_analysis,47,156
results,"With the help of feature engineering , the Feature - enhanced SVM achieves much better results .","[('With the help of', (0, 4)), ('achieves', (12, 13))]","[('feature engineering', (4, 6)), ('Feature - enhanced SVM', (8, 12)), ('much better results', (13, 16))]","[['Feature - enhanced SVM', 'achieves', 'much better results']]","[['feature engineering', 'has', 'Feature - enhanced SVM']]","[['Results', 'With the help of', 'feature engineering']]",[],[],[],[],[],[],sentiment_analysis,47,157
results,Our model achieves significantly better results than feature - enhanced SVM .,"[('achieves', (2, 3)), ('than', (6, 7))]","[('Our model', (0, 2)), ('significantly better results', (3, 6)), ('feature - enhanced SVM', (7, 11))]","[['Our model', 'achieves', 'significantly better results'], ['significantly better results', 'than', 'feature - enhanced SVM']]",[],[],"[['Results', 'has', 'Our model']]",[],[],[],[],[],sentiment_analysis,47,159
results,"Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .","[('performs', (14, 15))]","[('basic LSTM approach', (11, 14)), ('worst', (16, 17))]","[['basic LSTM approach', 'performs', 'worst']]",[],[],"[['Results', 'has', 'basic LSTM approach']]",[],[],[],[],[],sentiment_analysis,47,161
results,TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .,"[('obtains', (3, 4)), ('of', (6, 7)), ('over', (11, 12)), ('when', (13, 14)), ('taken into', (17, 19))]","[('TD - LSTM', (0, 3)), ('improvement', (5, 6)), ('1 - 2 %', (7, 11)), ('LSTM', (12, 13)), ('target signals', (14, 16)), ('consideration', (19, 20))]","[['TD - LSTM', 'obtains', 'improvement'], ['improvement', 'of', '1 - 2 %'], ['1 - 2 %', 'over', 'LSTM'], ['improvement', 'when', 'target signals'], ['target signals', 'taken into', 'consideration']]",[],[],"[['Results', 'has', 'TD - LSTM']]",[],[],[],[],[],sentiment_analysis,47,162
results,"MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .","[('achieves', (1, 2)), ('than', (4, 5)), ('on', (7, 8))]","[('MemNet', (0, 1)), ('better results', (2, 4)), ('other models', (5, 7)), ('Restaurant dataset', (9, 11))]","[['MemNet', 'achieves', 'better results'], ['better results', 'than', 'other models'], ['better results', 'on', 'Restaurant dataset']]",[],[],"[['Results', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,47,165
results,IAN considers separate representations of targets and obtains better result on the Laptop dataset .,"[('considers', (1, 2)), ('of', (4, 5)), ('obtains', (7, 8)), ('on', (10, 11))]","[('IAN', (0, 1)), ('separate representations', (2, 4)), ('targets', (5, 6)), ('better result', (8, 10)), ('Laptop dataset', (12, 14))]","[['IAN', 'considers', 'separate representations'], ['separate representations', 'of', 'targets'], ['IAN', 'obtains', 'better result'], ['better result', 'on', 'Laptop dataset']]",[],[],"[['Results', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,47,166
results,GRNN - G3 achieves competitive results on all datasets because of its three - way structure and special gated - RNN model .,"[('achieves', (3, 4)), ('on', (6, 7))]","[('GRNN - G3', (0, 3)), ('competitive results', (4, 6)), ('all datasets', (7, 9))]","[['GRNN - G3', 'achieves', 'competitive results'], ['competitive results', 'on', 'all datasets']]",[],[],"[['Results', 'has', 'GRNN - G3']]",[],[],[],[],[],sentiment_analysis,47,167
results,"In the contrast , our LCR - Rot model achieves the best results on the all datasets among all models .","[('achieves', (9, 10)), ('on', (13, 14)), ('among', (17, 18))]","[('LCR - Rot model', (5, 9)), ('best results', (11, 13)), ('all datasets', (15, 17)), ('all models', (18, 20))]","[['LCR - Rot model', 'achieves', 'best results'], ['best results', 'among', 'all models'], ['best results', 'on', 'all datasets']]",[],[],"[['Results', 'has', 'LCR - Rot model']]",[],[],[],[],[],sentiment_analysis,47,168
research-problem,Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer,[],"[('Aspect - term Sentiment Analysis', (2, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - term Sentiment Analysis']]",[],[],[],[],sentiment_analysis,48,2
research-problem,"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .",[],"[('aspect - term sentiment analysis ( ATSA )', (14, 22)), ('aspect - category sentiment analysis ( ACSA )', (23, 31))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect - term sentiment analysis ( ATSA )'], ['Contribution', 'has research problem', 'aspect - category sentiment analysis ( ACSA )']]",[],[],[],[],sentiment_analysis,48,13
research-problem,"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .",[],"[('ACSA', (0, 1))]",[],[],[],[],"[['Contribution', 'has research problem', 'ACSA']]",[],[],[],[],sentiment_analysis,48,14
research-problem,"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .",[],"[('ATSA', (5, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'ATSA']]",[],[],[],[],sentiment_analysis,48,15
model,"In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .","[('proposed', (5, 6)), ('named', (12, 13))]","[('classifier - agnostic framework', (7, 11)), ('Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET )', (13, 32))]","[['classifier - agnostic framework', 'named', 'Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET )']]",[],"[['Model', 'proposed', 'classifier - agnostic framework']]",[],[],[],[],[],[],sentiment_analysis,48,29
model,The variational autoencoder offers the flexibility to customize the model structure .,"[('offers', (3, 4)), ('to customize', (6, 8))]","[('variational autoencoder', (1, 3)), ('flexibility', (5, 6)), ('model structure', (9, 11))]","[['variational autoencoder', 'offers', 'flexibility'], ['flexibility', 'to customize', 'model structure']]",[],[],"[['Model', 'has', 'variational autoencoder']]",[],[],[],[],[],sentiment_analysis,48,30
model,"By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .","[('By regarding', (0, 2)), ('of', (6, 7)), ('as', (10, 11)), ('implicitly induces', (18, 20)), ('via', (23, 24))]","[('aspect sentiment polarity', (3, 6)), ('unlabeled data', (8, 10)), ('discrete latent variable', (12, 15)), ('model', (17, 18)), ('sentiment polarity', (21, 23)), ('variational inference', (25, 27))]","[['aspect sentiment polarity', 'as', 'discrete latent variable'], ['aspect sentiment polarity', 'of', 'unlabeled data'], ['model', 'implicitly induces', 'sentiment polarity'], ['sentiment polarity', 'via', 'variational inference']]","[['aspect sentiment polarity', 'has', 'model']]","[['Model', 'By regarding', 'aspect sentiment polarity']]",[],[],[],[],[],[],sentiment_analysis,48,33
model,"Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .","[('of', (4, 5)), ('extracted by', (9, 11)), ('inferred from', (21, 23))]","[('representation', (3, 4)), ('lexical context', (6, 8)), ('encoder', (12, 13)), ('aspect - term sentiment polarity', (15, 20)), ('specific ATSA classifier', (24, 27))]","[['aspect - term sentiment polarity', 'inferred from', 'specific ATSA classifier'], ['representation', 'of', 'lexical context'], ['lexical context', 'extracted by', 'encoder']]",[],[],"[['Model', 'has', 'aspect - term sentiment polarity'], ['Model', 'has', 'representation']]",[],[],[],[],[],sentiment_analysis,48,34
model,"In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .","[('by separating', (3, 5)), ('of', (7, 8)), ('becomes', (14, 15)), ('in', (18, 19))]","[('representation', (6, 7)), ('input sentence', (9, 11)), ('classifier', (13, 14)), ('independent module', (16, 18)), ('our framework', (19, 21))]","[['representation', 'of', 'input sentence'], ['classifier', 'becomes', 'independent module'], ['independent module', 'in', 'our framework']]","[['representation', 'has', 'classifier']]","[['Model', 'by separating', 'representation']]",[],[],[],[],[],[],sentiment_analysis,48,38
hyperparameters,"The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2 , the number of selfattention heads is 8 .",[],[],"[['number of selfattention heads', 'is', '8'], ['number of units', 'in', 'encoder and the decoder'], ['number of units', 'is', '100'], ['number of layers', 'of both', 'Transformer blocks'], ['Transformer blocks', 'is', '2'], ['latent variable', 'of', 'size 50']]",[],[],"[['Hyperparameters', 'has', 'number of selfattention heads'], ['Hyperparameters', 'has', 'number of units'], ['Hyperparameters', 'has', 'number of layers'], ['Hyperparameters', 'has', 'latent variable']]",[],[],[],[],[],sentiment_analysis,48,151
hyperparameters,"In this work , the KL weight is set to be 1e - 4 .","[('set to be', (8, 11))]","[('KL weight', (5, 7)), ('1e - 4', (11, 14))]","[['KL weight', 'set to be', '1e - 4']]",[],[],"[['Hyperparameters', 'has', 'KL weight']]",[],[],[],[],[],sentiment_analysis,48,158
baselines,"TC - LSTM : Two LSTMs are used to model the left and right context of the target separately , then the concatenation of two representations is used to predict the label .",[],[],"[['concatenation', 'of', 'two representations'], ['two representations', 'used to', 'predict'], ['Two LSTMs', 'used to', 'model'], ['left and right context', 'of', 'target']]","[['TC - LSTM', 'has', 'concatenation'], ['predict', 'has', 'label'], ['TC - LSTM', 'has', 'Two LSTMs'], ['model', 'has', 'left and right context']]",[],"[['Baselines', 'has', 'TC - LSTM']]",[],[],[],[],[],sentiment_analysis,48,164
baselines,"MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .",[],[],"[['MemNet', 'uses', 'attention mechanism'], ['attention mechanism', 'over', 'word embedding'], ['word embedding', 'over', 'multiple rounds'], ['attention mechanism', 'to aggregate', 'information'], ['information', 'in', 'sentence']]",[],[],"[['Baselines', 'has', 'MemNet']]",[],[],[],[],[],sentiment_analysis,48,165
baselines,IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,"[('adopts', (3, 4)), ('to derive', (6, 8)), ('of', (10, 11)), ('fed to', (22, 24))]","[('IAN', (0, 1)), ('two LSTMs', (4, 6)), ('representations', (9, 10)), ('context and the target phrase', (12, 17)), ('concatenation', (20, 21)), ('softmax layer', (25, 27))]","[['IAN', 'adopts', 'two LSTMs'], ['two LSTMs', 'to derive', 'representations'], ['representations', 'of', 'context and the target phrase'], ['concatenation', 'fed to', 'softmax layer']]","[['IAN', 'has', 'concatenation']]",[],"[['Baselines', 'has', 'IAN']]",[],[],[],[],[],sentiment_analysis,48,166
baselines,BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .,"[('models', (6, 7)), ('using', (11, 12)), ('makes use of', (18, 21)), ('to combine', (25, 27))]","[('BILSTM - ATT -G', (0, 4)), ('left and right contexts', (7, 11)), ('two attention - based LSTMs', (12, 17)), ('special gate layer', (22, 25)), ('two representations', (28, 30))]","[['BILSTM - ATT -G', 'models', 'left and right contexts'], ['left and right contexts', 'using', 'two attention - based LSTMs'], ['BILSTM - ATT -G', 'makes use of', 'special gate layer'], ['special gate layer', 'to combine', 'two representations']]",[],[],"[['Baselines', 'has', 'BILSTM - ATT -G']]",[],[],[],[],[],sentiment_analysis,48,168
baselines,"TNet - AS : Without using an attention module , TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer .","[('Without using', (4, 6))]","[('TNet - AS', (0, 3)), ('attention module', (7, 9))]","[['TNet - AS', 'Without using', 'attention module']]",[],[],"[['Baselines', 'has', 'TNet - AS']]",[],[],[],[],[],sentiment_analysis,48,170
results,"From the , the ASVAET is able to improve supervised performance consistently for all classifiers .","[('able to improve', (6, 9)), ('for', (12, 13))]","[('ASVAET', (4, 5)), ('supervised performance', (9, 11)), ('all classifiers', (13, 15))]","[['ASVAET', 'able to improve', 'supervised performance'], ['supervised performance', 'for', 'all classifiers']]",[],[],"[['Results', 'has', 'ASVAET']]",[],[],[],[],[],sentiment_analysis,48,197
results,"For the MemNet , the test accuracy can be improved by about 2 % by the TSSVAE , and so as the Macro - averaged F1 .","[('For', (0, 1)), ('improved by', (9, 11)), ('by', (14, 15))]","[('MemNet', (2, 3)), ('test accuracy', (5, 7)), ('about 2 %', (11, 14)), ('TSSVAE', (16, 17))]","[['test accuracy', 'improved by', 'about 2 %'], ['about 2 %', 'by', 'TSSVAE']]","[['MemNet', 'has', 'test accuracy']]","[['Results', 'For', 'MemNet']]",[],[],[],[],[],[],sentiment_analysis,48,198
results,The TNet - AS outperforms the other three models .,"[('outperforms', (4, 5))]","[('TNet - AS', (1, 4)), ('other three models', (6, 9))]","[['TNet - AS', 'outperforms', 'other three models']]",[],[],"[['Results', 'has', 'TNet - AS']]",[],[],[],[],[],sentiment_analysis,48,199
results,"Compared with the other two semi-supervised methods , the ASVAET also shows better results .","[('Compared with', (0, 2)), ('shows', (11, 12))]","[('other two semi-supervised methods', (3, 7)), ('ASVAET', (9, 10)), ('better results', (12, 14))]","[['ASVAET', 'shows', 'better results']]","[['other two semi-supervised methods', 'has', 'ASVAET']]","[['Results', 'Compared with', 'other two semi-supervised methods']]",[],[],[],[],[],[],sentiment_analysis,48,200
results,The ASVAET outperforms the compared semisupervised methods evidently .,"[('outperforms', (2, 3))]","[('compared semisupervised methods', (4, 7))]",[],[],[],[],[],"[['ASVAET', 'outperforms', 'compared semisupervised methods']]",[],[],[],sentiment_analysis,48,201
results,The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors .,"[('adoption of', (1, 3)), ('beneficial for', (8, 10)), ('compared with', (12, 14))]","[('indomain pre-trained word vectors', (3, 7)), ('performance', (11, 12)), ('Glove vectors', (15, 17))]","[['indomain pre-trained word vectors', 'beneficial for', 'performance'], ['performance', 'compared with', 'Glove vectors']]",[],"[['Results', 'adoption of', 'indomain pre-trained word vectors']]",[],[],[],[],[],[],sentiment_analysis,48,202
research-problem,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,[],"[('Multi-modal Sentiment Analysis', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi-modal Sentiment Analysis']]",[],[],[],[],sentiment_analysis,49,2
research-problem,"Traditionally , sentiment analysis has been applied to a wide variety of texts .",[],"[('sentiment analysis', (2, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentiment analysis']]",[],[],[],[],sentiment_analysis,49,12
model,"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .","[('propose', (5, 6)), ('employs', (10, 11)), ('for', (20, 21))]","[('novel method', (7, 9)), ('recurrent neural network based multimodal multi-utterance attention framework', (12, 20)), ('sentiment prediction', (21, 23))]","[['novel method', 'employs', 'recurrent neural network based multimodal multi-utterance attention framework'], ['recurrent neural network based multimodal multi-utterance attention framework', 'for', 'sentiment prediction']]",[],"[['Model', 'propose', 'novel method']]",[],[],[],[],[],[],sentiment_analysis,49,26
model,To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .,"[('focusing on', (12, 14)), ('computed between', (16, 18))]","[('novel fusion method', (8, 11)), ('inter-modality relations', (14, 16)), ('target utterance and its context', (19, 24))]","[['novel fusion method', 'focusing on', 'inter-modality relations'], ['inter-modality relations', 'computed between', 'target utterance and its context']]",[],[],"[['Model', 'propose', 'novel fusion method']]",[],[],[],[],[],sentiment_analysis,49,31
model,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,"[('attend to', (7, 9)), ('having', (13, 14)), ('with', (24, 25))]","[('attention mechanism', (1, 3)), ('important contextual utterances', (10, 13)), ('higher relatedness or similarity', (14, 18)), ('target utterance', (26, 28))]","[['attention mechanism', 'attend to', 'important contextual utterances'], ['important contextual utterances', 'having', 'higher relatedness or similarity'], ['higher relatedness or similarity', 'with', 'target utterance']]",[],[],"[['Model', 'has', 'attention mechanism']]",[],[],[],[],[],sentiment_analysis,49,37
model,"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .","[('attend over', (15, 17)), ('by computing', (20, 22)), ('among', (23, 24)), ('of', (26, 27))]","[('contextual utterances', (18, 20)), ('correlations', (22, 23)), ('modalities', (25, 26)), ('target utterance and the context utterances', (28, 34))]","[['contextual utterances', 'by computing', 'correlations'], ['correlations', 'among', 'modalities'], ['modalities', 'of', 'target utterance and the context utterances']]",[],"[['Model', 'attend over', 'contextual utterances']]",[],[],[],[],[],[],sentiment_analysis,49,38
model,The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .,"[('facilitates', (2, 3)), ('attending over', (7, 9)), ('generates', (14, 15)), ('when', (19, 20)), ('combined with', (26, 28))]","[('modality selection', (4, 6)), ('contextual utterances', (10, 12)), ('better multimodal feature representation', (15, 19)), ('modalities from the context', (21, 25)), ('modalities of the target utterance', (29, 34))]","[['modality selection', 'attending over', 'contextual utterances'], ['modality selection', 'generates', 'better multimodal feature representation'], ['better multimodal feature representation', 'when', 'modalities from the context'], ['modalities from the context', 'combined with', 'modalities of the target utterance']]",[],"[['Model', 'facilitates', 'modality selection']]",[],[],[],[],[],[],sentiment_analysis,49,40
hyperparameters,"We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .","[('use', (1, 2)), ('having', (4, 5)), ('followed by', (9, 11)), ('consisting of', (14, 16))]","[('Bi-directional GRUs', (2, 4)), ('300 neurons', (5, 7)), ('dense layer', (12, 14)), ('100 neurons', (16, 18))]","[['Bi-directional GRUs', 'having', '300 neurons'], ['300 neurons', 'followed by', 'dense layer'], ['dense layer', 'consisting of', '100 neurons']]",[],"[['Hyperparameters', 'use', 'Bi-directional GRUs']]",[],[],[],[],[],[],sentiment_analysis,49,149
hyperparameters,"Utilizing the dense layer , we project the input features of all the three modalities to the same dimensions .","[('Utilizing', (0, 1)), ('project', (6, 7)), ('of', (10, 11)), ('to', (15, 16))]","[('dense layer', (2, 4)), ('input features', (8, 10)), ('all the three modalities', (11, 15)), ('same dimensions', (17, 19))]","[['dense layer', 'project', 'input features'], ['input features', 'of', 'all the three modalities'], ['input features', 'to', 'same dimensions']]",[],"[['Hyperparameters', 'Utilizing', 'dense layer']]",[],[],[],[],[],[],sentiment_analysis,49,150
hyperparameters,We set dropout = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of regularization .,"[('set', (1, 2)), ('=', (3, 4)), ('as a measure of', (13, 17))]","[('dropout', (2, 3)), ('0.5 ( MOSI )', (4, 8)), ('0.3 ( MOSEI )', (9, 13)), ('regularization', (17, 18))]","[['dropout', '=', '0.3 ( MOSEI )'], ['regularization', 'set', 'dropout'], ['dropout', '=', '0.5 ( MOSI )'], ['dropout', '=', '0.3 ( MOSEI )']]",[],"[['Hyperparameters', 'as a measure of', 'regularization']]",[],[],[],[],[],[],sentiment_analysis,49,151
hyperparameters,"In addition , we also use dropout = 0.4 ( MOSI ) & 0.3 ( MOSEI ) for the Bi - GRU layers .",[],[],"[['Bi - GRU layers', 'use', 'dropout'], ['dropout', '=', '0.4 ( MOSI )']]",[],"[['Hyperparameters', 'for', 'Bi - GRU layers']]",[],[],[],[],[],[],sentiment_analysis,49,152
hyperparameters,"We employ ReLu activation function in the dense layers , and softmax activation in the final classification layer .",[],[],"[['ReLu activation function', 'in', 'dense layers'], ['softmax activation', 'in', 'final classification layer']]",[],"[['Hyperparameters', 'employ', 'ReLu activation function'], ['Hyperparameters', 'employ', 'softmax activation']]",[],[],[],[],[],[],sentiment_analysis,49,153
hyperparameters,"For training the network we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .","[('For', (0, 1)), ('set', (5, 6)), ('=', (9, 10)), ('use', (12, 13)), ('with', (15, 16)), ('train for', (22, 24))]","[('training the network', (1, 4)), ('batch size', (7, 9)), ('32', (10, 11)), ('Adam optimizer', (13, 15)), ('cross - entropy loss function', (16, 21)), ('50 epochs', (24, 26))]","[['training the network', 'set', 'batch size'], ['batch size', '=', '32'], ['training the network', 'use', 'Adam optimizer'], ['Adam optimizer', 'with', 'cross - entropy loss function'], ['training the network', 'train for', '50 epochs']]",[],"[['Hyperparameters', 'For', 'training the network']]",[],[],[],[],[],[],sentiment_analysis,49,154
results,"For MOSEI dataset , we obtain better performance with text .","[('For', (0, 1)), ('obtain', (5, 6)), ('with', (8, 9))]","[('MOSEI dataset', (1, 3)), ('better performance', (6, 8)), ('text', (9, 10))]","[['MOSEI dataset', 'obtain', 'better performance'], ['better performance', 'with', 'text']]",[],"[['Results', 'For', 'MOSEI dataset']]",[],[],[],[],[],[],sentiment_analysis,49,158
results,"For text - acoustic input pairs , we obtain the highest accuracies with 79. 74 % , 79.60 % and 79.32 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","[('obtain', (8, 9)), ('with', (12, 13)), ('for', (22, 23))]","[('text - acoustic input pairs', (1, 6)), ('highest accuracies', (10, 12)), ('79. 74 % , 79.60 % and 79.32 %', (13, 22)), ('MMMU - BA , MMUU - SA and MU - SA frameworks', (23, 35))]","[['text - acoustic input pairs', 'obtain', 'highest accuracies'], ['highest accuracies', 'with', '79. 74 % , 79.60 % and 79.32 %'], ['79. 74 % , 79.60 % and 79.32 %', 'for', 'MMMU - BA , MMUU - SA and MU - SA frameworks']]",[],[],"[['Results', 'For', 'text - acoustic input pairs']]",[],[],[],[],[],sentiment_analysis,49,160
results,"Finally , we experiment with tri-modal inputs and observe an improved performance of 79. 80 % , 79.76 % and 79.63 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","[('experiment with', (3, 5)), ('observe', (8, 9)), ('of', (12, 13)), ('for', (22, 23))]","[('tri-modal inputs', (5, 7)), ('improved performance', (10, 12)), ('79. 80 % , 79.76 % and 79.63 %', (13, 22)), ('MMMU - BA , MMUU - SA and MU - SA frameworks', (23, 35))]","[['tri-modal inputs', 'observe', 'improved performance'], ['improved performance', 'of', '79. 80 % , 79.76 % and 79.63 %'], ['79. 80 % , 79.76 % and 79.63 %', 'for', 'MMMU - BA , MMUU - SA and MU - SA frameworks']]",[],"[['Results', 'experiment with', 'tri-modal inputs']]",[],[],[],[],[],[],sentiment_analysis,49,162
results,The performance improvement was also found to be statistically significant ( T-test ) than the bimodality and uni-modality inputs .,"[('found to be', (5, 8)), ('than', (13, 14))]","[('performance improvement', (1, 3)), ('statistically significant ( T-test )', (8, 13)), ('bimodality and uni-modality inputs', (15, 19))]","[['performance improvement', 'found to be', 'statistically significant ( T-test )'], ['statistically significant ( T-test )', 'than', 'bimodality and uni-modality inputs']]",[],[],"[['Results', 'has', 'performance improvement']]",[],[],[],[],[],sentiment_analysis,49,164
results,"Further , we observe that the MMMU - BA framework reports the best accuracy of 79 . 80 % for the MOSEI dataset , thus supporting our claim that multi-modal attention framework ( i.e. MMMU - BA ) captures more information than the self - attention frameworks ( i.e. MMUU - SA & MU - SA ) .","[('observe', (3, 4)), ('reports', (10, 11)), ('of', (14, 15)), ('for', (19, 20))]","[('MMMU - BA framework', (6, 10)), ('best accuracy', (12, 14)), ('79 . 80 %', (15, 19)), ('MOSEI dataset', (21, 23))]","[['MMMU - BA framework', 'reports', 'best accuracy'], ['best accuracy', 'of', '79 . 80 %'], ['best accuracy', 'for', 'MOSEI dataset']]",[],"[['Results', 'observe', 'MMMU - BA framework']]",[],[],[],[],[],[],sentiment_analysis,49,165
research-problem,A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition,[],"[('EEG Emotion Recognition', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'EEG Emotion Recognition']]",[],[],[],[],sentiment_analysis,5,2
research-problem,"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .",[],"[('electroencephalograph ( EEG ) emotion recognition', (28, 34))]",[],[],[],[],"[['Contribution', 'has research problem', 'electroencephalograph ( EEG ) emotion recognition']]",[],[],[],[],sentiment_analysis,5,5
research-problem,"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .",[],"[('emotion recognition', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'emotion recognition']]",[],[],[],[],sentiment_analysis,5,13
model,"Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .","[('propose', (7, 8)), ('to learn', (14, 16)), ('for', (19, 20))]","[('novel neural network model BiHDM', (9, 14)), ('bi-hemispheric discrepancy', (17, 19)), ('EEG emotion recognition', (20, 23))]","[['novel neural network model BiHDM', 'to learn', 'bi-hemispheric discrepancy'], ['bi-hemispheric discrepancy', 'for', 'EEG emotion recognition']]",[],"[['Model', 'propose', 'novel neural network model BiHDM']]",[],[],[],[],[],[],sentiment_analysis,5,43
model,"BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .","[('aims to obtain', (1, 4)), ('between', (8, 9)), ('expected to contain', (17, 20)), ('to recognize', (23, 25))]","[('BiHDM', (0, 1)), ('deep discrepant features', (5, 8)), ('left and right hemispheres', (10, 14)), ('more discriminative information', (20, 23)), ('EEG emotion signals', (26, 29))]","[['BiHDM', 'aims to obtain', 'deep discrepant features'], ['deep discrepant features', 'between', 'left and right hemispheres'], ['left and right hemispheres', 'expected to contain', 'more discriminative information'], ['more discriminative information', 'to recognize', 'EEG emotion signals']]",[],[],"[['Model', 'has', 'BiHDM']]",[],[],[],[],[],sentiment_analysis,5,44
model,"Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .","[('to avoid losing', (2, 5)), ('of', (10, 11)), ('simplify', (16, 17)), ('by using', (22, 24)), ('which will construct', (31, 34)), ('generate', (39, 40)), ('for', (43, 44))]","[('intrinsic graph structural information', (6, 10)), ('EEG data', (11, 13)), ('graph structure learning process', (18, 22)), ('horizontal and vertical traversing RNNs', (25, 30)), ('complete relationship graph', (35, 38)), ('discriminative deep features', (40, 43)), ('all the EEG electrodes', (44, 48))]","[['graph structure learning process', 'to avoid losing', 'intrinsic graph structural information'], ['intrinsic graph structural information', 'of', 'EEG data'], ['graph structure learning process', 'by using', 'horizontal and vertical traversing RNNs'], ['horizontal and vertical traversing RNNs', 'which will construct', 'complete relationship graph'], ['complete relationship graph', 'generate', 'discriminative deep features'], ['discriminative deep features', 'for', 'all the EEG electrodes']]",[],"[['Model', 'simplify', 'graph structure learning process']]",[],[],[],[],[],[],sentiment_analysis,5,47
model,"After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .","[('After obtaining', (0, 2)), ('of', (5, 6)), ('extract', (11, 12)), ('between', (16, 17)), ('by performing', (19, 21)), ('for', (24, 25))]","[('deep features', (3, 5)), ('each electrodes', (6, 8)), ('asymmetric discrepancy information', (13, 16)), ('two hemispheres', (17, 19)), ('specific pairwise operations', (21, 24)), ('any paired symmetric electrodes', (25, 29))]","[['deep features', 'extract', 'asymmetric discrepancy information'], ['asymmetric discrepancy information', 'between', 'two hemispheres'], ['two hemispheres', 'by performing', 'specific pairwise operations'], ['specific pairwise operations', 'for', 'any paired symmetric electrodes'], ['deep features', 'of', 'each electrodes']]",[],"[['Model', 'After obtaining', 'deep features']]",[],[],[],[],[],[],sentiment_analysis,5,48
experimental-setup,"We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .",[],[],"[['released handcrafted features', 'to feed', 'model'], ['released handcrafted features', 'i.e.', 'differential entropy ( DE )'], ['differential entropy ( DE )', 'in', 'SEED and SEED - IV'], ['released handcrafted features', 'i.e.', 'Short - Time Fourier Transform ( STFT )'], ['Short - Time Fourier Transform ( STFT )', 'in', 'MPED']]",[],"[['Experimental setup', 'use', 'released handcrafted features']]",[],[],[],[],[],[],sentiment_analysis,5,158
experimental-setup,"Thus the sizes d N of the input sample X t are 5 62 , 5 62 and 1 62 for these three datasets , respectively .","[('of', (5, 6)), ('are', (11, 12))]","[('sizes d N', (2, 5)), ('input sample X t', (7, 11)), ('5 62 , 5 62 and 1 62', (12, 20))]","[['sizes d N', 'of', 'input sample X t'], ['input sample X t', 'are', '5 62 , 5 62 and 1 62']]",[],[],"[['Experimental setup', 'has', 'sizes d N']]",[],[],[],[],[],sentiment_analysis,5,159
experimental-setup,"Moreover , in the experiment , we respectively set the dimension d l of each electrode 's deep representation to 32 ; the parameters d g and K of the global high - level feature to 32 and 6 ; and the dimension do of the output feature to 16 without elaborate traversal .",[],[],"[['experiment', 'set', 'dimension do'], ['dimension do', 'of', 'output feature'], ['output feature', 'to', '16'], ['output feature', 'without', 'elaborate traversal'], ['experiment', 'set', 'parameters d g and K'], ['parameters d g and K', 'of', 'global high - level feature'], ['global high - level feature', 'to', '32 and 6'], ['experiment', 'set', 'dimension d l'], ['dimension d l', 'of', ""each electrode 's deep representation""], [""each electrode 's deep representation"", 'to', '32']]",[],"[['Experimental setup', 'in', 'experiment']]",[],[],[],[],[],[],sentiment_analysis,5,160
experimental-setup,"Specifically , we implemented BiHDM using Tensor Flow on one Nvidia 1080 Ti GPU .","[('implemented', (3, 4)), ('using', (5, 6)), ('on', (8, 9))]","[('BiHDM', (4, 5)), ('Tensor Flow', (6, 8)), ('one Nvidia 1080 Ti GPU', (9, 14))]","[['BiHDM', 'using', 'Tensor Flow'], ['Tensor Flow', 'on', 'one Nvidia 1080 Ti GPU']]",[],"[['Experimental setup', 'implemented', 'BiHDM']]",[],[],[],[],[],[],sentiment_analysis,5,161
experimental-setup,"The learning rate , momentum and weight decay rate are set as 0.003 , 0.9 and 0.95 respectively .","[('set as', (10, 12))]","[('learning rate , momentum and weight decay rate', (1, 9)), ('0.003 , 0.9 and 0.95', (12, 17))]","[['learning rate , momentum and weight decay rate', 'set as', '0.003 , 0.9 and 0.95']]",[],[],"[['Experimental setup', 'has', 'learning rate , momentum and weight decay rate']]",[],[],[],[],[],sentiment_analysis,5,162
experimental-setup,The network is trained using SGD with batch size of 200 .,"[('trained using', (3, 5)), ('with', (6, 7)), ('of', (9, 10))]","[('network', (1, 2)), ('SGD', (5, 6)), ('batch size', (7, 9)), ('200', (10, 11))]","[['network', 'trained using', 'SGD'], ['SGD', 'with', 'batch size'], ['batch size', 'of', '200']]",[],[],"[['Experimental setup', 'has', 'network']]",[],[],[],[],[],sentiment_analysis,5,163
experimental-setup,"In addition , we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section , and discuss the other two types of operations in section III - D.","[('adopt', (4, 5)), ('as', (7, 8)), ('of', (11, 12))]","[('subtraction', (6, 7)), ('pairwise operation', (9, 11)), ('BiHDM model', (13, 15))]","[['subtraction', 'as', 'pairwise operation'], ['pairwise operation', 'of', 'BiHDM model']]",[],"[['Experimental setup', 'adopt', 'subtraction']]",[],[],[],[],[],[],sentiment_analysis,5,164
experiments,1 ) The subject - dependent experiment :,[],"[('subject - dependent experiment', (3, 7))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'subject - dependent experiment']]","[['subject - dependent experiment', 'has', 'Baselines']]",sentiment_analysis,5,167
experiments,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .","[('using', (13, 14)), ('including', (17, 18))]","[('twelve methods', (14, 16)), ('linear support vector machine ( SVM )', (18, 25)), ('random forest ( RF )', (26, 31)), ('canonical correlation analysis ( CCA )', (32, 38)), ('group sparse canonical correlation analysis ( GSCCA )', (39, 47)), ('deep believe network ( DBN )', (48, 54)), ('graph regularization sparse linear regression ( GRSLR )', (55, 63)), ('graph convolutional neural network ( GCNN )', (64, 71)), ('dynamical graph convolutional neural network ( DGCNN )', (72, 80)), ('domain adversarial neural networks ( DANN )', (81, 88)), ('bi-hemisphere domain adversarial neural network ( BiDANN )', (89, 97)), ('EmotionMeter', (98, 99)), ('attention - long short - term memory ( A - LSTM )', (101, 113))]","[['twelve methods', 'including', 'linear support vector machine ( SVM )'], ['twelve methods', 'including', 'random forest ( RF )'], ['twelve methods', 'including', 'canonical correlation analysis ( CCA )'], ['twelve methods', 'including', 'group sparse canonical correlation analysis ( GSCCA )'], ['twelve methods', 'including', 'deep believe network ( DBN )'], ['twelve methods', 'including', 'graph regularization sparse linear regression ( GRSLR )'], ['twelve methods', 'including', 'graph convolutional neural network ( GCNN )'], ['twelve methods', 'including', 'dynamical graph convolutional neural network ( DGCNN )'], ['twelve methods', 'including', 'domain adversarial neural networks ( DANN )'], ['twelve methods', 'including', 'bi-hemisphere domain adversarial neural network ( BiDANN )'], ['twelve methods', 'including', 'EmotionMeter'], ['twelve methods', 'including', 'attention - long short - term memory ( A - LSTM )']]",[],[],[],[],"[['Baselines', 'using', 'twelve methods']]",[],[],[],sentiment_analysis,5,171
experiments,"From , we can see that the proposed BiHDM model outperforms all the compared methods on all the three public EEG emotional datasets , which verifies the effectiveness of BiHDM .","[('see', (4, 5)), ('outperforms', (10, 11)), ('on', (15, 16)), ('verifies', (25, 26))]","[('proposed BiHDM model', (7, 10)), ('all the compared methods', (11, 15)), ('all the three public EEG emotional datasets', (16, 23)), ('effectiveness of BiHDM', (27, 30))]","[['proposed BiHDM model', 'outperforms', 'all the compared methods'], ['all the compared methods', 'verifies', 'effectiveness of BiHDM'], ['all the compared methods', 'on', 'all the three public EEG emotional datasets']]",[],[],[],[],"[['Results', 'see', 'proposed BiHDM model']]",[],[],[],sentiment_analysis,5,175
experiments,"Especially for the result on SEED - IV , the proposed method improves over the state - of - the - art method Emotion - Meter by 4 % .","[('on', (4, 5)), ('improves over', (12, 14)), ('by', (26, 27))]","[('SEED - IV', (5, 8)), ('proposed method', (10, 12)), ('state - of - the - art method Emotion - Meter', (15, 26)), ('4 %', (27, 29))]","[['proposed method', 'improves over', 'state - of - the - art method Emotion - Meter'], ['state - of - the - art method Emotion - Meter', 'by', '4 %']]","[['SEED - IV', 'has', 'proposed method']]",[],[],[],"[['Results', 'on', 'SEED - IV']]",[],[],[],sentiment_analysis,5,176
experiments,"Besides , we can see that the compared method BiDANN , which also considers the bi-hemispheric asymmetry , achieves a comparable performance .","[('see that', (4, 6)), ('considers', (13, 14)), ('achieves', (18, 19))]","[('compared method BiDANN', (7, 10)), ('bi-hemispheric asymmetry', (15, 17)), ('comparable performance', (20, 22))]","[['compared method BiDANN', 'considers', 'bi-hemispheric asymmetry'], ['compared method BiDANN', 'achieves', 'comparable performance']]",[],[],[],[],"[['Results', 'see that', 'compared method BiDANN']]",[],[],[],sentiment_analysis,5,177
experiments,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .","[('shows', (0, 1)), ('see', (12, 13)), ('is', (14, 15)), ('than', (17, 18))]","[('t- test statistical analysis results', (2, 7)), ('BiHDM', (13, 14)), ('significantly better', (15, 17)), ('baseline method', (19, 21))]","[['t- test statistical analysis results', 'see', 'BiHDM'], ['BiHDM', 'is', 'significantly better'], ['significantly better', 'than', 'baseline method'], ['t- test statistical analysis results', 'see', 'BiHDM'], ['BiHDM', 'is', 'significantly better'], ['significantly better', 'than', 'baseline method']]",[],[],[],[],"[['Results', 'shows', 't- test statistical analysis results']]",[],"[['Results', 'has', 't- test statistical analysis results']]",[],sentiment_analysis,5,183
experiments,2 ) The subject - independent experiment :,[],"[('subject - independent experiment', (3, 7))]",[],[],[],[],[],[],[],"[['Tasks', 'has', 'subject - independent experiment']]","[['subject - independent experiment', 'has', 'Baselines']]",sentiment_analysis,5,188
experiments,"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .","[('use', (8, 9)), ('including', (11, 12))]","[('twelve methods', (9, 11)), ('Kullback - Leibler importance estimation procedure ( KLIEP )', (12, 21)), ('unconstrained least - squares importance fitting ( ULSIF )', (22, 31)), ('selective transfer machine ( STM )', (32, 38)), ('linear SVM , transfer component analysis ( TCA )', (39, 48)), ('transfer component analysis ( TCA )', (49, 55)), ('geodesic flow kernel ( GFK )', (56, 62)), ('DANN', (63, 64)), ('DGCNN', (65, 66)), ('deep adaptation network ( DAN )', (67, 73)), ('BiDANN', (74, 75)), ('A - LSTM', (77, 80))]","[['twelve methods', 'including', 'Kullback - Leibler importance estimation procedure ( KLIEP )'], ['twelve methods', 'including', 'unconstrained least - squares importance fitting ( ULSIF )'], ['twelve methods', 'including', 'selective transfer machine ( STM )'], ['twelve methods', 'including', 'linear SVM , transfer component analysis ( TCA )'], ['twelve methods', 'including', 'transfer component analysis ( TCA )'], ['twelve methods', 'including', 'geodesic flow kernel ( GFK )'], ['twelve methods', 'including', 'DANN'], ['twelve methods', 'including', 'DGCNN'], ['twelve methods', 'including', 'deep adaptation network ( DAN )'], ['twelve methods', 'including', 'BiDANN'], ['twelve methods', 'including', 'A - LSTM']]",[],[],[],[],"[['Baselines', 'use', 'twelve methods']]",[],[],[],sentiment_analysis,5,193
experiments,"The results are shown in From , it can be clearly seen that the proposed BiHDM method achieves the best performance in the three public datasets , which verifies the effectiveness of BiHDM in dealing with subject - independent EEG emotion recognition .","[('in', (4, 5)), ('seen that', (11, 13)), ('achieves', (17, 18)), ('verifies', (28, 29)), ('in dealing with', (33, 36))]","[('proposed BiHDM method', (14, 17)), ('best performance', (19, 21)), ('three public datasets', (23, 26)), ('effectiveness of BiHDM', (30, 33)), ('subject - independent EEG emotion recognition', (36, 42))]","[['proposed BiHDM method', 'achieves', 'best performance'], ['best performance', 'in', 'three public datasets'], ['best performance', 'verifies', 'effectiveness of BiHDM'], ['effectiveness of BiHDM', 'in dealing with', 'subject - independent EEG emotion recognition']]",[],[],[],[],"[['Results', 'seen that', 'proposed BiHDM method']]",[],[],[],sentiment_analysis,5,197
experiments,"For the three datasets , the improvements on accuracy are 2.2 % , 3.5 % and 2.4 % , respectively , when compared with the existing state - of - the - art methods .","[('For', (0, 1)), ('improvements on', (6, 8)), ('are', (9, 10)), ('compared with', (22, 24))]","[('three datasets', (2, 4)), ('accuracy', (8, 9)), ('2.2 % , 3.5 % and 2.4 %', (10, 18)), ('existing state - of - the - art methods', (25, 34))]","[['three datasets', 'improvements on', 'accuracy'], ['accuracy', 'are', '2.2 % , 3.5 % and 2.4 %'], ['2.2 % , 3.5 % and 2.4 %', 'compared with', 'existing state - of - the - art methods']]",[],[],[],[],"[['Results', 'For', 'three datasets']]",[],[],[],sentiment_analysis,5,198
experiments,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .",[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,5,200
research-problem,Exploiting Document Knowledge for Aspect - level Sentiment Classification,[],"[('Aspect - level Sentiment Classification', (4, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - level Sentiment Classification']]",[],[],[],[],sentiment_analysis,50,2
model,"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .","[('explore', (3, 4)), ('to incorporate', (7, 9))]","[('two transfer methods', (4, 7)), ('knowledge', (12, 13)), ('pretraining', (14, 15)), ('multi-task learning', (16, 18))]","[['two transfer methods', 'to incorporate', 'knowledge']]","[['knowledge', 'name', 'pretraining'], ['knowledge', 'name', 'multi-task learning']]","[['Model', 'explore', 'two transfer methods']]",[],[],[],[],[],[],sentiment_analysis,50,21
code,Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.,[],[],[],[],[],[],[],[],[],[],[],sentiment_analysis,50,24
hyperparameters,"In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .","[('to initialize', (12, 14)), ('when', (17, 18)), ('not conducted for', (20, 23))]","[('300 - dimension Glo Ve vectors', (4, 10)), ('E and E', (14, 17)), ('pretraining', (18, 19)), ('weight initialization', (23, 25))]","[['300 - dimension Glo Ve vectors', 'to initialize', 'E and E'], ['E and E', 'when', 'pretraining'], ['pretraining', 'not conducted for', 'weight initialization']]",[],[],"[['Hyperparameters', 'has', '300 - dimension Glo Ve vectors']]",[],[],[],[],[],sentiment_analysis,50,87
hyperparameters,These vectors are also used for initializing E in the pretraining phase .,"[('used for', (4, 6)), ('in', (8, 9))]","[('vectors', (1, 2)), ('initializing E', (6, 8)), ('pretraining phase', (10, 12))]","[['vectors', 'used for', 'initializing E'], ['initializing E', 'in', 'pretraining phase']]",[],[],"[['Hyperparameters', 'has', 'vectors']]",[],[],[],[],[],sentiment_analysis,50,88
hyperparameters,We randomly sample 20 % of the original training data from the aspectlevel dataset as the development set and only use the remaining 80 % for training .,"[('randomly sample', (1, 3)), ('of', (5, 6)), ('from', (10, 11)), ('as', (14, 15))]","[('20 %', (3, 5)), ('original training data', (7, 10)), ('aspectlevel dataset', (12, 14)), ('development set', (16, 18))]","[['20 %', 'as', 'development set'], ['20 %', 'of', 'original training data'], ['original training data', 'from', 'aspectlevel dataset']]",[],"[['Hyperparameters', 'randomly sample', '20 %']]",[],[],[],[],[],[],sentiment_analysis,50,90
hyperparameters,"For all experiments , the dimension of LSTM hidden vectors is set to 300 , ?","[('For', (0, 1)), ('of', (6, 7)), ('set to', (11, 13))]","[('all experiments', (1, 3)), ('dimension', (5, 6)), ('LSTM hidden vectors', (7, 10)), ('300', (13, 14))]","[['dimension', 'of', 'LSTM hidden vectors'], ['LSTM hidden vectors', 'set to', '300']]","[['all experiments', 'has', 'dimension']]","[['Hyperparameters', 'For', 'all experiments']]",[],[],[],[],[],[],sentiment_analysis,50,91
hyperparameters,"is set to 0.1 , and we use dropout with probability 0.5 on sentence / document representations before the output layer .","[('use', (7, 8)), ('with', (9, 10)), ('on', (12, 13)), ('before', (17, 18))]","[('dropout', (8, 9)), ('probability 0.5', (10, 12)), ('sentence / document representations', (13, 17)), ('output layer', (19, 21))]","[['dropout', 'with', 'probability 0.5'], ['probability 0.5', 'on', 'sentence / document representations'], ['sentence / document representations', 'before', 'output layer']]",[],"[['Hyperparameters', 'use', 'dropout']]",[],[],[],[],[],[],sentiment_analysis,50,92
hyperparameters,We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001 .,[],[],"[['RMSProp', 'as', 'optimizer'], ['optimizer', 'with', 'decay rate'], ['decay rate', 'set to', '0.9'], ['optimizer', 'with', 'base learning rate'], ['base learning rate', 'set to', '0.001']]",[],[],"[['Hyperparameters', 'use', 'RMSProp']]",[],[],[],[],[],sentiment_analysis,50,93
hyperparameters,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]","[['mini - batch size', 'set to', '32']]",[],[],"[['Hyperparameters', 'use', 'mini - batch size']]",[],[],[],[],[],sentiment_analysis,50,94
results,"We observe that PRET is very helpful , and consistently gives a 1 - 3 % increase in accuracy over LSTM + ATT across all datasets .","[('observe that', (1, 3)), ('is', (4, 5)), ('consistently gives', (9, 11)), ('in', (17, 18)), ('over', (19, 20))]","[('PRET', (3, 4)), ('very helpful', (5, 7)), ('1 - 3 % increase', (12, 17)), ('accuracy', (18, 19)), ('LSTM + ATT', (20, 23))]","[['PRET', 'consistently gives', '1 - 3 % increase'], ['1 - 3 % increase', 'in', 'accuracy'], ['accuracy', 'over', 'LSTM + ATT'], ['PRET', 'is', 'very helpful']]",[],"[['Results', 'observe that', 'PRET']]",[],[],[],[],[],[],sentiment_analysis,50,100
results,"MULT gives similar performance as LSTM + ATT on D1 and D2 , but improvements can be clearly observed for D3 and D4 .","[('gives', (1, 2)), ('as', (4, 5)), ('on', (8, 9))]","[('MULT', (0, 1)), ('similar performance', (2, 4)), ('LSTM + ATT', (5, 8)), ('D1 and D2', (9, 12))]","[['MULT', 'gives', 'similar performance'], ['similar performance', 'as', 'LSTM + ATT'], ['LSTM + ATT', 'on', 'D1 and D2']]",[],[],"[['Results', 'has', 'MULT']]",[],[],[],[],[],sentiment_analysis,50,102
results,The combination ( PRET + MULT ) over all yields better results .,"[('yields', (9, 10))]","[('combination ( PRET + MULT )', (1, 7)), ('better results', (10, 12))]","[['combination ( PRET + MULT )', 'yields', 'better results']]",[],[],"[['Results', 'has', 'combination ( PRET + MULT )']]",[],[],[],[],[],sentiment_analysis,50,103
results,( 2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .,"[('in', (8, 9)), ('of', (12, 13)), ('are', (16, 17))]","[('numbers of neutral examples', (4, 8)), ('test sets', (10, 12)), ('D3 and D4', (13, 16)), ('very small', (17, 19))]","[['numbers of neutral examples', 'in', 'test sets'], ['test sets', 'are', 'very small'], ['test sets', 'of', 'D3 and D4']]",[],[],"[['Results', 'has', 'numbers of neutral examples']]",[],[],[],[],[],sentiment_analysis,50,105
ablation-analysis,"( 2 ) Overall , transfers of the LSTM and embedding layer are more useful than the output layer .","[('of', (6, 7)), ('are', (12, 13)), ('than', (15, 16))]","[('transfers', (5, 6)), ('LSTM and embedding layer', (8, 12)), ('more useful', (13, 15)), ('output layer', (17, 19))]","[['transfers', 'of', 'LSTM and embedding layer'], ['LSTM and embedding layer', 'are', 'more useful'], ['more useful', 'than', 'output layer']]",[],[],"[['Ablation analysis', 'has', 'transfers']]",[],[],[],[],[],sentiment_analysis,50,115
ablation-analysis,( 3 ) Transfer of the embedding layer is more helpful on D3 and D4 .,"[('is', (8, 9)), ('on', (11, 12))]","[('Transfer of the embedding layer', (3, 8)), ('more helpful', (9, 11)), ('D3 and D4', (12, 15))]","[['Transfer of the embedding layer', 'is', 'more helpful'], ['more helpful', 'on', 'D3 and D4']]",[],[],"[['Ablation analysis', 'has', 'Transfer of the embedding layer']]",[],[],[],[],[],sentiment_analysis,50,117
ablation-analysis,Sentiment information is not adequately captured by Glo Ve word embeddings .,"[('not adequately captured by', (3, 7))]","[('Sentiment information', (0, 2)), ('Glo Ve word embeddings', (7, 11))]","[['Sentiment information', 'not adequately captured by', 'Glo Ve word embeddings']]",[],[],"[['Ablation analysis', 'has', 'Sentiment information']]",[],[],[],[],[],sentiment_analysis,50,119
research-problem,Fine - grained Sentiment Classification using BERT,[],"[('Fine - grained Sentiment Classification', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Fine - grained Sentiment Classification']]",[],[],[],[],sentiment_analysis,51,2
research-problem,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",[],"[('Sentiment classification', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment classification']]",[],[],[],[],sentiment_analysis,51,4
model,"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .","[('use', (5, 6)), ('finetune it for', (11, 14)), ('on', (21, 22))]","[('pretrained BERT model', (7, 10)), ('fine - grained sentiment classification task', (15, 21)), ('Stanford Sentiment Treebank ( SST ) dataset', (23, 30))]","[['pretrained BERT model', 'finetune it for', 'fine - grained sentiment classification task'], ['fine - grained sentiment classification task', 'on', 'Stanford Sentiment Treebank ( SST ) dataset']]",[],"[['Model', 'use', 'pretrained BERT model']]",[],[],[],[],[],[],sentiment_analysis,51,22
baselines,1 ) Word embeddings :,[],"[('Word embeddings', (2, 4))]",[],[],[],"[['Baselines', 'has', 'Word embeddings']]",[],[],[],[],"[['Word embeddings', 'has', 'word vectors']]",sentiment_analysis,51,127
baselines,"In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .","[('pretrained on', (7, 9)), ('such as', (12, 14)), ('averaged to get', (17, 20)), ('fed to', (27, 29)), ('to compute', (32, 34))]","[('word vectors', (5, 7)), ('large text corpus', (9, 12)), ('Wikipedia dump', (14, 16)), ('document vector', (21, 23)), ('sentiment classifier', (30, 32)), ('sentiment score', (35, 37))]","[['word vectors', 'averaged to get', 'document vector'], ['document vector', 'fed to', 'sentiment classifier'], ['sentiment classifier', 'to compute', 'sentiment score'], ['word vectors', 'pretrained on', 'large text corpus'], ['large text corpus', 'such as', 'Wikipedia dump']]",[],[],[],[],[],[],[],[],sentiment_analysis,51,128
baselines,2 ) Recursive networks :,[],"[('Recursive networks', (2, 4))]",[],[],[],"[['Baselines', 'has', 'Recursive networks']]",[],[],[],[],"[['Recursive networks', 'has', 'Various types of recursive neural networks ( RNN )']]",sentiment_analysis,51,129
baselines,Various types of recursive neural networks ( RNN ) have been applied on SST .,"[('applied on', (11, 13))]","[('Various types of recursive neural networks ( RNN )', (0, 9)), ('SST', (13, 14))]","[['Various types of recursive neural networks ( RNN )', 'applied on', 'SST']]",[],[],[],[],[],[],[],[],sentiment_analysis,51,130
baselines,3 ) Recurrent networks :,[],"[('Recurrent networks', (2, 4))]",[],[],[],"[['Baselines', 'has', 'Recurrent networks']]",[],[],[],[],"[['Recurrent networks', 'has', 'Sophisticated recurrent networks']]",sentiment_analysis,51,133
baselines,Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .,"[('such as', (3, 5)), ('applied on', (17, 19))]","[('Sophisticated recurrent networks', (0, 3)), ('left - to - right and bidrectional LSTM networks', (5, 14)), ('SST', (19, 20))]","[['Sophisticated recurrent networks', 'such as', 'left - to - right and bidrectional LSTM networks'], ['left - to - right and bidrectional LSTM networks', 'applied on', 'SST']]",[],[],[],[],[],[],[],[],sentiment_analysis,51,134
baselines,4 ) Convolutional networks :,[],"[('Convolutional networks', (2, 4))]",[],[],[],"[['Baselines', 'has', 'Convolutional networks']]",[],[],[],[],"[['Convolutional networks', 'has', 'input sequences']]",sentiment_analysis,51,135
baselines,"In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .","[('passed through', (8, 10)), ('as', (17, 18))]","[('input sequences', (5, 7)), ('1 - dimensional convolutional neural network', (11, 17)), ('feature extractors', (18, 20))]","[['input sequences', 'passed through', '1 - dimensional convolutional neural network'], ['1 - dimensional convolutional neural network', 'as', 'feature extractors']]",[],[],[],[],[],[],[],[],sentiment_analysis,51,136
results,"We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .","[('see that', (2, 4)), ('performs', (13, 14)), ('in terms of', (15, 18)), ('than', (19, 20))]","[('our model', (4, 6)), ('better', (14, 15)), ('accuracy', (18, 19)), ('many popular and sophisticated NLP models', (20, 26))]","[['our model', 'performs', 'better'], ['better', 'in terms of', 'accuracy'], ['accuracy', 'than', 'many popular and sophisticated NLP models']]",[],"[['Results', 'see that', 'our model']]",[],[],[],[],[],[],sentiment_analysis,51,145
research-problem,Context - Dependent Sentiment Analysis in User- Generated Videos,[],"[('Context - Dependent Sentiment Analysis', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Context - Dependent Sentiment Analysis']]",[],[],[],[],sentiment_analysis,6,2
research-problem,"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .",[],"[('identification of sentiments in videos', (13, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'identification of sentiments in videos']]",[],[],[],[],sentiment_analysis,6,4
research-problem,"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .",[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Sentiment analysis']]",[],[],[],[],sentiment_analysis,6,9
research-problem,"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .",[],"[('Emotion recognition', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion recognition']]",[],[],[],[],sentiment_analysis,6,11
research-problem,"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .",[],"[('multimodal sentiment analysis', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'multimodal sentiment analysis']]",[],[],[],[],sentiment_analysis,6,22
model,"In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .","[('develop', (11, 12)), ('based on', (14, 16)), ('takes', (23, 24)), ('as', (28, 29)), ('extracts', (31, 32))]","[('framework', (13, 14)), ('long shortterm memory ( LSTM )', (16, 22)), ('sequence of utterances', (25, 28)), ('input', (29, 30)), ('contextual utterancelevel features', (32, 35))]","[['framework', 'based on', 'long shortterm memory ( LSTM )'], ['long shortterm memory ( LSTM )', 'extracts', 'contextual utterancelevel features'], ['long shortterm memory ( LSTM )', 'takes', 'sequence of utterances'], ['sequence of utterances', 'as', 'input']]",[],"[['Model', 'develop', 'framework']]",[],[],[],[],[],[],sentiment_analysis,6,36
model,"Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .","[('preserves', (2, 3)), ('of', (6, 7)), ('enables', (9, 10)), ('to share', (12, 14)), ('providing', (17, 18)), ('to', (20, 21))]","[('sequential order', (4, 6)), ('utterances', (7, 8)), ('consecutive utterances', (10, 12)), ('information', (14, 15)), ('contextual information', (18, 20)), ('utterance - level sentiment classification process', (22, 28))]","[['sequential order', 'of', 'utterances'], ['consecutive utterances', 'to share', 'information'], ['information', 'providing', 'contextual information'], ['contextual information', 'to', 'utterance - level sentiment classification process']]",[],"[['Model', 'preserves', 'sequential order'], ['Model', 'enables', 'consecutive utterances']]",[],[],[],[],[],[],sentiment_analysis,6,40
results,"As expected , trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework .","[('help', (7, 8)), ('to outperform', (12, 14))]","[('trained contextual unimodal features', (3, 7)), ('hierarchical fusion framework', (9, 12)), ('non-hierarchical framework', (15, 17))]","[['trained contextual unimodal features', 'help', 'hierarchical fusion framework'], ['hierarchical fusion framework', 'to outperform', 'non-hierarchical framework']]",[],[],"[['Results', 'has', 'trained contextual unimodal features']]",[],[],[],[],[],sentiment_analysis,6,237
results,"The non-hierarchical model outperforms the baseline uni - SVM , which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline .","[('outperforms', (3, 4))]","[('non-hierarchical model', (1, 3)), ('baseline uni - SVM', (5, 9))]","[['non-hierarchical model', 'outperforms', 'baseline uni - SVM']]",[],[],"[['Results', 'has', 'non-hierarchical model']]",[],[],[],[],[],sentiment_analysis,6,240
results,It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .,"[('noted', (4, 5)), ('perform', (14, 15)), ('on', (17, 18))]","[('both sc - LSTM and bc - LSTM', (6, 14)), ('quite well', (15, 17)), ('multimodal emotion recognition and sentiment analysis datasets', (19, 26))]","[['both sc - LSTM and bc - LSTM', 'perform', 'quite well'], ['quite well', 'on', 'multimodal emotion recognition and sentiment analysis datasets']]",[],"[['Results', 'noted', 'both sc - LSTM and bc - LSTM']]",[],[],[],[],[],[],sentiment_analysis,6,242
results,"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .","[('has access to', (4, 7)), ('of', (13, 14)), ('performs', (19, 20)), ('on', (22, 23))]","[('bc - LSTM', (1, 4)), ('preceding and following information', (9, 13)), ('utterance sequence', (15, 17)), ('consistently better', (20, 22)), ('all the datasets over sc - LSTM', (23, 30))]","[['bc - LSTM', 'has access to', 'preceding and following information'], ['preceding and following information', 'of', 'utterance sequence'], ['utterance sequence', 'performs', 'consistently better'], ['consistently better', 'on', 'all the datasets over sc - LSTM']]",[],[],"[['Results', 'has', 'bc - LSTM']]",[],[],[],[],[],sentiment_analysis,6,243
results,The performance improvement is in the range of 0.3 % to 1.5 % on MOSI and MOUD datasets .,"[('is', (3, 4)), ('of', (7, 8)), ('on', (13, 14))]","[('performance improvement', (1, 3)), ('in the range', (4, 7)), ('0.3 % to 1.5 %', (8, 13)), ('MOSI and MOUD datasets', (14, 18))]","[['performance improvement', 'is', 'in the range'], ['in the range', 'of', '0.3 % to 1.5 %'], ['0.3 % to 1.5 %', 'on', 'MOSI and MOUD datasets']]",[],[],"[['Results', 'has', 'performance improvement']]",[],[],[],[],[],sentiment_analysis,6,245
results,"On the IEMOCAP dataset , the performance improvement of bc - LSTM and sc - LSTM over h- LSTM is in the range of 1 % to 5 % .","[('On', (0, 1)), ('of', (8, 9)), ('over', (16, 17)), ('in the range of', (20, 24))]","[('IEMOCAP dataset', (2, 4)), ('performance improvement', (6, 8)), ('bc - LSTM and sc - LSTM', (9, 16)), ('h- LSTM', (17, 19)), ('1 % to 5 %', (24, 29))]","[['performance improvement', 'of', 'bc - LSTM and sc - LSTM'], ['bc - LSTM and sc - LSTM', 'over', 'h- LSTM'], ['performance improvement', 'in the range of', '1 % to 5 %']]","[['IEMOCAP dataset', 'has', 'performance improvement']]","[['Results', 'On', 'IEMOCAP dataset']]",[],[],[],[],[],[],sentiment_analysis,6,246
results,Every LSTM network variant has outperformed the baseline uni - SVM on all the datasets by the margin of 2 % to 5 % ( see ) .,"[('outperformed', (5, 6)), ('on', (11, 12)), ('by the margin of', (15, 19))]","[('Every LSTM network variant', (0, 4)), ('baseline uni - SVM', (7, 11)), ('all the datasets', (12, 15)), ('2 % to 5 %', (19, 24))]","[['Every LSTM network variant', 'outperformed', 'baseline uni - SVM'], ['baseline uni - SVM', 'on', 'all the datasets'], ['all the datasets', 'by the margin of', '2 % to 5 %']]",[],[],"[['Results', 'has', 'Every LSTM network variant']]",[],[],[],[],[],sentiment_analysis,6,248
results,Experimental results in show that the proposed method outperformes by a significant margin .,"[('show', (3, 4)), ('outperformes by', (8, 10))]","[('proposed method', (6, 8)), ('significant margin', (11, 13))]","[['proposed method', 'outperformes by', 'significant margin']]",[],"[['Results', 'show', 'proposed method']]",[],[],[],[],[],[],sentiment_analysis,6,255
research-problem,MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS,[],"[('MULTI - MODAL EMOTION RECOGNITION', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'MULTI - MODAL EMOTION RECOGNITION']]",[],[],[],[],sentiment_analysis,7,2
research-problem,Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .,[],"[('Emotion recognition', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Emotion recognition']]",[],[],[],[],sentiment_analysis,7,4
approach,We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .,"[('explore', (1, 2))]","[('various deep learning based architectures', (2, 7))]",[],[],"[['Approach', 'explore', 'various deep learning based architectures']]",[],[],[],[],[],[],sentiment_analysis,7,15
approach,We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .,"[('combine them in', (2, 5)), ('to allow', (9, 11)), ('across', (13, 14)), ('using', (17, 18))]","[('ensemble based architecture', (6, 9)), ('training', (12, 13)), ('different modalities', (15, 17)), ('variations of the better individual models', (19, 25))]","[['ensemble based architecture', 'to allow', 'training'], ['training', 'across', 'different modalities'], ['different modalities', 'using', 'variations of the better individual models']]",[],[],[],[],"[['various deep learning based architectures', 'combine them in', 'ensemble based architecture']]",[],[],[],sentiment_analysis,7,16
approach,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .","[('consists of', (2, 4)), ('using techniques', (24, 26)), ('such as', (26, 28))]","[('Our ensemble', (0, 2)), ('Long Short Term Memory networks', (4, 9)), ('Convolution Neural Networks', (10, 13)), ('fully connected Multi - Layer Perceptrons', (14, 20)), ('Dropout', (28, 29)), ('adaptive optimizers', (30, 32)), ('Adam', (34, 35)), ('pretrained word - embedding models and Attention based RNN decoders', (36, 46))]","[['Our ensemble', 'consists of', 'Long Short Term Memory networks'], ['Our ensemble', 'consists of', 'Convolution Neural Networks'], ['Our ensemble', 'consists of', 'fully connected Multi - Layer Perceptrons'], ['Our ensemble', 'using techniques', 'Dropout'], ['Our ensemble', 'using techniques', 'adaptive optimizers'], ['adaptive optimizers', 'such as', 'Adam'], ['Our ensemble', 'using techniques', 'pretrained word - embedding models and Attention based RNN decoders']]",[],[],"[['Approach', 'has', 'Our ensemble']]",[],[],[],[],[],sentiment_analysis,7,17
approach,This allows us to individually target each modality and only perform feature fusion at the final stage .,"[('allows us to', (1, 4)), ('each', (6, 7)), ('at', (13, 14))]","[('individually target', (4, 6)), ('modality', (7, 8)), ('perform feature fusion', (10, 13)), ('final stage', (15, 17))]","[['perform feature fusion', 'at', 'final stage'], ['individually target', 'each', 'modality']]",[],"[['Approach', 'allows us to', 'perform feature fusion'], ['Approach', 'allows us to', 'individually target']]",[],[],[],[],[],[],sentiment_analysis,7,18
hyperparameters,"For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .",[],[],"[['text transcript', 'use', 'pretrained Glove embeddings'], ['pretrained Glove embeddings', 'of', 'dimension 300'], ['pretrained Glove embeddings', 'along with', 'maximum sequence length'], ['maximum sequence length', 'of', '500'], ['pretrained Glove embeddings', 'to obtain', '( 500,300 ) vector'], ['( 500,300 ) vector', 'for', 'each utterance'], ['text transcript', 'of', 'each of the utterance']]",[],[],"[['Hyperparameters', 'has', 'text transcript']]",[],[],[],[],[],sentiment_analysis,7,64
hyperparameters,"For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .","[('For', (0, 1)), ('for', (5, 6)), ('such as', (9, 11)), ('sample', (18, 19)), ('between', (23, 24)), ('split them into', (31, 34))]","[('Mocap data', (2, 4)), ('each different mode', (6, 9)), ('face , hand , head rotation', (11, 17)), ('feature values', (21, 23)), ('start and finish time values', (25, 30)), ('200 partitioned arrays', (34, 37))]","[['Mocap data', 'for', 'each different mode'], ['each different mode', 'such as', 'face , hand , head rotation'], ['each different mode', 'sample', 'feature values'], ['feature values', 'split them into', '200 partitioned arrays'], ['feature values', 'between', 'start and finish time values']]",[],"[['Hyperparameters', 'For', 'Mocap data']]",[],[],[],[],[],[],sentiment_analysis,7,65
hyperparameters,"We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .","[('average each of', (2, 5)), ('along', (8, 9)), ('concatenate all of them to obtain', (28, 34)), ('for', (39, 40))]","[('200 arrays', (6, 8)), ('columns ( 165 for faces , 18 for hands , and 6 for rotation )', (10, 25)), ('( 200,189 ) dimension vector', (34, 39)), ('utterance', (41, 42))]","[['200 arrays', 'concatenate all of them to obtain', '( 200,189 ) dimension vector'], ['( 200,189 ) dimension vector', 'for', 'utterance'], ['200 arrays', 'along', 'columns ( 165 for faces , 18 for hands , and 6 for rotation )']]",[],[],[],[],"[['Mocap data', 'average each of', '200 arrays']]",[],[],[],sentiment_analysis,7,66
results,"Our performance matches the prior state of the art , however the comparison is not fair .","[('matches', (2, 3))]","[('Our performance', (0, 2)), ('prior state of the art', (4, 9))]","[['Our performance', 'matches', 'prior state of the art']]",[],[],"[['Results', 'has', 'Our performance']]",[],[],[],[],[],sentiment_analysis,7,97
research-problem,Multimodal Speech Emotion Recognition and Ambiguity Resolution,[],"[('Multimodal Speech Emotion Recognition and Ambiguity Resolution', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multimodal Speech Emotion Recognition and Ambiguity Resolution']]",[],[],[],[],sentiment_analysis,8,2
research-problem,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,[],"[('Identifying emotion from speech', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Identifying emotion from speech']]",[],[],[],[],sentiment_analysis,8,4
research-problem,"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",[],"[('Speech Emotion Recognition ( SER )', (18, 24))]",[],[],[],[],"[['Contribution', 'has research problem', 'Speech Emotion Recognition ( SER )']]",[],[],[],[],sentiment_analysis,8,16
research-problem,"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .","[('explore', (5, 6)), ('for', (13, 14)), ('compare', (16, 17)), ('of', (19, 20)), ('with', (24, 25))]","[('implication of hand - crafted features', (7, 13)), ('SER', (14, 15)), ('performance', (18, 19)), ('lighter machine learning models', (20, 24)), ('heavily data - reliant deep learning models', (26, 33))]","[['performance', 'of', 'lighter machine learning models'], ['lighter machine learning models', 'with', 'heavily data - reliant deep learning models'], ['implication of hand - crafted features', 'for', 'SER']]",[],"[['Model', 'compare', 'performance'], ['Model', 'explore', 'implication of hand - crafted features']]",[],"[['Contribution', 'has research problem', 'SER']]",[],[],[],[],sentiment_analysis,8,18
model,"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .","[('combine', (4, 5)), ('from', (6, 7)), ('to understand', (10, 12)), ('aid', (18, 19))]","[('features', (5, 6)), ('textual modality', (8, 10)), ('correlation between different modalities', (13, 17)), ('ambiguity resolution', (19, 21))]","[['features', 'from', 'textual modality'], ['textual modality', 'to understand', 'correlation between different modalities'], ['textual modality', 'aid', 'ambiguity resolution']]",[],"[['Model', 'combine', 'features']]",[],[],[],[],[],[],sentiment_analysis,8,19
model,"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .","[('extract', (7, 8)), ('from', (10, 11)), ('of', (14, 15)), ('train', (19, 20))]","[('handcrafted features', (8, 10)), ('time domain', (12, 14)), ('audio signal', (16, 18)), ('respective models', (21, 23))]","[['handcrafted features', 'from', 'time domain'], ['time domain', 'of', 'audio signal'], ['handcrafted features', 'train', 'respective models']]",[],"[['Model', 'extract', 'handcrafted features']]",[],[],[],[],[],[],sentiment_analysis,8,21
model,"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .","[('In', (0, 1)), ('train', (6, 7)), ('namely', (12, 13))]","[('first approach', (2, 4)), ('traditional machine learning classifiers', (7, 11)), ('Random Forests', (14, 16)), ('Gradient Boosting', (17, 19)), ('Support Vector Machines', (20, 23)), ('Naive - Bayes', (24, 27)), ('Logistic Regression', (28, 30))]","[['first approach', 'train', 'traditional machine learning classifiers'], ['traditional machine learning classifiers', 'namely', 'Random Forests'], ['traditional machine learning classifiers', 'namely', 'Gradient Boosting'], ['traditional machine learning classifiers', 'namely', 'Support Vector Machines'], ['traditional machine learning classifiers', 'namely', 'Naive - Bayes'], ['traditional machine learning classifiers', 'namely', 'Logistic Regression']]",[],"[['Model', 'In', 'first approach']]",[],[],[],[],[],[],sentiment_analysis,8,22
model,"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .","[('build', (6, 7)), ('to recognize', (16, 18)), ('given', (19, 20))]","[('second approach', (2, 4)), ('Multi - Layer Perceptron and an LSTM classifier', (8, 16)), ('emotion', (18, 19)), ('speech signal', (21, 23))]","[['second approach', 'build', 'Multi - Layer Perceptron and an LSTM classifier'], ['Multi - Layer Perceptron and an LSTM classifier', 'to recognize', 'emotion'], ['emotion', 'given', 'speech signal']]",[],[],"[['Model', 'In', 'second approach']]",[],[],[],[],[],sentiment_analysis,8,23
experimental-setup,"We use librosa , a Python library , to process the audio files and extract features from them .","[('use', (1, 2)), ('to process', (8, 10))]","[('librosa , a Python library', (2, 7)), ('audio files', (11, 13))]","[['librosa , a Python library', 'to process', 'audio files']]",[],"[['Experimental setup', 'use', 'librosa , a Python library']]",[],[],[],[],[],[],sentiment_analysis,8,164
experimental-setup,"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .","[('to implement', (18, 20))]","[('scikit - learn and xgboost', (2, 7)), ('all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP', (20, 39))]","[['scikit - learn and xgboost', 'to implement', 'all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP']]",[],[],"[['Experimental setup', 'use', 'scikit - learn and xgboost']]",[],[],[],[],[],sentiment_analysis,8,165
experimental-setup,We use PyTorch to implement the LSTM classifiers described earlier .,"[('to implement', (3, 5))]","[('PyTorch', (2, 3)), ('LSTM classifiers', (6, 8))]","[['PyTorch', 'to implement', 'LSTM classifiers']]",[],[],"[['Experimental setup', 'use', 'PyTorch']]",[],[],[],[],[],sentiment_analysis,8,166
experimental-setup,"In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .","[('to regularize', (2, 4)), ('of', (7, 8)), ('use', (13, 14)), ('called', (20, 21)), ('where', (23, 24)), ('for', (31, 32))]","[('hidden space', (5, 7)), ('LSTM classifiers', (9, 11)), ('shut - off mechanism', (15, 19)), ('dropout', (21, 22)), ('fraction of neurons are not used', (25, 31)), ('final prediction', (32, 34))]","[['hidden space', 'use', 'shut - off mechanism'], ['shut - off mechanism', 'called', 'dropout'], ['shut - off mechanism', 'where', 'fraction of neurons are not used'], ['fraction of neurons are not used', 'for', 'final prediction'], ['hidden space', 'of', 'LSTM classifiers']]",[],"[['Experimental setup', 'to regularize', 'hidden space']]",[],[],[],[],[],[],sentiment_analysis,8,167
experimental-setup,We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,"[('randomly split', (1, 3)), ('into', (5, 6))]","[('our dataset', (3, 5)), ('train ( 80 % ) and test ( 20 % ) set', (7, 19))]","[['our dataset', 'into', 'train ( 80 % ) and test ( 20 % ) set']]",[],"[['Experimental setup', 'randomly split', 'our dataset']]",[],[],[],[],[],[],sentiment_analysis,8,169
experimental-setup,The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,"[('trained on', (4, 6))]","[('LSTM classifiers', (1, 3)), ('NVIDIA Titan X GPU', (7, 11))]","[['LSTM classifiers', 'trained on', 'NVIDIA Titan X GPU']]",[],[],"[['Experimental setup', 'has', 'LSTM classifiers']]",[],[],[],[],[],sentiment_analysis,8,171
experimental-setup,We stop the training when we do not see any improvement in validation performance for > 10 epochs .,"[('stop', (1, 2)), ('when', (4, 5)), ('in', (11, 12)), ('for', (14, 15))]","[('training', (3, 4)), ('do not see any improvement', (6, 11)), ('validation performance', (12, 14)), ('> 10 epochs', (15, 18))]","[['training', 'when', 'do not see any improvement'], ['do not see any improvement', 'in', 'validation performance'], ['validation performance', 'for', '> 10 epochs']]",[],"[['Experimental setup', 'stop', 'training']]",[],[],[],[],[],[],sentiment_analysis,8,172
results,"From , we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state - of - the art on this dataset .","[('either outperform or are comparable to', (12, 18))]","[('our simpler and lighter ML models', (6, 12)), ('much heavier current state - of - the art', (19, 28))]","[['our simpler and lighter ML models', 'either outperform or are comparable to', 'much heavier current state - of - the art']]",[],[],"[['Results', 'has', 'our simpler and lighter ML models']]",[],[],[],[],[],sentiment_analysis,8,199
results,Audio - only results :,[],"[('Audio - only results', (0, 4))]",[],[],[],"[['Results', 'has', 'Audio - only results']]",[],[],[],[],"[['Audio - only results', 'has', 'Performance of LSTM and ARE']]",sentiment_analysis,8,201
results,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,"[('reveals', (5, 6)), ('to learn', (15, 17)), ('as', (18, 19)), ('trained on', (22, 24)), ('achieves', (28, 29)), ('as compared to', (32, 35))]","[('Performance of LSTM and ARE', (0, 5)), ('deep models indeed need a lot of information', (7, 15)), ('features', (17, 18)), ('LSTM classifier', (20, 22)), ('eight - dimensional features', (24, 28)), ('very low accuracy', (29, 32)), ('end - to - end trained ARE', (36, 43))]","[['Performance of LSTM and ARE', 'reveals', 'deep models indeed need a lot of information'], ['deep models indeed need a lot of information', 'to learn', 'features'], ['features', 'as', 'LSTM classifier'], ['LSTM classifier', 'achieves', 'very low accuracy'], ['very low accuracy', 'as compared to', 'end - to - end trained ARE'], ['LSTM classifier', 'trained on', 'eight - dimensional features']]",[],[],[],[],[],[],[],[],sentiment_analysis,8,203
results,Text - only results :,[],"[('Text - only results', (0, 4))]",[],[],[],"[['Results', 'has', 'Text - only results']]",[],[],[],[],[],sentiment_analysis,8,207
results,We observe that the performance of all the models for this setting is similar .,"[('observe', (1, 2)), ('is', (12, 13))]","[('performance of all the models', (4, 9)), ('similar', (13, 14))]","[['performance of all the models', 'is', 'similar']]",[],[],[],[],"[['Text - only results', 'observe', 'performance of all the models']]",[],[],[],sentiment_analysis,8,208
results,c) Audio + Text results :,[],"[('Audio + Text results', (1, 5))]",[],[],[],"[['Results', 'has', 'Audio + Text results']]",[],[],[],[],[],sentiment_analysis,8,212
results,We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .,"[('combining', (3, 4)), ('gives', (8, 9)), ('of', (12, 13))]","[('audio and text features', (4, 8)), ('boost', (11, 12)), ('? 14 % for all the metrics', (13, 20))]","[['audio and text features', 'gives', 'boost'], ['boost', 'of', '? 14 % for all the metrics']]",[],[],[],[],"[['Audio + Text results', 'combining', 'audio and text features']]",[],[],[],sentiment_analysis,8,213
results,"Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .","[('conclude that', (4, 6)), ('are', (10, 11)), ('to have achieved', (13, 16))]","[('our simple ML methods', (6, 10)), ('very robust', (11, 13)), ('comparable performance', (16, 18))]","[['our simple ML methods', 'are', 'very robust'], ['very robust', 'to have achieved', 'comparable performance']]",[],"[['Results', 'conclude that', 'our simple ML methods']]",[],[],[],[],[],[],sentiment_analysis,8,219
research-problem,Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,[],"[('Aspect Polarity Classification and Aspect Term Extraction', (10, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect Polarity Classification and Aspect Term Extraction']]",[],[],[],[],sentiment_analysis,9,2
research-problem,Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,[],"[('Aspect - based sentiment analysis ( ABSA )', (0, 8)), ('aspect term extraction ( ATE )', (25, 31)), ('aspect polarity classification ( APC )', (32, 38))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis ( ABSA )'], ['Contribution', 'has research problem', 'aspect term extraction ( ATE )'], ['Contribution', 'has research problem', 'aspect polarity classification ( APC )']]",[],[],[],[],sentiment_analysis,9,8
research-problem,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,[],"[('aspect term polarity inferring', (10, 14)), ('aspect term extraction', (19, 22))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect term polarity inferring'], ['Contribution', 'has research problem', 'aspect term extraction']]",[],[],[],[],sentiment_analysis,9,9
research-problem,"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .",[],"[('aspect polarity classification', (28, 31))]",[],[],[],[],"[['Contribution', 'has research problem', 'aspect polarity classification']]",[],[],[],[],sentiment_analysis,9,13
research-problem,"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .",[],"[('Aspect - based sentiment analysis', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Aspect - based sentiment analysis']]",[],[],[],[],sentiment_analysis,9,16
research-problem,The APC task is a kind of classification problem .,[],"[('APC', (1, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'APC']]",[],[],[],[],sentiment_analysis,9,23
research-problem,"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .",[],"[('ATE', (10, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'ATE']]",[],[],[],[],sentiment_analysis,9,24
model,"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .","[('proposes', (20, 21)), ('for', (25, 26))]","[('multi-task learning model', (22, 25)), ('aspect - based sentiment analysis', (26, 31))]","[['multi-task learning model', 'for', 'aspect - based sentiment analysis']]",[],"[['Model', 'proposes', 'multi-task learning model']]",[],[],[],[],[],[],sentiment_analysis,9,32
model,The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,"[('is', (10, 11))]","[('LCF - ATEPC 3 model', (1, 6)), ('novel multilingual and multi-task - oriented model', (12, 19))]","[['LCF - ATEPC 3 model', 'is', 'novel multilingual and multi-task - oriented model']]",[],[],"[['Model', 'has', 'LCF - ATEPC 3 model']]",[],[],[],[],[],sentiment_analysis,9,34
model,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .","[('based on', (4, 6)), ('integrates', (14, 15)), ('namely', (24, 25))]","[('multi-head self - attention ( MHSA )', (6, 13)), ('pre-trained and the local context focus mechanism', (16, 23)), ('LCF - ATEPC', (25, 28))]","[['pre-trained and the local context focus mechanism', 'namely', 'LCF - ATEPC']]",[],"[['Model', 'based on', 'multi-head self - attention ( MHSA )'], ['Model', 'integrates', 'pre-trained and the local context focus mechanism']]",[],[],[],[],[],[],sentiment_analysis,9,36
model,"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .","[('training on', (1, 3)), ('adapted to', (19, 21)), ('automatically extracting', (27, 29)), ('predicting', (32, 33))]","[('small amount of annotated data of aspect and their polarity', (4, 14)), ('large - scale dataset', (22, 26)), ('aspects', (30, 31)), ('sentiment polarities', (34, 36))]","[['small amount of annotated data of aspect and their polarity', 'adapted to', 'large - scale dataset'], ['large - scale dataset', 'automatically extracting', 'aspects'], ['large - scale dataset', 'predicting', 'sentiment polarities']]",[],"[['Model', 'training on', 'small amount of annotated data of aspect and their polarity']]",[],[],[],[],[],[],sentiment_analysis,9,37
code,The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,[],"[('https://github.com/yangheng95/LCF-ATEPC', (8, 9))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/yangheng95/LCF-ATEPC']]",[],[],[],[],sentiment_analysis,9,48
baselines,"ATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .","[('is', (3, 4)), ('for', (10, 11)), ('applies', (16, 17)), ('to focus on', (20, 23)), ('in', (26, 27))]","[('ATAE - LSTM', (0, 3)), ('classical LSTM - based network', (5, 10)), ('APC task', (12, 14)), ('attention mechanism', (18, 20)), ('important words', (24, 26)), ('context', (28, 29))]","[['ATAE - LSTM', 'is', 'classical LSTM - based network'], ['classical LSTM - based network', 'applies', 'attention mechanism'], ['attention mechanism', 'to focus on', 'important words'], ['important words', 'in', 'context'], ['classical LSTM - based network', 'for', 'APC task']]",[],[],"[['Baselines', 'has', 'ATAE - LSTM']]",[],[],[],[],[],sentiment_analysis,9,192
baselines,ATSM -S Peng et al.,[],"[('ATSM -S', (0, 2))]",[],[],[],"[['Baselines', 'has', 'ATSM -S']]",[],[],[],[],[],sentiment_analysis,9,195
baselines,is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .,"[('is', (0, 1)), ('of', (4, 5)), ('for', (8, 9))]","[('baseline model', (2, 4)), ('ATSM variations', (6, 8)), ('Chinese language - oriented ABSA task', (9, 15))]","[['baseline model', 'of', 'ATSM variations'], ['ATSM variations', 'for', 'Chinese language - oriented ABSA task']]",[],[],[],[],"[['ATSM -S', 'is', 'baseline model']]",[],[],[],sentiment_analysis,9,196
baselines,GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs .,"[('is', (1, 2)), ('for', (6, 7)), ('aimed to solve', (9, 12))]","[('GANN', (0, 1)), ('novel neural network model', (2, 6)), ('APC task', (7, 9)), ('shortcomings of traditional RNNs and CNNs', (13, 19))]","[['GANN', 'is', 'novel neural network model'], ['novel neural network model', 'aimed to solve', 'shortcomings of traditional RNNs and CNNs'], ['novel neural network model', 'for', 'APC task']]",[],[],"[['Baselines', 'has', 'GANN']]",[],[],[],[],[],sentiment_analysis,9,198
baselines,"AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .","[('is', (2, 3)), ('based on', (7, 9)), ('aims to solve', (15, 18))]","[('AEN', (0, 1)), ('attentional encoder network', (4, 7)), ('pretrained BERT model', (10, 13)), ('aspect polarity classification', (19, 22))]","[['AEN', 'is', 'attentional encoder network'], ['attentional encoder network', 'aims to solve', 'aspect polarity classification'], ['attentional encoder network', 'based on', 'pretrained BERT model']]",[],[],"[['Baselines', 'has', 'AEN']]",[],[],[],[],[],sentiment_analysis,9,201
baselines,"BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .","[('is', (2, 3)), ('for', (8, 9))]","[('BERT', (0, 1)), ('BERT - adapted model', (4, 8)), ('Review Reading Comprehension ( RRC ) task', (9, 16))]","[['BERT', 'is', 'BERT - adapted model'], ['BERT - adapted model', 'for', 'Review Reading Comprehension ( RRC ) task']]",[],[],"[['Baselines', 'has', 'BERT']]",[],[],[],[],[],sentiment_analysis,9,202
baselines,BERT - BASE is the basic pretrained BERT model .,"[('is', (3, 4))]","[('BERT - BASE', (0, 3)), ('basic pretrained BERT model', (5, 9))]","[['BERT - BASE', 'is', 'basic pretrained BERT model']]",[],[],"[['Baselines', 'has', 'BERT - BASE']]",[],[],[],[],[],sentiment_analysis,9,203
baselines,"We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .","[('adapt it to', (1, 4)), ('equips', (9, 10)), ('to automatically extract', (13, 16)), ('classify', (19, 20)), ('as', (22, 23))]","[('ABSA multi-task learning', (4, 7)), ('same ability', (11, 13)), ('aspect terms', (16, 18)), ('aspects polarity', (20, 22)), ('LCF - ATEPC model', (23, 27))]","[['ABSA multi-task learning', 'equips', 'same ability'], ['same ability', 'to automatically extract', 'aspect terms'], ['same ability', 'classify', 'aspects polarity'], ['aspects polarity', 'as', 'LCF - ATEPC model']]",[],[],[],[],"[['BERT - BASE', 'adapt it to', 'ABSA multi-task learning']]",[],[],[],sentiment_analysis,9,204
baselines,BERT - ADA,[],"[('BERT - ADA', (0, 3))]",[],[],[],"[['Baselines', 'has', 'BERT - ADA']]",[],[],[],[],[],sentiment_analysis,9,207
baselines,"is a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .","[('is', (0, 1)), ('proposed for', (9, 11)), ('finetuned', (16, 17)), ('on', (22, 23))]","[('domain - adapted BERT - based model', (2, 9)), ('APC task', (12, 14)), ('BERT - BASE model', (18, 22)), ('task - related corpus', (23, 27))]","[['domain - adapted BERT - based model', 'finetuned', 'BERT - BASE model'], ['BERT - BASE model', 'on', 'task - related corpus'], ['domain - adapted BERT - based model', 'proposed for', 'APC task']]",[],[],[],[],"[['BERT - ADA', 'is', 'domain - adapted BERT - based model']]",[],[],[],sentiment_analysis,9,209
baselines,"LCF - ATEPC 5 is the multi -task learning model for the ATE and APC tasks , which is based on the the BERT - SPC model and local context focus mechanism .","[('is', (4, 5)), ('for', (10, 11))]","[('LCF - ATEPC', (0, 3)), ('multi -task learning model', (6, 10)), ('ATE and APC tasks', (12, 16))]","[['LCF - ATEPC', 'is', 'multi -task learning model'], ['multi -task learning model', 'for', 'ATE and APC tasks']]",[],[],"[['Baselines', 'has', 'LCF - ATEPC']]",[],[],[],[],[],sentiment_analysis,9,211
baselines,LCF - ATE are the variations of the LCF - ATEPC model which only optimize for the ATE task .,"[('are', (3, 4)), ('optimize for', (14, 16))]","[('LCF - ATE', (0, 3)), ('variations of the LCF - ATEPC model', (5, 12)), ('ATE task', (17, 19))]","[['LCF - ATE', 'are', 'variations of the LCF - ATEPC model'], ['variations of the LCF - ATEPC model', 'optimize for', 'ATE task']]",[],[],"[['Baselines', 'has', 'LCF - ATE']]",[],[],[],[],[],sentiment_analysis,9,212
baselines,LCF - APC are the variations of LCF - ATEPC and it only optimize for the APC task during training process .,"[('are', (3, 4)), ('optimize for', (13, 15)), ('during', (18, 19))]","[('LCF - APC', (0, 3)), ('variations of LCF - ATEPC', (5, 10)), ('APC task', (16, 18)), ('training process', (19, 21))]","[['LCF - APC', 'are', 'variations of LCF - ATEPC'], ['variations of LCF - ATEPC', 'optimize for', 'APC task'], ['APC task', 'during', 'training process']]",[],[],"[['Baselines', 'has', 'LCF - APC']]",[],[],[],[],[],sentiment_analysis,9,213
results,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .","[('works better on', (3, 6))]","[('CDM layer', (1, 3)), ('twitter dataset', (6, 8))]","[['CDM layer', 'works better on', 'twitter dataset']]",[],[],"[['Results', 'has', 'CDM layer']]",[],[],[],[],[],sentiment_analysis,9,238
results,"After optimizing the model parameters according to the empirical result , the joint model based on BERT - BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets , such as BERT - PT , AEN - BERT , SDGCN - BERT , and soon .",[],[],"[['joint model based on BERT - BASE', 'even surpassed', 'other proposed BERT based improved models'], ['other proposed BERT based improved models', 'on', 'some datasets'], ['joint model based on BERT - BASE', 'achieved', 'hopeful performance'], ['hopeful performance', 'on', 'all three datasets']]",[],[],"[['Results', 'has', 'joint model based on BERT - BASE']]",[],[],[],[],[],sentiment_analysis,9,244
results,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .","[('Compared with', (0, 2)), ('significantly improves', (11, 13)), ('of', (19, 20))]","[('BERT - BASE model', (3, 7)), ('BERT - SPC', (8, 11)), ('accuracy and F 1 score', (14, 19)), ('aspect polarity classification', (20, 23))]","[['BERT - SPC', 'significantly improves', 'accuracy and F 1 score'], ['accuracy and F 1 score', 'of', 'aspect polarity classification']]","[['BERT - BASE model', 'has', 'BERT - SPC']]","[['Results', 'Compared with', 'BERT - BASE model']]",[],[],[],[],[],[],sentiment_analysis,9,246
results,"In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .","[('for', (3, 4)), ('has increased', (11, 13)), ('of', (17, 18)), ('on', (20, 21)), ('up to', (23, 25))]","[('first time', (5, 7)), ('BERT - SPC', (8, 11)), ('F 1 score', (14, 17)), ('ATE subtask', (18, 20)), ('three datasets', (21, 23)), ('99 %', (25, 27))]","[['BERT - SPC', 'has increased', 'F 1 score'], ['F 1 score', 'up to', '99 %'], ['F 1 score', 'of', 'ATE subtask'], ['ATE subtask', 'on', 'three datasets']]","[['first time', 'has', 'BERT - SPC']]","[['Results', 'for', 'first time']]",[],[],[],[],[],[],sentiment_analysis,9,247
results,"ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .","[('is', (3, 4)), ('of', (7, 8)), ('adopts', (13, 14)), ('to generate', (17, 19))]","[('ATEPC - Fusion', (0, 3)), ('supplementary scheme', (5, 7)), ('LCF mechanism', (8, 10)), ('moderate approach', (15, 17)), ('local context features', (19, 22))]","[['ATEPC - Fusion', 'adopts', 'moderate approach'], ['moderate approach', 'to generate', 'local context features'], ['ATEPC - Fusion', 'is', 'supplementary scheme'], ['supplementary scheme', 'of', 'LCF mechanism']]",[],[],"[['Results', 'has', 'ATEPC - Fusion']]",[],[],[],[],[],sentiment_analysis,9,248
results,The experimental results show that its performance is also better than the existing BERT - based models .,"[('show', (3, 4)), ('better than', (9, 11))]","[('performance', (6, 7)), ('existing BERT - based models', (12, 17))]","[['performance', 'better than', 'existing BERT - based models']]",[],[],[],[],"[['ATEPC - Fusion', 'show', 'performance']]",[],[],[],sentiment_analysis,9,249
research-problem,Deep Learning For Smile Recognition,[],"[('Smile Recognition', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Smile Recognition']]",[],[],[],[],smile_recognition,0,2
research-problem,"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .",[],"[('facial expression recognition', (22, 25))]",[],[],[],[],"[['Contribution', 'has research problem', 'facial expression recognition']]",[],[],[],[],smile_recognition,0,4
model,The input images are fed into a convolution comprising a convolutional and a subsampling layer .,"[('fed into', (4, 6)), ('comprising', (8, 9))]","[('input images', (1, 3)), ('convolution', (7, 8)), ('convolutional and a subsampling layer', (10, 15))]","[['input images', 'fed into', 'convolution'], ['convolution', 'comprising', 'convolutional and a subsampling layer']]",[],[],"[['Model', 'has', 'input images']]",[],[],[],[],[],smile_recognition,0,56
model,That convolution maybe followed by more convolutions to become gradually more invariant to distortions in the input .,"[('followed by', (3, 5)), ('to become', (7, 9)), ('to', (12, 13)), ('in', (14, 15))]","[('convolution', (1, 2)), ('more convolutions', (5, 7)), ('more invariant', (10, 12)), ('distortions', (13, 14)), ('input', (16, 17))]","[['convolution', 'followed by', 'more convolutions'], ['more convolutions', 'to become', 'more invariant'], ['more invariant', 'to', 'distortions'], ['distortions', 'in', 'input']]",[],[],"[['Model', 'has', 'convolution']]",[],[],[],[],[],smile_recognition,0,57
model,"In the second stage , a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions .",[],[],"[['regular neural network', 'follows', 'convolutions'], ['convolutions', 'to discriminate', 'features'], ['features', 'learned by', 'convolutions']]","[['second stage', 'has', 'regular neural network']]","[['Model', 'In', 'second stage']]",[],[],[],[],[],"[['second stage', 'has', 'output layer']]",smile_recognition,0,58
model,The output layer consists of two units for smile or no smile .,"[('consists of', (3, 5)), ('for', (7, 8))]","[('output layer', (1, 3)), ('two units', (5, 7)), ('smile or no smile', (8, 12))]","[['output layer', 'consists of', 'two units'], ['two units', 'for', 'smile or no smile']]",[],[],[],[],[],[],[],[],smile_recognition,0,59
model,"The novelty of this approach is that the exact number of convolutions , number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3 .","[('is', (5, 6)), ('are', (22, 23)), ('subject to', (26, 28))]","[('novelty', (1, 2)), ('exact number of convolutions , number of hidden layers and size of hidden layers', (8, 22)), ('not fixed', (23, 25)), ('model selection', (29, 31))]","[['novelty', 'is', 'exact number of convolutions , number of hidden layers and size of hidden layers'], ['exact number of convolutions , number of hidden layers and size of hidden layers', 'subject to', 'model selection'], ['exact number of convolutions , number of hidden layers and size of hidden layers', 'are', 'not fixed']]",[],[],"[['Model', 'has', 'novelty']]",[],[],[],[],[],smile_recognition,0,60
experimental-setup,"Due to training time constraints , some parameters have been fixed to reasonable and empirical values , such as the size of convolutions ( 5 5 pixels , 32 feature maps ) and the size of subsamplings ( 2 2 pixels using max pooling ) .","[('fixed to', (10, 12)), ('such as', (17, 19))]","[('some parameters', (6, 8)), ('reasonable and empirical values', (12, 16)), ('size of convolutions ( 5 5 pixels , 32 feature maps )', (20, 32)), ('size of subsamplings ( 2 2 pixels using max pooling )', (34, 45))]","[['some parameters', 'fixed to', 'reasonable and empirical values'], ['reasonable and empirical values', 'such as', 'size of convolutions ( 5 5 pixels , 32 feature maps )'], ['reasonable and empirical values', 'such as', 'size of subsamplings ( 2 2 pixels using max pooling )']]",[],[],"[['Experimental setup', 'has', 'some parameters']]",[],[],[],[],[],smile_recognition,0,62
experimental-setup,"All layers use ReLU units , except of softmax being used in the output layer .","[('use', (2, 3)), ('except of', (6, 8)), ('used in', (10, 12))]","[('All layers', (0, 2)), ('ReLU units', (3, 5)), ('softmax', (8, 9)), ('output layer', (13, 15))]","[['All layers', 'except of', 'softmax'], ['softmax', 'used in', 'output layer'], ['All layers', 'use', 'ReLU units']]",[],[],"[['Experimental setup', 'has', 'All layers']]",[],[],[],[],[],smile_recognition,0,63
experimental-setup,The learning rate is fixed to ? = 0.01 and not subject to model selection as it would significantly prolong the model selection .,"[('fixed to', (4, 6)), ('not subject to', (10, 13))]","[('learning rate', (1, 3)), ('? = 0.01', (6, 9)), ('model selection', (13, 15))]","[['learning rate', 'not subject to', 'model selection'], ['learning rate', 'fixed to', '? = 0.01']]",[],[],"[['Experimental setup', 'has', 'learning rate']]",[],[],[],[],[],smile_recognition,0,64
experimental-setup,"The same considerations apply to the momentum , which is fixed to = 0.9 .","[('apply to', (3, 5)), ('fixed to', (10, 12))]","[('momentum', (6, 7)), ('= 0.9', (12, 14))]","[['momentum', 'fixed to', '= 0.9']]",[],"[['Experimental setup', 'apply to', 'momentum']]",[],[],[],[],[],[],smile_recognition,0,65
experimental-setup,The entire database has been randomly split into a 60% / 20 % / 20 % training / validation / test ratio .,"[('randomly split into', (5, 8))]","[('entire database', (1, 3)), ('60% / 20 % / 20 % training / validation / test ratio', (9, 22))]","[['entire database', 'randomly split into', '60% / 20 % / 20 % training / validation / test ratio']]",[],[],"[['Experimental setup', 'has', 'entire database']]",[],[],[],[],[],smile_recognition,0,66
experimental-setup,The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time .,"[('implemented using', (3, 5)), ('generated', (9, 10)), ('executed on', (13, 15))]","[('Lasagne', (5, 6)), ('CUDA code', (10, 12)), ('Tesla K40c', (16, 18))]","[['CUDA code', 'executed on', 'Tesla K40c']]",[],"[['Experimental setup', 'generated', 'CUDA code'], ['Experimental setup', 'implemented using', 'Lasagne']]",[],[],[],[],[],[],smile_recognition,0,71
experimental-setup,Stochastic gradient descent with a batch size of 500 is used .,"[('with', (3, 4)), ('of', (7, 8))]","[('Stochastic gradient descent', (0, 3)), ('batch size', (5, 7)), ('500', (8, 9))]","[['Stochastic gradient descent', 'with', 'batch size'], ['batch size', 'of', '500']]",[],[],"[['Experimental setup', 'has', 'Stochastic gradient descent']]",[],[],[],[],[],smile_recognition,0,72
experimental-setup,"contains the four parameters to be optimized : the number of convolutions , the number of hidden layers , the number of units per hidden layer and the dropout factor .","[('contains', (0, 1)), ('to be', (4, 6))]","[('four parameters', (2, 4)), ('optimized', (6, 7)), ('number of convolutions', (9, 12)), ('number of hidden layers', (14, 18)), ('number of units per hidden layer', (20, 26)), ('dropout factor', (28, 30))]","[['four parameters', 'to be', 'optimized']]","[['four parameters', 'name', 'number of convolutions'], ['four parameters', 'name', 'number of hidden layers'], ['four parameters', 'name', 'number of units per hidden layer'], ['four parameters', 'name', 'dropout factor']]","[['Experimental setup', 'contains', 'four parameters']]",[],[],[],[],[],[],smile_recognition,0,73
experimental-setup,Each model was trained for 50 epochs in the model selection .,"[('trained for', (3, 5)), ('in', (7, 8))]","[('model', (1, 2)), ('50 epochs', (5, 7)), ('model selection', (9, 11))]","[['model', 'trained for', '50 epochs'], ['50 epochs', 'in', 'model selection']]",[],[],"[['Experimental setup', 'has', 'model']]",[],[],[],[],[],smile_recognition,0,76
research-problem,"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .",[],"[('temporal and causal relation extraction and classification', (11, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'temporal and causal relation extraction and classification']]",[],[],[],[],temporal_information_extraction,0,4
model,"The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .","[('includes', (3, 4)), ('one for', (9, 11)), ('other for', (14, 16))]","[('CATENA system', (1, 3)), ('two main classification modules', (4, 8)), ('temporal', (11, 12)), ('causal relations', (16, 18))]","[['CATENA system', 'includes', 'two main classification modules'], ['two main classification modules', 'other for', 'causal relations'], ['two main classification modules', 'one for', 'temporal']]",[],[],"[['Model', 'name', 'CATENA system']]",[],[],[],[],[],temporal_information_extraction,0,19
model,"As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .","[('take as input', (6, 9)), ('annotated with', (11, 13)), ('according to', (19, 21)), ('including', (24, 25))]","[('document', (10, 11)), ('temporal entities', (17, 19)), ('TimeML guidelines', (21, 23)), ('document creation time ( DCT )', (26, 32)), ('events and time expressions ( timexes )', (33, 40))]","[['document', 'annotated with', 'temporal entities'], ['temporal entities', 'including', 'document creation time ( DCT )'], ['temporal entities', 'including', 'events and time expressions ( timexes )'], ['temporal entities', 'according to', 'TimeML guidelines']]",[],"[['Model', 'take as input', 'document']]",[],[],[],[],[],[],temporal_information_extraction,0,20
model,"The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .","[('is', (2, 3)), ('with', (6, 7)), ('set between', (12, 14)), ('of', (15, 16))]","[('output', (1, 2)), ('same document', (4, 6)), ('temporal links ( TLINKs )', (7, 12)), ('pairs', (14, 15)), ('temporal entities', (16, 18))]","[['output', 'is', 'same document'], ['same document', 'with', 'temporal links ( TLINKs )'], ['temporal links ( TLINKs )', 'set between', 'pairs'], ['pairs', 'of', 'temporal entities']]",[],[],"[['Model', 'has', 'output']]",[],[],[],[],[],temporal_information_extraction,0,21
model,The document is also annotated with causal relations ( CLINKs ) between event pairs .,"[('annotated with', (4, 6)), ('between', (11, 12))]","[('document', (1, 2)), ('causal relations ( CLINKs )', (6, 11)), ('event pairs', (12, 14))]","[['document', 'annotated with', 'causal relations ( CLINKs )'], ['causal relations ( CLINKs )', 'between', 'event pairs']]",[],[],[],[],[],[],"[['output', 'has', 'document']]",[],temporal_information_extraction,0,22
model,"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .","[('for', (2, 3)), ('rely both on', (8, 11)), ('in which', (17, 19)), ('after running', (24, 26)), ('fed into', (38, 40))]","[('modules', (1, 2)), ('temporal and causal relation classification', (3, 8)), ('sieve - based architecture', (12, 16)), ('remaining unlabelled pairs', (20, 23)), ('rule - based component', (27, 31)), ('transitive reasoner', (35, 37)), ('supervised classifier', (41, 43))]","[['modules', 'for', 'temporal and causal relation classification'], ['temporal and causal relation classification', 'rely both on', 'sieve - based architecture'], ['sieve - based architecture', 'in which', 'remaining unlabelled pairs'], ['remaining unlabelled pairs', 'after running', 'rule - based component'], ['temporal and causal relation classification', 'rely both on', 'transitive reasoner'], ['transitive reasoner', 'fed into', 'supervised classifier']]",[],[],[],[],[],[],"[['output', 'has', 'modules']]",[],temporal_information_extraction,0,23
results,"The evaluation shows that CATENA is the best performing system in both tasks , even if in Task C best precision and best recall are yielded by and , respectively .","[('is', (5, 6)), ('in', (10, 11))]","[('CATENA', (4, 5)), ('best performing system', (7, 10)), ('both tasks', (11, 13))]","[['CATENA', 'is', 'best performing system'], ['best performing system', 'in', 'both tasks']]",[],[],"[['Results', 'has', 'CATENA']]",[],[],[],[],[],temporal_information_extraction,0,150
results,"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the best results on event - DCT and event - event pairs .","[('performs best on', (9, 12)), ('while', (21, 22)), ('achieves', (24, 25)), ('on', (28, 29))]","[('timex - timex and event - timex relations', (12, 20)), ('CAEVO', (22, 23)), ('best results', (26, 28)), ('event - DCT and event - event pairs', (29, 37))]","[['timex - timex and event - timex relations', 'while', 'CAEVO'], ['CAEVO', 'achieves', 'best results'], ['best results', 'on', 'event - DCT and event - event pairs']]",[],[],[],[],"[['CATENA', 'performs best on', 'timex - timex and event - timex relations']]",[],[],[],temporal_information_extraction,0,156
ablation-analysis,"As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .","[('running', (3, 4)), ('after', (8, 9)), ('results in', (20, 22))]","[('transitive closure module', (5, 8)), ('temporal rule - based sieve ( RB + TR )', (10, 20)), ('improving recall', (22, 24))]","[['transitive closure module', 'after', 'temporal rule - based sieve ( RB + TR )'], ['temporal rule - based sieve ( RB + TR )', 'results in', 'improving recall']]",[],"[['Ablation analysis', 'running', 'transitive closure module']]",[],[],[],[],[],[],temporal_information_extraction,0,160
ablation-analysis,Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .,"[('Combining', (0, 1)), ('yields', (14, 15)), ('compared with', (18, 20)), ('in', (27, 28))]","[('rule - based and machine - learned sieves ( RB + ML )', (1, 14)), ('slight improvement', (16, 18)), ('enabling only the machine - learned sieve', (20, 27)), ('system ( ML )', (29, 33))]","[['rule - based and machine - learned sieves ( RB + ML )', 'yields', 'slight improvement'], ['slight improvement', 'compared with', 'enabling only the machine - learned sieve'], ['enabling only the machine - learned sieve', 'in', 'system ( ML )']]",[],"[['Ablation analysis', 'Combining', 'rule - based and machine - learned sieves ( RB + ML )']]",[],[],[],[],[],[],temporal_information_extraction,0,161
ablation-analysis,Introducing the temporal reasoner module between the two sieves ( RB + TR + ML ) proves to be even more beneficial .,"[('Introducing', (0, 1)), ('between', (5, 6)), ('proves to be', (16, 19))]","[('temporal reasoner module', (2, 5)), ('two sieves ( RB + TR + ML )', (7, 16)), ('even more beneficial', (19, 22))]","[['temporal reasoner module', 'between', 'two sieves ( RB + TR + ML )'], ['two sieves ( RB + TR + ML )', 'proves to be', 'even more beneficial']]",[],"[['Ablation analysis', 'Introducing', 'temporal reasoner module']]",[],[],[],[],[],[],temporal_information_extraction,0,162
research-problem,A Structured Learning Approach to Temporal Relation Extraction,[],"[('Temporal Relation Extraction', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Temporal Relation Extraction']]",[],[],[],[],temporal_information_extraction,1,2
research-problem,Identifying temporal relations between events is an essential step towards natural language understanding .,[],"[('Identifying temporal relations between events', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Identifying temporal relations between events']]",[],[],[],[],temporal_information_extraction,1,4
research-problem,"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .",[],"[('temporal processing', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'temporal processing']]",[],[],[],[],temporal_information_extraction,1,13
approach,"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .","[('propose', (5, 6)), ('to', (10, 11)), ('where', (15, 16)), ('are', (18, 19)), ('based on', (20, 22)), ('from', (23, 24))]","[('structured learning approach', (7, 10)), ('temporal relation extraction', (11, 14)), ('local models', (16, 18)), ('updated', (19, 20)), ('feedback', (22, 23)), ('global inferences', (24, 26))]","[['structured learning approach', 'to', 'temporal relation extraction'], ['temporal relation extraction', 'where', 'local models'], ['local models', 'are', 'updated'], ['updated', 'based on', 'feedback'], ['feedback', 'from', 'global inferences']]",[],"[['Approach', 'propose', 'structured learning approach']]",[],[],[],[],[],[],temporal_information_extraction,1,17
approach,"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .","[('gives rise to', (4, 7)), ('take advantage of', (15, 18))]","[('structured approach', (1, 3)), ('semisupervised method', (8, 10)), ('readily available unlabeled data', (19, 23))]","[['structured approach', 'gives rise to', 'semisupervised method'], ['semisupervised method', 'take advantage of', 'readily available unlabeled data']]",[],[],"[['Approach', 'has', 'structured approach']]",[],[],[],[],[],temporal_information_extraction,1,18
baselines,The first is the regularized averaged perceptron ( AP ) implemented in the LBJava package and is a local method .,"[('implemented in', (10, 12))]","[('regularized averaged perceptron ( AP )', (4, 10)), ('LBJava package', (13, 15))]","[['regularized averaged perceptron ( AP )', 'implemented in', 'LBJava package']]",[],[],"[['Baselines', 'has', 'regularized averaged perceptron ( AP )']]",[],[],[],[],[],temporal_information_extraction,1,199
baselines,"On top of the first baseline , we performed global inference in Eq .","[('On top of', (0, 3)), ('performed', (8, 9))]","[('first baseline', (4, 6)), ('global inference', (9, 11))]","[['first baseline', 'performed', 'global inference']]",[],"[['Baselines', 'On top of', 'first baseline']]",[],[],[],[],[],[],temporal_information_extraction,1,200
baselines,"Both of them used the same feature set ( i.e. , as designed in ) as in the proposed structured perceptron ( SP ) and CoDL for fair comparisons .","[('used', (3, 4)), ('as in', (15, 17))]","[('same feature set', (5, 8)), ('proposed structured perceptron ( SP )', (18, 24)), ('CoDL', (25, 26))]","[['same feature set', 'as in', 'proposed structured perceptron ( SP )'], ['same feature set', 'as in', 'CoDL']]",[],"[['Baselines', 'used', 'same feature set']]",[],[],[],[],[],[],temporal_information_extraction,1,202
results,TE3 Task C - Relation Only,[],"[('TE3 Task C - Relation Only', (0, 6))]",[],[],[],"[['Results', 'has', 'TE3 Task C - Relation Only']]",[],[],[],[],[],temporal_information_extraction,1,205
results,"We can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .","[('see', (2, 3)), ('is about', (7, 9)), ('better than', (11, 13)), ('in', (16, 17))]","[('UT - Time', (4, 7)), ('3 %', (9, 11)), ('AP - 1', (13, 16)), ('absolute value of F 1', (18, 23))]","[['UT - Time', 'is about', '3 %'], ['3 %', 'better than', 'AP - 1'], ['AP - 1', 'in', 'absolute value of F 1']]",[],[],[],[],"[['TE3 Task C - Relation Only', 'see', 'UT - Time']]",[],[],[],temporal_information_extraction,1,214
results,"On top of AP - 2 , a global inference step enforcing symmetry and transitivity constraints ( "" AP + ILP "" ) can further improve the F 1 score by 9.3 % , which is consistent with previous observations .","[('On top of', (0, 3)), ('enforcing', (11, 12)), ('further improve', (24, 26)), ('by', (30, 31))]","[('AP - 2', (3, 6)), ('global inference step', (8, 11)), ('symmetry and transitivity constraints', (12, 16)), ('F 1 score', (27, 30)), ('9.3 %', (31, 33))]","[['global inference step', 'enforcing', 'symmetry and transitivity constraints'], ['symmetry and transitivity constraints', 'further improve', 'F 1 score'], ['F 1 score', 'by', '9.3 %']]","[['AP - 2', 'has', 'global inference step']]",[],[],[],"[['TE3 Task C - Relation Only', 'On top of', 'AP - 2']]",[],[],[],temporal_information_extraction,1,220
results,"SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .","[('further improved', (3, 5)), ('in', (7, 8)), ('reaching', (28, 29)), ('of', (33, 34))]","[('SP + ILP', (0, 3)), ('performance', (6, 7)), ('precision , recall , and F 1', (8, 15)), ('F 1 score', (30, 33)), ('67.2 %', (34, 36))]","[['SP + ILP', 'further improved', 'performance'], ['performance', 'in', 'precision , recall , and F 1'], ['SP + ILP', 'reaching', 'F 1 score'], ['F 1 score', 'of', '67.2 %']]",[],[],[],[],[],[],"[['TE3 Task C - Relation Only', 'has', 'SP + ILP']]",[],temporal_information_extraction,1,221
results,TE3 Task C,[],"[('TE3 Task C', (0, 3))]",[],[],[],"[['Results', 'has', 'TE3 Task C']]",[],[],[],[],"[['TE3 Task C', 'has', 'AP + ILP']]",temporal_information_extraction,1,223
results,"The improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .",[],[],"[['AP + ILP', 'even worse than', 'AP'], ['SP + ILP', 'over', 'AP'], ['AP', 'was', 'small']]",[],[],[],[],"[['TE3 Task C', 'improvement of', 'SP + ILP']]",[],[],[],temporal_information_extraction,1,232
results,"By applying the postfiltering method proposed in Sec. 4 , we were able to achieve better performances using SP + ILP ( line 5 ) , which shows the effectiveness of this strategy .","[('applying', (1, 2)), ('able to achieve', (12, 15)), ('using', (17, 18))]","[('postfiltering method', (3, 5)), ('better performances', (15, 17)), ('SP + ILP', (18, 21))]","[['postfiltering method', 'able to achieve', 'better performances'], ['better performances', 'using', 'SP + ILP']]",[],[],[],[],"[['TE3 Task C', 'applying', 'postfiltering method']]",[],[],[],temporal_information_extraction,1,233
results,Comparison with CAEVO,[],"[('Comparison with CAEVO', (0, 3))]",[],[],[],"[['Results', 'has', 'Comparison with CAEVO']]",[],[],[],[],"[['Comparison with CAEVO', 'has', 'SP + ILP']]",temporal_information_extraction,1,239
results,"SP + ILP outperformed CAEVO and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .","[('outperformed', (3, 4))]","[('SP + ILP', (0, 3)), ('CAEVO', (4, 5))]","[['SP + ILP', 'outperformed', 'CAEVO']]",[],[],[],[],[],[],[],[],temporal_information_extraction,1,249
research-problem,Bag of Tricks for Efficient Text Classification,[],"[('Efficient Text Classification', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Efficient Text Classification']]",[],[],[],[],text-classification,2,2
research-problem,This paper explores a simple and efficient baseline for text classification .,[],"[('text classification', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'text classification']]",[],[],[],[],text-classification,2,4
model,shows a simple linear model with rank constraint .,"[('shows', (0, 1)), ('with', (5, 6))]","[('simple linear model', (2, 5)), ('rank constraint', (6, 8))]","[['simple linear model', 'with', 'rank constraint']]",[],"[['Model', 'shows', 'simple linear model']]",[],[],[],[],[],[],text-classification,2,22
model,The first weight matrix A is a look - up table over the words .,"[('is a', (5, 7)), ('over', (11, 12))]","[('weight matrix A', (2, 5)), ('look - up table', (7, 11)), ('words', (13, 14))]","[['weight matrix A', 'is a', 'look - up table'], ['look - up table', 'over', 'words']]",[],[],"[['Model', 'has', 'weight matrix A']]",[],[],[],[],[],text-classification,2,23
model,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .","[('are then averaged into', (3, 7)), ('in turn fed to', (13, 17))]","[('word representations', (1, 3)), ('text representation', (8, 10)), ('linear classifier', (18, 20))]","[['word representations', 'are then averaged into', 'text representation'], ['text representation', 'in turn fed to', 'linear classifier']]",[],[],"[['Model', 'has', 'word representations']]",[],[],[],[],[],text-classification,2,24
model,We use the softmax function f to compute the probability distribution over the predefined classes .,"[('use', (1, 2)), ('to compute', (6, 8)), ('over', (11, 12))]","[('softmax function f', (3, 6)), ('probability distribution', (9, 11)), ('predefined classes', (13, 15))]","[['softmax function f', 'to compute', 'probability distribution'], ['probability distribution', 'over', 'predefined classes']]",[],"[['Model', 'use', 'softmax function f']]",[],[],[],[],[],[],text-classification,2,27
model,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .","[('to improve', (2, 4)), ('use', (9, 10)), ('based on', (14, 16))]","[('running time', (5, 7)), ('hierarchical softmax', (11, 13)), ('Huffman coding tree', (17, 20))]","[['running time', 'use', 'hierarchical softmax'], ['hierarchical softmax', 'based on', 'Huffman coding tree']]",[],"[['Model', 'to improve', 'running time']]",[],[],[],[],[],[],text-classification,2,34
model,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .","[('as', (8, 9)), ('to capture', (11, 13)), ('about', (16, 17))]","[('bag of n-grams', (5, 8)), ('additional features', (9, 11)), ('some partial information', (13, 16)), ('local word order', (18, 21))]","[['bag of n-grams', 'as', 'additional features'], ['additional features', 'to capture', 'some partial information'], ['some partial information', 'about', 'local word order']]",[],[],"[['Model', 'use', 'bag of n-grams']]",[],[],[],[],[],text-classification,2,45
model,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,"[('in practice', (4, 6)), ('achieving', (7, 8)), ('to methods that explicitly use', (10, 15))]","[('very efficient', (2, 4)), ('comparable results', (8, 10)), ('order', (16, 17))]","[['comparable results', 'to methods that explicitly use', 'order']]",[],[],[],[],"[['bag of n-grams', 'achieving', 'comparable results'], ['bag of n-grams', 'in practice', 'very efficient']]",[],[],[],text-classification,2,46
model,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .","[('maintain', (1, 2)), ('of', (8, 9)), ('by using', (11, 13)), ('with', (16, 17)), ('only used', (28, 30))]","[('fast and memory efficient mapping', (3, 8)), ('n-grams', (10, 11)), ('hashing trick', (14, 16)), ('same hashing function as in', (18, 23)), ('10M bins', (24, 26)), ('bigrams', (30, 31))]","[['fast and memory efficient mapping', 'of', 'n-grams'], ['n-grams', 'by using', 'hashing trick'], ['hashing trick', 'with', 'same hashing function as in'], ['hashing trick', 'with', '10M bins'], ['10M bins', 'only used', 'bigrams']]",[],"[['Model', 'maintain', 'fast and memory efficient mapping']]",[],[],[],[],[],[],text-classification,2,47
tasks,Sentiment analysis,[],"[('Sentiment analysis', (0, 2))]",[],[],[],"[['Tasks', 'has', 'Sentiment analysis']]",[],[],[],[],"[['Sentiment analysis', 'has', 'Hyperparameters']]",text-classification,2,53
tasks,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .","[('hidden units', (3, 5)), ('run', (6, 7)), ('for', (8, 9)), ('with', (11, 12)), ('selected on', (15, 17)), ('from', (20, 21))]","[('10', (2, 3)), ('fastText', (7, 8)), ('5 epochs', (9, 11)), ('learning rate', (13, 15)), ('validation set', (18, 20)), ('{ 0.05 , 0.1 , 0.25 , 0.5 }', (21, 30))]","[['fastText', 'for', '5 epochs'], ['5 epochs', 'with', 'learning rate'], ['learning rate', 'selected on', 'validation set'], ['learning rate', 'from', '{ 0.05 , 0.1 , 0.25 , 0.5 }']]",[],[],[],[],"[['Hyperparameters', 'run', 'fastText'], ['Hyperparameters', 'hidden units', '10']]",[],[],[],text-classification,2,60
tasks,"On this task , adding bigram information improves the performance by 1 - 4 % .","[('adding', (4, 5)), ('improves the performance by', (7, 11))]","[('bigram information', (5, 7)), ('1 - 4 %', (11, 15))]","[['bigram information', 'improves the performance by', '1 - 4 %']]",[],[],[],[],"[['Results', 'adding', 'bigram information']]",[],[],[],text-classification,2,61
tasks,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","[('slightly better than', (4, 7)), ('bit worse than', (17, 20))]","[('our accuracy', (1, 3)), ('char - CNN and char - CRNN', (7, 14)), ('VDCNN', (20, 21))]","[['our accuracy', 'bit worse than', 'VDCNN'], ['our accuracy', 'slightly better than', 'char - CNN and char - CRNN']]",[],[],[],[],[],[],"[['Results', 'has', 'our accuracy']]",[],text-classification,2,62
tasks,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","[('increase', (4, 5)), ('using', (9, 10)), ('for example with', (13, 16)), ('performance on', (19, 21)), ('goes up to', (22, 25))]","[('accuracy slightly', (6, 8)), ('more n-grams', (10, 12)), ('trigrams', (16, 17)), ('Sogou', (21, 22)), ('97.1 %', (25, 27))]","[['accuracy slightly', 'using', 'more n-grams'], ['more n-grams', 'for example with', 'trigrams'], ['trigrams', 'performance on', 'Sogou'], ['Sogou', 'goes up to', '97.1 %']]",[],[],[],[],"[['Results', 'increase', 'accuracy slightly']]",[],[],[],text-classification,2,63
tasks,Tag prediction,[],"[('Tag prediction', (0, 2))]",[],[],[],"[['Tasks', 'has', 'Tag prediction']]",[],[],[],[],"[['Tag prediction', 'has', 'Results']]",text-classification,2,69
tasks,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .","[('At test time', (0, 3)), ('needs to compute', (5, 8)), ('for', (10, 11)), ('makes it', (15, 17)), ('gives', (24, 25)), ('when', (30, 31))]","[('Tagspace', (4, 5)), ('scores', (9, 10)), ('all the classes', (11, 14)), ('relatively slow', (17, 19)), ('our fast inference', (21, 24)), ('significant speed - up', (26, 30)), ('number of classes is large ( more than 300 K here )', (32, 44))]","[['Tagspace', 'needs to compute', 'scores'], ['scores', 'for', 'all the classes'], ['all the classes', 'makes it', 'relatively slow'], ['our fast inference', 'gives', 'significant speed - up'], ['significant speed - up', 'when', 'number of classes is large ( more than 300 K here )']]",[],[],[],[],"[['Results', 'At test time', 'Tagspace'], ['Results', 'At test time', 'our fast inference']]",[],[],[],text-classification,2,84
tasks,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .","[('more than an order of magnitude', (4, 10)), ('to obtain', (11, 13)), ('with', (14, 15))]","[('faster', (10, 11)), ('model', (13, 14)), ('better quality', (16, 18))]","[['faster', 'to obtain', 'model'], ['model', 'with', 'better quality']]",[],[],[],[],"[['Results', 'more than an order of magnitude', 'faster']]",[],[],[],text-classification,2,85
research-problem,Universal Sentence Encoder,[],"[('Universal Sentence Encoder', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Universal Sentence Encoder']]",[],[],[],[],text-classification,6,2
research-problem,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,[],"[('encoding sentences into embedding vectors that specifically target transfer learning', (4, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'encoding sentences into embedding vectors that specifically target transfer learning']]",[],[],[],[],text-classification,6,4
research-problem,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,[],"[('transfer learning using sentence embeddings', (3, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'transfer learning using sentence embeddings']]",[],[],[],[],text-classification,6,9
research-problem,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .",[],"[('transfer learning via sentence embeddings', (1, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'transfer learning via sentence embeddings']]",[],[],[],[],text-classification,6,10
code,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,[],"[('https://tfhub.dev/google/universal-sentence-encoder/1', (16, 17))]",[],[],[],[],"[['Contribution', 'Code', 'https://tfhub.dev/google/universal-sentence-encoder/1']]",[],[],[],[],text-classification,6,32
model,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,"[('based', (2, 3)), ('constructs', (6, 7)), ('using', (9, 10))]","[('sentence encoding model', (3, 6)), ('sentence embeddings', (7, 9)), ('encoding sub - graph of the transformer architecture', (11, 19))]","[['sentence encoding model', 'constructs', 'sentence embeddings'], ['sentence embeddings', 'using', 'encoding sub - graph of the transformer architecture']]",[],[],[],[],"[['Transformer', 'based', 'sentence encoding model']]",[],[],[],text-classification,6,44
model,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,"[('uses', (4, 5)), ('to compute', (6, 8)), ('of', (11, 12)), ('that take into account', (16, 20))]","[('attention', (5, 6)), ('context aware representations', (8, 11)), ('words in a sentence', (12, 16)), ('ordering and identity of all the other words', (22, 30))]","[['attention', 'to compute', 'context aware representations'], ['context aware representations', 'of', 'words in a sentence'], ['words in a sentence', 'that take into account', 'ordering and identity of all the other words']]",[],[],[],[],"[['Transformer', 'uses', 'attention']]",[],[],[],text-classification,6,45
model,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,"[('converted', (6, 7)), ('to a', (7, 9)), ('by computing', (14, 16)), ('of', (21, 22))]","[('context aware word representations', (1, 5)), ('fixed length sentence encoding vector', (9, 14)), ('element - wise sum', (17, 21)), ('representations at each word position', (23, 28))]","[['context aware word representations', 'to a', 'fixed length sentence encoding vector'], ['fixed length sentence encoding vector', 'by computing', 'element - wise sum'], ['element - wise sum', 'of', 'representations at each word position']]",[],[],[],[],"[['Transformer', 'converted', 'context aware word representations']]",[],[],[],text-classification,6,46
model,The encoding model is designed to be as general purpose as possible .,"[('designed to be', (4, 7))]","[('encoding model', (1, 3)), ('general purpose', (8, 10))]","[['encoding model', 'designed to be', 'general purpose']]",[],[],[],[],[],[],"[['Transformer', 'has', 'encoding model']]",[],text-classification,6,48
model,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,"[('accomplished by', (2, 4)), ('whereby', (7, 8)), ('is used to feed', (12, 16))]","[('using multi-task learning', (4, 7)), ('a single encoding model', (8, 12)), ('multiple downstream tasks', (16, 19))]","[['using multi-task learning', 'whereby', 'a single encoding model'], ['a single encoding model', 'is used to feed', 'multiple downstream tasks']]",[],[],[],[],"[['general purpose', 'accomplished by', 'using multi-task learning']]",[],[],[],text-classification,6,49
model,Deep Averaging Network ( DAN ),[],"[('Deep Averaging Network ( DAN )', (0, 6))]",[],[],[],"[['Model', 'has', 'Deep Averaging Network ( DAN )']]",[],[],[],[],[],text-classification,6,54
model,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,"[('whereby', (14, 15)), ('are first', (21, 23)), ('then passed through', (26, 29)), ('to produce', (37, 39))]","[('input embeddings for words and bi-grams', (15, 21)), ('averaged together', (23, 25)), ('feedforward deep neural network ( DNN )', (30, 37)), ('sentence embeddings', (39, 41))]","[['input embeddings for words and bi-grams', 'then passed through', 'feedforward deep neural network ( DNN )'], ['feedforward deep neural network ( DNN )', 'to produce', 'sentence embeddings'], ['input embeddings for words and bi-grams', 'are first', 'averaged together']]",[],[],[],[],"[['Deep Averaging Network ( DAN )', 'whereby', 'input embeddings for words and bi-grams']]",[],[],[],text-classification,6,55
model,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",[],[],"[['lowercased PTB tokenized string', 'outputs', '512 dimensional sentence embedding']]",[],[],[],[],"[['Deep Averaging Network ( DAN )', 'takes as input', 'lowercased PTB tokenized string']]",[],[],[],text-classification,6,56
model,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,"[('use', (2, 3)), ('whereby', (6, 7)), ('used to supply', (12, 15)), ('for', (17, 18))]","[('mul-titask learning', (4, 6)), ('single DAN encoder', (8, 11)), ('sentence embeddings', (15, 17)), ('multiple downstream tasks', (18, 21))]","[['mul-titask learning', 'whereby', 'single DAN encoder'], ['single DAN encoder', 'used to supply', 'sentence embeddings'], ['sentence embeddings', 'for', 'multiple downstream tasks']]",[],[],[],[],"[['Deep Averaging Network ( DAN )', 'use', 'mul-titask learning']]",[],[],[],text-classification,6,58
model,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,"[('primary advantage', (1, 3)), ('of', (3, 4)), ('is', (7, 8))]","[('compute time', (9, 11)), ('linear in the length', (12, 16)), ('input sequence', (18, 20))]","[['compute time', 'is', 'linear in the length'], ['linear in the length', 'of', 'input sequence']]",[],[],[],[],"[['Deep Averaging Network ( DAN )', 'primary advantage', 'compute time']]",[],[],[],text-classification,6,59
tasks,MR : Movie review snippet sentiment on a five star scale .,"[('on', (6, 7))]","[('MR', (0, 1)), ('Movie review snippet sentiment', (2, 6)), ('five star scale', (8, 11))]","[['Movie review snippet sentiment', 'on', 'five star scale']]","[['MR', 'has', 'Movie review snippet sentiment']]",[],"[['Tasks', 'has', 'MR']]",[],[],[],[],[],text-classification,6,69
tasks,CR : Sentiment of sentences mined from customer reviews .,"[('mined from', (5, 7))]","[('CR', (0, 1)), ('Sentiment of sentences', (2, 5)), ('customer reviews', (7, 9))]","[['Sentiment of sentences', 'mined from', 'customer reviews']]","[['CR', 'has', 'Sentiment of sentences']]",[],"[['Tasks', 'has', 'CR']]",[],[],[],[],[],text-classification,6,70
tasks,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,"[('from', (5, 6))]","[('SUBJ', (0, 1)), ('Subjectivity of sentences', (2, 5)), ('movie reviews and plot summaries', (6, 11))]","[['Subjectivity of sentences', 'from', 'movie reviews and plot summaries']]","[['SUBJ', 'has', 'Subjectivity of sentences']]",[],"[['Tasks', 'has', 'SUBJ']]",[],[],[],[],[],text-classification,6,71
tasks,MPQA : Phrase level opinion polarity from news data .,"[('from', (6, 7))]","[('MPQA', (0, 1)), ('Phrase level opinion polarity', (2, 6)), ('news data', (7, 9))]","[['Phrase level opinion polarity', 'from', 'news data']]","[['MPQA', 'has', 'Phrase level opinion polarity']]",[],"[['Tasks', 'has', 'MPQA']]",[],[],[],[],[],text-classification,6,72
tasks,TREC : Fine grained question classification sourced from TREC .,[],[],"[['Fine grained question classification', 'sourced from', 'TREC']]","[['TREC', 'has', 'Fine grained question classification']]",[],"[['Tasks', 'has', 'TREC']]",[],[],[],[],[],text-classification,6,73
tasks,SST : Binary phrase level sentiment classification .,[],"[('SST', (0, 1)), ('Binary phrase level sentiment classification', (2, 7))]",[],"[['SST', 'has', 'Binary phrase level sentiment classification']]",[],"[['Tasks', 'has', 'SST']]",[],[],[],[],[],text-classification,6,74
tasks,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,"[('between', (9, 10)), ('scored by', (12, 14))]","[('STS Benchmark', (0, 2)), ('Semantic textual similarity ( STS )', (3, 9)), ('sentence pairs', (10, 12)), ('Pearson correlation with human judgments', (14, 19))]","[['Semantic textual similarity ( STS )', 'between', 'sentence pairs'], ['sentence pairs', 'scored by', 'Pearson correlation with human judgments']]","[['STS Benchmark', 'has', 'Semantic textual similarity ( STS )']]",[],"[['Tasks', 'has', 'STS Benchmark']]",[],[],[],[],[],text-classification,6,75
tasks,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,"[('from', (4, 5)), ('on', (8, 9))]","[('WEAT', (0, 1)), ('Word pairs', (2, 4)), ('psychology literature', (6, 8)), ('implicit association tests ( IAT )', (9, 15))]","[['Word pairs', 'from', 'psychology literature'], ['psychology literature', 'on', 'implicit association tests ( IAT )']]","[['WEAT', 'has', 'Word pairs']]",[],"[['Tasks', 'has', 'WEAT']]",[],[],[],[],[],text-classification,6,76
baselines,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .","[('For', (0, 1)), ('use', (6, 7)), ('trained on', (17, 19)), ('of', (21, 22))]","[('word level transfer', (1, 4)), ('word embeddings from a word2 vec skip - gram model', (7, 17)), ('corpus', (20, 21)), ('news data', (22, 24))]","[['word level transfer', 'use', 'word embeddings from a word2 vec skip - gram model'], ['word embeddings from a word2 vec skip - gram model', 'trained on', 'corpus'], ['corpus', 'of', 'news data']]",[],"[['Baselines', 'For', 'word level transfer']]",[],[],[],[],[],[],text-classification,6,84
baselines,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,"[('input to', (7, 9))]","[('convolutional neural network models', (14, 18)), ('CNN', (19, 20)), ('DAN', (23, 24))]",[],"[['convolutional neural network models', 'name', 'CNN']]",[],[],[],"[['word embeddings from a word2 vec skip - gram model', 'input to', 'convolutional neural network models'], ['word embeddings from a word2 vec skip - gram model', 'input to', 'DAN']]",[],[],[],text-classification,6,85
baselines,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,"[('are trained without using', (6, 10))]","[('Additional baseline', (0, 2)), ('CNN and DAN models', (2, 6)), ('pretrained word or sentence embeddings', (11, 16))]","[['CNN and DAN models', 'are trained without using', 'pretrained word or sentence embeddings']]","[['Additional baseline', 'has', 'CNN and DAN models']]",[],"[['Baselines', 'has', 'Additional baseline']]",[],[],[],[],[],text-classification,6,87
results,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,[],[],"[['transfer learning', 'from', 'transformer based sentence encoder'], ['transformer based sentence encoder', 'performs as good or better than', 'transfer learning'], ['transfer learning', 'from', 'DAN encoder']]",[],"[['Results', 'observe', 'transfer learning']]",[],[],[],[],[],[],text-classification,6,112
results,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,"[('make use of', (2, 5)), ('tend to perform better than', (9, 14))]","[('Models', (0, 1)), ('sentence level transfer learning', (5, 9)), ('models that only use word level transfer', (14, 21))]","[['Models', 'make use of', 'sentence level transfer learning'], ['sentence level transfer learning', 'tend to perform better than', 'models that only use word level transfer']]",[],[],"[['Results', 'has', 'Models']]",[],[],[],[],[],text-classification,6,114
results,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .","[('for', (4, 5)), ('can achieve', (14, 16))]","[('smaller quantities of data', (5, 9)), ('sentence level transfer learning', (10, 14)), ('surprisingly good task performance', (16, 20))]","[['sentence level transfer learning', 'for', 'smaller quantities of data'], ['smaller quantities of data', 'can achieve', 'surprisingly good task performance']]",[],[],"[['Results', 'observe', 'sentence level transfer learning']]",[],[],[],[],[],text-classification,6,117
results,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .","[('As', (0, 1)), ('approach the performance of', (16, 20))]","[('training set size increases', (2, 6)), ('models that do not make use of transfer learning', (7, 16)), ('other models', (21, 23))]","[['models that do not make use of transfer learning', 'approach the performance of', 'other models']]","[['training set size increases', 'has', 'models that do not make use of transfer learning']]","[['Results', 'As', 'training set size increases']]",[],[],[],[],[],[],text-classification,6,118
research-problem,Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion,[],"[('Grapheme - to - Phoneme Conversion', (6, 12))]",[],[],[],[],"[['Contribution', 'has research problem', 'Grapheme - to - Phoneme Conversion']]",[],[],[],[],text-to-speech_synthesis,0,2
research-problem,Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .,[],"[('Grapheme - to - phoneme ( G2P ) conversion', (0, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'Grapheme - to - phoneme ( G2P ) conversion']]",[],[],[],[],text-to-speech_synthesis,0,4
approach,"Inspired by the knowledge distillation in computer vision and natural language processing , in this work , we propose the token - level ensemble distillation for G2P conversion , to address the practical problems mentioned above .","[('propose', (18, 19)), ('for', (25, 26))]","[('token - level ensemble distillation', (20, 25)), ('G2P conversion', (26, 28))]","[['token - level ensemble distillation', 'for', 'G2P conversion']]",[],"[['Approach', 'propose', 'token - level ensemble distillation']]",[],[],[],[],[],[],text-to-speech_synthesis,0,22
approach,"First , we use knowledge distillation to leverage the large amount of unlabeled words .","[('use', (3, 4)), ('to leverage', (6, 8))]","[('knowledge distillation', (4, 6)), ('large amount of unlabeled words', (9, 14))]","[['knowledge distillation', 'to leverage', 'large amount of unlabeled words']]",[],"[['Approach', 'use', 'knowledge distillation']]",[],[],[],[],[],[],text-to-speech_synthesis,0,23
approach,"Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .","[('train', (3, 4)), ('to generate', (7, 9)), ('as well as', (12, 15)), ('given', (18, 19))]","[('teacher model', (5, 7)), ('phoneme sequence', (10, 12)), ('probability distribution', (16, 18)), ('unlabeled grapheme sequence', (19, 22))]","[['teacher model', 'to generate', 'phoneme sequence'], ['phoneme sequence', 'as well as', 'probability distribution'], ['probability distribution', 'given', 'unlabeled grapheme sequence']]",[],[],[],[],"[['knowledge distillation', 'train', 'teacher model']]",[],[],[],text-to-speech_synthesis,0,24
approach,"Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .","[('train', (3, 4)), ('for', (15, 16)), ('to get', (17, 19)), ('transfer', (23, 24)), ('to', (30, 31)), ('suitable for', (38, 40))]","[('variety of models ( CNN , RNN and Transformer )', (5, 15)), ('ensemble', (16, 17)), ('higher accuracy', (19, 21)), ('knowledge of the ensemble models', (25, 30)), ('light - weight model', (32, 36)), ('online deployment', (40, 42))]","[['knowledge of the ensemble models', 'to', 'light - weight model'], ['light - weight model', 'suitable for', 'online deployment'], ['variety of models ( CNN , RNN and Transformer )', 'for', 'ensemble'], ['ensemble', 'to get', 'higher accuracy']]",[],"[['Approach', 'transfer', 'knowledge of the ensemble models'], ['Approach', 'train', 'variety of models ( CNN , RNN and Transformer )']]",[],[],[],[],[],[],text-to-speech_synthesis,0,25
approach,"Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .","[('adopt', (3, 4)), ('instead of', (5, 7)), ('as', (10, 11))]","[('Transformer', (4, 5)), ('RNN or CNN', (7, 10)), ('basic encoder - decoder model structure', (12, 18))]","[['Transformer', 'as', 'basic encoder - decoder model structure'], ['Transformer', 'instead of', 'RNN or CNN']]",[],"[['Approach', 'adopt', 'Transformer']]",[],[],[],[],[],[],text-to-speech_synthesis,0,26
experiments,Ensemble Model,[],"[('Ensemble Model', (0, 2))]",[],[],[],[],[],[],[],"[['Model', 'has', 'Ensemble Model']]",[],text-to-speech_synthesis,0,110
experiments,"We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .","[('use', (1, 2))]","[('4 Transformer models', (2, 5)), ('3 CNN models', (6, 9)), ('3 Bi - LSTM models', (10, 15))]",[],[],[],[],[],"[['Ensemble Model', 'use', '4 Transformer models'], ['Ensemble Model', 'use', '3 CNN models'], ['Ensemble Model', 'use', '3 Bi - LSTM models']]",[],[],[],text-to-speech_synthesis,0,112
experiments,The 4 Transformer models share the same hidden size ( 256 ) but vary in the number of the encoder - decoder layers .,"[('share', (4, 5)), ('vary in', (13, 15))]","[('4 Transformer models', (1, 4)), ('same hidden size ( 256 )', (6, 12)), ('number of the encoder - decoder layers', (16, 23))]","[['4 Transformer models', 'vary in', 'number of the encoder - decoder layers'], ['4 Transformer models', 'share', 'same hidden size ( 256 )']]",[],[],[],[],[],[],"[['Ensemble Model', 'has', '4 Transformer models']]",[],text-to-speech_synthesis,0,113
experiments,"For the 3 CNN models , they share the same hidden size ( 256 ) but vary in the number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ) respectively .","[('share', (7, 8)), ('vary in', (16, 18))]","[('3 CNN models', (2, 5)), ('same hidden size ( 256 )', (9, 15)), ('number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 )', (19, 49))]","[['3 CNN models', 'vary in', 'number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 )'], ['3 CNN models', 'share', 'same hidden size ( 256 )']]",[],[],[],[],[],[],"[['Ensemble Model', 'has', '3 CNN models']]",[],text-to-speech_synthesis,0,114
experiments,"For the 3 Bi - LSTM models , they share the same number of encoder - decoder layers ( 1 - 1 ) , but with different hidden sizes ( 256 , 384 and 512 ) .","[('share', (9, 10)), ('with different', (25, 27))]","[('3 Bi - LSTM models', (2, 7)), ('same number of encoder - decoder layers ( 1 - 1 )', (11, 23)), ('hidden sizes ( 256 , 384 and 512 )', (27, 36))]","[['3 Bi - LSTM models', 'share', 'same number of encoder - decoder layers ( 1 - 1 )'], ['3 Bi - LSTM models', 'with different', 'hidden sizes ( 256 , 384 and 512 )']]",[],[],[],[],[],[],"[['Ensemble Model', 'has', '3 Bi - LSTM models']]",[],text-to-speech_synthesis,0,115
experiments,Student Model,[],"[('Student Model', (0, 2))]",[],[],[],[],[],[],[],"[['Model', 'has', 'Student Model']]",[],text-to-speech_synthesis,0,116
experiments,We choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .,"[('choose', (1, 2)), ('use', (8, 9))]","[('Transformer', (2, 3)), ('default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder )', (10, 26))]",[],[],[],[],[],"[['Student Model', 'use', 'default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder )'], ['Student Model', 'choose', 'Transformer']]",[],[],[],text-to-speech_synthesis,0,117
experiments,We implement experiments with the fairseq - py 4 library in Py-Torch .,"[('implement experiments with', (1, 4))]","[('fairseq - py 4 library in Py-Torch', (5, 12))]",[],[],[],[],[],"[['Experimental setup', 'implement experiments with', 'fairseq - py 4 library in Py-Torch']]",[],[],[],text-to-speech_synthesis,0,120
experiments,We use Adam optimizer for all models and follow the learning rate schedule in .,"[('use', (1, 2)), ('for', (4, 5)), ('follow', (8, 9))]","[('Adam optimizer', (2, 4)), ('all models', (5, 7)), ('learning rate schedule', (10, 13))]","[['Adam optimizer', 'for', 'all models'], ['Adam optimizer', 'follow', 'learning rate schedule']]",[],[],[],[],"[['Experimental setup', 'use', 'Adam optimizer']]",[],[],[],text-to-speech_synthesis,0,121
experiments,"The dropout is 0.3 for Bi - LSTM and CNN models , while the residual dropout , attention dropout and ReLU dropout for Transformer models is 0.2 , 0.4 , 0.4 respectively .",[],[],"[['residual dropout , attention dropout and ReLU dropout', 'for', 'Transformer models'], ['Transformer models', 'is', '0.2 , 0.4 , 0.4'], ['dropout', 'is', '0.3'], ['0.3', 'for', 'Bi - LSTM and CNN models']]",[],[],[],[],[],[],"[['Experimental setup', 'has', 'residual dropout , attention dropout and ReLU dropout'], ['Experimental setup', 'has', 'dropout']]",[],text-to-speech_synthesis,0,122
experiments,We train each model on 8 NVIDIA M40 GPUs .,"[('train', (1, 2)), ('on', (4, 5))]","[('each model', (2, 4)), ('8 NVIDIA M40 GPUs', (5, 9))]","[['each model', 'on', '8 NVIDIA M40 GPUs']]",[],[],[],[],"[['Experimental setup', 'train', 'each model']]",[],[],[],text-to-speech_synthesis,0,124
experiments,Each GPU contains roughly 4000 tokens in one mini-batch .,"[('contains', (2, 3)), ('in', (6, 7))]","[('roughly 4000 tokens', (3, 6)), ('one mini-batch', (7, 9))]","[['roughly 4000 tokens', 'in', 'one mini-batch']]",[],[],[],[],"[['8 NVIDIA M40 GPUs', 'contains', 'roughly 4000 tokens']]",[],[],[],text-to-speech_synthesis,0,125
experiments,We use beam search during inference and set beam size to 10 .,"[('during', (4, 5)), ('set', (7, 8))]","[('beam search', (2, 4)), ('inference', (5, 6)), ('beam size to 10', (8, 12))]","[['beam search', 'set', 'beam size to 10'], ['beam search', 'during', 'inference']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'beam search']]",[],text-to-speech_synthesis,0,126
experiments,We use WER ( word error rate ) and PER ( phoneme error rate ) to measure the accuracy of G2P conversion .,"[('to measure', (15, 17))]","[('WER ( word error rate ) and PER ( phoneme error rate )', (2, 15)), ('accuracy of G2P conversion', (18, 22))]","[['WER ( word error rate ) and PER ( phoneme error rate )', 'to measure', 'accuracy of G2P conversion']]",[],[],[],[],[],[],"[['Experimental setup', 'use', 'WER ( word error rate ) and PER ( phoneme error rate )']]",[],text-to-speech_synthesis,0,127
results,"We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .","[('on', (8, 9))]","[('CMUDict', (9, 10))]",[],[],"[['Results', 'on', 'CMUDict']]",[],[],[],[],[],[],text-to-speech_synthesis,0,132
results,"It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .","[('can be seen', (1, 4)), ('achieves', (18, 19)), ('of', (29, 30)), ('outperforming', (34, 35)), ('by', (36, 37))]","[('our method on 6 - layer encoder and 6 - layer decoder Transformer', (5, 18)), ('new state - of - the - art result', (20, 29)), ('19.88 % WER', (30, 33)), ('NSGD', (35, 36)), ('4.22 % WER', (37, 40))]","[['our method on 6 - layer encoder and 6 - layer decoder Transformer', 'achieves', 'new state - of - the - art result'], ['new state - of - the - art result', 'of', '19.88 % WER'], ['new state - of - the - art result', 'outperforming', 'NSGD'], ['NSGD', 'by', '4.22 % WER']]",[],[],[],[],"[['CMUDict', 'can be seen', 'our method on 6 - layer encoder and 6 - layer decoder Transformer']]",[],[],[],text-to-speech_synthesis,0,136
ablation-analysis,"We first study the effect of distilling from unlabeled source words , as shown in .","[('study', (2, 3))]","[('effect of distilling from unlabeled source words', (4, 11))]",[],[],"[['Ablation analysis', 'study', 'effect of distilling from unlabeled source words']]",[],[],[],[],[],[],text-to-speech_synthesis,0,145
ablation-analysis,"It can be seen that unlabeled source words can boost the accuracy by nearly 1 % WER , demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation .","[('boost', (9, 10)), ('by', (12, 13)), ('demonstrating the effectiveness by', (18, 22)), ('into', (26, 27))]","[('accuracy', (11, 12)), ('nearly 1 % WER', (13, 17)), ('introducing abundant unlabeled data', (22, 26)), ('knowledge distillation', (27, 29))]","[['accuracy', 'by', 'nearly 1 % WER'], ['accuracy', 'demonstrating the effectiveness by', 'introducing abundant unlabeled data'], ['introducing abundant unlabeled data', 'into', 'knowledge distillation']]",[],[],[],[],"[['effect of distilling from unlabeled source words', 'boost', 'accuracy']]",[],[],[],text-to-speech_synthesis,0,146
ablation-analysis,"Furthermore , we study the effect of ensemble teacher model in knowledge distillation .",[],"[('effect of ensemble teacher model in knowledge distillation', (5, 13))]",[],[],[],"[['Ablation analysis', 'study', 'effect of ensemble teacher model in knowledge distillation']]",[],[],[],[],[],text-to-speech_synthesis,0,152
ablation-analysis,"As shown in , the ensemble teacher model can boost the accuracy by more than 1 % WER , compared with the single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ) , which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation .","[('boost', (9, 10)), ('by', (12, 13)), ('compared with', (19, 21))]","[('accuracy', (11, 12)), ('more than 1 % WER', (13, 18)), ('single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder )', (22, 40))]","[['accuracy', 'by', 'more than 1 % WER'], ['accuracy', 'compared with', 'single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder )']]",[],[],[],[],"[['effect of ensemble teacher model in knowledge distillation', 'boost', 'accuracy']]",[],[],[],text-to-speech_synthesis,0,153
ablation-analysis,"At last , we compare Transformer with RNN and CNN based models , without using knowledge distillation and unlabeled data , as shown in .","[('compare', (4, 5)), ('with', (6, 7)), ('without using', (13, 15))]","[('Transformer', (5, 6)), ('RNN and CNN based models', (7, 12)), ('knowledge distillation and unlabeled data', (15, 20))]","[['Transformer', 'with', 'RNN and CNN based models'], ['Transformer', 'without using', 'knowledge distillation and unlabeled data']]","[['Transformer', 'outperforms', 'RNN and CNN based models']]","[['Ablation analysis', 'compare', 'Transformer']]",[],[],[],[],[],[],text-to-speech_synthesis,0,154
ablation-analysis,"We can see that Transformer model outperforms the RNN and CNN based models used in previous works , demonstrating the advantage of Transformer model .",[],[],[],[],[],[],[],[],[],[],[],text-to-speech_synthesis,0,155
results,"We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .","[('by', (21, 22))]","[('CNN with NSGD', (14, 17)), ('internal dataset', (26, 28))]",[],"[['internal dataset', 'outperforms', 'CNN with NSGD']]",[],"[['Results', 'on', 'internal dataset']]",[],[],"[['CNN with NSGD', 'by', '3.52 % WER']]",[],[],text-to-speech_synthesis,0,157
results,"Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .","[('outperforms', (2, 3)), ('demonstrates', (12, 13)), ('for', (18, 19))]","[('3.52 % WER', (7, 10)), ('effectiveness of our method', (14, 18)), ('G2P conversion', (19, 21))]","[['effectiveness of our method', 'for', 'G2P conversion']]",[],[],[],[],"[['CNN with NSGD', 'demonstrates', 'effectiveness of our method']]",[],[],[],text-to-speech_synthesis,0,158
research-problem,"FastSpeech : Fast , Robust and Controllable Text to Speech",[],"[('Text to Speech', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text to Speech']]",[],[],[],[],text-to-speech_synthesis,1,2
research-problem,Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .,[],"[('Neural network based end - to - end text to speech ( TTS )', (0, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural network based end - to - end text to speech ( TTS )']]",[],[],[],[],text-to-speech_synthesis,1,4
research-problem,Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .,[],"[('Text to speech ( TTS )', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text to speech ( TTS )']]",[],[],[],[],text-to-speech_synthesis,1,15
research-problem,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,[],"[('Neural network based TTS', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural network based TTS']]",[],[],[],[],text-to-speech_synthesis,1,18
model,"Considering the monotonous alignment between text and speech , to speedup mel- spectrogram generation , in this work , we propose a novel model , FastSpeech , which takes a text ( phoneme ) sequence as input and generates mel-spectrograms non-autoregressively .","[('propose', (20, 21)), ('takes', (28, 29)), ('generates', (38, 39))]","[('FastSpeech', (25, 26)), ('text ( phoneme ) sequence as input', (30, 37)), ('mel-spectrograms non-autoregressively', (39, 41))]","[['FastSpeech', 'generates', 'mel-spectrograms non-autoregressively'], ['FastSpeech', 'takes', 'text ( phoneme ) sequence as input']]",[],"[['Model', 'propose', 'FastSpeech']]",[],[],[],[],[],[],text-to-speech_synthesis,1,28
model,It adopts a feed - forward network based on the self - attention in Transformer and 1D convolution .,"[('adopts', (1, 2)), ('based on', (7, 9))]","[('feed - forward network', (3, 7)), ('self - attention in Transformer and 1D convolution', (10, 18))]","[['feed - forward network', 'based on', 'self - attention in Transformer and 1D convolution']]",[],"[['Model', 'adopts', 'feed - forward network']]",[],[],[],[],[],[],text-to-speech_synthesis,1,29
model,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .","[('up - samples', (33, 36)), ('to match', (58, 60))]","[('length regulator', (30, 32)), ('phoneme sequence according to the phoneme duration', (37, 44)), ('length of the mel-spectrogram sequence', (61, 66))]","[['length regulator', 'up - samples', 'phoneme sequence according to the phoneme duration'], ['phoneme sequence according to the phoneme duration', 'to match', 'length of the mel-spectrogram sequence']]",[],[],"[['Model', 'adopts', 'length regulator']]",[],[],[],[],[],text-to-speech_synthesis,1,30
model,"The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .","[('built on', (3, 5)), ('predicts', (11, 12))]","[('phoneme duration predictor', (6, 9)), ('duration of each phoneme', (13, 17))]","[['phoneme duration predictor', 'predicts', 'duration of each phoneme']]",[],[],[],[],"[['length regulator', 'built on', 'phoneme duration predictor']]",[],[],[],text-to-speech_synthesis,1,31
experimental-setup,"We first train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs , with batchsize of 16 sentences on each GPU .","[('train', (2, 3)), ('on', (8, 9)), ('with', (14, 15))]","[('autoregressive Transformer TTS model', (4, 8)), ('4 NVIDIA V100 GPUs', (9, 13)), ('batchsize of 16 sentences', (15, 19))]","[['autoregressive Transformer TTS model', 'on', '4 NVIDIA V100 GPUs'], ['4 NVIDIA V100 GPUs', 'with', 'batchsize of 16 sentences']]",[],"[['Experimental setup', 'train', 'autoregressive Transformer TTS model']]",[],[],[],[],[],[],text-to-speech_synthesis,1,130
experimental-setup,"We use the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 and follow the same learning rate schedule in .","[('use', (1, 2)), ('with', (5, 6))]","[('Adam optimizer', (3, 5)), ('? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9', (6, 20))]","[['Adam optimizer', 'with', '? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9']]",[],"[['Experimental setup', 'use', 'Adam optimizer']]",[],[],[],[],[],[],text-to-speech_synthesis,1,131
experimental-setup,"In addition , we also leverage sequence - level knowledge distillation that has achieved good performance in non-autoregressive machine translation to transfer the knowledge from the teacher model to the student model .","[('leverage', (5, 6))]","[('sequence - level knowledge distillation', (6, 11))]",[],[],"[['Experimental setup', 'leverage', 'sequence - level knowledge distillation']]",[],[],[],[],[],[],text-to-speech_synthesis,1,134
experimental-setup,"In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .","[('In the inference process', (0, 4)), ('transformed into', (13, 15))]","[('output mel-spectrograms', (6, 8)), ('audio samples using the pretrained WaveGlow', (15, 21))]","[['output mel-spectrograms', 'transformed into', 'audio samples using the pretrained WaveGlow']]",[],"[['Experimental setup', 'In the inference process', 'output mel-spectrograms']]",[],[],[],[],[],[],text-to-speech_synthesis,1,139
results,Audio Quality,[],"[('Audio Quality', (0, 2))]",[],[],[],"[['Results', 'has', 'Audio Quality']]",[],[],[],[],[],text-to-speech_synthesis,1,142
results,We conduct the MOS ( mean opinion score ) evaluation on the test set to measure the audio quality .,"[('conduct', (1, 2)), ('on', (10, 11)), ('to measure', (14, 16))]","[('MOS ( mean opinion score ) evaluation', (3, 10)), ('test set', (12, 14)), ('audio quality', (17, 19))]","[['MOS ( mean opinion score ) evaluation', 'on', 'test set'], ['test set', 'to measure', 'audio quality']]",[],[],[],[],"[['Audio Quality', 'conduct', 'MOS ( mean opinion score ) evaluation']]",[],[],[],text-to-speech_synthesis,1,143
results,Robustness,[],"[('Robustness', (0, 1))]",[],[],[],"[['Results', 'has', 'Robustness']]",[],[],[],[],[],text-to-speech_synthesis,1,155
results,"It can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .","[('can be seen that', (1, 5)), ('not robust to', (8, 11)), ('gets', (15, 16)), ('effectively eliminate', (24, 26)), ('to improve', (30, 32))]","[('Transformer TTS', (5, 7)), ('hard cases', (12, 14)), ('34 % error rate', (16, 20)), ('FastSpeech', (22, 23)), ('word repeating and skipping', (26, 30)), ('intelligibility', (32, 33))]","[['FastSpeech', 'effectively eliminate', 'word repeating and skipping'], ['word repeating and skipping', 'to improve', 'intelligibility'], ['Transformer TTS', 'not robust to', 'hard cases'], ['hard cases', 'gets', '34 % error rate']]",[],[],[],[],"[['Robustness', 'can be seen that', 'FastSpeech'], ['Robustness', 'can be seen that', 'Transformer TTS']]",[],[],[],text-to-speech_synthesis,1,159
results,Voice Speed,[],"[('Voice Speed', (0, 2))]",[],[],[],"[['Results', 'has', 'Voice Speed']]",[],[],[],[],[],text-to-speech_synthesis,1,166
results,"As demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .","[('demonstrated by', (1, 3)), ('can adjust the voice speed', (7, 12)), ('with', (19, 20))]","[('samples', (4, 5)), ('FastSpeech', (6, 7)), ('from 0.5x to 1.5 x smoothly', (12, 18)), ('stable and almost unchanged pitch', (20, 25))]","[['FastSpeech', 'can adjust the voice speed', 'from 0.5x to 1.5 x smoothly'], ['from 0.5x to 1.5 x smoothly', 'with', 'stable and almost unchanged pitch']]","[['samples', 'has', 'FastSpeech']]",[],[],[],"[['Voice Speed', 'demonstrated by', 'samples']]",[],[],[],text-to-speech_synthesis,1,169
ablation-analysis,1D Convolution in FFT Block,[],"[('1D Convolution in FFT Block', (0, 5))]",[],[],[],"[['Ablation analysis', 'has', '1D Convolution in FFT Block']]",[],[],[],[],[],text-to-speech_synthesis,1,189
ablation-analysis,"We propose to replace the original fully connected layer ( adopted in Transformer ) with 1D convolution in FFT block , as described in Section 3.1 .","[('replace', (3, 4))]","[('original fully connected layer', (5, 9))]",[],[],[],[],[],"[['1D Convolution in FFT Block', 'replace', 'original fully connected layer']]",[],[],[],text-to-speech_synthesis,1,190
ablation-analysis,"As shown in , replacing 1D convolution with fully connected layer results in - 0.113 CMOS , which demonstrates the effectiveness of 1D convolution .","[('results in', (11, 13))]","[('- 0.113 CMOS', (13, 16))]",[],[],[],[],[],"[['1D Convolution in FFT Block', 'results in', '- 0.113 CMOS']]",[],[],[],text-to-speech_synthesis,1,192
ablation-analysis,Sequence - Level Knowledge Distillation,[],"[('Sequence - Level Knowledge Distillation', (0, 5))]",[],[],[],"[['Ablation analysis', 'has', 'Sequence - Level Knowledge Distillation']]",[],[],[],[],[],text-to-speech_synthesis,1,193
ablation-analysis,"We find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .","[('removing', (3, 4)), ('results in', (9, 11))]","[('sequence - level knowledge distillation', (4, 9)), ('- 0.325 CMOS', (11, 14))]","[['sequence - level knowledge distillation', 'results in', '- 0.325 CMOS']]",[],[],[],[],"[['Sequence - Level Knowledge Distillation', 'removing', 'sequence - level knowledge distillation']]",[],[],[],text-to-speech_synthesis,1,196
research-problem,Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis,[],"[('Text - To - Speech Synthesis', (7, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text - To - Speech Synthesis']]",[],[],[],[],text-to-speech_synthesis,2,2
research-problem,"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .",[],"[('text - to - speech ( TTS ) synthesis', (9, 18))]",[],[],[],[],"[['Contribution', 'has research problem', 'text - to - speech ( TTS ) synthesis']]",[],[],[],[],text-to-speech_synthesis,2,4
research-problem,The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .,[],"[('build a TTS system', (7, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'build a TTS system']]",[],[],[],[],text-to-speech_synthesis,2,10
approach,Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .,"[('to decouple', (3, 5)), ('from', (7, 8)), ('by independently training', (10, 13)), ('captures', (20, 21)), ('training', (27, 28)), ('on', (33, 34)), ('conditioned on', (37, 39))]","[('speaker modeling', (5, 7)), ('speech synthesis', (8, 10)), ('speaker - discriminative embedding network', (14, 19)), ('space of speaker characteristics', (22, 26)), ('high quality TTS model', (29, 33)), ('smaller dataset', (35, 37)), ('representation learned by the first network', (40, 46))]","[['speaker modeling', 'from', 'speech synthesis'], ['speaker modeling', 'by independently training', 'speaker - discriminative embedding network'], ['speaker - discriminative embedding network', 'captures', 'space of speaker characteristics'], ['speaker modeling', 'training', 'high quality TTS model'], ['high quality TTS model', 'conditioned on', 'representation learned by the first network'], ['high quality TTS model', 'on', 'smaller dataset']]",[],"[['Approach', 'to decouple', 'speaker modeling']]",[],[],[],[],[],[],text-to-speech_synthesis,2,18
approach,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,"[('train', (1, 2)), ('on', (6, 7)), ('to determine', (11, 13))]","[('speaker embedding network', (3, 6)), ('speaker verification task', (8, 11)), ('two different utterances were spoken by the same speaker', (14, 23))]","[['speaker embedding network', 'on', 'speaker verification task'], ['speaker verification task', 'to determine', 'two different utterances were spoken by the same speaker']]",[],"[['Approach', 'train', 'speaker embedding network']]",[],[],[],[],[],[],text-to-speech_synthesis,2,20
approach,"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .","[('trained on', (11, 13)), ('containing', (15, 16)), ('from', (20, 21))]","[('untranscribed speech', (13, 15)), ('reverberation and background noise', (16, 20)), ('large number of speakers', (22, 26))]","[['untranscribed speech', 'containing', 'reverberation and background noise'], ['reverberation and background noise', 'from', 'large number of speakers']]",[],"[['Approach', 'trained on', 'untranscribed speech']]",[],[],[],[],[],[],text-to-speech_synthesis,2,21
results,Speech naturalness,[],"[('Speech naturalness', (0, 2))]",[],[],[],"[['Results', 'has', 'Speech naturalness']]",[],[],[],[],"[['Speech naturalness', 'has', 'proposed model']]",text-to-speech_synthesis,2,115
results,"The proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .","[('achieved', (3, 4)), ('in', (7, 8))]","[('proposed model', (1, 3)), ('about 4.0 MOS', (4, 7)), ('all datasets', (8, 10))]","[['proposed model', 'achieved', 'about 4.0 MOS'], ['about 4.0 MOS', 'in', 'all datasets']]",[],[],[],[],[],[],[],[],text-to-speech_synthesis,2,123
results,"Most importantly , the audio generated by our model for unseen speakers is deemed to be at least as natural as that generated for seen speakers .","[('for', (9, 10)), ('is deemed to be at least as natural as', (12, 21))]","[('audio generated', (4, 6)), ('unseen speakers', (10, 12)), ('generated for seen speakers', (22, 26))]","[['audio generated', 'for', 'unseen speakers'], ['unseen speakers', 'is deemed to be at least as natural as', 'generated for seen speakers']]",[],[],[],[],[],[],"[['Speech naturalness', 'has', 'audio generated']]",[],text-to-speech_synthesis,2,125
results,"Surprisingly , the MOS on unseen speakers is higher than that of seen speakers , by as much as 0.2 points on LibriSpeech .","[('is higher than', (7, 10)), ('by as much as', (15, 19)), ('on', (21, 22))]","[('MOS on unseen speakers', (3, 7)), ('seen speakers', (12, 14)), ('0.2 points', (19, 21)), ('LibriSpeech', (22, 23))]","[['MOS on unseen speakers', 'is higher than', 'seen speakers'], ['seen speakers', 'by as much as', '0.2 points'], ['0.2 points', 'on', 'LibriSpeech']]",[],[],[],[],[],[],"[['Speech naturalness', 'has', 'MOS on unseen speakers']]",[],text-to-speech_synthesis,2,126
results,Speaker similarity,[],"[('Speaker similarity', (0, 2))]",[],[],[],"[['Results', 'has', 'Speaker similarity']]",[],[],[],[],[],text-to-speech_synthesis,2,131
results,"The scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .","[('scores for', (1, 3)), ('tend to be higher than those for', (6, 13)), ('reflecting', (15, 16))]","[('VCTK model', (4, 6)), ('LibriSpeech', (13, 14)), ('cleaner nature of the dataset', (17, 22))]","[['VCTK model', 'tend to be higher than those for', 'LibriSpeech'], ['LibriSpeech', 'reflecting', 'cleaner nature of the dataset']]",[],[],[],[],"[['Speaker similarity', 'scores for', 'VCTK model']]",[],[],[],text-to-speech_synthesis,2,135
results,"For seen speakers on VCTK , the proposed model performs about as well as the baseline which uses an embedding lookup table for speaker conditioning .","[('For', (0, 1)), ('performs about as well as', (9, 14)), ('uses', (17, 18))]","[('seen speakers on VCTK', (1, 5)), ('proposed model', (7, 9)), ('baseline', (15, 16)), ('embedding lookup table for speaker conditioning', (19, 25))]","[['proposed model', 'performs about as well as', 'baseline'], ['baseline', 'uses', 'embedding lookup table for speaker conditioning']]","[['seen speakers on VCTK', 'has', 'proposed model']]",[],[],[],"[['Speaker similarity', 'For', 'seen speakers on VCTK']]",[],[],[],text-to-speech_synthesis,2,137
results,Speaker verification,[],"[('Speaker verification', (0, 2))]",[],[],[],"[['Results', 'has', 'Speaker verification']]",[],[],[],[],[],text-to-speech_synthesis,2,152
results,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the synthesized speech is typically most similar to the ground truth voices .","[('trained on', (10, 12)), ('most similar to', (28, 31))]","[('LibriSpeech', (21, 22)), ('synthesized speech', (24, 26)), ('ground truth voices', (32, 35))]","[['synthesized speech', 'most similar to', 'ground truth voices']]","[['LibriSpeech', 'has', 'synthesized speech']]",[],[],[],"[['Speaker verification', 'trained on', 'LibriSpeech']]",[],[],[],text-to-speech_synthesis,2,160
results,"On this 20 voice discrimination task we obtain an EER of 2.86 % , demonstrating that , while the synthetic speech tends to be close to the target speaker ( cosine similarity > 0.6 , and as in ) , it is nearly always even closer to other synthetic utterances for the same speaker ( similarity > 0.7 ) .","[('On', (0, 1)), ('obtain', (7, 8))]","[('20 voice discrimination task', (2, 6)), ('EER of 2.86 %', (9, 13))]","[['20 voice discrimination task', 'obtain', 'EER of 2.86 %']]",[],[],[],[],"[['Speaker verification', 'On', '20 voice discrimination task']]",[],[],[],text-to-speech_synthesis,2,164
results,Speaker embedding space,[],"[('Speaker embedding space', (0, 3))]",[],[],[],"[['Results', 'has', 'Speaker embedding space']]",[],[],[],[],"[['Speaker embedding space', 'has', 'PCA visualization']]",text-to-speech_synthesis,2,166
results,The PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .,"[('shows', (6, 7)), ('tend to lie very close to', (10, 16)), ('in', (22, 23))]","[('PCA visualization', (1, 3)), ('synthesized utterances', (8, 10)), ('real speech from the same speaker', (16, 22)), ('embedding space', (24, 26))]","[['PCA visualization', 'shows', 'synthesized utterances'], ['synthesized utterances', 'tend to lie very close to', 'real speech from the same speaker'], ['real speech from the same speaker', 'in', 'embedding space']]",[],[],[],[],[],[],[],[],text-to-speech_synthesis,2,169
results,"However , synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t - SNE visualization ( right ) where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker .",[],[],"[['t - SNE visualization', 'where', 'utterances'], ['utterances', 'from', 'each synthetic speaker'], ['each synthetic speaker', 'from', 'distinct cluster'], ['distinct cluster', 'adjacent to', 'cluster of real utterances'], ['cluster of real utterances', 'from', 'corresponding speaker'], ['t - SNE visualization', 'demonstrated', 'synthetic utterances'], ['synthetic utterances', 'easily distinguishable from', 'real human speech']]",[],[],[],[],[],[],"[['Speaker embedding space', 'has', 't - SNE visualization']]",[],text-to-speech_synthesis,2,170
results,Number of speaker encoder training speakers,[],"[('Number of speaker encoder training speakers', (0, 6))]",[],[],[],"[['Results', 'has', 'Number of speaker encoder training speakers']]",[],[],[],[],[],text-to-speech_synthesis,2,173
results,"As the number of training speakers increases , both naturalness and similarity improve significantly .","[('As', (0, 1)), ('improve significantly', (12, 14))]","[('number of training speakers', (2, 6)), ('increases', (6, 7)), ('both naturalness and similarity', (8, 12))]","[['both naturalness and similarity', 'As', 'increases']]","[['increases', 'has', 'number of training speakers']]",[],[],[],"[['Number of speaker encoder training speakers', 'improve significantly', 'both naturalness and similarity']]",[],[],[],text-to-speech_synthesis,2,185
results,Fictitious speakers,[],"[('Fictitious speakers', (0, 2))]",[],[],[],"[['Results', 'has', 'Fictitious speakers']]",[],[],[],[],[],text-to-speech_synthesis,2,190
results,Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .,[],[],"[['synthesizer', 'on', 'random points'], ['random points', 'in', 'speaker embedding space']]",[],[],[],[],"[['Fictitious speakers', 'conditioning', 'synthesizer'], ['Fictitious speakers', 'Bypassing', 'speaker encoder network']]",[],[],[],text-to-speech_synthesis,2,191
research-problem,"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .",[],"[('generating real - valued data', (33, 38))]",[],[],[],[],"[['Contribution', 'has research problem', 'generating real - valued data']]",[],[],[],[],text_generation,0,4
research-problem,"However , it has limitations when the goal is for generating sequences of discrete tokens .",[],"[('generating sequences of discrete tokens', (10, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'generating sequences of discrete tokens']]",[],[],[],[],text_generation,0,5
research-problem,Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .,[],"[('Generating sequential synthetic data', (0, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generating sequential synthetic data']]",[],[],[],[],text_generation,0,13
model,"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .","[('consider', (15, 16)), ('as', (20, 21))]","[('sequence generation procedure', (17, 20)), ('sequential decision making process', (22, 26))]","[['sequence generation procedure', 'as', 'sequential decision making process']]",[],"[['Model', 'consider', 'sequence generation procedure']]",[],[],[],[],[],[],text_generation,0,32
model,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,[],[],"[['generative model', 'treated as', 'agent of reinforcement learning ( RL )'], ['action', 'is', 'next token to be generated'], ['state', 'is', 'generated tokens so far']]","[['generative model', 'has', 'action'], ['generative model', 'has', 'state']]",[],"[['Model', 'has', 'generative model']]",[],[],[],[],[],text_generation,0,33
model,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .","[('employ', (27, 28)), ('to evaluate', (30, 32)), ('feedback', (35, 36)), ('to guide', (38, 40)), ('of', (42, 43))]","[('sequence', (11, 12)), ('discriminator', (29, 30)), ('evaluation', (37, 38)), ('learning', (41, 42)), ('generative model', (44, 46))]","[['discriminator', 'feedback', 'evaluation'], ['evaluation', 'to guide', 'learning'], ['learning', 'of', 'generative model'], ['discriminator', 'to evaluate', 'sequence']]",[],"[['Model', 'employ', 'discriminator']]",[],[],[],[],[],[],text_generation,0,34
model,"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .","[('regard', (22, 23)), ('as a', (26, 28))]","[('generative model', (13, 15)), ('stochastic parametrized policy', (28, 31))]","[['generative model', 'as a', 'stochastic parametrized policy']]",[],"[['Model', 'regard', 'generative model']]",[],[],[],[],[],[],text_generation,0,35
model,"In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .","[('In', (0, 1)), ('employ', (6, 7)), ('to approximate', (13, 15))]","[('policy gradient', (2, 4)), ('Monte Carlo ( MC ) search', (7, 13)), ('state - action value', (16, 20))]","[['policy gradient', 'employ', 'Monte Carlo ( MC ) search'], ['Monte Carlo ( MC ) search', 'to approximate', 'state - action value']]",[],"[['Model', 'In', 'policy gradient']]",[],[],[],[],[],[],text_generation,0,36
model,"We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .","[('directly train', (1, 3)), ('via', (9, 10)), ('naturally avoids', (14, 16)), ('for', (19, 20)), ('in a', (22, 24))]","[('policy ( generative model )', (4, 9)), ('policy gradient', (10, 12)), ('differentiation difficulty', (17, 19)), ('discrete data', (20, 22)), ('conventional GAN', (24, 26))]","[['policy ( generative model )', 'via', 'policy gradient'], ['policy gradient', 'naturally avoids', 'differentiation difficulty'], ['differentiation difficulty', 'for', 'discrete data'], ['discrete data', 'in a', 'conventional GAN']]",[],"[['Model', 'directly train', 'policy ( generative model )']]",[],[],[],[],[],[],text_generation,0,37
hyperparameters,"To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .","[('first initialize', (8, 10)), ('of an', (12, 14)), ('following', (16, 17)), ('as', (26, 27)), ('describing', (29, 30))]","[('parameters', (11, 12)), ('LSTM network', (14, 16)), ('normal distribution N ( 0 , 1 )', (18, 26)), ('oracle', (28, 29)), ('real data distribution G oracle ( x t |x 1 , . . . , x t?1 )', (31, 49))]","[['parameters', 'of an', 'LSTM network'], ['LSTM network', 'following', 'normal distribution N ( 0 , 1 )'], ['normal distribution N ( 0 , 1 )', 'as', 'oracle'], ['oracle', 'describing', 'real data distribution G oracle ( x t |x 1 , . . . , x t?1 )']]",[],"[['Hyperparameters', 'first initialize', 'parameters']]",[],[],[],[],[],[],text_generation,0,185
hyperparameters,"In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label",[],[],"[['training set', 'for', 'discriminator'], ['discriminator', 'is comprised by', 'generated examples'], ['generated examples', 'with', 'label 0'], ['generated examples', 'with', 'instances'], ['instances', 'from', 'S'], ['S', 'with', 'label']]","[['SeqGAN algorithm', 'has', 'training set']]","[['Hyperparameters', 'In', 'SeqGAN algorithm']]",[],[],[],[],[],[],text_generation,0,187
,"For different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 . ",[],[],[],[],[],[],[],[],[],[],[],text_generation,0,189
hyperparameters,Dropout ) and L2 regularization are used to avoid over-fitting .,"[('to avoid', (7, 9))]","[('Dropout ) and L2 regularization', (0, 5)), ('over-fitting', (9, 10))]","[['Dropout ) and L2 regularization', 'to avoid', 'over-fitting']]",[],[],"[['Hyperparameters', 'has', 'Dropout ) and L2 regularization']]",[],[],[],[],[],text_generation,0,190
baselines,The first model is a random token generation .,[],"[('random token generation', (5, 8))]",[],[],[],"[['Baselines', 'has', 'random token generation']]",[],[],[],[],[],text_generation,0,192
baselines,The second one is the MLE trained LSTM G ? .,[],"[('MLE trained LSTM G', (5, 9))]",[],[],[],"[['Baselines', 'has', 'MLE trained LSTM G']]",[],[],[],[],[],text_generation,0,193
baselines,The third one is scheduled sampling .,[],"[('scheduled sampling', (4, 6))]",[],[],[],"[['Baselines', 'has', 'scheduled sampling']]",[],[],[],[],[],text_generation,0,194
baselines,The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .,[],"[('Policy Gradient with BLEU ( PG - BLEU )', (5, 14))]",[],[],[],"[['Baselines', 'has', 'Policy Gradient with BLEU ( PG - BLEU )']]",[],[],[],[],[],text_generation,0,195
hyperparameters,"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .","[('training process', (6, 8)), ('from', (10, 11)), ('into', (20, 21)), ('towards', (23, 24)), ('with', (33, 34))]","[('scheduled sampling', (2, 4)), ('gradually changes', (8, 10)), ('fully guided scheme feeding the true previous tokens', (12, 20)), ('LSTM', (21, 22)), ('less guided scheme which mostly feeds the LSTM', (25, 33)), ('generated tokens', (35, 37))]","[['scheduled sampling', 'training process', 'gradually changes'], ['gradually changes', 'from', 'fully guided scheme feeding the true previous tokens'], ['fully guided scheme feeding the true previous tokens', 'into', 'LSTM'], ['LSTM', 'towards', 'less guided scheme which mostly feeds the LSTM'], ['less guided scheme which mostly feeds the LSTM', 'with', 'generated tokens']]",[],[],"[['Hyperparameters', 'In', 'scheduled sampling']]",[],[],[],[],[],text_generation,0,196
hyperparameters,A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .,"[('to control', (6, 8)), ('of', (10, 11)), ('with', (15, 16))]","[('curriculum rate ?', (1, 4)), ('probability', (9, 10)), ('replacing the true tokens', (11, 15)), ('generated ones', (17, 19))]","[['curriculum rate ?', 'to control', 'probability'], ['probability', 'of', 'replacing the true tokens'], ['replacing the true tokens', 'with', 'generated ones']]",[],[],"[['Hyperparameters', 'has', 'curriculum rate ?']]",[],[],[],[],[],text_generation,0,197
results,"Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly .","[('see', (10, 11)), ('outperforms', (17, 18))]","[('impact of SeqGAN', (12, 15)), ('other baselines significantly', (18, 21))]","[['impact of SeqGAN', 'outperforms', 'other baselines significantly']]",[],"[['Results', 'see', 'impact of SeqGAN']]",[],[],[],[],[],[],text_generation,0,202
results,"A significance T - test on the NLL oracle score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models .",[],[],"[['significance T - test', 'on', 'NLL oracle score distribution'], ['NLL oracle score distribution', 'of', 'generated sequences'], ['generated sequences', 'from', 'compared models'], ['compared models', 'demonstrates', 'significant improvement'], ['significant improvement', 'of', 'SeqGAN'], ['SeqGAN', 'over all', 'compared models']]",[],[],"[['Results', 'has', 'significance T - test']]",[],[],[],[],[],text_generation,0,203
results,"After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly .","[('After about', (0, 2)), ('both', (6, 7)), ('converge to', (16, 18)), ('improve the limit of', (28, 32)), ('with', (34, 35)), ('as', (38, 39))]","[('150 training epochs', (2, 5)), ('maximum likelihood estimation and the schedule sampling methods', (8, 16)), ('relatively high NLL oracle score', (19, 24)), ('SeqGAN', (26, 27)), ('generator', (33, 34)), ('same structure', (36, 38)), ('baselines', (40, 41))]","[['150 training epochs', 'both', 'maximum likelihood estimation and the schedule sampling methods'], ['maximum likelihood estimation and the schedule sampling methods', 'converge to', 'relatively high NLL oracle score'], ['SeqGAN', 'improve the limit of', 'generator'], ['generator', 'with', 'same structure'], ['same structure', 'as', 'baselines']]","[['relatively high NLL oracle score', 'has', 'SeqGAN']]","[['Results', 'After about', '150 training epochs']]",[],[],[],[],[],[],text_generation,0,205
results,This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE .,"[('indicates', (1, 2)), ('to', (9, 10)), ('to breakthrough', (14, 16))]","[('prospect of applying adversarial training strategies', (3, 9)), ('discrete sequence generative models', (10, 14)), ('limitations of MLE', (17, 20))]","[['prospect of applying adversarial training strategies', 'to', 'discrete sequence generative models'], ['discrete sequence generative models', 'to breakthrough', 'limitations of MLE']]",[],"[['Results', 'indicates', 'prospect of applying adversarial training strategies']]",[],[],[],[],[],[],text_generation,0,206
results,"Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data .","[('outperforms', (3, 4))]","[('SeqGAN', (2, 3)), ('PG - BLEU', (4, 7))]","[['SeqGAN', 'outperforms', 'PG - BLEU']]",[],[],"[['Results', 'has', 'SeqGAN']]",[],[],[],[],[],text_generation,0,207
research-problem,Adversarial Ranking for Language Generation,[],"[('Language Generation', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Language Generation']]",[],[],[],[],text_generation,1,2
model,"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .","[('propose', (5, 6))]","[('novel adversarial learning framework', (7, 11)), ('RankGAN', (12, 13))]",[],"[['novel adversarial learning framework', 'name', 'RankGAN']]","[['Model', 'propose', 'novel adversarial learning framework']]",[],[],[],[],[],[],text_generation,1,28
model,RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,"[('learns', (1, 2)), ('from', (4, 5)), ('between', (9, 10)), ('in', (20, 21))]","[('model', (3, 4)), ('relative ranking information', (6, 9)), ('the machine - written and the human - written sentences', (10, 20)), ('adversarial framework', (22, 24))]","[['model', 'from', 'relative ranking information'], ['relative ranking information', 'between', 'the machine - written and the human - written sentences'], ['the machine - written and the human - written sentences', 'in', 'adversarial framework']]",[],"[['Model', 'learns', 'model']]",[],[],[],[],[],[],text_generation,1,29
model,"In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .","[('relax', (6, 7)), ('of', (9, 10)), ('to', (12, 13))]","[('training', (8, 9)), ('discriminator', (11, 12)), ('learning - to - rank optimization problem', (14, 21))]","[['training', 'of', 'discriminator'], ['training', 'to', 'learning - to - rank optimization problem']]",[],"[['Model', 'relax', 'training']]",[],[],[],[],[],[],text_generation,1,30
model,"Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .","[('proposed', (3, 4)), ('consists of', (7, 9))]","[('new adversarial network', (4, 7)), ('two neural network models', (9, 13)), ('a generator and a ranker', (14, 19))]","[['new adversarial network', 'consists of', 'two neural network models']]","[['two neural network models', 'name', 'a generator and a ranker']]","[['Model', 'proposed', 'new adversarial network']]",[],[],[],[],[],[],text_generation,1,31
model,"As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .","[('to rank', (15, 17)), ('lower than', (22, 24)), ('with respect to', (28, 31)), ('which is', (34, 36))]","[('train', (12, 13)), ('ranker', (14, 15)), ('machine - written sentences', (18, 22)), ('human - written sentences', (24, 28)), ('reference sentence', (32, 34)), ('human-written', (36, 37))]","[['ranker', 'to rank', 'machine - written sentences'], ['machine - written sentences', 'with respect to', 'reference sentence'], ['reference sentence', 'which is', 'human-written'], ['machine - written sentences', 'lower than', 'human - written sentences']]","[['train', 'has', 'ranker']]",[],"[['Model', 'propose', 'train']]",[],[],[],[],[],text_generation,1,32
model,"Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .","[('train', (3, 4)), ('to synthesize', (6, 8)), ('which confuse', (9, 11)), ('so that', (13, 15)), ('ranked higher than', (20, 23))]","[('generator', (5, 6)), ('sentences', (8, 9)), ('ranker', (12, 13)), ('machine - written sentences', (15, 19)), ('human - written sentences', (23, 27))]","[['generator', 'so that', 'machine - written sentences'], ['machine - written sentences', 'ranked higher than', 'human - written sentences'], ['generator', 'to synthesize', 'sentences'], ['sentences', 'which confuse', 'ranker']]",[],"[['Model', 'train', 'generator']]",[],[],[],[],[],[],text_generation,1,33
model,"During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .","[('During', (0, 1)), ('adopt', (4, 5)), ('to overcome', (9, 11))]","[('learning', (1, 2)), ('policy gradient technique', (6, 9)), ('non-differentiable problem', (12, 14))]","[['learning', 'adopt', 'policy gradient technique'], ['policy gradient technique', 'to overcome', 'non-differentiable problem']]",[],"[['Model', 'During', 'learning']]",[],[],[],[],[],[],text_generation,1,34
model,"Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .","[('viewing', (3, 4)), ('evaluating', (11, 12)), ('through', (14, 15)), ('able to make', (21, 24)), ('of', (26, 27)), ('helps', (36, 37)), ('to learn', (39, 41))]","[('set of data samples collectively', (5, 10)), ('quality', (13, 14)), ('relative ranking', (15, 17)), ('discriminator', (19, 20)), ('better assessment', (24, 26)), ('samples', (31, 32)), ('generator', (38, 39)), ('better', (41, 42))]","[['set of data samples collectively', 'evaluating', 'quality'], ['quality', 'through', 'relative ranking'], ['quality', 'of', 'samples'], ['discriminator', 'able to make', 'better assessment'], ['better assessment', 'helps', 'generator'], ['generator', 'to learn', 'better']]","[['samples', 'has', 'discriminator']]","[['Model', 'viewing', 'set of data samples collectively']]",[],[],[],[],[],[],text_generation,1,35
model,Our method is suitable for language learning in comparison to conventional GANs .,"[('suitable for', (3, 5)), ('in comparison to', (7, 10))]","[('language learning', (5, 7)), ('conventional GANs', (10, 12))]","[['language learning', 'in comparison to', 'conventional GANs']]",[],"[['Model', 'suitable for', 'language learning']]",[],[],[],[],[],[],text_generation,1,36
results,Simulation on synthetic data,"[('on', (1, 2))]","[('synthetic data', (2, 4))]",[],[],"[['Results', 'on', 'synthetic data']]",[],[],[],[],[],"[['synthetic data', 'has', 'RankGAN']]",text_generation,1,168
results,It can be seen that the proposed RankGAN performs more favourably against the compared methods .,"[('performs', (8, 9)), ('against', (11, 12))]","[('RankGAN', (7, 8)), ('more favourably', (9, 11)), ('compared methods', (13, 15))]","[['RankGAN', 'performs', 'more favourably'], ['more favourably', 'against', 'compared methods']]",[],[],[],[],[],[],[],[],text_generation,1,189
results,"While MLE , PG - BLEU and SeqGAN tend to converge after 200 training epochs , the proposed RankGAN consistently improves the language generator and achieves relatively lower NLL score .","[('tend to', (8, 10)), ('after', (11, 12)), ('consistently improves', (19, 21)), ('achieves', (25, 26))]","[('MLE , PG - BLEU and SeqGAN', (1, 8)), ('converge', (10, 11)), ('200 training epochs', (12, 15)), ('proposed RankGAN', (17, 19)), ('language generator', (22, 24)), ('relatively lower', (26, 28)), ('NLL score', (28, 30))]","[['MLE , PG - BLEU and SeqGAN', 'tend to', 'converge'], ['converge', 'after', '200 training epochs'], ['proposed RankGAN', 'consistently improves', 'language generator'], ['proposed RankGAN', 'achieves', 'relatively lower']]","[['relatively lower', 'has', 'NLL score']]",[],[],[],[],[],"[['synthetic data', 'has', 'MLE , PG - BLEU and SeqGAN'], ['synthetic data', 'has', 'proposed RankGAN']]",[],text_generation,1,191
results,It is worth noting that the proposed RankGAN achieves better performance than that of PG - BLEU .,"[('worth noting', (2, 4)), ('achieves', (8, 9)), ('than', (11, 12))]","[('proposed RankGAN', (6, 8)), ('better performance', (9, 11)), ('PG - BLEU', (14, 17))]","[['proposed RankGAN', 'achieves', 'better performance'], ['better performance', 'than', 'PG - BLEU']]",[],[],[],[],"[['synthetic data', 'worth noting', 'proposed RankGAN']]",[],[],[],text_generation,1,193
results,Results on Chinese poems composition,[],"[('Chinese poems composition', (2, 5))]",[],[],[],"[['Results', 'on', 'Chinese poems composition']]",[],[],[],[],[],text_generation,1,203
results,"Following the evaluation protocol in , we compute the BLEU - 2 score and estimate the similarity between the human - written poem and the machine - created one .","[('estimate', (14, 15)), ('between', (17, 18))]","[('similarity', (16, 17)), ('human - written poem and the machine - created one', (19, 29))]","[['similarity', 'between', 'human - written poem and the machine - created one']]",[],[],[],[],"[['Chinese poems composition', 'estimate', 'similarity']]",[],[],[],text_generation,1,209
results,It can be seen that the proposed Rank GAN performs more favourably compared to the state - of - the - art methods in terms of BLEU - 2 score .,"[('seen that', (3, 5)), ('performs', (9, 10)), ('compared to', (12, 14)), ('in terms of', (23, 26))]","[('proposed Rank GAN', (6, 9)), ('more favourably', (10, 12)), ('state - of - the - art methods', (15, 23)), ('BLEU - 2 score', (26, 30))]","[['proposed Rank GAN', 'performs', 'more favourably'], ['more favourably', 'compared to', 'state - of - the - art methods'], ['state - of - the - art methods', 'in terms of', 'BLEU - 2 score']]",[],[],[],[],"[['Chinese poems composition', 'seen that', 'proposed Rank GAN']]",[],[],[],text_generation,1,211
results,RankGAN outperforms the compared method in terms of the human evaluation score .,"[('outperforms', (1, 2)), ('in terms of', (5, 8))]","[('RankGAN', (0, 1)), ('compared method', (3, 5)), ('human evaluation score', (9, 12))]","[['RankGAN', 'outperforms', 'compared method']]","[['human evaluation score', 'has', 'RankGAN']]",[],[],[],"[['Chinese poems composition', 'in terms of', 'human evaluation score']]",[],[],[],text_generation,1,238
results,Results on COCO image captions,[],"[('COCO image captions', (2, 5))]",[],[],[],"[['Results', 'on', 'COCO image captions']]",[],[],[],[],"[['COCO image captions', 'has', 'RankGAN']]",text_generation,1,240
results,RankGAN achieves better performance than the other methods in terms of different BLEU scores .,"[('achieves', (1, 2)), ('than', (4, 5)), ('in terms of', (8, 11))]","[('RankGAN', (0, 1)), ('better performance', (2, 4)), ('other methods', (6, 8)), ('different BLEU scores', (11, 14))]","[['RankGAN', 'achieves', 'better performance'], ['better performance', 'than', 'other methods'], ['other methods', 'in terms of', 'different BLEU scores']]",[],[],[],[],[],[],[],[],text_generation,1,249
results,"These examples show that our model is able to generate fluent , novel sentences that are not existing in the training set .","[('able to generate', (7, 10)), ('not existing in', (16, 19))]","[('our model', (4, 6)), ('fluent , novel sentences', (10, 14)), ('training set', (20, 22))]","[['our model', 'able to generate', 'fluent , novel sentences'], ['fluent , novel sentences', 'not existing in', 'training set']]",[],[],[],[],[],[],"[['COCO image captions', 'has', 'our model']]",[],text_generation,1,251
results,"As can be seen , the human - written sentences get the highest score comparing to the language models .","[('get', (10, 11)), ('comparing to', (14, 16))]","[('human - written sentences', (6, 10)), ('highest score', (12, 14)), ('language models', (17, 19))]","[['human - written sentences', 'get', 'highest score'], ['highest score', 'comparing to', 'language models']]",[],[],[],[],[],[],"[['COCO image captions', 'has', 'human - written sentences']]",[],text_generation,1,257
results,"Among the GANs approaches , RankGAN receives better score than SeqGAN , which is consistent to the finding in the Chinese poem composition .","[('Among', (0, 1)), ('receives', (6, 7)), ('than', (9, 10))]","[('GANs approaches', (2, 4)), ('RankGAN', (5, 6)), ('better score', (7, 9)), ('SeqGAN', (10, 11))]","[['RankGAN', 'receives', 'better score'], ['better score', 'than', 'SeqGAN']]","[['GANs approaches', 'has', 'RankGAN']]",[],[],[],"[['human - written sentences', 'Among', 'GANs approaches']]",[],[],[],text_generation,1,258
results,Results on Shakespeare 's plays,[],"[(""Shakespeare 's plays"", (2, 5))]",[],[],[],"[['Results', 'on', ""Shakespeare 's plays""]]",[],[],[],[],"[[""Shakespeare 's plays"", 'has', 'proposed method']]",text_generation,1,260
results,"As can be seen , the proposed method achieves consistently higher BLEU score than the other methods in terms of the different n-grams criteria .","[('achieves', (8, 9)), ('than', (13, 14)), ('in terms of', (17, 20))]","[('proposed method', (6, 8)), ('consistently higher BLEU score', (9, 13)), ('other methods', (15, 17)), ('different n-grams criteria', (21, 24))]","[['proposed method', 'achieves', 'consistently higher BLEU score'], ['consistently higher BLEU score', 'than', 'other methods'], ['other methods', 'in terms of', 'different n-grams criteria']]",[],[],[],[],[],[],[],[],text_generation,1,266
results,"The results indicate the proposed RankGAN is able to capture the transition pattern among the words , even if the training sentences are novel , delicate and complicated .","[('is able to capture', (6, 10)), ('among', (13, 14))]","[('proposed RankGAN', (4, 6)), ('transition pattern', (11, 13)), ('words', (15, 16))]","[['proposed RankGAN', 'is able to capture', 'transition pattern'], ['transition pattern', 'among', 'words']]",[],[],[],[],[],[],"[[""Shakespeare 's plays"", 'has', 'proposed RankGAN']]",[],text_generation,1,267
research-problem,Long Text Generation via Adversarial Training with Leaked Information,[],"[('Text Generation', (1, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text Generation']]",[],[],[],[],text_generation,2,2
research-problem,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .",[],"[('Automatically generating coherent and semantically meaningful text', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatically generating coherent and semantically meaningful text']]",[],[],[],[],text_generation,2,4
approach,"In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .","[('called', (10, 11)), ('to address', (13, 15))]","[('Leak GAN', (11, 13)), ('both the non-informativeness and the sparsity issues', (15, 22))]","[['Leak GAN', 'to address', 'both the non-informativeness and the sparsity issues']]",[],"[['Approach', 'called', 'Leak GAN']]",[],[],[],[],[],[],text_generation,2,33
approach,LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .,"[('providing', (6, 7)), ('from', (9, 10)), ('to', (12, 13)), ('by borrowing', (15, 17)), ('in', (20, 21))]","[('LeakGAN', (0, 1)), ('richer information', (7, 9)), ('discriminator', (11, 12)), ('generator', (14, 15)), ('recent advances', (18, 20)), ('hierarchical reinforcement learning', (21, 24))]","[['LeakGAN', 'providing', 'richer information'], ['richer information', 'from', 'discriminator'], ['discriminator', 'to', 'generator'], ['generator', 'by borrowing', 'recent advances'], ['recent advances', 'in', 'hierarchical reinforcement learning']]",[],[],"[['Approach', 'has', 'LeakGAN']]",[],[],[],[],[],text_generation,2,34
approach,"As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .","[('introduce', (6, 7)), ('consists of', (13, 15))]","[('hierarchical generator G', (8, 11)), ('high - level MANAGER module', (16, 21)), ('low - level WORKER module', (23, 28))]","[['hierarchical generator G', 'consists of', 'high - level MANAGER module'], ['hierarchical generator G', 'consists of', 'low - level WORKER module']]",[],"[['Approach', 'introduce', 'hierarchical generator G']]",[],[],[],[],[],[],text_generation,2,35
approach,The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .,"[('along', (3, 4)), ('serves as', (11, 13))]","[('MANAGER', (1, 2)), ('shortterm memory network ( LSTM )', (4, 10)), ('mediator', (14, 15))]","[['MANAGER', 'along', 'shortterm memory network ( LSTM )'], ['MANAGER', 'serves as', 'mediator']]",[],[],"[['Approach', 'has', 'MANAGER']]",[],[],[],[],[],text_generation,2,36
approach,"In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep .","[('receives', (5, 6)), ('form', (28, 29)), ('for', (32, 33))]","[(""generator D 's high - level feature representation"", (6, 14)), ('guiding goal', (30, 32)), ('WORKER module', (34, 36))]","[[""generator D 's high - level feature representation"", 'form', 'guiding goal'], ['guiding goal', 'for', 'WORKER module']]",[],[],[],[],"[['MANAGER', 'receives', ""generator D 's high - level feature representation""]]",[],[],[],text_generation,2,37
approach,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .","[('given', (2, 3)), ('produced by', (6, 8)), ('first encodes', (15, 17)), ('with', (20, 21)), ('then combines', (24, 26)), ('of', (28, 29)), ('to take', (35, 37)), ('at', (40, 41))]","[('goal embedding', (4, 6)), ('MAN - AGER', (9, 12)), ('WORKER', (14, 15)), ('current generated words', (17, 20)), ('another LSTM', (21, 23)), ('output', (27, 28)), ('the LSTM and the goal embedding', (29, 35)), ('final action', (38, 40)), ('current state', (41, 43))]","[['goal embedding', 'produced by', 'MAN - AGER'], ['WORKER', 'then combines', 'output'], ['output', 'of', 'the LSTM and the goal embedding'], ['output', 'to take', 'final action'], ['final action', 'at', 'current state'], ['WORKER', 'first encodes', 'current generated words'], ['current generated words', 'with', 'another LSTM']]","[['MAN - AGER', 'has', 'WORKER']]","[['Approach', 'given', 'goal embedding']]",[],[],[],[],[],[],text_generation,2,40
hyperparameters,GAN Setting .,[],"[('GAN Setting', (0, 2))]",[],[],[],"[['Hyperparameters', 'has', 'GAN Setting']]",[],[],[],[],[],text_generation,2,163
hyperparameters,"For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .","[('choose', (5, 6)), ('as', (9, 10))]","[('CNN architecture', (7, 9)), ('feature extractor and the binary classifier', (11, 17))]","[['CNN architecture', 'as', 'feature extractor and the binary classifier']]",[],[],[],[],"[['GAN Setting', 'choose', 'CNN architecture']]",[],[],[],text_generation,2,164
hyperparameters,"For the synthetic data experiment , the CNN kernel size ranges from 1 to T .","[('For', (0, 1)), ('ranges', (10, 11))]","[('synthetic data experiment', (2, 5)), ('CNN kernel size', (7, 10)), ('from 1 to T', (11, 15))]","[['CNN kernel size', 'ranges', 'from 1 to T']]","[['synthetic data experiment', 'has', 'CNN kernel size']]",[],[],[],"[['GAN Setting', 'For', 'synthetic data experiment']]",[],[],[],text_generation,2,166
hyperparameters,The number of each kernel is between 100 and 200 .,"[('number of', (1, 3)), ('between', (6, 7))]","[('each kernel', (3, 5)), ('100 and 200', (7, 10))]","[['each kernel', 'between', '100 and 200']]",[],[],[],[],"[['GAN Setting', 'number of', 'each kernel']]",[],[],[],text_generation,2,167
hyperparameters,"In this case , the feature of text is a 1,720 dimensional vector .","[('is', (8, 9))]","[('feature of text', (5, 8)), ('1,720 dimensional vector', (10, 13))]","[['feature of text', 'is', '1,720 dimensional vector']]",[],[],[],[],[],[],"[['GAN Setting', 'has', 'feature of text']]",[],text_generation,2,168
hyperparameters,Dropout with the keep rate 0.75 and L2 regularization are performed to avoid overfitting .,"[('with', (1, 2)), ('performed to avoid', (10, 13))]","[('Dropout', (0, 1)), ('keep rate 0.75', (3, 6)), ('L2 regularization', (7, 9)), ('overfitting', (13, 14))]","[['Dropout', 'performed to avoid', 'overfitting'], ['overfitting', 'with', 'keep rate 0.75'], ['overfitting', 'with', 'L2 regularization']]",[],[],[],[],[],[],"[['GAN Setting', 'has', 'Dropout']]",[],text_generation,2,169
hyperparameters,"For the generator , we adopt LSTM as the architectures of MANAGER and WORKER to capture the sequence context information .","[('adopt', (5, 6)), ('as', (7, 8)), ('to capture', (14, 16))]","[('generator', (2, 3)), ('LSTM', (6, 7)), ('architectures of MANAGER and WORKER', (9, 14)), ('sequence context information', (17, 20))]","[['generator', 'adopt', 'LSTM'], ['LSTM', 'as', 'architectures of MANAGER and WORKER'], ['architectures of MANAGER and WORKER', 'to capture', 'sequence context information']]",[],[],[],[],[],[],"[['GAN Setting', 'For', 'generator']]",[],text_generation,2,170
hyperparameters,The MANAGER produces the 16 - dimensional goal embedding feature vector wt using the feature map extracted by CNN .,"[('produces', (2, 3)), ('using', (12, 13)), ('extracted by', (16, 18))]","[('MANAGER', (1, 2)), ('16 - dimensional goal embedding feature vector', (4, 11)), ('feature map', (14, 16)), ('CNN', (18, 19))]","[['MANAGER', 'produces', '16 - dimensional goal embedding feature vector'], ['16 - dimensional goal embedding feature vector', 'using', 'feature map'], ['feature map', 'extracted by', 'CNN']]",[],[],[],[],[],[],"[['GAN Setting', 'has', 'MANAGER']]",[],text_generation,2,171
hyperparameters,The goal duration time c is a hyperparameter set as 4 after some preliminary experiments .,"[('is a', (5, 7)), ('set as', (8, 10)), ('after', (11, 12))]","[('goal duration time c', (1, 5)), ('hyperparameter', (7, 8)), ('4', (10, 11)), ('some preliminary experiments', (12, 15))]","[['goal duration time c', 'is a', 'hyperparameter'], ['hyperparameter', 'set as', '4'], ['4', 'after', 'some preliminary experiments']]",[],[],[],[],[],[],"[['GAN Setting', 'has', 'goal duration time c']]",[],text_generation,2,172
results,Synthetic Data Experiments,[],"[('Synthetic Data Experiments', (0, 3))]",[],[],[],"[['Results', 'has', 'Synthetic Data Experiments']]",[],[],[],[],[],text_generation,2,178
results,"( i ) In the pre-training stage , LeakGAN has already shown observable performance superiority compared to other models , which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones .","[('In', (3, 4)), ('already shown', (10, 12)), ('compared to', (15, 17))]","[('pre-training stage', (5, 7)), ('LeakGAN', (8, 9)), ('observable performance superiority', (12, 15)), ('other models', (17, 19))]","[['LeakGAN', 'already shown', 'observable performance superiority'], ['observable performance superiority', 'compared to', 'other models']]","[['pre-training stage', 'has', 'LeakGAN']]",[],[],[],"[['Synthetic Data Experiments', 'In', 'pre-training stage']]",[],[],[],text_generation,2,182
results,"( ii ) In the adversarial training stage , Leak GAN shows a better speed of convergence , and the local minimum it explores is significantly better than previous results .","[('shows', (11, 12)), ('explores', (23, 24)), ('is', (24, 25)), ('than', (27, 28))]","[('adversarial training stage', (5, 8)), ('Leak GAN', (9, 11)), ('better speed of convergence', (13, 17)), ('local minimum', (20, 22)), ('significantly better', (25, 27)), ('previous results', (28, 30))]","[['Leak GAN', 'shows', 'better speed of convergence'], ['Leak GAN', 'explores', 'local minimum'], ['local minimum', 'is', 'significantly better'], ['significantly better', 'than', 'previous results']]","[['adversarial training stage', 'has', 'Leak GAN']]",[],[],[],[],[],"[['Synthetic Data Experiments', 'In', 'adversarial training stage']]",[],text_generation,2,183
results,Long Text Generation : EMNLP2017 WMT News,[],"[('Long Text Generation : EMNLP2017 WMT News', (0, 7))]",[],[],[],"[['Results', 'has', 'Long Text Generation : EMNLP2017 WMT News']]",[],[],[],[],[],text_generation,2,185
results,"In all measured metrics , LeakGAN shows significant performance gain compared to baseline models .","[('In', (0, 1)), ('shows', (6, 7)), ('compared to', (10, 12))]","[('all measured metrics', (1, 4)), ('LeakGAN', (5, 6)), ('significant performance gain', (7, 10)), ('baseline models', (12, 14))]","[['LeakGAN', 'shows', 'significant performance gain'], ['significant performance gain', 'compared to', 'baseline models']]","[['all measured metrics', 'has', 'LeakGAN']]",[],[],[],"[['Long Text Generation : EMNLP2017 WMT News', 'In', 'all measured metrics']]",[],[],[],text_generation,2,196
results,Middle Text Generation : COCO Image Captions,[],"[('Middle Text Generation : COCO Image Captions', (0, 7))]",[],[],[],"[['Results', 'has', 'Middle Text Generation : COCO Image Captions']]",[],[],[],[],[],text_generation,2,198
results,The results of the BLEU scores on the COCO dataset indicate that Leak GAN performs significantly better than baseline models in mid-length text generation task .,"[('of', (2, 3)), ('on', (6, 7)), ('indicate', (10, 11)), ('performs', (14, 15)), ('than', (17, 18)), ('in', (20, 21))]","[('BLEU scores', (4, 6)), ('COCO dataset', (8, 10)), ('Leak GAN', (12, 14)), ('significantly better', (15, 17)), ('baseline models', (18, 20)), ('mid-length text generation task', (21, 25))]","[['BLEU scores', 'on', 'COCO dataset'], ['COCO dataset', 'indicate', 'Leak GAN'], ['Leak GAN', 'performs', 'significantly better'], ['significantly better', 'than', 'baseline models'], ['baseline models', 'in', 'mid-length text generation task']]",[],[],[],[],"[['Middle Text Generation : COCO Image Captions', 'of', 'BLEU scores']]",[],[],[],text_generation,2,208
results,Short Text Generation : Chinese Poems,[],"[('Short Text Generation : Chinese Poems', (0, 6))]",[],[],[],"[['Results', 'has', 'Short Text Generation : Chinese Poems']]",[],[],[],[],[],text_generation,2,209
results,The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .,"[('indicate', (5, 6)), ('successfully handles', (8, 10))]","[('LeakGAN', (7, 8)), ('short text generation tasks', (11, 15))]","[['LeakGAN', 'successfully handles', 'short text generation tasks']]",[],[],[],[],"[['Short Text Generation : Chinese Poems', 'indicate', 'LeakGAN']]",[],[],[],text_generation,2,214
results,Turing Test and Generated Samples,[],"[('Turing Test and Generated Samples', (0, 5))]",[],[],[],"[['Results', 'has', 'Turing Test and Generated Samples']]",[],[],[],[],"[['Turing Test and Generated Samples', 'has', 'performance']]",text_generation,2,223
results,The performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .,[],[],"[['performance', 'on', 'two datasets'], ['two datasets', 'indicates that', 'generated sentences'], ['generated sentences', 'of', 'Leak GAN'], ['Leak GAN', 'than those of', 'SeqGAN'], ['SeqGAN', 'of', 'higher global consistency'], ['SeqGAN', 'of', 'better readability']]",[],[],[],[],[],[],[],[],text_generation,2,231
research-problem,An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,[],"[('Dialogue Generation', (14, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'Dialogue Generation']]",[],[],[],[],text_generation,3,2
research-problem,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",[],"[('Automatic dialogue generation', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic dialogue generation']]",[],[],[],[],text_generation,3,12
research-problem,"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",[],"[('conversation generation', (2, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'conversation generation']]",[],[],[],[],text_generation,3,20
model,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .","[('propose', (6, 7)), ('to learn', (14, 16))]","[('novel Auto - Encoder Matching model', (8, 14)), ('utterance - level dependency', (16, 20))]","[['novel Auto - Encoder Matching model', 'to learn', 'utterance - level dependency']]",[],"[['Model', 'propose', 'novel Auto - Encoder Matching model']]",[],[],[],[],[],[],text_generation,3,24
model,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .","[('use', (6, 7)), ('to learn', (10, 12)), ('of', (15, 16)), ('in', (19, 20))]","[('two auto- encoders', (7, 10)), ('semantic representations', (13, 15)), ('inputs and responses', (16, 19)), ('unsupervised style', (21, 23))]","[['two auto- encoders', 'to learn', 'semantic representations'], ['semantic representations', 'of', 'inputs and responses'], ['inputs and responses', 'in', 'unsupervised style']]",[],"[['Model', 'use', 'two auto- encoders']]",[],[],[],[],[],[],text_generation,3,25
model,"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .","[('given', (2, 3)), ('taught to learn', (13, 16))]","[('utterance - level representations', (4, 8)), ('mapping module', (10, 12)), ('utterance - level dependency', (17, 21))]","[['mapping module', 'taught to learn', 'utterance - level dependency']]","[['utterance - level representations', 'has', 'mapping module']]","[['Model', 'given', 'utterance - level representations']]",[],[],[],[],[],[],text_generation,3,26
hyperparameters,"For dialogue generation , we set the maximum length to 15 words for each generated sentence .","[('set', (5, 6)), ('to', (9, 10)), ('for', (12, 13))]","[('maximum length', (7, 9)), ('15 words', (10, 12)), ('each generated sentence', (13, 16))]","[['maximum length', 'to', '15 words'], ['15 words', 'for', 'each generated sentence']]",[],"[['Hyperparameters', 'set', 'maximum length']]",[],[],[],[],[],[],text_generation,3,86
hyperparameters,"Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .",[],"[('hidden size', (12, 14)), ('512', (15, 16)), ('embedding size', (17, 19)), ('64', (20, 21)), ('vocabulary size', (22, 24)), ('40 K', (25, 27))]",[],"[['hidden size', 'has', '512'], ['vocabulary size', 'has', '40 K'], ['embedding size', 'has', '64']]",[],"[['Hyperparameters', 'set', 'hidden size'], ['Hyperparameters', 'set', 'vocabulary size'], ['Hyperparameters', 'set', 'embedding size']]",[],[],[],[],[],text_generation,3,87
hyperparameters,"The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .","[('updated by', (3, 5)), ('initialized by', (16, 18)), ('from', (19, 20))]","[('parameters', (1, 2)), ('Adam algorithm ( Kingma and Ba , 2014 )', (6, 15)), ('sampling', (18, 19)), ('uniform distribution ( [? 0.1 , 0.1 ] )', (21, 30))]","[['parameters', 'initialized by', 'sampling'], ['sampling', 'from', 'uniform distribution ( [? 0.1 , 0.1 ] )'], ['parameters', 'updated by', 'Adam algorithm ( Kingma and Ba , 2014 )']]",[],[],"[['Hyperparameters', 'has', 'parameters']]",[],[],[],[],[],text_generation,3,88
hyperparameters,The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?,"[('trained in', (10, 12)), ('with', (13, 14)), ('of', (17, 18))]","[('initial learning rate', (1, 4)), ('0.002', (5, 6)), ('model', (8, 9)), ('minibatches', (12, 13)), ('batch size', (15, 17)), ('256', (18, 19))]","[['model', 'trained in', 'minibatches'], ['minibatches', 'with', 'batch size'], ['batch size', 'of', '256']]","[['initial learning rate', 'has', '0.002']]",[],"[['Hyperparameters', 'has', 'initial learning rate'], ['Hyperparameters', 'has', 'model']]",[],[],[],[],[],text_generation,3,89
results,The proposed AEM model significantly outperforms the Seq2Seq model .,"[('significantly outperforms', (4, 6))]","[('proposed AEM model', (1, 4)), ('Seq2Seq model', (7, 9))]","[['proposed AEM model', 'significantly outperforms', 'Seq2Seq model']]",[],[],"[['Results', 'has', 'proposed AEM model']]",[],[],[],[],[],text_generation,3,97
results,It demonstrates the effectiveness of utterance - level dependency on improving the quality of generated text .,"[('demonstrates', (1, 2)), ('on improving', (9, 11))]","[('effectiveness of utterance - level dependency', (3, 9)), ('quality of generated text', (12, 16))]","[['effectiveness of utterance - level dependency', 'on improving', 'quality of generated text']]",[],"[['Results', 'demonstrates', 'effectiveness of utterance - level dependency']]",[],[],[],[],[],[],text_generation,3,98
results,The improvement from the AEM model to the AEM + Attention model 2 is 0.68 BLEU - 4 point .,"[('improvement from', (1, 3)), ('to', (6, 7)), ('is', (13, 14))]","[('AEM model', (4, 6)), ('AEM + Attention model', (8, 12)), ('0.68 BLEU - 4 point', (14, 19))]","[['AEM model', 'to', 'AEM + Attention model'], ['AEM + Attention model', 'is', '0.68 BLEU - 4 point']]",[],"[['Results', 'improvement from', 'AEM model']]",[],[],[],[],[],[],text_generation,3,100
results,We find that the AEM model achieves significant improvement on the diversity of generated text .,"[('find', (1, 2)), ('achieves', (6, 7)), ('on', (9, 10))]","[('AEM model', (4, 6)), ('significant improvement', (7, 9)), ('diversity of generated text', (11, 15))]","[['AEM model', 'achieves', 'significant improvement'], ['significant improvement', 'on', 'diversity of generated text']]",[],"[['Results', 'find', 'AEM model']]",[],[],[],[],[],[],text_generation,3,104
results,"Also , it should be noticed that the attention mechanism performs almost the same compared to the AEM model ( 31.2 K vs. 34.6 K in terms of Dist - 3 ) , which indicates that the utterance - level dependency and the word - level dependency are both indispensable for dialogue generation .","[('noticed that', (5, 7)), ('performs', (10, 11)), ('compared to', (14, 16))]","[('attention mechanism', (8, 10)), ('almost the same', (11, 14)), ('AEM model', (17, 19))]","[['attention mechanism', 'performs', 'almost the same'], ['almost the same', 'compared to', 'AEM model']]",[],"[['Results', 'noticed that', 'attention mechanism']]",[],[],[],[],[],[],text_generation,3,106
results,"Therefore , by combining the two dependencies together , the AEM + Attention model achieves the best results .","[('combining', (3, 4)), ('achieves', (14, 15))]","[('two dependencies', (5, 7)), ('AEM + Attention model', (10, 14)), ('best results', (16, 18))]","[['AEM + Attention model', 'achieves', 'best results']]","[['two dependencies', 'has', 'AEM + Attention model']]","[['Results', 'combining', 'two dependencies']]",[],[],[],[],[],[],text_generation,3,107
results,shows the results of human evaluation .,"[('of', (3, 4))]","[('human evaluation', (4, 6))]",[],[],"[['Results', 'of', 'human evaluation']]",[],[],[],[],[],"[['human evaluation', 'has', 'inter-annotator agreement']]",text_generation,3,120
results,The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .,"[('is', (3, 4))]","[('inter-annotator agreement', (1, 3)), ('satisfactory', (4, 5))]","[['inter-annotator agreement', 'is', 'satisfactory']]",[],[],[],[],[],[],[],[],text_generation,3,121
results,"The Pearson 's correlation coefficient is 0.69 on coherence and 0.57 on fluency , with p < 0.0001 .",[],[],"[[""Pearson 's correlation coefficient"", 'is', '0.57'], ['0.57', 'on', 'fluency'], [""Pearson 's correlation coefficient"", 'is', '0.69'], ['0.69', 'on', 'coherence']]",[],[],[],[],[],[],"[['human evaluation', 'has', ""Pearson 's correlation coefficient""]]",[],text_generation,3,122
results,"First , it is clear that the AEM model outperforms the Seq2Seq model with a large margin , which proves the effectiveness of the AEM model on generating high quality responses .","[('clear that', (4, 6)), ('outperforms', (9, 10)), ('with', (13, 14))]","[('AEM model', (7, 9)), ('Seq2Seq model', (11, 13)), ('large margin', (15, 17))]","[['AEM model', 'outperforms', 'Seq2Seq model'], ['Seq2Seq model', 'with', 'large margin']]",[],[],[],[],"[['human evaluation', 'clear that', 'AEM model']]",[],[],[],text_generation,3,123
results,"Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .","[('interesting to note that with', (4, 9)), ('decreased slightly in', (16, 19)), ('increased significantly in', (23, 26))]","[('attention mechanism', (10, 12)), ('coherence', (14, 15)), ('Seq2Seq model', (20, 22)), ('AEM model', (27, 29))]","[['coherence', 'decreased slightly in', 'Seq2Seq model'], ['Seq2Seq model', 'increased significantly in', 'AEM model']]","[['attention mechanism', 'has', 'coherence']]",[],[],[],"[['human evaluation', 'interesting to note that with', 'attention mechanism']]",[],[],[],text_generation,3,124
results,"Therefore , it is expected that the AEM + Attention model achieves the best G-score .","[('expected that', (4, 6)), ('achieves', (11, 12))]","[('AEM + Attention model', (7, 11)), ('best G-score', (13, 15))]","[['AEM + Attention model', 'achieves', 'best G-score']]",[],[],[],[],"[['human evaluation', 'expected that', 'AEM + Attention model']]",[],[],[],text_generation,3,126
research-problem,Generating Text through Adversarial Training using Skip - Thought Vectors,[],"[('Generating Text', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generating Text']]",[],[],[],[],text_generation,4,2
research-problem,Attempts have been made for utilizing GANs with word embeddings for text generation .,[],"[('text generation', (11, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'text generation']]",[],[],[],[],text_generation,4,7
research-problem,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,[],"[('natural language text generation', (9, 13))]",[],[],[],[],"[['Contribution', 'has research problem', 'natural language text generation']]",[],[],[],[],text_generation,4,11
code,This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,[],"[('https://github.com/enigmaeth/skip-thought-gan', (6, 7))]",[],[],[],[],"[['Contribution', 'Code', 'https://github.com/enigmaeth/skip-thought-gan']]",[],[],[],[],text_generation,4,14
hyperparameters,The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .,[],[],"[['Skip - Thought encoder', 'encodes', 'sentences'], ['sentences', 'with', 'length'], ['length', 'less than', '30 words'], ['sentences', 'using', '2400 GRU units'], ['2400 GRU units', 'with', 'word vector'], ['word vector', 'dimensionality of', '620'], ['620', 'to produce', '4800 - dimensional combineskip vectors']]",[],[],"[['Hyperparameters', 'has', 'Skip - Thought encoder']]",[],[],[],[],[],text_generation,4,71
hyperparameters,"The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments","[('with', (6, 7)), ('being', (11, 12)), ('found to be', (27, 30))]","[('combine - skip vectors', (1, 5)), ('first 2400 dimensions', (8, 11)), ('uni-skip model', (12, 14)), ('last 2400 bi-skip model', (16, 20)), ('best performing in the experiments', (31, 36))]","[['combine - skip vectors', 'with', 'last 2400 bi-skip model'], ['last 2400 bi-skip model', 'found to be', 'best performing in the experiments'], ['combine - skip vectors', 'with', 'first 2400 dimensions'], ['first 2400 dimensions', 'being', 'uni-skip model']]",[],[],"[['Hyperparameters', 'has', 'combine - skip vectors']]",[],[],[],[],[],text_generation,4,72
research-problem,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,[],"[('Text Modeling', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text Modeling']]",[],[],[],[],text_generation,5,2
research-problem,"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .",[],"[('generative text modeling', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'generative text modeling']]",[],[],[],[],text_generation,5,4
model,"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .","[('propose', (1, 2)), ('as', (8, 9)), ('in', (11, 12))]","[('dilated CNN', (6, 8)), ('decoder', (10, 11)), ('VAE', (12, 13))]","[['dilated CNN', 'as', 'decoder'], ['decoder', 'in', 'VAE']]",[],"[['Model', 'propose', 'dilated CNN']]",[],[],[],[],[],[],text_generation,5,33
model,"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .","[('exploit', (13, 14)), ('for', (17, 18)), ('in varying the amount of', (20, 25))]","[('dilated CNN', (15, 17)), ('flexibility', (19, 20)), ('conditioning context', (25, 27))]","[['dilated CNN', 'for', 'flexibility'], ['flexibility', 'in varying the amount of', 'conditioning context']]",[],"[['Model', 'exploit', 'dilated CNN']]",[],[],[],[],[],[],text_generation,5,34
hyperparameters,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .,[],[],"[['LSTMs and CNNs', 'as', 'decoders'], ['LSTM', 'as', 'encoder'], ['encoder', 'for', 'VAE']]",[],"[['Hyperparameters', 'explore', 'LSTMs and CNNs'], ['Hyperparameters', 'use', 'LSTM']]",[],[],[],[],[],[],text_generation,5,154
hyperparameters,"For CNNs , we explore several different configurations .","[('For', (0, 1))]","[('CNNs', (1, 2))]",[],[],"[['Hyperparameters', 'For', 'CNNs']]",[],[],[],[],[],[],text_generation,5,155
hyperparameters,"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .","[('set', (1, 2)), ('to be', (6, 8))]","[('convolution filter size', (3, 6)), ('3', (8, 9))]","[['convolution filter size', 'to be', '3']]",[],[],[],[],"[['CNNs', 'set', 'convolution filter size']]",[],[],[],text_generation,5,156
hyperparameters,We use Gumbel - softmax to sample y from q ( y|x ) .,"[('to sample', (5, 7))]","[('Gumbel - softmax', (2, 5)), ('y from q ( y|x )', (7, 13))]","[['Gumbel - softmax', 'to sample', 'y from q ( y|x )']]",[],[],"[['Hyperparameters', 'use', 'Gumbel - softmax']]",[],[],[],[],[],text_generation,5,165
hyperparameters,We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .,"[('of', (5, 6)), ('for', (8, 9)), ('set', (13, 14)), ('to be', (18, 20))]","[('vocabulary size', (3, 5)), ('20 k', (6, 8)), ('both data sets', (9, 12)), ('word embedding dimension', (15, 18)), ('512', (20, 21))]","[['word embedding dimension', 'to be', '512'], ['vocabulary size', 'of', '20 k'], ['20 k', 'for', 'both data sets']]",[],"[['Hyperparameters', 'set', 'word embedding dimension']]","[['Hyperparameters', 'use', 'vocabulary size']]",[],[],[],[],[],text_generation,5,168
hyperparameters,"The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .","[('for', (4, 5)), ('in', (6, 7)), ('is', (9, 10))]","[('number of channels', (1, 4)), ('convolutions', (5, 6)), ('CNN decoders', (7, 9)), ('512 internally', (10, 12)), ('1024 externally', (13, 15))]","[['number of channels', 'for', 'convolutions'], ['convolutions', 'is', '512 internally'], ['convolutions', 'is', '1024 externally']]","[['CNN decoders', 'has', 'number of channels']]","[['Hyperparameters', 'in', 'CNN decoders']]",[],[],[],[],[],[],text_generation,5,170
hyperparameters,"We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?","[('to optimize', (3, 5)), ('selected from', (12, 14))]","[('Adam', (2, 3)), ('all models', (5, 7)), ('learning rate', (9, 11)), ('[ 2e - 3 , 1 e - 3 , 7.5 e - 4 ]', (14, 29))]","[['Adam', 'to optimize', 'all models'], ['learning rate', 'selected from', '[ 2e - 3 , 1 e - 3 , 7.5 e - 4 ]']]",[],[],"[['Hyperparameters', 'use', 'Adam'], ['Hyperparameters', 'use', 'learning rate']]",[],[],[],[],[],text_generation,5,173
hyperparameters,"Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .","[('find', (3, 4)), ('to perform', (13, 15))]","[('learning rate', (4, 6)), ('1e - 3 and ?1 = 0.5', (6, 13)), ('best', (16, 17))]","[['1e - 3 and ?1 = 0.5', 'to perform', 'best']]","[['learning rate', 'has', '1e - 3 and ?1 = 0.5']]","[['Hyperparameters', 'find', 'learning rate']]",[],[],[],[],[],[],text_generation,5,175
hyperparameters,"We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .","[('select', (1, 2)), ('of', (4, 5)), ('from', (12, 13))]","[('dropout ratio', (2, 4)), ('LSTMs ( both encoder and decoder )', (5, 12)), ('0.3 , 0.5', (14, 17))]","[['dropout ratio', 'of', 'LSTMs ( both encoder and decoder )'], ['LSTMs ( both encoder and decoder )', 'from', '0.3 , 0.5']]",[],"[['Hyperparameters', 'select', 'dropout ratio']]",[],[],[],[],[],[],text_generation,5,176
hyperparameters,"Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .","[('for', (7, 8)), ('selected from', (17, 19))]","[('drop word', (5, 7)), ('LSTM decoder', (9, 11)), ('drop word ratio', (13, 16)), ('0 , 0.3 , 0.5 , 0.7', (20, 27))]","[['drop word', 'for', 'LSTM decoder'], ['drop word ratio', 'selected from', '0 , 0.3 , 0.5 , 0.7']]",[],[],"[['Hyperparameters', 'use', 'drop word'], ['Hyperparameters', 'use', 'drop word ratio']]",[],[],[],[],[],text_generation,5,177
hyperparameters,"For the CNN decoder , we use a dropout ratio of 0.1 at each layer .","[('use', (6, 7)), ('of', (10, 11)), ('at', (12, 13))]","[('CNN decoder', (2, 4)), ('dropout ratio', (8, 10)), ('0.1', (11, 12)), ('each layer', (13, 15))]","[['CNN decoder', 'use', 'dropout ratio'], ['dropout ratio', 'of', '0.1'], ['0.1', 'at', 'each layer']]",[],[],"[['Hyperparameters', 'For', 'CNN decoder']]",[],[],[],[],[],text_generation,5,178
hyperparameters,We use batch size of 32 and all model are trained for 40 epochs .,"[('of', (4, 5))]","[('batch size', (2, 4)), ('32', (5, 6))]","[['batch size', 'of', '32']]",[],[],"[['Hyperparameters', 'use', 'batch size']]",[],[],[],[],[],text_generation,5,180
hyperparameters,"Following , we use KL cost annealing strategy .",[],"[('KL cost annealing strategy', (4, 8))]",[],[],[],"[['Hyperparameters', 'use', 'KL cost annealing strategy']]",[],[],[],[],[],text_generation,5,182
hyperparameters,We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,"[('set', (1, 2)), ('of', (5, 6)), ('to be', (9, 11)), ('increase', (13, 14)), ('until', (16, 17))]","[('initial weight', (3, 5)), ('KL cost term', (6, 9)), ('0.01', (11, 12)), ('linearly', (15, 16)), ('given iteration T', (18, 21))]","[['initial weight', 'of', 'KL cost term'], ['KL cost term', 'increase', 'linearly'], ['linearly', 'until', 'given iteration T'], ['KL cost term', 'to be', '0.01']]",[],[],[],[],"[['KL cost annealing strategy', 'set', 'initial weight']]",[],[],[],text_generation,5,183
results,The results for language modeling are shown in .,"[('for', (2, 3))]","[('language modeling', (3, 5))]",[],[],"[['Results', 'for', 'language modeling']]",[],[],[],[],[],[],text_generation,5,186
results,"For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .",[],[],"[['VAE results', 'improve over', 'LM results'], ['LM results', 'from', '338.3'], ['338.3', 'to', '336.2'], ['LM results', 'from', '335.4'], ['335.4', 'to', '333.9'], ['LM results', 'from', '345.3'], ['345.3', 'to', '337.8']]","[['SCNN , MCNN and LCNN', 'has', 'VAE results']]",[],[],[],"[['language modeling', 'For', 'SCNN , MCNN and LCNN']]",[],[],[],text_generation,5,191
results,"When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .","[('When', (0, 1)), ('used as', (3, 5)), ('obtain', (9, 10)), ('between using', (14, 16))]","[('LCNN', (1, 2)), ('decoder', (6, 7)), ('optimal trade off', (11, 14)), ('contextual information and latent representation', (16, 21))]","[['LCNN', 'used as', 'decoder'], ['decoder', 'obtain', 'optimal trade off'], ['optimal trade off', 'between using', 'contextual information and latent representation']]",[],[],[],[],"[['language modeling', 'When', 'LCNN']]",[],[],[],text_generation,5,195
results,"LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .",[],[],"[['LCNN - VAE', 'achieves', 'NLL'], ['NLL', 'of', '333.9'], ['LCNN - VAE', 'improves over', 'LSTM - LM'], ['LSTM - LM', 'with', 'NLL'], ['NLL', 'of', '334.9']]",[],[],[],[],[],[],"[['language modeling', 'has', 'LCNN - VAE']]",[],text_generation,5,196
ablation-analysis,We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,"[('see', (2, 3)), ('of', (14, 15))]","[('SCNN - VAE - Semi', (4, 9)), ('best classification accuracy', (11, 14)), ('65.5', (15, 16))]","[['best classification accuracy', 'of', '65.5']]","[['SCNN - VAE - Semi', 'has', 'best classification accuracy']]","[['Ablation analysis', 'see', 'SCNN - VAE - Semi']]",[],[],[],[],[],[],text_generation,5,216
ablation-analysis,"On the other hand , LCNN - VAE - Semi has the best NLL result .",[],"[('LCNN - VAE - Semi', (5, 10)), ('best NLL result', (12, 15))]",[],"[['LCNN - VAE - Semi', 'has', 'best NLL result']]",[],"[['Ablation analysis', 'has', 'LCNN - VAE - Semi']]",[],[],[],[],[],text_generation,5,218
research-problem,Abstractive Text Summarization by Incorporating Reader Comments,[],"[('Abstractive Text Summarization', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Text Summarization']]",[],[],[],[],text_summarization,0,2
research-problem,"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .",[],"[('neural abstractive summarization', (1, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural abstractive summarization']]",[],[],[],[],text_summarization,0,4
research-problem,"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .",[],"[('reader - aware abstractive summary generation', (10, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'reader - aware abstractive summary generation']]",[],[],[],[],text_summarization,0,5
research-problem,"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :",[],"[('abstractive summarization', (2, 4))]",[],[],[],[],"[['Contribution', 'has research problem', 'abstractive summarization']]",[],[],[],[],text_summarization,0,6
model,"In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .","[('propose', (5, 6)), ('named', (9, 10)), ('incorporates', (19, 20)), ('to improve', (22, 24))]","[('summarization framework', (7, 9)), ('reader - aware summary generator ( RASG )', (10, 18)), ('reader comments', (20, 22)), ('summarization performance', (25, 27))]","[['summarization framework', 'incorporates', 'reader comments'], ['reader comments', 'to improve', 'summarization performance'], ['summarization framework', 'named', 'reader - aware summary generator ( RASG )']]",[],"[['Model', 'propose', 'summarization framework']]",[],[],[],[],[],[],text_summarization,0,51
model,"Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .","[('employed', (9, 10)), ('as', (10, 11))]","[('seq2seq architecture with attention mechanism', (3, 8)), ('basic summary generator', (12, 15))]","[['seq2seq architecture with attention mechanism', 'as', 'basic summary generator']]",[],"[['Model', 'employed', 'seq2seq architecture with attention mechanism']]",[],[],[],[],[],[],text_summarization,0,52
model,"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the "" reader focused aspect "" .","[('calculate alignment between', (2, 5)), ('regarded as', (18, 20)), ('representing', (22, 23))]","[('reader comments words and document words', (6, 12)), ('reader attention', (20, 22)), ('reader focused aspect', (25, 28))]","[['reader comments words and document words', 'regarded as', 'reader attention'], ['reader attention', 'representing', 'reader focused aspect']]",[],"[['Model', 'calculate alignment between', 'reader comments words and document words']]",[],[],[],[],[],[],text_summarization,0,53
model,"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .","[('treat', (3, 4)), ('as', (8, 9)), ('a.k.a.', (17, 18))]","[('decoder attention weights', (5, 8)), ('focused aspect of the generated summary', (10, 16)), ('decoder focused aspect', (20, 23))]","[['decoder attention weights', 'as', 'focused aspect of the generated summary'], ['focused aspect of the generated summary', 'a.k.a.', 'decoder focused aspect']]",[],"[['Model', 'treat', 'decoder attention weights']]",[],[],[],[],[],[],text_summarization,0,54
model,"After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .","[('After', (0, 1)), ('designed', (8, 9)), ('to measure', (9, 11)), ('between', (13, 14))]","[('each decoding step', (1, 4)), ('supervisor', (6, 7)), ('distance', (12, 13)), ('reader focused aspect and the decoder focused aspect', (15, 23))]","[['each decoding step', 'designed', 'supervisor'], ['supervisor', 'to measure', 'distance'], ['distance', 'between', 'reader focused aspect and the decoder focused aspect']]",[],"[['Model', 'After', 'each decoding step']]",[],[],[],[],[],[],text_summarization,0,55
model,The training of our framework RASG is conducted in an adversarial way .,"[('training of', (1, 3)), ('conducted in', (7, 9))]","[('framework RASG', (4, 6)), ('adversarial way', (10, 12))]","[['framework RASG', 'conducted in', 'adversarial way']]",[],"[['Model', 'training of', 'framework RASG']]",[],[],[],[],[],[],text_summarization,0,57
baselines,( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .,[],"[('S2S', (3, 4)), ('Sequence - to - sequence framework', (5, 11))]",[],"[['S2S', 'name', 'Sequence - to - sequence framework']]",[],"[['Baselines', 'has', 'S2S']]",[],[],[],[],[],text_summarization,0,224
baselines,"( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .","[('add', (7, 8)), ('in', (17, 18))]","[('S2SR', (3, 4)), ('reader attention on attention distribution ? t', (9, 16)), ('decoding step', (19, 21))]","[['S2SR', 'add', 'reader attention on attention distribution ? t'], ['reader attention on attention distribution ? t', 'in', 'decoding step']]",[],[],"[['Baselines', 'has', 'S2SR']]",[],[],[],[],[],text_summarization,0,225
baselines,"( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .","[('propose to use', (5, 8))]","[('CGU', (3, 4)), ('convolutional gated unit', (9, 12))]","[['CGU', 'propose to use', 'convolutional gated unit']]",[],[],"[['Baselines', 'has', 'CGU']]",[],[],[],[],[],text_summarization,0,226
baselines,"( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .","[('selects', (13, 14))]","[('LEAD1', (3, 4)), ('first sentence of document as the summary', (15, 22))]","[['LEAD1', 'selects', 'first sentence of document as the summary']]",[],[],"[['Baselines', 'has', 'LEAD1']]",[],[],[],[],[],text_summarization,0,227
baselines,"( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .","[('propose to build', (5, 8)), ('add', (12, 13)), ('as', (15, 16)), ('use', (19, 20)), ('to represent', (21, 23))]","[('TextRank', (3, 4)), ('graph', (9, 10)), ('each sentence', (13, 15)), ('vertex', (17, 18)), ('link', (20, 21)), ('semantic similarity', (23, 25))]","[['TextRank', 'add', 'each sentence'], ['each sentence', 'as', 'vertex'], ['TextRank', 'use', 'link'], ['link', 'to represent', 'semantic similarity'], ['TextRank', 'propose to build', 'graph']]",[],[],"[['Baselines', 'has', 'TextRank']]",[],[],[],[],[],text_summarization,0,228
experimental-setup,We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .,"[('implement', (1, 2)), ('in', (4, 5)), ('on', (7, 8))]","[('our experiments', (2, 4)), ('TensorFlow', (5, 6)), ('NVIDIA P40 GPU', (9, 12))]","[['our experiments', 'in', 'TensorFlow'], ['TensorFlow', 'on', 'NVIDIA P40 GPU']]",[],"[['Experimental setup', 'implement', 'our experiments']]",[],[],[],[],[],[],text_summarization,0,231
experimental-setup,The word embedding dimension is set to 256 and the number of hidden units is 512 .,"[('set to', (5, 7))]","[('word embedding dimension', (1, 4)), ('256', (7, 8)), ('number of hidden units', (10, 14)), ('512', (15, 16))]","[['word embedding dimension', 'set to', '256']]","[['number of hidden units', 'has', '512']]",[],"[['Experimental setup', 'has', 'word embedding dimension'], ['Experimental setup', 'has', 'number of hidden units']]",[],[],[],[],[],text_summarization,0,232
experimental-setup,We use Adagrad optimizer as our optimizing algorithm .,"[('use', (1, 2)), ('as', (4, 5))]","[('Adagrad optimizer', (2, 4)), ('optimizing algorithm', (6, 8))]","[['Adagrad optimizer', 'as', 'optimizing algorithm']]",[],"[['Experimental setup', 'use', 'Adagrad optimizer']]",[],[],[],[],[],[],text_summarization,0,234
experimental-setup,We employ beam search with beam size 5 to generate more fluency summary sentence .,"[('employ', (1, 2)), ('with', (4, 5)), ('to generate', (8, 10))]","[('beam search', (2, 4)), ('beam size 5', (5, 8)), ('more fluency summary sentence', (10, 14))]","[['beam search', 'with', 'beam size 5'], ['beam size 5', 'to generate', 'more fluency summary sentence']]",[],"[['Experimental setup', 'employ', 'beam search']]",[],[],[],[],[],[],text_summarization,0,235
results,"We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .","[('see that', (1, 3)), ('achieves', (4, 5)), ('over', (15, 16)), ('in terms of', (26, 29))]","[('RASG', (3, 4)), ('11.0 % , 9.1 % and 6.6 % increment', (6, 15)), ('state - of - the - art method CGU', (17, 26)), ('ROUGE - 1 , ROUGE - 2 and ROUGE - L', (29, 40))]","[['RASG', 'achieves', '11.0 % , 9.1 % and 6.6 % increment'], ['11.0 % , 9.1 % and 6.6 % increment', 'over', 'state - of - the - art method CGU'], ['state - of - the - art method CGU', 'in terms of', 'ROUGE - 1 , ROUGE - 2 and ROUGE - L']]",[],"[['Results', 'see that', 'RASG']]",[],[],[],[],[],[],text_summarization,0,240
results,It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .,"[('worth noticing', (2, 4)), ('achieves better performance than', (9, 13))]","[('baseline model S2SR', (6, 9)), ('S2S', (13, 14))]","[['baseline model S2SR', 'achieves better performance than', 'S2S']]",[],"[['Results', 'worth noticing', 'baseline model S2SR']]",[],[],[],[],[],[],text_summarization,0,241
ablation-analysis,The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .,[],[],"[['discriminator', 'provides', 'scalar training signal L g c'], ['scalar training signal L g c', 'for', 'generator training'], ['discriminator', 'provides', 'feature vector F ( m t )'], ['feature vector F ( m t )', 'for', 'goal tracker']]",[],[],"[['Ablation analysis', 'has', 'discriminator']]",[],[],[],[],[],text_summarization,0,247
ablation-analysis,"Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .","[('increment of', (5, 7)), ('from', (9, 10)), ('in terms of', (21, 24))]","[('17.51 %', (7, 9)), ('RASG w / o GTD to RASG w / o GT', (10, 21)), ('ROUGE - L', (24, 27))]","[['17.51 %', 'from', 'RASG w / o GTD to RASG w / o GT'], ['RASG w / o GTD to RASG w / o GT', 'in terms of', 'ROUGE - L']]",[],[],[],[],"[['discriminator', 'increment of', '17.51 %']]",[],[],[],text_summarization,0,248
ablation-analysis,"As for the effectiveness of goal tracker , compared with RASG and RASG w / o GT , RASG w/ o GTD offers a decrease of 45. 23 % and 17.88 % in terms of ROUGE - 1 , respectively .","[('compared with', (8, 10)), ('offers', (22, 23)), ('decrease of', (24, 26)), ('in terms of', (32, 35))]","[('goal tracker', (5, 7)), ('RASG and RASG w / o GT', (10, 17)), ('RASG w/ o GTD', (18, 22)), ('45. 23 % and 17.88 %', (26, 32)), ('ROUGE - 1', (35, 38))]","[['goal tracker', 'compared with', 'RASG and RASG w / o GT'], ['RASG and RASG w / o GT', 'offers', 'RASG w/ o GTD'], ['RASG w/ o GTD', 'decrease of', '45. 23 % and 17.88 %'], ['45. 23 % and 17.88 %', 'in terms of', 'ROUGE - 1']]",[],[],"[['Ablation analysis', 'has', 'goal tracker']]",[],[],[],[],[],text_summarization,0,249
ablation-analysis,"Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .","[('offers a decrease of', (5, 9)), ('compared with', (13, 15)), ('in terms of', (16, 19))]","[('RASG w/o DM', (2, 5)), ('10 . 22 %', (9, 13)), ('RASG', (15, 16)), ('ROUGE - L', (19, 22))]","[['RASG w/o DM', 'offers a decrease of', '10 . 22 %'], ['10 . 22 %', 'compared with', 'RASG'], ['RASG', 'in terms of', 'ROUGE - L']]",[],[],"[['Ablation analysis', 'has', 'RASG w/o DM']]",[],[],[],[],[],text_summarization,0,252
research-problem,Mixture Content Selection for Diverse Sequence Generation,[],"[('Diverse Sequence Generation', (4, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Diverse Sequence Generation']]",[],[],[],[],text_summarization,1,2
research-problem,Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .,[],"[('Generating diverse sequences', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generating diverse sequences']]",[],[],[],[],text_summarization,1,4
research-problem,Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .,[],"[('Generating target sequences given a source sequence', (0, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Generating target sequences given a source sequence']]",[],[],[],[],text_summarization,1,12
research-problem,"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .",[],"[('sequence generation', (8, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'sequence generation']]",[],[],[],[],text_summarization,1,30
model,"In this paper , we present a method for diverse generation that separates diversification and generation stages .","[('separates', (12, 13))]","[('diversification and generation stages', (13, 17))]",[],[],"[['Model', 'separates', 'diversification and generation stages']]",[],[],[],[],[],[],text_summarization,1,34
model,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .","[('leverages', (3, 4)), ('map', (7, 8))]","[('diversification stage', (1, 3)), ('content selection', (4, 6)), ('source to multiple sequences', (9, 13))]","[['diversification stage', 'leverages', 'content selection'], ['content selection', 'map', 'source to multiple sequences']]",[],[],"[['Model', 'has', 'diversification stage']]",[],[],[],[],[],text_summarization,1,35
model,The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,"[('uses', (3, 4)), ('generate', (11, 12))]","[('generation stage', (1, 3)), ('standard encoder - decoder model', (5, 10)), ('target sequence given each selected content from the source', (13, 22))]","[['generation stage', 'uses', 'standard encoder - decoder model'], ['standard encoder - decoder model', 'generate', 'target sequence given each selected content from the source']]",[],[],"[['Model', 'has', 'generation stage']]",[],[],[],[],[],text_summarization,1,36
model,We present a generic module called SELECTOR that is specialized for diversification .,"[('present', (1, 2)), ('called', (5, 6)), ('specialized for', (9, 11))]","[('generic module', (3, 5)), ('SELECTOR', (6, 7)), ('diversification', (11, 12))]","[['generic module', 'called', 'SELECTOR'], ['SELECTOR', 'specialized for', 'diversification']]",[],"[['Model', 'present', 'generic module']]",[],[],[],[],[],[],text_summarization,1,37
model,This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .,"[('used as', (4, 6)), ('to', (12, 13)), ('for', (19, 20))]","[('plug - and - play', (7, 12)), ('arbitrary encoder - decoder model', (14, 19)), ('generation without architecture change', (20, 24))]","[['plug - and - play', 'to', 'arbitrary encoder - decoder model'], ['arbitrary encoder - decoder model', 'for', 'generation without architecture change']]",[],[],[],[],"[['generic module', 'used as', 'plug - and - play']]",[],[],[],text_summarization,1,38
baselines,Beam Search,[],"[('Beam Search', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Beam Search']]",[],[],[],[],[],text_summarization,1,161
baselines,This baseline keeps K hypotheses with highest log-probability scores at each decoding step .,"[('keeps', (2, 3)), ('with', (5, 6)), ('at', (9, 10))]","[('K hypotheses', (3, 5)), ('highest log-probability scores', (6, 9)), ('each decoding step', (10, 13))]","[['K hypotheses', 'with', 'highest log-probability scores'], ['highest log-probability scores', 'at', 'each decoding step']]",[],[],[],[],"[['Beam Search', 'keeps', 'K hypotheses']]",[],[],[],text_summarization,1,162
baselines,Truncated Sampling,[],"[('Truncated Sampling', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Truncated Sampling']]",[],[],[],[],[],text_summarization,1,163
baselines,This baseline randomly samples words from top - 10 candidates of the distribution at the decoding step .,"[('randomly samples', (2, 4)), ('from', (5, 6)), ('of', (10, 11)), ('at', (13, 14))]","[('words', (4, 5)), ('top - 10 candidates', (6, 10)), ('distribution', (12, 13)), ('decoding step', (15, 17))]","[['words', 'from', 'top - 10 candidates'], ['top - 10 candidates', 'of', 'distribution'], ['distribution', 'at', 'decoding step']]",[],[],[],[],"[['Truncated Sampling', 'randomly samples', 'words']]",[],[],[],text_summarization,1,164
baselines,Mixture Decoder,[],"[('Mixture Decoder', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Mixture Decoder']]",[],[],[],[],[],text_summarization,1,165
baselines,This baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .,"[('constructs', (2, 3)), ('with', (10, 11)), ('conducts', (21, 22))]","[('hard - MoE of K decoders', (4, 10)), ('uniform mixing coefficient', (11, 14)), ('parallel greedy decoding', (22, 25))]","[['hard - MoE of K decoders', 'with', 'uniform mixing coefficient']]",[],[],[],[],"[['Mixture Decoder', 'conducts', 'parallel greedy decoding'], ['Mixture Decoder', 'constructs', 'hard - MoE of K decoders']]",[],[],[],text_summarization,1,166
baselines,Mixture Selector ( Ours ),[],"[('Mixture Selector ( Ours )', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Mixture Selector ( Ours )']]",[],[],[],[],[],text_summarization,1,168
baselines,We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence .,"[('construct', (1, 2)), ('with', (9, 10)), ('infers', (14, 15))]","[('hard - MoE of K SELECTORs', (3, 9)), ('uniform mixing coefficient', (10, 13)), ('K different focus from source sequence', (15, 21))]","[['hard - MoE of K SELECTORs', 'with', 'uniform mixing coefficient'], ['uniform mixing coefficient', 'infers', 'K different focus from source sequence']]",[],[],[],[],"[['Mixture Selector ( Ours )', 'construct', 'hard - MoE of K SELECTORs']]",[],[],[],text_summarization,1,169
experimental-setup,"For all experiments , we tie the weights of the encoder embedding , the decoder embedding , and the decoder output layers .","[('tie', (5, 6)), ('of', (8, 9))]","[('weights', (7, 8)), ('encoder embedding', (10, 12)), ('decoder embedding', (14, 16)), ('decoder output layers', (19, 22))]","[['weights', 'of', 'encoder embedding'], ['weights', 'of', 'decoder embedding'], ['weights', 'of', 'decoder output layers']]",[],"[['Experimental setup', 'tie', 'weights']]",[],[],[],[],[],[],text_summarization,1,201
experimental-setup,We train up to 20 epochs and select the checkpoint with the best oracle metric .,"[('train', (1, 2)), ('select', (7, 8)), ('with', (10, 11))]","[('up to 20 epochs', (2, 6)), ('checkpoint', (9, 10)), ('best oracle metric', (12, 15))]","[['up to 20 epochs', 'select', 'checkpoint'], ['checkpoint', 'with', 'best oracle metric']]",[],"[['Experimental setup', 'train', 'up to 20 epochs']]",[],[],[],[],[],[],text_summarization,1,203
,"We use Adam ( Kingma and Ba , 2015 ) optimizer with learning rate 0.001 and momentum parmeters ? 1 = 0.9 and ? 2 = 0.999 . ",[],[],[],[],[],[],[],[],[],[],[],text_summarization,1,204
experimental-setup,Minibatch size is 64 and 32 for question generation and abstractive summarization .,"[('is', (2, 3)), ('for', (6, 7))]","[('Minibatch size', (0, 2)), ('64 and 32', (3, 6)), ('question generation and abstractive summarization', (7, 12))]","[['Minibatch size', 'is', '64 and 32'], ['64 and 32', 'for', 'question generation and abstractive summarization']]",[],[],"[['Experimental setup', 'has', 'Minibatch size']]",[],[],[],[],[],text_summarization,1,205
experimental-setup,"All models are implemented in PyTorch and trained on single Tesla P40 GPU , based on NAVER Smart Machine Learning ( NSML ) platform .","[('implemented in', (3, 5)), ('trained on', (7, 9)), ('based on', (14, 16))]","[('PyTorch', (5, 6)), ('single Tesla P40 GPU', (9, 13)), ('NAVER Smart Machine Learning ( NSML ) platform', (16, 24))]","[['single Tesla P40 GPU', 'based on', 'NAVER Smart Machine Learning ( NSML ) platform']]",[],"[['Experimental setup', 'implemented in', 'PyTorch'], ['Experimental setup', 'trained on', 'single Tesla P40 GPU']]",[],[],[],[],[],[],text_summarization,1,206
results,Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .,"[('in', (13, 14))]","[('Diversity vs. Accuracy Trade - off', (0, 6))]",[],[],[],"[['Results', 'has', 'Diversity vs. Accuracy Trade - off']]",[],[],[],[],[],text_summarization,1,208
results,The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .,"[('show', (2, 3)), ('outperforms', (8, 9)), ('achieves', (19, 20)), ('between', (25, 26))]","[('mixture SELECTOR method', (5, 8)), ('all baselines', (9, 11)), ('Top - 1 and oracle metrics', (12, 18)), ('best trade - off', (21, 25)), ('diversity and accuracy', (26, 29))]","[['mixture SELECTOR method', 'achieves', 'best trade - off'], ['best trade - off', 'between', 'diversity and accuracy'], ['mixture SELECTOR method', 'outperforms', 'all baselines']]","[['all baselines', 'in', 'Top - 1 and oracle metrics']]",[],[],[],"[['Diversity vs. Accuracy Trade - off', 'show', 'mixture SELECTOR method']]",[],[],[],text_summarization,1,209
results,"Notably , our method scores state - of - the - art BLEU - 4 in question generation on SQuAD and ROUGE comparable to state - of - the - art methods in abstractive summarization in CNN - DM ( See also for state - of - the - art results in CNN - DM ) .",[],[],"[['our method', 'scores', 'state - of - the - art BLEU'], ['state - of - the - art BLEU', 'in', 'question generation'], ['question generation', 'on', 'SQuAD and ROUGE'], ['SQuAD and ROUGE', 'in', 'abstractive summarization in CNN - DM']]",[],[],[],[],[],[],"[['Diversity vs. Accuracy Trade - off', 'has', 'our method']]",[],text_summarization,1,213
results,Diversity vs. Number of Mixtures,[],"[('Diversity vs. Number of Mixtures', (0, 5))]",[],[],[],"[['Results', 'has', 'Diversity vs. Number of Mixtures']]",[],[],[],[],[],text_summarization,1,214
results,Here we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .,"[('compare', (2, 3)), ('in', (9, 10))]","[('effect of number of mixtures', (4, 9)), ('SELECTOR and Mixture Decoder', (11, 15))]","[['effect of number of mixtures', 'in', 'SELECTOR and Mixture Decoder']]",[],[],[],[],"[['Diversity vs. Number of Mixtures', 'compare', 'effect of number of mixtures']]",[],[],[],text_summarization,1,215
results,show that pairwise similarity increases ( diversity ?) when the number of mixtures increases for Mixture Decoder .,"[('show', (0, 1)), ('when', (8, 9)), ('for', (14, 15))]","[('pairwise similarity increases ( diversity ?)', (2, 8)), ('number of mixtures increases', (10, 14)), ('Mixture Decoder', (15, 17))]","[['Mixture Decoder', 'show', 'pairwise similarity increases ( diversity ?)'], ['pairwise similarity increases ( diversity ?)', 'when', 'number of mixtures increases']]",[],[],[],[],"[['Diversity vs. Number of Mixtures', 'for', 'Mixture Decoder']]",[],[],[],text_summarization,1,216
research-problem,Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation,[],"[('Multi - Task Summarization', (4, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Multi - Task Summarization']]",[],[],[],[],text_summarization,10,2
research-problem,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .",[],"[('abstractive summarization', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'abstractive summarization']]",[],[],[],[],text_summarization,10,5
research-problem,"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .",[],"[('abstractive text summarization', (6, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'abstractive text summarization']]",[],[],[],[],text_summarization,10,17
model,"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .","[('present', (4, 5)), ('based on', (9, 11)), ('empirically show', (19, 21)), ('to share', (26, 28)), ('between', (32, 33)), ('keeping', (39, 40))]","[('novel multi-task learning architectures', (5, 9)), ('multi-layered encoder and decoder models', (11, 16)), ('substantially better', (24, 26)), ('higherlevel semantic layers', (29, 32)), ('three aforementioned tasks', (34, 37)), ('lower - level ( lexico- syntactic ) layers unshared', (41, 50))]","[['novel multi-task learning architectures', 'empirically show', 'substantially better'], ['substantially better', 'to share', 'higherlevel semantic layers'], ['higherlevel semantic layers', 'keeping', 'lower - level ( lexico- syntactic ) layers unshared'], ['higherlevel semantic layers', 'between', 'three aforementioned tasks'], ['novel multi-task learning architectures', 'based on', 'multi-layered encoder and decoder models']]",[],"[['Model', 'present', 'novel multi-task learning architectures']]",[],[],[],[],[],[],text_summarization,10,22
results,Pointer + Coverage Baseline,[],"[('Pointer + Coverage Baseline', (0, 4))]",[],[],[],"[['Results', 'has', 'Pointer + Coverage Baseline']]",[],[],[],[],[],text_summarization,10,144
results,"4 On Gigaword dataset , our baseline model ( with pointer only , since coverage not needed for this single - sentence summarization task ) performs better than all previous works , as shown in .","[('On', (1, 2)), ('performs better than', (25, 28))]","[('Gigaword dataset', (2, 4)), ('baseline model', (6, 8)), ('all previous works', (28, 31))]","[['baseline model', 'performs better than', 'all previous works']]","[['Gigaword dataset', 'has', 'baseline model']]",[],[],[],"[['Pointer + Coverage Baseline', 'On', 'Gigaword dataset']]",[],[],[],text_summarization,10,149
results,Multi - Task with Entailment Generation,[],"[('Multi - Task with Entailment Generation', (0, 6))]",[],[],[],"[['Results', 'has', 'Multi - Task with Entailment Generation']]",[],[],[],[],[],text_summarization,10,150
results,"4 . shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN / DailyMail ( p < 0.01 in ROUGE - 1 / ROUGE - L / METEOR and p < 0.05 in ROUGE - 2 ) and Gigaword ( p < 0.01 on all metrics ) datasets , showing that entailment generation task is inducing useful inference skills to the summarization task ( also see analysis examples in Sec. 7 ) .","[('shows', (2, 3)), ('better than', (8, 10))]","[('multi-task setting', (5, 7)), ('our strong baseline models', (10, 14))]","[['multi-task setting', 'better than', 'our strong baseline models']]",[],[],[],[],"[['Multi - Task with Entailment Generation', 'shows', 'multi-task setting']]",[],[],[],text_summarization,10,152
results,"For multi-task learning with question generation , the improvements are statistically significant in ROUGE - 1 ( p < 0.01 ) , ROUGE - L ( p < 0.05 ) , and METEOR ( p < 0.01 ) for CNN / DailyMail and in all metrics ( p < 0.01 ) for Gigaword , compared to the respective baseline models .",[],[],"[['multi-task learning with question generation', 'improvements', 'statistically significant'], ['statistically significant', 'for', 'CNN / DailyMail'], ['CNN / DailyMail', 'in', 'ROUGE - 1 ( p < 0.01 )'], ['CNN / DailyMail', 'in', 'ROUGE - L ( p < 0.05 )'], ['CNN / DailyMail', 'in', 'METEOR ( p < 0.01 )'], ['statistically significant', 'for', 'Gigaword'], ['Gigaword', 'in', 'all metrics ( p < 0.01 )']]",[],"[['Results', 'For', 'multi-task learning with question generation']]",[],[],[],[],[],[],text_summarization,10,154
ablation-analysis,Soft - sharing vs. Hard - sharing,[],"[('Soft - sharing vs. Hard - sharing', (0, 7))]",[],[],[],"[['Ablation analysis', 'has', 'Soft - sharing vs. Hard - sharing']]",[],[],[],[],[],text_summarization,10,192
ablation-analysis,"As described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .","[('choose', (7, 8)), ('over', (11, 12)), ('because of', (15, 17))]","[('soft - sharing', (8, 11)), ('hard - sharing', (12, 15)), ('more expressive parameter sharing', (18, 22))]","[['soft - sharing', 'over', 'hard - sharing'], ['soft - sharing', 'because of', 'more expressive parameter sharing']]",[],[],[],[],"[['Soft - sharing vs. Hard - sharing', 'choose', 'soft - sharing']]",[],[],[],text_summarization,10,193
ablation-analysis,Empirical results in 8 prove that soft - sharing method is statistically significantly better than hard - sharing with p < 0.001 in all metrics .,"[('in', (2, 3)), ('prove', (4, 5)), ('is statistically significantly better than', (10, 15)), ('with', (18, 19))]","[('soft - sharing method', (6, 10)), ('hard - sharing', (15, 18)), ('p < 0.001', (19, 22)), ('all metrics', (23, 25))]","[['soft - sharing method', 'is statistically significantly better than', 'hard - sharing'], ['hard - sharing', 'with', 'p < 0.001'], ['p < 0.001', 'in', 'all metrics']]",[],[],[],[],"[['Soft - sharing vs. Hard - sharing', 'prove', 'soft - sharing method']]",[],[],[],text_summarization,10,194
ablation-analysis,Quantitative Improvements in Entailment,[],"[('Quantitative Improvements in Entailment', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Quantitative Improvements in Entailment']]",[],[],[],[],[],text_summarization,10,203
ablation-analysis,We found that our 2 - way MTL model with entailment generation reduces this extraneous count by 17.2 % w.r.t. the baseline .,"[('found', (1, 2)), ('with', (9, 10)), ('reduces', (12, 13)), ('by', (16, 17))]","[('our 2 - way MTL model', (3, 9)), ('entailment generation', (10, 12)), ('extraneous count', (14, 16)), ('17.2 % w.r.t. the baseline', (17, 22))]","[['our 2 - way MTL model', 'with', 'entailment generation'], ['entailment generation', 'reduces', 'extraneous count'], ['extraneous count', 'by', '17.2 % w.r.t. the baseline']]",[],[],[],[],"[['Quantitative Improvements in Entailment', 'found', 'our 2 - way MTL model']]",[],[],[],text_summarization,10,208
ablation-analysis,Quantitative Improvements in Saliency Detection,[],"[('Quantitative Improvements in Saliency Detection', (0, 5))]",[],[],[],"[['Ablation analysis', 'has', 'Quantitative Improvements in Saliency Detection']]",[],[],[],[],[],text_summarization,10,210
ablation-analysis,"The results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .","[('results are', (1, 3)), ('versus', (22, 23)), ('is', (25, 26))]","[('2 - way - QG MTL model ( with question generation )', (10, 22)), ('baseline improvement', (23, 25)), ('stat. significant ( p < 0.01 )', (26, 33))]","[['2 - way - QG MTL model ( with question generation )', 'versus', 'baseline improvement'], ['baseline improvement', 'is', 'stat. significant ( p < 0.01 )']]",[],[],[],[],"[['Quantitative Improvements in Saliency Detection', 'results are', '2 - way - QG MTL model ( with question generation )']]",[],[],[],text_summarization,10,213
ablation-analysis,"Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .",[],"[('Qualitative Examples on Entailment and Saliency Improvements', (0, 7)), ('summaries', (12, 13))]",[],[],[],"[['Ablation analysis', 'has', 'Qualitative Examples on Entailment and Saliency Improvements']]",[],[],[],[],"[['Qualitative Examples on Entailment and Saliency Improvements', 'has', '3 - way multi-task model']]",text_summarization,10,215
ablation-analysis,"Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .","[('generates', (8, 9)), ('are both better at', (11, 15))]","[('3 - way multi-task model', (3, 8)), ('logical entailment', (15, 17)), ('contain more salient information', (18, 22))]",[],[],[],[],[],"[['summaries', 'are both better at', 'logical entailment'], ['summaries', 'are both better at', 'contain more salient information']]","[['3 - way multi-task model', 'generates', 'summaries']]",[],[],text_summarization,10,218
research-problem,Global Encoding for Abstractive Summarization,[],"[('Abstractive Summarization', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Summarization']]",[],[],[],[],text_summarization,11,2
research-problem,"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .",[],"[('neural abstractive summarization', (12, 15))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural abstractive summarization']]",[],[],[],[],text_summarization,11,10
model,"To tackle this problem , we propose a model of global encoding for abstractive summarization .","[('of', (9, 10)), ('for', (12, 13))]","[('global encoding', (10, 12)), ('abstractive summarization', (13, 15))]","[['global encoding', 'for', 'abstractive summarization']]",[],"[['Model', 'of', 'global encoding']]",[],[],[],[],[],[],text_summarization,11,23
model,We set a convolutional gated unit to perform global encoding on the source context .,"[('set', (1, 2)), ('to perform', (6, 8)), ('on', (10, 11))]","[('convolutional gated unit', (3, 6)), ('global encoding', (8, 10)), ('source context', (12, 14))]","[['convolutional gated unit', 'to perform', 'global encoding'], ['global encoding', 'on', 'source context']]",[],"[['Model', 'set', 'convolutional gated unit']]",[],[],[],[],[],[],text_summarization,11,24
model,"The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .",[],[],"[['gate', 'based on', 'convolutional neural network ( CNN )'], ['gate', 'filters', 'each encoder output'], ['each encoder output', 'based on', 'global context'], ['global context', 'due to', 'parameter sharing']]",[],[],"[['Model', 'has', 'gate']]",[],[],[],[],[],text_summarization,11,25
experimental-setup,We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU .,"[('implement', (1, 2)), ('in', (4, 5))]","[('our experiments', (2, 4)), ('PyTorch on an NVIDIA 1080 Ti GPU', (5, 12))]","[['our experiments', 'in', 'PyTorch on an NVIDIA 1080 Ti GPU']]",[],"[['Experimental setup', 'implement', 'our experiments']]",[],[],[],[],[],[],text_summarization,11,88
experimental-setup,The word embedding dimension and the number of hidden units are both 512 .,"[('word embedding dimension and the number of hidden units', (1, 10))]","[('512', (12, 13))]",[],[],"[['Experimental setup', 'word embedding dimension and the number of hidden units', '512']]",[],[],[],[],[],[],text_summarization,11,89
experimental-setup,"In both experiments , the batch size is set to 64 .","[('set to', (8, 10))]","[('batch size', (5, 7)), ('64', (10, 11))]","[['batch size', 'set to', '64']]",[],[],"[['Experimental setup', 'has', 'batch size']]",[],[],[],[],[],text_summarization,11,90
experimental-setup,"We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 .","[('use', (1, 2)), ('with', (11, 12))]","[('Adam optimizer ( Kingma and Ba , 2014 )', (2, 11)), ('default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8', (13, 33))]","[['Adam optimizer ( Kingma and Ba , 2014 )', 'with', 'default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8']]",[],"[['Experimental setup', 'use', 'Adam optimizer ( Kingma and Ba , 2014 )']]",[],[],[],[],[],[],text_summarization,11,91
experimental-setup,The learning rate is halved every epoch .,"[('halved', (4, 5))]","[('learning rate', (1, 3)), ('every epoch', (5, 7))]","[['learning rate', 'halved', 'every epoch']]",[],[],"[['Experimental setup', 'has', 'learning rate']]",[],[],[],[],[],text_summarization,11,92
experimental-setup,"Gradient clipping is applied with range [ - 10 , 10 ] .","[('applied with', (3, 5))]","[('Gradient clipping', (0, 2)), ('range [ - 10 , 10 ]', (5, 12))]","[['Gradient clipping', 'applied with', 'range [ - 10 , 10 ]']]",[],[],"[['Experimental setup', 'has', 'Gradient clipping']]",[],[],[],[],[],text_summarization,11,93
baselines,Baselines for LCSTS are introduced in the following .,"[('for', (1, 2)), ('are', (3, 4))]","[('LCSTS', (2, 3))]",[],[],"[['Baselines', 'for', 'LCSTS']]",[],[],[],[],[],"[['LCSTS', 'has', 'RNN and RNN - context']]",text_summarization,11,100
baselines,"RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .","[('without and with', (11, 14))]","[('RNN and RNN - context', (0, 5)), ('RNNbased seq2seq models', (7, 10)), ('attention mechanism', (14, 16))]","[['RNNbased seq2seq models', 'without and with', 'attention mechanism']]","[['RNN and RNN - context', 'are', 'RNNbased seq2seq models']]",[],[],[],[],[],[],[],text_summarization,11,101
baselines,Copy - Net is the attention - based seq2seq model with the copy mechanism .,"[('is', (3, 4)), ('with', (10, 11))]","[('Copy - Net', (0, 3)), ('attention - based seq2seq model', (5, 10)), ('copy mechanism', (12, 14))]","[['Copy - Net', 'is', 'attention - based seq2seq model'], ['attention - based seq2seq model', 'with', 'copy mechanism']]",[],[],[],[],[],[],"[['LCSTS', 'has', 'Copy - Net']]",[],text_summarization,11,102
baselines,SRB is a model that improves semantic relevance between source text and summary .,"[('improves', (5, 6)), ('between', (8, 9))]","[('SRB', (0, 1)), ('semantic relevance', (6, 8)), ('source text and summary', (9, 13))]","[['SRB', 'improves', 'semantic relevance'], ['semantic relevance', 'between', 'source text and summary']]",[],[],[],[],[],[],"[['LCSTS', 'has', 'SRB']]",[],text_summarization,11,103
baselines,DRGD is the conventional seq2seq with a deep recurrent generative decoder .,"[('is', (1, 2)), ('with', (5, 6))]","[('DRGD', (0, 1)), ('conventional seq2seq', (3, 5)), ('deep recurrent generative decoder', (7, 11))]","[['DRGD', 'is', 'conventional seq2seq'], ['conventional seq2seq', 'with', 'deep recurrent generative decoder']]",[],[],[],[],[],[],"[['LCSTS', 'has', 'DRGD']]",[],text_summarization,11,104
baselines,"As to the baselines for Gigaword , ABS and ABS + are the models with local attention and handcrafted features .","[('are', (11, 12)), ('with', (14, 15))]","[('Gigaword', (5, 6)), ('ABS and ABS +', (7, 11)), ('models', (13, 14)), ('local attention and handcrafted features', (15, 20))]","[['ABS and ABS +', 'are', 'models'], ['models', 'with', 'local attention and handcrafted features']]","[['Gigaword', 'has', 'ABS and ABS +']]",[],"[['Baselines', 'for', 'Gigaword']]",[],[],[],[],[],text_summarization,11,105
baselines,Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size .,"[('is', (1, 2)), ('with', (7, 8)), ('to control', (11, 13))]","[('Feats', (0, 1)), ('fully RNN seq2seq model', (3, 7)), ('some specific methods', (8, 11)), ('vocabulary size', (14, 16))]","[['Feats', 'is', 'fully RNN seq2seq model'], ['fully RNN seq2seq model', 'with', 'some specific methods'], ['some specific methods', 'to control', 'vocabulary size']]",[],[],[],[],[],[],"[['Gigaword', 'has', 'Feats']]",[],text_summarization,11,106
baselines,RAS - LSTM and RAS - Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively .,"[('are', (7, 8)), ('with', (10, 11))]","[('RAS - LSTM and RAS - Elman', (0, 7)), ('seq2seq models', (8, 10)), ('convolutional encoder', (12, 14)), ('LSTM decoder', (16, 18)), ('Elman RNN decoder', (20, 23))]","[['RAS - LSTM and RAS - Elman', 'are', 'seq2seq models'], ['seq2seq models', 'with', 'convolutional encoder'], ['seq2seq models', 'with', 'LSTM decoder'], ['seq2seq models', 'with', 'Elman RNN decoder']]",[],[],[],[],[],[],"[['Gigaword', 'has', 'RAS - LSTM and RAS - Elman']]",[],text_summarization,11,107
baselines,SEASS is a seq2seq model with a selective gate mechanism .,"[('is a', (1, 3)), ('with', (5, 6))]","[('SEASS', (0, 1)), ('seq2seq model', (3, 5)), ('selective gate mechanism', (7, 10))]","[['SEASS', 'is a', 'seq2seq model'], ['seq2seq model', 'with', 'selective gate mechanism']]",[],[],[],[],[],[],"[['Gigaword', 'has', 'SEASS']]",[],text_summarization,11,108
baselines,DRGD is also a baseline for Gigaword .,"[('baseline for', (4, 6))]","[('DRGD', (0, 1)), ('Gigaword', (6, 7))]","[['DRGD', 'baseline for', 'Gigaword']]","[['Gigaword', 'has', 'DRGD']]",[],[],[],[],[],[],[],text_summarization,11,109
results,"In the experiments on the two datasets , our model achieves advantages of ROUGE score over the baselines , and the advantages of ROUGE score on the LCSTS are significant .",[],[],"[['our model', 'achieves', 'advantages'], ['advantages', 'of', 'ROUGE score'], ['ROUGE score', 'over', 'baselines'], ['ROUGE score', 'on', 'LCSTS'], ['LCSTS', 'are', 'significant']]","[['two datasets', 'has', 'our model']]","[['Results', 'on', 'two datasets']]",[],[],[],[],[],[],text_summarization,11,115
results,"Compared with the conventional seq2seq model , our model owns an advantage of ROUGE - 2 score 3.7 and 1.5 on the LCSTS and Gigaword respectively .","[('Compared with', (0, 2)), ('owns an advantage', (9, 12)), ('of', (12, 13)), ('on', (20, 21))]","[('conventional seq2seq model', (3, 6)), ('our model', (7, 9)), ('ROUGE - 2 score 3.7 and 1.5', (13, 20)), ('LCSTS and Gigaword', (22, 25))]","[['conventional seq2seq model', 'owns an advantage', 'our model'], ['our model', 'of', 'ROUGE - 2 score 3.7 and 1.5'], ['ROUGE - 2 score 3.7 and 1.5', 'on', 'LCSTS and Gigaword']]",[],"[['Results', 'Compared with', 'conventional seq2seq model']]",[],[],[],[],[],[],text_summarization,11,118
research-problem,Selective Encoding for Abstractive Sentence Summarization,[],"[('Abstractive Sentence Summarization', (3, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Sentence Summarization']]",[],[],[],[],text_summarization,12,2
research-problem,"The second level representation is tailored for sentence summarization task , which leads to better performance .",[],"[('sentence summarization', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentence summarization']]",[],[],[],[],text_summarization,12,8
model,In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,"[('propose', (4, 5))]","[('Selective Encoding for Abstractive Sentence Summarization ( SEASS )', (5, 14))]",[],[],"[['Model', 'propose', 'Selective Encoding for Abstractive Sentence Summarization ( SEASS )']]",[],[],[],[],[],[],text_summarization,12,35
model,"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .","[('treat', (1, 2)), ('as a', (5, 7))]","[('sentence summarization', (3, 5)), ('threephase task', (7, 9)), ('encoding', (10, 11)), ('selection', (12, 13)), ('decoding', (15, 16))]","[['sentence summarization', 'as a', 'threephase task']]","[['threephase task', 'name', 'encoding'], ['threephase task', 'name', 'selection'], ['threephase task', 'name', 'decoding']]","[['Model', 'treat', 'sentence summarization']]",[],[],[],[],[],[],text_summarization,12,36
model,"It consists of a sentence encoder , a selective gate network , and a summary decoder .","[('consists of', (1, 3))]","[('sentence encoder', (4, 6)), ('selective gate network', (8, 11)), ('summary decoder', (14, 16))]",[],[],"[['Model', 'consists of', 'sentence encoder'], ['Model', 'consists of', 'selective gate network'], ['Model', 'consists of', 'summary decoder']]",[],[],[],[],[],[],text_summarization,12,37
model,"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .","[('First', (0, 1)), ('reads', (5, 6)), ('through', (9, 10)), ('to construct', (13, 15))]","[('sentence encoder', (3, 5)), ('input words', (7, 9)), ('RNN unit', (11, 13)), ('first level sentence representation', (16, 20))]","[['sentence encoder', 'reads', 'input words'], ['input words', 'through', 'RNN unit'], ['RNN unit', 'to construct', 'first level sentence representation']]",[],"[['Model', 'First', 'sentence encoder']]",[],[],[],[],[],[],text_summarization,12,38
model,Then the selective gate network selects the encoded information to construct the second level sentence representation .,"[('selects', (5, 6)), ('to construct', (9, 11))]","[('selective gate network', (2, 5)), ('encoded information', (7, 9)), ('second level sentence representation', (12, 16))]","[['selective gate network', 'selects', 'encoded information'], ['encoded information', 'to construct', 'second level sentence representation']]",[],[],"[['Model', 'has', 'selective gate network']]",[],[],[],[],[],text_summarization,12,39
model,"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .","[('controls', (3, 4)), ('from', (7, 8)), ('by applying', (11, 13)), ('according to', (16, 18))]","[('selective mechanism', (1, 3)), ('information flow', (5, 7)), ('encoder to decoder', (8, 11)), ('gate network', (14, 16)), ('sentence information', (19, 21))]","[['selective mechanism', 'controls', 'information flow'], ['information flow', 'from', 'encoder to decoder'], ['encoder to decoder', 'by applying', 'gate network'], ['gate network', 'according to', 'sentence information']]",[],[],"[['Model', 'has', 'selective mechanism']]",[],[],[],[],[],text_summarization,12,40
model,"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .","[('generates', (7, 8)), ('using', (10, 11))]","[('attention - equipped decoder', (3, 7)), ('summary', (9, 10)), ('second level sentence representation', (12, 16))]","[['attention - equipped decoder', 'generates', 'summary'], ['summary', 'using', 'second level sentence representation']]",[],[],"[['Model', 'has', 'attention - equipped decoder']]",[],[],[],[],[],text_summarization,12,41
hyperparameters,We initialize model parameters randomly using a Gaussian distribution with Xavier scheme .,"[('initialize', (1, 2)), ('using', (5, 6)), ('with', (9, 10))]","[('model parameters randomly', (2, 5)), ('Gaussian distribution', (7, 9)), ('Xavier scheme', (10, 12))]","[['model parameters randomly', 'using', 'Gaussian distribution'], ['Gaussian distribution', 'with', 'Xavier scheme']]",[],"[['Hyperparameters', 'initialize', 'model parameters randomly']]",[],[],[],[],[],[],text_summarization,12,164
hyperparameters,We use Adam as our optimizing algorithm .,"[('use', (1, 2)), ('as', (3, 4))]","[('Adam', (2, 3)), ('optimizing algorithm', (5, 7))]","[['Adam', 'as', 'optimizing algorithm']]",[],"[['Hyperparameters', 'use', 'Adam']]",[],[],[],[],[],[],text_summarization,12,165
hyperparameters,"For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .","[('learning rate', (10, 12)), ('two momentum parameters', (16, 19))]","[('0.001', (14, 15)), ('? 1 = 0.9 and ? 2 = 0.999', (19, 28))]",[],[],[],[],[],"[['Adam', 'learning rate', '0.001'], ['Adam', 'two momentum parameters', '? 1 = 0.9 and ? 2 = 0.999']]",[],[],[],text_summarization,12,166
hyperparameters,"During training , we test the model performance ( ROUGE - 2 F1 ) on development set for every 2,000 batches .","[('During', (0, 1)), ('test', (4, 5)), ('on', (14, 15)), ('for', (17, 18))]","[('training', (1, 2)), ('model performance ( ROUGE - 2 F1 )', (6, 14)), ('development set', (15, 17)), ('every 2,000 batches', (18, 21))]","[['training', 'test', 'model performance ( ROUGE - 2 F1 )'], ['model performance ( ROUGE - 2 F1 )', 'on', 'development set'], ['development set', 'for', 'every 2,000 batches']]",[],"[['Hyperparameters', 'During', 'training']]",[],[],[],[],[],[],text_summarization,12,167
hyperparameters,"We also apply gradient clipping with range [ ? 5 , 5 ] during training .","[('apply', (2, 3)), ('with', (5, 6)), ('during', (13, 14))]","[('gradient clipping', (3, 5)), ('range [ ? 5 , 5 ]', (6, 13)), ('training', (14, 15))]","[['gradient clipping', 'with', 'range [ ? 5 , 5 ]'], ['range [ ? 5 , 5 ]', 'during', 'training']]",[],"[['Hyperparameters', 'apply', 'gradient clipping']]",[],[],[],[],[],[],text_summarization,12,169
hyperparameters,"To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .","[('To both speedup', (0, 3)), ('by', (14, 15))]","[('training and converge quickly', (4, 8)), ('mini-batch size 64', (11, 14)), ('grid search', (15, 17))]","[['mini-batch size 64', 'by', 'grid search'], ['grid search', 'To both speedup', 'training and converge quickly']]",[],[],"[['Hyperparameters', 'use', 'mini-batch size 64']]",[],[],[],[],[],text_summarization,12,170
baselines,"ABS + Based on ABS model , further with two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer implemented in .","[('Based on', (2, 4)), ('further with', (7, 9)), ('for', (13, 14)), ('with', (18, 19)), ('in', (22, 23))]","[('ABS +', (0, 2)), ('ABS model', (4, 6)), ('two - layer LSTMs', (9, 13)), ('encoder - decoder', (15, 18)), ('500 hidden units', (19, 22)), ('each layer', (23, 25))]","[['ABS +', 'Based on', 'ABS model'], ['ABS model', 'further with', 'two - layer LSTMs'], ['two - layer LSTMs', 'for', 'encoder - decoder'], ['encoder - decoder', 'with', '500 hidden units'], ['500 hidden units', 'in', 'each layer']]",[],[],"[['Baselines', 'has', 'ABS +']]",[],[],[],[],[],text_summarization,12,178
baselines,s 2 s+ att,[],"[('s 2 s+ att', (0, 4))]",[],[],[],[],[],[],[],"[['ABS +', 'Based on', 's 2 s+ att']]",[],text_summarization,12,179
baselines,"We also implement a sequence - to sequence model with attention as our baseline and denote it as "" s2 s + att "" .","[('implement', (2, 3)), ('with', (9, 10))]","[('sequence - to sequence model', (4, 9)), ('attention', (10, 11))]","[['sequence - to sequence model', 'with', 'attention']]",[],[],[],[],"[['s 2 s+ att', 'implement', 'sequence - to sequence model']]",[],[],[],text_summarization,12,180
results,Our SEASS model with beam search outperforms all baseline models by a large margin .,"[('with', (3, 4)), ('outperforms', (6, 7)), ('by', (10, 11))]","[('SEASS model', (1, 3)), ('beam search', (4, 6)), ('all baseline models', (7, 10)), ('large margin', (12, 14))]","[['SEASS model', 'with', 'beam search'], ['beam search', 'outperforms', 'all baseline models'], ['all baseline models', 'by', 'large margin']]",[],[],[],[],[],[],"[['English Gigaword', 'has', 'SEASS model']]",[],text_summarization,12,189
results,"Even for greedy search , our model still performs better than other methods which used beam search .","[('Even for', (0, 2)), ('still performs better than', (7, 11))]","[('greedy search', (2, 4)), ('other methods', (11, 13))]","[['greedy search', 'still performs better than', 'other methods']]",[],[],[],[],"[['SEASS model', 'Even for', 'greedy search']]",[],[],[],text_summarization,12,190
results,"For the popular ROUGE - 2 metric , our SEASS model achieves 17.54 F1 score and performs better than the previous works .","[('For the popular', (0, 3)), ('achieves', (11, 12)), ('performs better than', (16, 19))]","[('ROUGE - 2 metric', (3, 7)), ('17.54 F1 score', (12, 15)), ('previous works', (20, 22))]","[['ROUGE - 2 metric', 'performs better than', 'previous works'], ['ROUGE - 2 metric', 'achieves', '17.54 F1 score']]",[],[],[],[],"[['SEASS model', 'For the popular', 'ROUGE - 2 metric']]",[],[],[],text_summarization,12,191
results,"Compared to the ABS model , our model has a 6.22 ROUGE - 2 F1 relative gain .","[('Compared to', (0, 2))]","[('ABS model', (3, 5)), ('6.22 ROUGE - 2 F1 relative gain', (10, 17))]",[],"[['ABS model', 'has', '6.22 ROUGE - 2 F1 relative gain']]",[],[],[],"[['SEASS model', 'Compared to', 'ABS model']]",[],[],[],text_summarization,12,192
results,"Compared to the highest CAs 2s baseline , our model achieves 1.57 ROUGE - 2 F1 improvement and passes the significant test according to the official ROUGE script .","[('achieves', (10, 11)), ('passes', (18, 19)), ('according to', (22, 24))]","[('highest CAs 2s baseline', (3, 7)), ('1.57 ROUGE - 2 F1 improvement', (11, 17)), ('significant test', (20, 22)), ('official ROUGE script', (25, 28))]","[['highest CAs 2s baseline', 'passes', 'significant test'], ['significant test', 'according to', 'official ROUGE script'], ['highest CAs 2s baseline', 'achieves', '1.57 ROUGE - 2 F1 improvement']]",[],[],[],[],[],[],"[['SEASS model', 'Compared to', 'highest CAs 2s baseline']]",[],text_summarization,12,193
results,DUC 2004,[],"[('DUC 2004', (0, 2))]",[],[],[],"[['Results', 'has', 'DUC 2004']]",[],[],[],[],"[['DUC 2004', 'has', 'SEASS']]",text_summarization,12,196
results,"As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .","[('outperforms', (6, 7)), ('achieves', (12, 13)), ('for', (18, 19))]","[('SEASS', (5, 6)), ('all the baseline methods', (7, 11)), ('29.21 , 9.56 and 25.51', (13, 18)), ('ROUGE 1 , 2 and L recall', (19, 26))]","[['SEASS', 'achieves', '29.21 , 9.56 and 25.51'], ['29.21 , 9.56 and 25.51', 'for', 'ROUGE 1 , 2 and L recall'], ['SEASS', 'outperforms', 'all the baseline methods']]",[],[],[],[],[],[],[],[],text_summarization,12,199
results,"Compared to the ABS + model which is tuned using DUC 2003 data , our model performs significantly better by 1.07 ROUGE - 2 recall score and is trained only with English Gigaword sentence - summary data without being tuned using DUC data .",[],[],"[['ABS + model', 'tuned using', 'DUC 2003 data'], ['our model', 'performs', 'significantly better'], ['significantly better', 'by', '1.07 ROUGE - 2 recall score']]","[['ABS + model', 'has', 'our model']]",[],[],[],"[['DUC 2004', 'Compared to', 'ABS + model']]",[],[],[],text_summarization,12,200
research-problem,Coarse-to-Fine Attention Models for Document Summarization,[],"[('Document Summarization', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Document Summarization']]",[],[],[],[],text_summarization,13,2
approach,"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .","[('to scale', (4, 6)), ('prune down', (15, 17))]","[('attention models', (6, 8)), ('length of the source sequence', (18, 23))]",[],[],"[['Approach', 'prune down', 'length of the source sequence'], ['Approach', 'to scale', 'attention models']]",[],[],[],[],[],[],text_summarization,13,16
approach,"Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .","[('Instead of', (0, 2)), ('of', (8, 9)), ('to use', (17, 19))]","[('naively attending to all the words', (2, 8)), ('source', (10, 11)), ('two - layer hierarchical attention', (20, 25))]","[['two - layer hierarchical attention', 'Instead of', 'naively attending to all the words'], ['naively attending to all the words', 'of', 'source']]",[],"[['Approach', 'to use', 'two - layer hierarchical attention']]",[],[],[],[],[],[],text_summarization,13,17
approach,"For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .","[('For', (0, 1)), ('means', (5, 6)), ('into', (9, 10)), ('sparsely attending to', (14, 17)), ('using', (25, 26)), ('applying', (30, 31)), ('over', (35, 36)), ('call', (40, 41))]","[('document summarization', (1, 3)), ('dividing the document', (6, 9)), ('chunks of text', (10, 13)), ('one or a few chunks at a time', (17, 25)), ('hard attention', (26, 28)), ('usual full attention', (32, 35)), ('those chunks', (36, 38)), ('coarse - to - fine attention', (43, 49))]","[['document summarization', 'call', 'coarse - to - fine attention'], ['document summarization', 'means', 'dividing the document'], ['dividing the document', 'into', 'chunks of text'], ['document summarization', 'sparsely attending to', 'one or a few chunks at a time'], ['one or a few chunks at a time', 'applying', 'usual full attention'], ['usual full attention', 'over', 'those chunks'], ['one or a few chunks at a time', 'using', 'hard attention']]",[],"[['Approach', 'For', 'document summarization']]",[],[],[],[],[],[],text_summarization,13,18
experimental-setup,"We train with minibatch stochastic gradient descent ( SGD ) with batch size 20 for 20 epochs , renormalizing gradients below norm 5 .","[('train with', (1, 3)), ('with', (10, 11)), ('for', (14, 15)), ('renormalizing gradients', (18, 20))]","[('minibatch stochastic gradient descent ( SGD )', (3, 10)), ('batch size 20', (11, 14)), ('20 epochs', (15, 17)), ('below norm 5', (20, 23))]","[['minibatch stochastic gradient descent ( SGD )', 'with', 'batch size 20'], ['batch size 20', 'for', '20 epochs'], ['minibatch stochastic gradient descent ( SGD )', 'renormalizing gradients', 'below norm 5']]",[],"[['Experimental setup', 'train with', 'minibatch stochastic gradient descent ( SGD )']]",[],[],[],[],[],[],text_summarization,13,185
experimental-setup,"We initialize the learning rate to 0.1 for the top - level encoder and 1 for the rest of the model , and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing .",[],[],"[['learning rate', 'to', '1'], ['1', 'for', 'rest of the model'], ['learning rate', 'to', '0.1'], ['0.1', 'for', 'top - level encoder'], ['learning rate', 'begin decaying it by', 'a factor of 0.5'], ['a factor of 0.5', 'after', 'validation perplexity'], ['validation perplexity', 'stops', 'decreasing']]",[],"[['Experimental setup', 'initialize', 'learning rate']]",[],[],[],[],[],[],text_summarization,13,186
experimental-setup,"We use 2 layer LSTMs with 500 hidden units , and we initialize word embeddings with 300 dimensional word2vec embeddings .",[],[],"[['2 layer LSTMs', 'with', '500 hidden units'], ['word embeddings', 'with', '300 dimensional word2vec embeddings']]",[],"[['Experimental setup', 'use', '2 layer LSTMs']]","[['Experimental setup', 'initialize', 'word embeddings']]",[],[],[],[],[],text_summarization,13,187
experimental-setup,"We initialize all other parameters as uniform in the interval [ ? 0.1 , 0.1 ] .","[('as uniform in', (5, 8))]","[('all other parameters', (2, 5)), ('interval [ ? 0.1 , 0.1 ]', (9, 16))]","[['all other parameters', 'as uniform in', 'interval [ ? 0.1 , 0.1 ]']]",[],[],"[['Experimental setup', 'initialize', 'all other parameters']]",[],[],[],[],[],text_summarization,13,188
experimental-setup,"For convolutional layers , we use a kernel width of 6 and 600 filters .","[('For', (0, 1)), ('use', (5, 6)), ('of', (9, 10))]","[('convolutional layers', (1, 3)), ('kernel width', (7, 9)), ('6 and 600 filters', (10, 14))]","[['convolutional layers', 'use', 'kernel width'], ['kernel width', 'of', '6 and 600 filters']]",[],"[['Experimental setup', 'For', 'convolutional layers']]",[],[],[],[],[],[],text_summarization,13,189
experimental-setup,Positional embeddings have dimension 25 .,"[('have', (2, 3))]","[('Positional embeddings', (0, 2)), ('dimension 25', (3, 5))]","[['Positional embeddings', 'have', 'dimension 25']]",[],[],"[['Experimental setup', 'has', 'Positional embeddings']]",[],[],[],[],[],text_summarization,13,190
experimental-setup,We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .,"[('between', (3, 4)), ('to regularize', (15, 17))]","[('dropout', (2, 3)), ('stacked LSTM hidden states and before the final word generator layer', (4, 15)), ('dropout probability 0.3', (19, 22))]","[['dropout', 'between', 'stacked LSTM hidden states and before the final word generator layer'], ['stacked LSTM hidden states and before the final word generator layer', 'to regularize', 'dropout probability 0.3']]",[],[],"[['Experimental setup', 'use', 'dropout']]",[],[],[],[],[],text_summarization,13,191
experimental-setup,"At test time , we run beam search to produce the summary with a beam size of 5 .","[('At', (0, 1)), ('run', (5, 6)), ('to produce', (8, 10)), ('with', (12, 13))]","[('test time', (1, 3)), ('beam search', (6, 8)), ('summary', (11, 12)), ('beam size of 5', (14, 18))]","[['test time', 'run', 'beam search'], ['beam search', 'to produce', 'summary'], ['summary', 'with', 'beam size of 5']]",[],"[['Experimental setup', 'At', 'test time']]",[],[],[],[],[],[],text_summarization,13,192
experimental-setup,Our models are implemented using Torch based on a past version of the Open NMT system,"[('implemented using', (3, 5)), ('based on', (6, 8))]","[('Torch', (5, 6)), ('past version of the Open NMT system', (9, 16))]","[['Torch', 'based on', 'past version of the Open NMT system']]",[],"[['Experimental setup', 'implemented using', 'Torch']]",[],[],[],[],[],[],text_summarization,13,193
experimental-setup,4 . We ran our experiments on a 12GB Geforce GTX Titan X GPU .,"[('ran', (3, 4)), ('on', (6, 7))]","[('our experiments', (4, 6)), ('12GB Geforce GTX Titan X GPU', (8, 14))]","[['our experiments', 'on', '12GB Geforce GTX Titan X GPU']]",[],"[['Experimental setup', 'ran', 'our experiments']]",[],[],[],[],[],[],text_summarization,13,194
results,The ILP model ROUGE scores are surprisingly low .,"[('ROUGE scores', (3, 5))]","[('ILP model', (1, 3)), ('surprisingly low', (6, 8))]","[['ILP model', 'ROUGE scores', 'surprisingly low']]",[],[],"[['Results', 'has', 'ILP model']]",[],[],[],[],[],text_summarization,13,204
results,C2 F results are significantly worse than soft attention results .,"[('significantly worse than', (4, 7))]","[('C2 F', (0, 2)), ('soft attention results', (7, 10))]","[['C2 F', 'significantly worse than', 'soft attention results']]",[],[],"[['Results', 'has', 'C2 F']]",[],[],[],[],[],text_summarization,13,213
ablation-analysis,Sharpness of Attention,[],"[('Sharpness of Attention', (0, 3))]",[],[],[],"[['Ablation analysis', 'has', 'Sharpness of Attention']]",[],[],[],[],[],text_summarization,13,234
ablation-analysis,We compute the entropy numbers by averaging over all generated words in the validation set .,"[('compute', (1, 2)), ('by averaging', (5, 7)), ('in', (11, 12))]","[('entropy numbers', (3, 5)), ('over all generated words', (7, 11)), ('validation set', (13, 15))]","[['entropy numbers', 'by averaging', 'over all generated words'], ['over all generated words', 'in', 'validation set']]",[],[],[],[],"[['Sharpness of Attention', 'compute', 'entropy numbers']]",[],[],[],text_summarization,13,237
ablation-analysis,We note that the entropy of C2F is very low ( before taking the argmax at test time ) .,"[('note', (1, 2)), ('is', (7, 8))]","[('entropy of C2F', (4, 7)), ('very low', (8, 10))]","[['entropy of C2F', 'is', 'very low']]",[],[],[],[],"[['Sharpness of Attention', 'note', 'entropy of C2F']]",[],[],[],text_summarization,13,239
ablation-analysis,This is exactly what we had hoped for - we will see that the model in fact learns to focus on only a few top - level chunks of the document over the course of generation .,"[('learns to focus on', (17, 21)), ('of', (28, 29)), ('over', (31, 32))]","[('model', (14, 15)), ('few top - level chunks', (23, 28)), ('document', (30, 31)), ('course of generation', (33, 36))]","[['model', 'learns to focus on', 'few top - level chunks'], ['few top - level chunks', 'of', 'document'], ['document', 'over', 'course of generation']]",[],[],[],[],[],[],"[['Sharpness of Attention', 'has', 'model']]",[],text_summarization,13,240
ablation-analysis,Attention Heatmaps,[],"[('Attention Heatmaps', (0, 2))]",[],[],[],"[['Ablation analysis', 'has', 'Attention Heatmaps']]",[],[],[],[],[],text_summarization,13,245
ablation-analysis,"In HIER , we observe that the attention becomes washed out ( in accord with its high entropy ) and is essentially averaging all of the encoder hidden states .","[('In', (0, 1)), ('observe', (4, 5)), ('averaging', (22, 23))]","[('HIER', (1, 2)), ('attention becomes washed out', (7, 11)), ('all of the encoder hidden states', (23, 29))]","[['HIER', 'observe', 'attention becomes washed out'], ['attention becomes washed out', 'averaging', 'all of the encoder hidden states']]",[],[],[],[],"[['Attention Heatmaps', 'In', 'HIER']]",[],[],[],text_summarization,13,250
ablation-analysis,"In C2 F , we see that we get very sharp attention on some rows as we had hoped .","[('get', (8, 9)), ('on', (12, 13))]","[('C2 F', (1, 3)), ('very sharp attention', (9, 12)), ('some rows', (13, 15))]","[['C2 F', 'get', 'very sharp attention'], ['very sharp attention', 'on', 'some rows']]",[],[],[],[],[],[],"[['Attention Heatmaps', 'In', 'C2 F']]",[],text_summarization,13,255
research-problem,Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,[],"[('Abstractive Sentence Summarization', (11, 14))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Sentence Summarization']]",[],[],[],[],text_summarization,14,2
research-problem,"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .",[],"[('sentence summarization', (7, 9))]",[],[],[],[],"[['Contribution', 'has research problem', 'sentence summarization']]",[],[],[],[],text_summarization,14,4
model,"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .","[('propose', (10, 11))]","[('entailment - aware encoder', (15, 19)), ('entailment - aware decoder', (21, 25))]",[],[],"[['Model', 'propose', 'entailment - aware encoder'], ['Model', 'propose', 'entailment - aware decoder']]",[],[],[],[],[],[],text_summarization,14,27
model,"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .","[('share', (1, 2)), ('of', (4, 5)), ('with', (9, 10))]","[('encoder', (3, 4)), ('summarization generation system', (6, 9)), ('entailment recognition system', (11, 14))]","[['encoder', 'of', 'summarization generation system'], ['summarization generation system', 'with', 'entailment recognition system']]",[],"[['Model', 'share', 'encoder']]",[],[],[],[],[],[],text_summarization,14,28
model,"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .","[('encourages', (15, 16)), ('to produce', (22, 24)), ('entailed by', (25, 27))]","[('entailment Reward Augmented Maximum Likelihood ( RAML ) training', (5, 14)), ('decoder of the summarization system', (17, 22)), ('summary', (24, 25)), ('source', (28, 29))]","[['entailment Reward Augmented Maximum Likelihood ( RAML ) training', 'encourages', 'decoder of the summarization system'], ['decoder of the summarization system', 'to produce', 'summary'], ['summary', 'entailed by', 'source']]",[],[],"[['Model', 'propose', 'entailment Reward Augmented Maximum Likelihood ( RAML ) training']]",[],[],[],[],[],text_summarization,14,29
baselines,ABS . first apply the seq2seq model to abstractive sentence summarization .,"[('apply', (3, 4)), ('to', (7, 8))]","[('ABS', (0, 1)), ('seq2seq model', (5, 7)), ('abstractive sentence summarization', (8, 11))]","[['ABS', 'apply', 'seq2seq model'], ['seq2seq model', 'to', 'abstractive sentence summarization']]",[],[],"[['Baselines', 'has', 'ABS']]",[],[],[],[],[],text_summarization,14,151
baselines,ABS +. propose a neural machine translation model with two - layer LSTMs for the encoder - decoder .,"[('propose', (2, 3)), ('with', (8, 9)), ('for', (13, 14))]","[('neural machine translation model', (4, 8)), ('two - layer LSTMs', (9, 13)), ('encoder - decoder', (15, 18))]","[['neural machine translation model', 'with', 'two - layer LSTMs'], ['two - layer LSTMs', 'for', 'encoder - decoder']]",[],[],[],[],"[['ABS +', 'propose', 'neural machine translation model']]",[],[],[],text_summarization,14,153
baselines,Seq2seq .,[],"[('Seq2seq', (0, 1))]",[],[],[],"[['Baselines', 'has', 'Seq2seq']]",[],[],[],[],[],text_summarization,14,154
baselines,This is a standard seq2seq model with attention mechanism .,"[('is a', (1, 3)), ('with', (6, 7))]","[('standard seq2seq model', (3, 6)), ('attention mechanism', (7, 9))]","[['standard seq2seq model', 'with', 'attention mechanism']]",[],[],[],[],"[['Seq2seq', 'is a', 'standard seq2seq model']]",[],[],[],text_summarization,14,155
baselines,Seq2seq + MTL .,[],"[('Seq2seq + MTL', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + MTL']]",[],[],[],[],[],text_summarization,14,156
baselines,"This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .","[('with', (5, 6)), ('applies', (12, 13)), ('to', (20, 21))]","[('entailment - aware encoder', (6, 10)), ('multi-task learning ( MTL ) framework', (14, 20)), ('seq2seq model', (21, 23))]","[['entailment - aware encoder', 'applies', 'multi-task learning ( MTL ) framework'], ['multi-task learning ( MTL ) framework', 'to', 'seq2seq model']]",[],[],[],[],"[['Seq2seq + MTL', 'with', 'entailment - aware encoder']]",[],[],[],text_summarization,14,157
baselines,Seq2seq + MTL ( Share decoder ) .,[],"[('Seq2seq + MTL ( Share decoder )', (0, 7))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + MTL ( Share decoder )']]",[],[],[],[],[],text_summarization,14,158
baselines,propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .,"[('propose', (0, 1)), ('in which', (10, 12)), ('is shared for', (14, 17))]","[('multi - task learning ( MTL ) framework', (2, 10)), ('decoder', (13, 14)), ('summarization generation and entailment generation task', (17, 23))]","[['multi - task learning ( MTL ) framework', 'in which', 'decoder'], ['decoder', 'is shared for', 'summarization generation and entailment generation task']]",[],[],[],[],"[['Seq2seq + MTL ( Share decoder )', 'propose', 'multi - task learning ( MTL ) framework']]",[],[],[],text_summarization,14,159
baselines,Seq2seq + ERAML .,[],"[('Seq2seq + ERAML', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + ERAML']]",[],[],[],[],[],text_summarization,14,160
baselines,"This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .","[('with', (5, 6)), ('conducts', (12, 13))]","[('entailment - aware decoder', (6, 10)), ('Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework', (14, 24))]","[['entailment - aware decoder', 'conducts', 'Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework']]",[],[],[],[],"[['Seq2seq + ERAML', 'with', 'entailment - aware decoder']]",[],[],[],text_summarization,14,161
baselines,Seq2seq + ROUGE -2 RAML .,[],"[('Seq2seq + ROUGE -2 RAML', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + ROUGE -2 RAML']]",[],[],[],[],[],text_summarization,14,162
baselines,We apply ROUGE - 2 RAML training for seq2seq model .,"[('apply', (1, 2)), ('for', (7, 8))]","[('ROUGE - 2 RAML training', (2, 7)), ('seq2seq model', (8, 10))]","[['ROUGE - 2 RAML training', 'for', 'seq2seq model']]",[],[],[],[],"[['Seq2seq + ROUGE -2 RAML', 'apply', 'ROUGE - 2 RAML training']]",[],[],[],text_summarization,14,163
baselines,Seq2seq + RL .,[],"[('Seq2seq + RL', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + RL']]",[],[],[],[],[],text_summarization,14,164
baselines,We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .,"[('implement', (1, 2)), ('with', (12, 13)), ('of', (15, 16))]","[('Reinforcement Learning ( RL ) models', (2, 8)), ('reward metrics', (13, 15)), ('Entailment and ROUGE - 2', (16, 21))]","[['Reinforcement Learning ( RL ) models', 'with', 'reward metrics'], ['reward metrics', 'of', 'Entailment and ROUGE - 2']]",[],[],[],[],"[['Seq2seq + RL', 'implement', 'Reinforcement Learning ( RL ) models']]",[],[],[],text_summarization,14,165
baselines,Seq2seq + selective .,[],"[('Seq2seq + selective', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Seq2seq + selective']]",[],[],[],[],[],text_summarization,14,166
baselines,employ a selective encoding model to control the information flow from encoder to decoder .,"[('employ', (0, 1)), ('to control', (5, 7)), ('from', (10, 11))]","[('selective encoding model', (2, 5)), ('information flow', (8, 10)), ('encoder to decoder', (11, 14))]","[['selective encoding model', 'to control', 'information flow'], ['information flow', 'from', 'encoder to decoder']]",[],[],[],[],"[['Seq2seq + selective', 'employ', 'selective encoding model']]",[],[],[],text_summarization,14,167
results,Experimental Results : Gigaword Corpus,[],"[('Gigaword Corpus', (3, 5))]",[],[],[],"[['Results', 'on', 'Gigaword Corpus']]",[],[],[],[],"[['Gigaword Corpus', 'has', 'Our model']]",text_summarization,14,172
results,Our model performs better than the previous works .,"[('performs better than', (2, 5))]","[('Our model', (0, 2)), ('previous works', (6, 8))]","[['Our model', 'performs better than', 'previous works']]",[],[],[],[],[],[],[],[],text_summarization,14,176
results,Experimental Results : DUC 2004,[],"[('DUC 2004', (3, 5))]",[],[],[],"[['Results', 'on', 'DUC 2004']]",[],[],[],[],[],text_summarization,14,177
results,"In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .","[('show', (5, 6)), ('achieves', (15, 16)), ('over', (18, 19)), ('surpassing', (22, 23)), ('by', (24, 25)), ('without', (42, 43)), ('on', (46, 47))]","[('Seq2seq + selective + MTL + ERAML model', (7, 15)), ('significant improvements', (16, 18)), ('baseline models', (19, 21)), ('Feats2s', (23, 24)), ('0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L', (25, 42)), ('fine - tuning', (43, 46))]","[['Seq2seq + selective + MTL + ERAML model', 'achieves', 'significant improvements'], ['significant improvements', 'over', 'baseline models'], ['significant improvements', 'surpassing', 'Feats2s'], ['Feats2s', 'by', '0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L'], ['0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L', 'without', 'fine - tuning']]",[],[],[],[],"[['DUC 2004', 'show', 'Seq2seq + selective + MTL + ERAML model']]",[],[],[],text_summarization,14,181
ablation-analysis,Does our summarization model learn entailment knowledge ?,[],"[('Does our summarization model learn entailment knowledge ?', (0, 8))]",[],[],[],"[['Ablation analysis', 'has', 'Does our summarization model learn entailment knowledge ?']]",[],[],[],[],[],text_summarization,14,189
ablation-analysis,"For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .","[('For', (0, 1)), ('for', (10, 11)), ('is', (13, 14)), ('entailment score is', (24, 27))]","[('test set', (2, 4)), ('average entailment score', (7, 10)), ('reference', (12, 13)), ('0.72', (14, 15)), ('basic seq2seq model', (19, 22)), ('0.46', (28, 29))]","[['average entailment score', 'for', 'reference'], ['reference', 'is', '0.72'], ['average entailment score', 'for', 'basic seq2seq model'], ['basic seq2seq model', 'entailment score is', '0.46']]","[['test set', 'has', 'average entailment score']]",[],[],[],"[['Does our summarization model learn entailment knowledge ?', 'For', 'test set']]",[],[],[],text_summarization,14,192
ablation-analysis,"When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .","[('adopt', (2, 3)), ('rises to', (9, 11)), ('for', (12, 13))]","[('entailmentbased strategies', (3, 5)), ('entailment score', (7, 9)), ('0.63', (11, 12)), ('seq2seq model', (13, 15))]","[['entailment score', 'rises to', '0.63'], ['0.63', 'for', 'seq2seq model']]","[['entailmentbased strategies', 'has', 'entailment score']]",[],[],[],"[['Does our summarization model learn entailment knowledge ?', 'adopt', 'entailmentbased strategies']]",[],[],[],text_summarization,14,193
ablation-analysis,"Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .","[('Note', (0, 1)), ('is', (5, 6)), ('for', (7, 8)), ('with', (10, 11)), ('filter out', (22, 24)), ('in', (26, 27))]","[('entailment score', (3, 5)), ('0.57', (6, 7)), ('seq2seq model', (8, 10)), ('selective encoding', (11, 13)), ('selective mechanism', (19, 21)), ('secondary information', (24, 26)), ('input', (28, 29))]","[['entailment score', 'is', '0.57'], ['0.57', 'for', 'seq2seq model'], ['seq2seq model', 'with', 'selective encoding'], ['selective mechanism', 'filter out', 'secondary information'], ['secondary information', 'in', 'input']]",[],[],[],[],"[['Does our summarization model learn entailment knowledge ?', 'Note', 'entailment score']]",[],"[['Does our summarization model learn entailment knowledge ?', 'has', 'selective mechanism']]",[],text_summarization,14,194
ablation-analysis,Entailment - aware selective model achieves a high entailment reward of 0.71 .,"[('achieves', (5, 6)), ('of', (10, 11))]","[('Entailment - aware selective model', (0, 5)), ('high entailment reward', (7, 10)), ('0.71', (11, 12))]","[['Entailment - aware selective model', 'achieves', 'high entailment reward'], ['high entailment reward', 'of', '0.71']]",[],[],[],[],[],[],"[['Does our summarization model learn entailment knowledge ?', 'has', 'Entailment - aware selective model']]",[],text_summarization,14,195
ablation-analysis,"In part at least , we can conclude that our model has successfully learned entailment knowledge .","[('conclude', (7, 8)), ('successfully learned', (12, 14))]","[('our model', (9, 11)), ('entailment knowledge', (14, 16))]","[['our model', 'successfully learned', 'entailment knowledge']]",[],[],[],[],"[['Does our summarization model learn entailment knowledge ?', 'conclude', 'our model']]",[],[],[],text_summarization,14,196
ablation-analysis,Is it less abstractive for our model ?,[],"[('Is it less abstractive for our model ?', (0, 8))]",[],[],[],"[['Ablation analysis', 'has', 'Is it less abstractive for our model ?']]",[],[],[],[],[],text_summarization,14,198
ablation-analysis,"shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .","[('shows that', (0, 2)), ('produces', (5, 6)), ('than', (21, 22)), ('indicating', (25, 26))]","[('seq2seq model', (3, 5)), ('more novel words', (6, 9)), ('our model', (22, 24)), ('lower degree of abstraction', (27, 31))]","[['seq2seq model', 'produces', 'more novel words'], ['more novel words', 'than', 'our model'], ['our model', 'indicating', 'lower degree of abstraction']]",[],[],[],[],"[['Is it less abstractive for our model ?', 'shows that', 'seq2seq model']]",[],[],[],text_summarization,14,202
ablation-analysis,"However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .","[('exclude', (4, 5)), ('generates', (24, 25)), ('suggesting that', (29, 31)), ('provides', (33, 34)), ('for', (37, 38))]","[('all the words not in the reference', (5, 12)), ('model', (23, 24)), ('more novel words', (25, 28)), ('our model', (31, 33)), ('compromise solution', (35, 37)), ('informativeness and correctness', (38, 41))]","[['model', 'generates', 'more novel words'], ['more novel words', 'suggesting that', 'our model'], ['our model', 'provides', 'compromise solution'], ['compromise solution', 'for', 'informativeness and correctness']]","[['all the words not in the reference', 'has', 'model']]",[],[],[],"[['Is it less abstractive for our model ?', 'exclude', 'all the words not in the reference']]",[],[],[],text_summarization,14,203
ablation-analysis,6.6.3 Could the entailment recognition also be improved ?,[],"[('Could the entailment recognition also be improved ?', (1, 9))]",[],[],[],"[['Ablation analysis', 'has', 'Could the entailment recognition also be improved ?']]",[],[],[],[],[],text_summarization,14,205
ablation-analysis,shows that our summarization model with MTL outperforms basic seq2seq model .,"[('shows', (0, 1)), ('with', (5, 6)), ('outperforms', (7, 8))]","[('our summarization model', (2, 5)), ('MTL', (6, 7)), ('basic seq2seq model', (8, 11))]","[['our summarization model', 'with', 'MTL'], ['MTL', 'outperforms', 'basic seq2seq model']]",[],[],[],[],"[['Could the entailment recognition also be improved ?', 'shows', 'our summarization model']]",[],[],[],text_summarization,14,208
ablation-analysis,"As ? increases , the accuracy of entailment recognition improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .","[('of', (6, 7))]","[('accuracy', (5, 6)), ('entailment recognition', (7, 9))]","[['accuracy', 'of', 'entailment recognition']]",[],[],[],[],[],[],"[['Could the entailment recognition also be improved ?', 'has', 'accuracy']]",[],text_summarization,14,209
research-problem,Structure - Infused Copy Mechanisms for Abstractive Summarization,[],"[('Abstractive Summarization', (6, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Summarization']]",[],[],[],[],text_summarization,2,2
research-problem,Seq2seq learning has produced promising results on summarization .,[],"[('summarization', (7, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'summarization']]",[],[],[],[],text_summarization,2,4
model,In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .,"[('incorporating', (10, 11)), ('in', (14, 15)), ('to help', (18, 20)), ('identify', (22, 23)), ('compose', (28, 29)), ('that preserve', (30, 32))]","[('source syntactic structure', (11, 14)), ('neural sentence summarization', (15, 18)), ('system', (21, 22)), ('summary - worthy content', (23, 27)), ('summaries', (29, 30)), ('important meaning of the source texts', (33, 39))]","[['source syntactic structure', 'in', 'neural sentence summarization'], ['neural sentence summarization', 'to help', 'system'], ['system', 'identify', 'summary - worthy content'], ['system', 'compose', 'summaries'], ['summaries', 'that preserve', 'important meaning of the source texts']]",[],"[['Model', 'incorporating', 'source syntactic structure']]",[],[],[],[],[],[],text_summarization,2,25
model,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .,"[('present', (1, 2)), ('to facilitate copying', (7, 10)), ('to', (14, 15)), ('based on', (17, 19)), ('in', (24, 25))]","[('structure - infused copy mechanisms', (2, 7)), ('source words and relations', (10, 14)), ('summary', (16, 17)), ('semantic and structural importance', (20, 24)), ('source sentences', (26, 28))]","[['structure - infused copy mechanisms', 'to facilitate copying', 'source words and relations'], ['source words and relations', 'to', 'summary'], ['summary', 'based on', 'semantic and structural importance'], ['semantic and structural importance', 'in', 'source sentences']]",[],"[['Model', 'present', 'structure - infused copy mechanisms']]",[],[],[],[],[],[],text_summarization,2,26
results,We first report results on the Gigaword valid - 2000 dataset in .,"[('on', (4, 5))]","[('Gigaword valid - 2000 dataset', (6, 11))]",[],[],"[['Results', 'on', 'Gigaword valid - 2000 dataset']]",[],[],[],[],[],[],text_summarization,2,215
results,"We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .","[('present', (1, 2)), ('measures', (18, 19)), ('between', (29, 30))]","[('R - 1 , R - 2 , and R - L scores', (2, 15)), ('overlapped unigrams , bigrams , and longest common subsequences', (20, 29)), ('system and reference summaries', (31, 35))]","[['R - 1 , R - 2 , and R - L scores', 'measures', 'overlapped unigrams , bigrams , and longest common subsequences'], ['overlapped unigrams , bigrams , and longest common subsequences', 'between', 'system and reference summaries']]",[],[],[],[],"[['Gigaword valid - 2000 dataset', 'present', 'R - 1 , R - 2 , and R - L scores']]",[],[],[],text_summarization,2,216
results,"Overall , we observe that models equipped with the structure - infused copy mechanisms are superior to the baseline , suggesting that combining source syntactic structure with the copy mechanism is effective .","[('observe', (3, 4)), ('with', (7, 8)), ('superior to', (15, 17))]","[('models', (5, 6)), ('structure - infused copy mechanisms', (9, 14)), ('baseline', (18, 19))]","[['models', 'with', 'structure - infused copy mechanisms'], ['structure - infused copy mechanisms', 'superior to', 'baseline']]",[],[],[],[],"[['Gigaword valid - 2000 dataset', 'observe', 'models']]",[],[],[],text_summarization,2,221
results,"We found that the "" Struct + Hidden "" architecture , which directly concatenates structural embeddings with the encoder hidden states , outperforms "" Struct + Input "" despite that the latter requires more parameters .","[('found', (1, 2)), ('directly concatenates', (12, 14)), ('with', (16, 17)), ('outperforms', (22, 23))]","[('"" Struct + Hidden "" architecture', (4, 10)), ('structural embeddings', (14, 16)), ('encoder hidden states', (18, 21)), ('"" Struct + Input ""', (23, 28))]","[['"" Struct + Hidden "" architecture', 'directly concatenates', 'structural embeddings'], ['structural embeddings', 'with', 'encoder hidden states'], ['"" Struct + Hidden "" architecture', 'outperforms', '"" Struct + Input ""']]",[],[],[],[],"[['Gigaword valid - 2000 dataset', 'found', '"" Struct + Hidden "" architecture']]",[],[],[],text_summarization,2,222
results,""" Struct + 2 Way + Word "" also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .","[('demonstrates', (9, 10)), ('achieving', (13, 14)), ('for', (28, 29))]","[('Struct + 2 Way + Word', (1, 7)), ('strong performance', (10, 12)), ('43.21 % , 21. 84 % , and 40.86 % F 1 scores', (14, 27)), ('R - 1 , R - 2 , and R - L', (29, 41))]","[['Struct + 2 Way + Word', 'demonstrates', 'strong performance'], ['Struct + 2 Way + Word', 'achieving', '43.21 % , 21. 84 % , and 40.86 % F 1 scores'], ['43.21 % , 21. 84 % , and 40.86 % F 1 scores', 'for', 'R - 1 , R - 2 , and R - L']]",[],[],[],[],[],[],"[['Gigaword valid - 2000 dataset', 'has', 'Struct + 2 Way + Word']]",[],text_summarization,2,223
research-problem,Concept Pointer Network for Abstractive Summarization,[],"[('Abstractive Summarization', (4, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Summarization']]",[],[],[],[],text_summarization,3,2
research-problem,Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,[],"[('Abstractive summarization ( ABS )', (0, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive summarization ( ABS )']]",[],[],[],[],text_summarization,3,14
model,"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .","[('propose', (7, 8)), ('based on', (11, 13)), ('encourages the generation of', (18, 22))]","[('novel model', (9, 11)), ('concept pointer generator', (14, 17)), ('conceptual and abstract words', (22, 26))]","[['novel model', 'based on', 'concept pointer generator'], ['concept pointer generator', 'encourages the generation of', 'conceptual and abstract words']]",[],"[['Model', 'propose', 'novel model']]",[],[],[],[],[],[],text_summarization,3,26
model,"As a hidden benefit , the model also alleviates the OOV problems .","[('alleviates', (8, 9))]","[('OOV problems', (10, 12))]",[],[],"[['Model', 'alleviates', 'OOV problems']]",[],[],[],[],[],[],text_summarization,3,27
model,"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .","[('uses', (2, 3)), ('to capture', (5, 7)), ('from', (10, 11)), ('employs', (17, 18)), ('to generalize', (20, 22)), ('according to', (25, 27))]","[('pointer network', (3, 5)), ('salient information', (8, 10)), ('source text', (12, 14)), ('another pointer', (18, 20)), ('detailed words', (23, 25)), ('upper level of expressions', (28, 32))]","[['another pointer', 'to generalize', 'detailed words'], ['detailed words', 'according to', 'upper level of expressions'], ['pointer network', 'to capture', 'salient information'], ['salient information', 'from', 'source text']]",[],"[['Model', 'employs', 'another pointer'], ['Model', 'uses', 'pointer network']]",[],[],[],[],[],[],text_summarization,3,28
model,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,"[('adaptive so as to cater for', (4, 10)), ('with', (12, 13))]","[('optimization function', (1, 3)), ('different datasets', (10, 12)), ('distantly - supervised training', (13, 17))]","[['optimization function', 'adaptive so as to cater for', 'different datasets'], ['different datasets', 'with', 'distantly - supervised training']]",[],[],"[['Model', 'has', 'optimization function']]",[],[],[],[],[],text_summarization,3,34
model,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .","[('optimized end - to - end', (4, 10)), ('using', (10, 11)), ('with', (14, 15))]","[('network', (1, 2)), ('reinforcement learning', (11, 13)), ('distant - supervision strategy', (16, 20))]","[['network', 'using', 'reinforcement learning'], ['reinforcement learning', 'with', 'distant - supervision strategy']]",[],"[['Model', 'optimized end - to - end', 'network']]",[],[],[],[],[],[],text_summarization,3,35
experimental-setup,We initialize word embeddings with 128 - d vectors and fine - tune them during training .,"[('initialize', (1, 2)), ('fine - tune', (10, 13))]","[('word embeddings', (2, 4)), ('128 - d vectors', (5, 9)), ('during training', (14, 16))]","[['word embeddings', 'fine - tune', 'during training'], ['word embeddings', 'initialize', '128 - d vectors']]",[],[],"[['Experimental setup', 'has', 'word embeddings']]",[],[],[],[],[],text_summarization,3,159
experimental-setup,The vocabulary size was set to 150 k for both the source and target text .,"[('set to', (4, 6)), ('for', (8, 9))]","[('vocabulary size', (1, 3)), ('150 k', (6, 8)), ('both the source and target text', (9, 15))]","[['vocabulary size', 'set to', '150 k'], ['150 k', 'for', 'both the source and target text']]",[],[],"[['Experimental setup', 'has', 'vocabulary size']]",[],[],[],[],[],text_summarization,3,161
experimental-setup,The hidden state size was set to 256 .,"[('set to', (5, 7))]","[('hidden state size', (1, 4)), ('256', (7, 8))]","[['hidden state size', 'set to', '256']]",[],[],"[['Experimental setup', 'has', 'hidden state size']]",[],[],[],[],[],text_summarization,3,162
code,"Our code is available on https :// github.com/wprojectsn/codes , and the vocabularies and candidate concepts are also included .",[],"[('https :// github.com/wprojectsn/codes', (5, 8))]",[],[],[],[],"[['Contribution', 'Code', 'https :// github.com/wprojectsn/codes']]",[],[],[],[],text_summarization,3,165
experimental-setup,We trained our models on a single GTX TI - TAN GPU machine .,"[('on', (4, 5))]","[('models', (3, 4)), ('single GTX TI - TAN GPU machine', (6, 13))]","[['models', 'on', 'single GTX TI - TAN GPU machine']]",[],[],"[['Experimental setup', 'has', 'models']]",[],[],[],[],[],text_summarization,3,166
experimental-setup,We used the Adagrad optimizer with a batch size of 64 to minimize the loss .,"[('used', (1, 2)), ('with', (5, 6)), ('to minimize', (11, 13))]","[('Adagrad optimizer', (3, 5)), ('batch size of 64', (7, 11)), ('loss', (14, 15))]","[['Adagrad optimizer', 'with', 'batch size of 64'], ['batch size of 64', 'to minimize', 'loss']]",[],"[['Experimental setup', 'used', 'Adagrad optimizer']]",[],[],[],[],[],[],text_summarization,3,167
experimental-setup,"The initial learning rate and the accumulator value were set to 0.15 and 0.1 , respectively .","[('set to', (9, 11))]","[('initial learning rate', (1, 4)), ('accumulator value', (6, 8)), ('0.15', (11, 12)), ('0.1', (13, 14))]",[],"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","[['Experimental setup', 'set to', '0.1'], ['Experimental setup', 'set to', '0.15']]",[],[],[],[],[],[],text_summarization,3,168
experimental-setup,We used gradient clipping with a maximum gradient norm of 2 .,"[('with', (4, 5)), ('of', (9, 10))]","[('gradient clipping', (2, 4)), ('maximum gradient norm', (6, 9)), ('2', (10, 11))]","[['gradient clipping', 'with', 'maximum gradient norm'], ['maximum gradient norm', 'of', '2']]",[],[],"[['Experimental setup', 'used', 'gradient clipping']]",[],[],[],[],[],text_summarization,3,169
experimental-setup,"We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .",[],[],"[['RL rewards', 'for', 'RG - L'], ['RG - L', 'at', '50 K iterations'], ['50 K iterations', 'on', 'Gigaword'], ['RG - L', 'at', '95 K iterations'], ['95 K iterations', 'on', 'DUC - 2004'], ['concept pointer generator', 'for', '450 k iterations'], ['450 k iterations', 'yielded', 'best performance']]",[],"[['Experimental setup', 'optimization using', 'RL rewards'], ['Experimental setup', 'trained', 'concept pointer generator']]",[],[],[],[],[],[],text_summarization,3,173
experimental-setup,We took the distancesupervised training at 5 K iterations on DUC - 2004 and at 6.5 K iterations on Gigaword .,[],[],"[['distancesupervised training', 'at', '5 K iterations'], ['5 K iterations', 'on', 'DUC - 2004'], ['distancesupervised training', 'at', '6.5 K iterations'], ['6.5 K iterations', 'on', 'Gigaword']]",[],"[['Experimental setup', 'took', 'distancesupervised training']]",[],[],[],[],[],[],text_summarization,3,174
baselines,ABS + is a tuned ABS model with additional features .,"[('is a', (2, 4))]","[('ABS +', (0, 2)), ('tuned ABS model', (4, 7))]","[['ABS +', 'is a', 'tuned ABS model']]",[],[],"[['Baselines', 'has', 'ABS +']]",[],[],[],[],[],text_summarization,3,177
baselines,RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .,"[('is a', (4, 6)), ('with', (13, 14))]","[('RAS - Elman', (0, 3)), ('convolution encoder', (6, 8)), ('Elman RNN decoder', (10, 13)), ('attention', (14, 15))]","[['RAS - Elman', 'is a', 'convolution encoder'], ['RAS - Elman', 'is a', 'Elman RNN decoder'], ['Elman RNN decoder', 'with', 'attention']]",[],[],"[['Baselines', 'has', 'RAS - Elman']]",[],[],[],[],[],text_summarization,3,179
baselines,Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .,"[('is', (3, 4)), ('equipped with', (15, 17))]","[('Seq2seq + att', (0, 3)), ('two - layer BiLSTM encoder', (4, 9)), ('one - layer LSTM decoder', (10, 15)), ('attention', (17, 18))]","[['Seq2seq + att', 'is', 'two - layer BiLSTM encoder'], ['Seq2seq + att', 'is', 'one - layer LSTM decoder'], ['one - layer LSTM decoder', 'equipped with', 'attention']]",[],[],"[['Baselines', 'has', 'Seq2seq + att']]",[],[],[],[],[],text_summarization,3,180
baselines,lvt5 k - lsent uses temporal attention to keep track of the past attentive weights of the decoder and restrains the repetition in later sequences .,"[('uses', (4, 5)), ('to keep track of', (7, 11)), ('of', (15, 16)), ('restrains', (19, 20)), ('in', (22, 23))]","[('lvt5 k - lsent', (0, 4)), ('temporal attention', (5, 7)), ('past attentive weights', (12, 15)), ('decoder', (17, 18)), ('repetition', (21, 22)), ('later sequences', (23, 25))]","[['lvt5 k - lsent', 'uses', 'temporal attention'], ['temporal attention', 'to keep track of', 'past attentive weights'], ['past attentive weights', 'of', 'decoder'], ['temporal attention', 'restrains', 'repetition'], ['repetition', 'in', 'later sequences']]",[],[],"[['Baselines', 'has', 'lvt5 k - lsent']]",[],[],[],[],[],text_summarization,3,181
baselines,SEASS includes an additional selective gate to control information flow from the encoder to the decoder .,"[('includes', (1, 2)), ('to control', (6, 8)), ('from', (10, 11)), ('to', (13, 14))]","[('SEASS', (0, 1)), ('additional selective gate', (3, 6)), ('information flow', (8, 10)), ('encoder', (12, 13)), ('decoder', (15, 16))]","[['SEASS', 'includes', 'additional selective gate'], ['additional selective gate', 'to control', 'information flow'], ['information flow', 'from', 'encoder'], ['encoder', 'to', 'decoder']]",[],[],"[['Baselines', 'has', 'SEASS']]",[],[],[],[],[],text_summarization,3,182
baselines,Pointer - generator is an integrated pointer network and seq2seq model .,"[('is', (3, 4))]","[('Pointer - generator', (0, 3)), ('integrated pointer network', (5, 8)), ('seq2seq model', (9, 11))]","[['Pointer - generator', 'is', 'integrated pointer network'], ['Pointer - generator', 'is', 'seq2seq model']]",[],[],"[['Baselines', 'has', 'Pointer - generator']]",[],[],[],[],[],text_summarization,3,183
baselines,CGU ) sets a convolutional gated unit and self - attention for global encoding .,"[('sets', (2, 3)), ('for', (11, 12))]","[('CGU', (0, 1)), ('convolutional gated unit', (4, 7)), ('self - attention', (8, 11)), ('global encoding', (12, 14))]","[['CGU', 'for', 'global encoding'], ['global encoding', 'sets', 'convolutional gated unit'], ['global encoding', 'sets', 'self - attention']]",[],[],"[['Baselines', 'has', 'CGU']]",[],[],[],[],[],text_summarization,3,186
results,We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .,"[('observe', (1, 2))]","[('model', (4, 5))]",[],[],"[['Results', 'observe', 'model']]",[],[],[],[],[],"[['model', 'outperformed', 'all the strong state of - the - art models'], ['model', 'outperformed', 'in all metrics']]",text_summarization,3,191
results,"In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .","[('In terms of', (0, 3)), ('improvements made', (9, 11)), ('across', (23, 24))]","[('pointer generator performance', (4, 7)), ('concept pointer', (13, 15)), ('statistically significant ( p < 0.01 )', (16, 23)), ('all metrics', (24, 26))]","[['concept pointer', 'improvements made', 'statistically significant ( p < 0.01 )'], ['statistically significant ( p < 0.01 )', 'across', 'all metrics']]","[['pointer generator performance', 'has', 'concept pointer']]","[['Results', 'In terms of', 'pointer generator performance']]",[],[],[],[],[],[],text_summarization,3,192
research-problem,Entity Commonsense Representation for Neural Abstractive Summarization,[],"[('Abstractive Summarization', (5, 7))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Summarization']]",[],[],[],[],text_summarization,4,2
research-problem,Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .,[],"[('Text summarization', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text summarization']]",[],[],[],[],text_summarization,4,13
model,"To this end , we present a method to effectively apply linked entities in sequence - tosequence models , called Entity2Topic ( E2T ) .","[('method', (7, 8)), ('in', (13, 14)), ('called', (19, 20))]","[('effectively apply linked entities', (9, 13)), ('sequence - tosequence models', (14, 18)), ('Entity2Topic ( E2T )', (20, 24))]","[['effectively apply linked entities', 'in', 'sequence - tosequence models'], ['sequence - tosequence models', 'called', 'Entity2Topic ( E2T )']]",[],"[['Model', 'method', 'effectively apply linked entities']]",[],[],[],[],[],[],text_summarization,4,31
model,E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .,"[('module', (3, 4))]","[('E2T', (0, 1))]",[],[],"[['Model', 'module', 'E2T']]",[],[],[],[],[],[],text_summarization,4,32
model,"The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .","[('encodes', (2, 3)), ('by', (10, 11)), ('constructs', (19, 20)), ('representing', (22, 23)), ('of', (25, 26)), ('informs', (33, 34)), ('about', (36, 37))]","[('entities extracted from the original text', (4, 10)), ('entity linking system ( ELS )', (12, 18)), ('vector', (21, 22)), ('topic', (24, 25)), ('summary to be generated', (27, 31)), ('decoder', (35, 36)), ('constructed topic vector', (38, 41))]","[['entities extracted from the original text', 'by', 'entity linking system ( ELS )'], ['vector', 'representing', 'topic'], ['topic', 'of', 'summary to be generated'], ['decoder', 'about', 'constructed topic vector']]",[],[],[],[],"[['E2T', 'encodes', 'entities extracted from the original text'], ['E2T', 'constructs', 'vector'], ['E2T', 'informs', 'decoder']]",[],[],[],text_summarization,4,33
model,We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention .,[],[],"[['entity encoders', 'with', 'selective disambiguation'], ['entity encoders', 'constructing', 'topic vectors'], ['topic vectors', 'using', 'firm attention']]",[],"[['Model', 'using', 'entity encoders']]",[],[],[],[],[],[],text_summarization,4,35
experimental-setup,"For both datasets , we further reduce the size of the input , output , and entity vocabularies to at most 50 K as suggested in and replace less frequent words to "" < unk > "" .",[],[],"[['size of the input , output , and entity vocabularies', 'to', 'at most 50 K'], ['less frequent words', 'to', '< unk >']]",[],"[['Experimental setup', 'reduce', 'size of the input , output , and entity vocabularies'], ['Experimental setup', 'replace', 'less frequent words']]",[],[],[],[],[],[],text_summarization,4,177
experimental-setup,We use 300D Glove 6 and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors .,"[('use', (1, 2)), ('initialize', (12, 13))]","[('300D Glove', (2, 4)), ('1000D wiki2vec', (6, 8)), ('pre-trained vectors', (9, 11)), ('word and entity vectors', (14, 18))]","[['word and entity vectors', 'use', 'pre-trained vectors']]","[['pre-trained vectors', 'name', '300D Glove'], ['pre-trained vectors', 'name', '1000D wiki2vec']]","[['Experimental setup', 'initialize', 'word and entity vectors']]",[],[],[],[],[],[],text_summarization,4,178
experimental-setup,"For GRUs , we set the state size to 500 .","[('For', (0, 1)), ('set', (4, 5)), ('to', (8, 9))]","[('GRUs', (1, 2)), ('state size', (6, 8)), ('500', (9, 10))]","[['GRUs', 'set', 'state size'], ['state size', 'to', '500']]",[],"[['Experimental setup', 'For', 'GRUs']]",[],[],[],[],[],[],text_summarization,4,179
experimental-setup,"For CNN , we set h = 3 , 4 , 5 with 400 , 300 , 300 feature maps , respectively .","[('set', (4, 5)), ('with', (12, 13))]","[('CNN', (1, 2)), ('h = 3 , 4 , 5', (5, 12)), ('400 , 300 , 300 feature maps', (13, 20))]","[['CNN', 'set', 'h = 3 , 4 , 5'], ['h = 3 , 4 , 5', 'with', '400 , 300 , 300 feature maps']]",[],[],"[['Experimental setup', 'For', 'CNN']]",[],[],[],[],[],text_summarization,4,180
experimental-setup,"For firm attention , k is tuned by calculating the perplexity of the model starting with smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ) and stopping when the perplexity of the model becomes worse than the previous model .","[('tuned', (6, 7)), ('by calculating', (7, 9)), ('of', (11, 12)), ('starting with', (14, 16)), ('stopping when', (35, 37)), ('than', (44, 45))]","[('firm attention', (1, 3)), ('k', (4, 5)), ('perplexity', (10, 11)), ('model', (13, 14)), ('smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... )', (16, 34)), ('perplexity of the model becomes worse', (38, 44)), ('previous model', (46, 48))]","[['firm attention', 'tuned', 'k'], ['k', 'by calculating', 'perplexity'], ['perplexity', 'of', 'model'], ['model', 'starting with', 'smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... )'], ['model', 'stopping when', 'perplexity of the model becomes worse'], ['perplexity of the model becomes worse', 'than', 'previous model']]",[],[],"[['Experimental setup', 'For', 'firm attention']]",[],[],[],[],[],text_summarization,4,181
experimental-setup,We use dropout on all non-linear connections with a dropout rate of 0.5 .,"[('use', (1, 2)), ('on', (3, 4)), ('with', (7, 8))]","[('dropout', (2, 3)), ('all non-linear connections', (4, 7)), ('dropout rate of 0.5', (9, 13))]","[['dropout', 'on', 'all non-linear connections'], ['all non-linear connections', 'with', 'dropout rate of 0.5']]",[],"[['Experimental setup', 'use', 'dropout']]",[],[],[],[],[],[],text_summarization,4,183
experimental-setup,"We set the batch sizes of Gigaword and CNN datasets to 80 and 10 , respectively .","[('set', (1, 2)), ('of', (5, 6)), ('to', (10, 11))]","[('batch sizes', (3, 5)), ('Gigaword and CNN datasets', (6, 10)), ('80 and 10', (11, 14))]","[['batch sizes', 'of', 'Gigaword and CNN datasets'], ['Gigaword and CNN datasets', 'to', '80 and 10']]",[],"[['Experimental setup', 'set', 'batch sizes']]",[],[],[],[],[],[],text_summarization,4,184
experimental-setup,"Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule , with l 2 constraint ( Hinton et al. , 2012 ) of 3 .","[('Training is done via', (0, 4)), ('over', (7, 8)), ('with', (10, 11))]","[('stochastic gradient descent', (4, 7)), ('shuffled mini-batches', (8, 10)), ('Adadelta update rule', (12, 15)), ('l 2 constraint ( Hinton et al. , 2012 ) of 3', (17, 29))]","[['stochastic gradient descent', 'over', 'shuffled mini-batches'], ['shuffled mini-batches', 'with', 'Adadelta update rule'], ['shuffled mini-batches', 'with', 'l 2 constraint ( Hinton et al. , 2012 ) of 3']]",[],"[['Experimental setup', 'Training is done via', 'stochastic gradient descent']]",[],[],[],[],[],[],text_summarization,4,185
experimental-setup,We perform early stopping using a subset of the given development dataset .,"[('perform', (1, 2)), ('using', (4, 5))]","[('early stopping', (2, 4)), ('subset of the given development dataset', (6, 12))]","[['early stopping', 'using', 'subset of the given development dataset']]",[],"[['Experimental setup', 'perform', 'early stopping']]",[],[],[],[],[],[],text_summarization,4,186
experimental-setup,We use beam search of size 10 to generate the summary .,"[('of size', (4, 6)), ('to generate', (7, 9))]","[('beam search', (2, 4)), ('10', (6, 7)), ('summary', (10, 11))]","[['beam search', 'of size', '10'], ['10', 'to generate', 'summary']]",[],[],"[['Experimental setup', 'use', 'beam search']]",[],[],[],[],[],text_summarization,4,187
baselines,"For the Gigaword dataset , we compare our models with the following abstractive baselines :","[('For', (0, 1)), ('compare our models with', (6, 10))]","[('Gigaword dataset', (2, 4))]",[],[],"[['Baselines', 'For', 'Gigaword dataset']]",[],[],[],"[['Gigaword dataset', 'compare our models with', 'Feat2s'], ['Gigaword dataset', 'compare our models with', 'Luong - NMT'], ['Gigaword dataset', 'compare our models with', 'RAS - Elman'], ['Gigaword dataset', 'compare our models with', 'ABS +'], ['Gigaword dataset', 'compare our models with', 'SEASS']]",[],[],text_summarization,4,189
baselines,"ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .",[],[],"[['Feat2s', 'is an', 'RNN sequence - to - sequence model'], ['RNN sequence - to - sequence model', 'with', 'lexical and statistical features'], ['lexical and statistical features', 'in', 'encoder'], ['Luong - NMT', 'is a', 'two - layer LSTM encoder - decoder model'], ['RAS - Elman', 'uses', 'attentive CNN encoder'], ['RAS - Elman', 'uses', 'Elman RNN decoder'], ['ABS +', 'is a', 'fine tuned version of ABS'], ['fine tuned version of ABS', 'uses', 'attentive CNN encoder'], ['fine tuned version of ABS', 'uses', 'NNLM decoder'], ['SEASS', 'uses', 'BiGRU encoders'], ['SEASS', 'uses', 'GRU decoders'], ['GRU decoders', 'with', 'selective encoding']]",[],[],[],[],[],[],[],[],text_summarization,4,190
baselines,"For the CNN dataset , we compare our models with the following extractive and abstractive baselines :","[('compare our models with', (6, 10))]","[('CNN dataset', (2, 4))]",[],[],[],"[['Baselines', 'For', 'CNN dataset']]",[],[],"[['CNN dataset', 'compare our models with', 'Distraction - M3'], ['CNN dataset', 'compare our models with', 'GBA'], ['CNN dataset', 'compare our models with', 'Lead - 3'], ['CNN dataset', 'compare our models with', 'Bi - GRU'], ['CNN dataset', 'compare our models with', 'LexRank']]",[],[],text_summarization,4,191
baselines,"Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .",[],[],"[['Distraction - M3', 'uses', 'sequence - to - sequence abstractive model'], ['sequence - to - sequence abstractive model', 'with', 'distraction - based networks'], ['GBA', 'is a', 'graph - based attentional neural abstractive model'], ['Lead - 3', 'extracts', 'first three sentences of the document'], ['first three sentences of the document', 'as', 'summary'], ['Bi - GRU', 'is a', 'non-hierarchical one - layer sequence - to - sequence abstractive baseline'], ['LexRank', 'extracts', 'texts'], ['texts', 'using', 'LexRank']]",[],[],[],[],[],[],[],[],text_summarization,4,192
results,"In Gigaword dataset where the texts are short , our best model achieves a comparable performance with the current state - of - the - art .","[('In', (0, 1)), ('where', (3, 4)), ('achieves', (12, 13)), ('with', (16, 17))]","[('Gigaword dataset', (1, 3)), ('texts are short', (5, 8)), ('our best model', (9, 12)), ('comparable performance', (14, 16)), ('current state - of - the - art', (18, 26))]","[['Gigaword dataset', 'where', 'texts are short'], ['our best model', 'achieves', 'comparable performance'], ['comparable performance', 'with', 'current state - of - the - art']]","[['Gigaword dataset', 'has', 'our best model']]","[['Results', 'In', 'Gigaword dataset']]",[],[],[],[],[],[],text_summarization,4,198
results,"In CNN dataset where the texts are longer , our best model outperforms all the previous models .","[('where', (3, 4)), ('outperforms', (12, 13))]","[('CNN dataset', (1, 3)), ('texts are longer', (5, 8)), ('our best model', (9, 12)), ('all the previous models', (13, 17))]","[['CNN dataset', 'where', 'texts are longer'], ['our best model', 'outperforms', 'all the previous models']]","[['CNN dataset', 'has', 'our best model']]",[],"[['Results', 'In', 'CNN dataset']]",[],[],[],[],[],text_summarization,4,199
results,"Overall , E2T achieves a significant improvement over the baseline model BASE , with at least 2 ROUGE - 1 points increase in the Gigaword dataset and 6 ROUGE - 1 points increase in the CNN dataset .",[],[],"[['E2T', 'achieves', 'significant improvement'], ['significant improvement', 'over', 'baseline model BASE'], ['baseline model BASE', 'with', 'at least 2 ROUGE'], ['1 points increase', 'in', 'Gigaword dataset'], ['baseline model BASE', 'with', '6 ROUGE'], ['1 points increase', 'in', 'CNN dataset']]","[['at least 2 ROUGE', 'has', '1 points increase'], ['6 ROUGE', 'has', '1 points increase']]",[],"[['Results', 'has', 'E2T']]",[],[],[],[],[],text_summarization,4,201
results,"Among the model variants , the CNN - based encoder with selective disambiguation and firm attention performs the best .","[('Among', (0, 1)), ('with', (10, 11)), ('performs', (16, 17))]","[('model variants', (2, 4)), ('CNN - based encoder', (6, 10)), ('selective disambiguation', (11, 13)), ('firm attention', (14, 16)), ('best', (18, 19))]","[['CNN - based encoder', 'with', 'selective disambiguation'], ['CNN - based encoder', 'with', 'firm attention'], ['CNN - based encoder', 'performs', 'best']]","[['model variants', 'has', 'CNN - based encoder']]","[['Results', 'Among', 'model variants']]",[],[],[],[],[],[],text_summarization,4,203
research-problem,"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization",[],"[('Neural Summarization', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Summarization']]",[],[],[],[],text_summarization,5,2
research-problem,"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .",[],"[('abstractive sentence summarization', (14, 17))]",[],[],[],[],"[['Contribution', 'has research problem', 'abstractive sentence summarization']]",[],[],[],[],text_summarization,5,12
approach,"Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .","[('combine', (18, 19))]","[('seq2seq and template based summarization approaches', (20, 26))]",[],[],"[['Approach', 'combine', 'seq2seq and template based summarization approaches']]",[],[],[],[],[],[],text_summarization,5,30
approach,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .","[('call', (1, 2)), ('consists of', (10, 12))]","[('summarization system', (3, 5)), ('Re 3 Sum', (5, 8)), ('three modules', (12, 14)), ('Retrieve', (15, 16)), ('Rerank', (17, 18)), ('Rewrite', (19, 20))]","[['Re 3 Sum', 'consists of', 'three modules']]","[['summarization system', 'name', 'Re 3 Sum'], ['three modules', 'name', 'Retrieve'], ['three modules', 'name', 'Rerank'], ['three modules', 'name', 'Rewrite']]","[['Approach', 'call', 'summarization system']]",[],[],[],[],[],[],text_summarization,5,31
approach,We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .,"[('utilize', (1, 2)), ('to find out', (12, 15)), ('from', (18, 19))]","[('widely - used Information Retrieval ( IR ) platform', (3, 12)), ('candidate soft templates', (15, 18)), ('training corpus', (20, 22))]","[['widely - used Information Retrieval ( IR ) platform', 'to find out', 'candidate soft templates'], ['candidate soft templates', 'from', 'training corpus']]",[],"[['Approach', 'utilize', 'widely - used Information Retrieval ( IR ) platform']]",[],[],[],[],[],[],text_summarization,5,32
approach,"Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .","[('extend', (3, 4)), ('to jointly learn', (7, 10))]","[('seq2seq model', (5, 7)), ('template saliency measurement ( Rerank )', (10, 16)), ('final summary generation ( Rewrite )', (17, 23))]","[['seq2seq model', 'to jointly learn', 'template saliency measurement ( Rerank )'], ['seq2seq model', 'to jointly learn', 'final summary generation ( Rewrite )']]",[],"[['Approach', 'extend', 'seq2seq model']]",[],[],[],[],[],[],text_summarization,5,33
approach,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and each candidate template into hidden states .","[('applied to', (11, 13)), ('into', (21, 22))]","[('Recurrent Neural Network ( RNN ) encoder', (3, 10)), ('convert', (13, 14)), ('input sentence', (15, 17)), ('each candidate template', (18, 21)), ('hidden states', (22, 24))]","[['Recurrent Neural Network ( RNN ) encoder', 'applied to', 'convert'], ['convert', 'into', 'hidden states']]","[['hidden states', 'name', 'input sentence'], ['hidden states', 'name', 'each candidate template']]",[],"[['Approach', 'has', 'Recurrent Neural Network ( RNN ) encoder']]",[],[],[],[],[],text_summarization,5,34
approach,"In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .","[('In', (0, 1)), ('measure', (4, 5)), ('of', (7, 8)), ('according to', (11, 13)), ('to', (17, 18))]","[('Rerank', (1, 2)), ('informativeness', (6, 7)), ('candidate template', (9, 11)), ('hidden state relevance', (14, 17)), ('input sentence', (19, 21))]","[['Rerank', 'measure', 'informativeness'], ['informativeness', 'of', 'candidate template'], ['candidate template', 'according to', 'hidden state relevance'], ['hidden state relevance', 'to', 'input sentence']]",[],"[['Approach', 'In', 'Rerank']]",[],[],[],[],[],[],text_summarization,5,35
approach,The candidate template with the highest predicted informativeness is regarded as the actual soft template .,"[('with', (3, 4)), ('is regarded as', (8, 11))]","[('highest predicted informativeness', (5, 8)), ('actual soft template', (12, 15))]","[['highest predicted informativeness', 'is regarded as', 'actual soft template']]",[],[],[],[],"[['candidate template', 'with', 'highest predicted informativeness']]",[],[],[],text_summarization,5,36
approach,"In Rewrite , the summary is generated according to the hidden states of both the sentence and template .","[('generated', (6, 7)), ('according to', (7, 9)), ('of both', (12, 14))]","[('Rewrite', (1, 2)), ('summary', (4, 5)), ('hidden states', (10, 12)), ('sentence and template', (15, 18))]","[['Rewrite', 'generated', 'summary'], ['summary', 'according to', 'hidden states'], ['hidden states', 'of both', 'sentence and template']]",[],[],"[['Approach', 'In', 'Rewrite']]",[],[],[],[],[],text_summarization,5,37
code,Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,[],"[('http://www4.comp.polyu.edu.hk/cszqcao/', (7, 8))]",[],[],[],[],"[['Contribution', 'Code', 'http://www4.comp.polyu.edu.hk/cszqcao/']]",[],[],[],[],text_summarization,5,43
experimental-setup,We use the popular seq2seq framework Open - NMT 5 as the starting point .,"[('use', (1, 2))]","[('popular seq2seq framework', (3, 6)), ('Open - NMT', (6, 9))]",[],"[['popular seq2seq framework', 'name', 'Open - NMT']]","[['Experimental setup', 'use', 'popular seq2seq framework']]",[],[],[],[],[],[],text_summarization,5,139
experimental-setup,"To make our model more general , we retain the default settings of Open NMT to build the network architecture .","[('retain', (8, 9)), ('of', (12, 13)), ('to build', (15, 17))]","[('default settings', (10, 12)), ('Open NMT', (13, 15)), ('network architecture', (18, 20))]","[['default settings', 'of', 'Open NMT'], ['Open NMT', 'to build', 'network architecture']]",[],"[['Experimental setup', 'retain', 'default settings']]",[],[],[],[],[],[],text_summarization,5,140
experimental-setup,"Specifically , the dimensions of word embeddings and RNN are both 500 , and the encoder and decoder structures are two - layer bidirectional Long Short Term Memory Networks ( LSTMs ) .","[('dimensions', (3, 4)), ('are both', (9, 11)), ('are', (19, 20))]","[('word embeddings and RNN', (5, 9)), ('500', (11, 12)), ('encoder and decoder structures', (15, 19)), ('two - layer bidirectional Long Short Term Memory Networks ( LSTMs )', (20, 32))]","[['encoder and decoder structures', 'are', 'two - layer bidirectional Long Short Term Memory Networks ( LSTMs )'], ['word embeddings and RNN', 'are both', '500']]",[],"[['Experimental setup', 'dimensions', 'word embeddings and RNN']]","[['Experimental setup', 'has', 'encoder and decoder structures']]",[],[],[],[],[],text_summarization,5,141
experimental-setup,"On our computer ( GPU : GTX 1080 , Memory : 16G , CPU : i7-7700 K ) , the training spends about 2 days .","[('On', (0, 1)), ('GPU', (4, 5)), ('Memory', (9, 10)), ('CPU', (13, 14)), ('training spends', (20, 22))]","[('our computer', (1, 3)), ('GTX 1080', (6, 8)), ('16G', (11, 12)), ('i7-7700 K', (15, 17)), ('about 2 days', (22, 25))]","[['our computer', 'Memory', '16G'], ['our computer', 'CPU', 'i7-7700 K'], ['our computer', 'GPU', 'GTX 1080'], ['our computer', 'training spends', 'about 2 days']]",[],"[['Experimental setup', 'On', 'our computer']]",[],[],[],[],[],[],text_summarization,5,144
experimental-setup,"During test , we use beam search of size 5 to generate summaries .","[('During test', (0, 2)), ('of size', (7, 9)), ('to generate', (10, 12))]","[('beam search', (5, 7)), ('5', (9, 10)), ('summaries', (12, 13))]","[['beam search', 'of size', '5'], ['5', 'to generate', 'summaries']]",[],"[['Experimental setup', 'During test', 'beam search']]",[],[],[],[],[],[],text_summarization,5,145
experimental-setup,"We add the argument "" replace unk "" to replace the generated unknown words with the source word that holds the highest attention weight .","[('add', (1, 2)), ('to replace', (8, 10)), ('with', (14, 15)), ('that holds', (18, 20))]","[('argument', (3, 4)), ('replace unk', (5, 7)), ('generated unknown words', (11, 14)), ('source word', (16, 18)), ('highest attention weight', (21, 24))]","[['replace unk', 'to replace', 'generated unknown words'], ['generated unknown words', 'with', 'source word'], ['source word', 'that holds', 'highest attention weight']]","[['argument', 'name', 'replace unk']]","[['Experimental setup', 'add', 'argument']]",[],[],[],[],[],[],text_summarization,5,146
experimental-setup,"Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument "" alpha 1 "" to encourage longer generation , like .","[('introduce', (13, 14)), ('to encourage', (23, 25))]","[('additional length penalty argument', (15, 19)), ('alpha 1', (20, 22)), ('longer generation', (25, 27))]","[['alpha 1', 'to encourage', 'longer generation']]","[['additional length penalty argument', 'name', 'alpha 1']]","[['Experimental setup', 'introduce', 'additional length penalty argument']]",[],[],[],[],[],[],text_summarization,5,147
baselines,OpenNMT,[],"[('OpenNMT', (0, 1))]",[],[],[],"[['Baselines', 'has', 'OpenNMT']]",[],[],[],[],[],text_summarization,5,151
baselines,We also implement the standard attentional seq2seq model with OpenNMT .,"[('implement', (2, 3))]","[('standard attentional seq2seq model', (4, 8))]",[],[],[],[],[],"[['OpenNMT', 'implement', 'standard attentional seq2seq model']]",[],[],[],text_summarization,5,152
baselines,FTSum encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries .,"[('encoded', (1, 2)), ('extracted from', (4, 6)), ('to improve', (9, 11)), ('both', (11, 12))]","[('FTSum', (0, 1)), ('facts', (3, 4)), ('source sentence', (7, 9)), ('faithfulness', (13, 14)), ('informativeness', (15, 16)), ('generated summaries', (17, 19))]","[['FTSum', 'encoded', 'facts'], ['facts', 'extracted from', 'source sentence'], ['source sentence', 'to improve', 'generated summaries'], ['generated summaries', 'both', 'faithfulness'], ['generated summaries', 'both', 'informativeness']]",[],[],"[['Baselines', 'has', 'FTSum']]",[],[],[],[],[],text_summarization,5,156
baselines,"In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named "" PIPELINE "" .",[],"[('PIPELINE', (19, 20))]",[],[],[],"[['Baselines', 'has', 'PIPELINE']]",[],[],[],[],[],text_summarization,5,157
baselines,"However , it trains the Rerank module and Rewrite module in pipeline .","[('trains', (3, 4))]","[('Rerank module', (5, 7)), ('Rewrite module', (8, 10))]",[],[],[],[],[],"[['PIPELINE', 'trains', 'Rerank module'], ['PIPELINE', 'trains', 'Rewrite module']]",[],[],[],text_summarization,5,159
results,We also examine the performance of directly regarding soft templates as output summaries .,"[('examine', (2, 3)), ('of directly regarding', (5, 8)), ('as', (10, 11))]","[('performance', (4, 5)), ('soft templates', (8, 10)), ('output summaries', (11, 13))]","[['performance', 'of directly regarding', 'soft templates'], ['soft templates', 'as', 'output summaries']]",[],"[['Results', 'examine', 'performance']]",[],[],[],[],[],[],text_summarization,5,163
results,We introduce five types of different soft templates :,"[('introduce', (1, 2))]","[('five types of different soft templates', (2, 8))]",[],[],[],[],[],"[['soft templates', 'introduce', 'five types of different soft templates']]",[],[],[],text_summarization,5,164
results,"As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .","[('performance of', (5, 7)), ('is', (8, 9))]","[('Random', (7, 8)), ('terrible', (9, 10))]","[['Random', 'is', 'terrible']]",[],[],[],[],"[['five types of different soft templates', 'performance of', 'Random']]",[],[],[],text_summarization,5,176
results,"Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .","[('largely outperforms', (1, 3))]","[('Rerank', (0, 1)), ('First', (3, 4))]","[['Rerank', 'largely outperforms', 'First']]",[],[],[],[],[],[],"[['five types of different soft templates', 'has', 'Rerank']]",[],text_summarization,5,177
results,"Likewise , comparing Max and First , we observe that the improving capacity of the Retrieve module is high .","[('comparing', (2, 3)), ('observe that', (8, 10)), ('of', (13, 14)), ('is', (17, 18))]","[('Max and First', (3, 6)), ('improving capacity', (11, 13)), ('Retrieve module', (15, 17)), ('high', (18, 19))]","[['Max and First', 'observe that', 'improving capacity'], ['improving capacity', 'of', 'Retrieve module'], ['Retrieve module', 'is', 'high']]",[],[],[],[],"[['five types of different soft templates', 'comparing', 'Max and First']]",[],[],[],text_summarization,5,179
results,Notice that Optimal greatly exceeds all the state - of - the - art approaches .,"[('greatly exceeds', (3, 5))]","[('Optimal', (2, 3)), ('all the state - of - the - art approaches', (5, 15))]","[['Optimal', 'greatly exceeds', 'all the state - of - the - art approaches']]",[],[],[],[],[],[],"[['five types of different soft templates', 'has', 'Optimal']]",[],text_summarization,5,180
results,"We also measure the linguistic quality of generated summaries from various aspects , and the results are present in .","[('measure', (2, 3)), ('of', (6, 7))]","[('linguistic quality', (4, 6)), ('generated summaries', (7, 9))]","[['linguistic quality', 'of', 'generated summaries']]",[],"[['Results', 'measure', 'linguistic quality']]",[],[],[],[],[],[],text_summarization,5,183
results,"As can be seen from the rows "" LEN DIF "" and "" LESS 3 "" , the performance of Re 3 Sum is almost the same as that of soft templates .","[('performance of', (18, 20)), ('almost the same as', (24, 28))]","[('Re 3 Sum', (20, 23)), ('soft templates', (30, 32))]","[['Re 3 Sum', 'almost the same as', 'soft templates']]",[],[],[],[],"[['linguistic quality', 'performance of', 'Re 3 Sum']]",[],[],[],text_summarization,5,184
results,"In this section , we investigate how soft templates affect our model .","[('investigate', (5, 6)), ('affect', (9, 10))]","[('soft templates', (7, 9)), ('our model', (10, 12))]","[['soft templates', 'affect', 'our model']]",[],"[['Results', 'investigate', 'soft templates']]",[],[],[],[],[],[],text_summarization,5,204
results,"As illustrated in , the more high - quality templates are provided , the higher ROUGE scores are achieved .","[('provided', (11, 12)), ('achieved', (18, 19))]","[('more high - quality templates', (5, 10)), ('higher ROUGE scores', (14, 17))]","[['more high - quality templates', 'achieved', 'higher ROUGE scores']]",[],[],[],[],"[['soft templates', 'provided', 'more high - quality templates']]",[],[],[],text_summarization,5,206
results,"Next , we manually inspect the summaries generated by different methods .","[('manually inspect', (3, 5)), ('generated by', (7, 9))]","[('summaries', (6, 7)), ('different methods', (9, 11))]","[['summaries', 'generated by', 'different methods']]",[],[],[],[],"[['soft templates', 'manually inspect', 'summaries']]",[],[],[],text_summarization,5,210
results,We find the outputs of Re 3 Sum are usually longer and more flu - ent than the outputs of OpenNMT .,"[('find', (1, 2)), ('longer and more flu - ent than', (10, 17))]","[('outputs of Re 3 Sum', (3, 8)), ('outputs of OpenNMT', (18, 21))]","[['outputs of Re 3 Sum', 'longer and more flu - ent than', 'outputs of OpenNMT']]",[],[],[],[],"[['summaries', 'find', 'outputs of Re 3 Sum']]",[],[],[],text_summarization,5,211
results,"As can be seen , with different templates given , our model is likely to generate dissimilar summaries .","[('with', (5, 6)), ('likely to generate', (13, 16))]","[('different templates given', (6, 9)), ('our model', (10, 12)), ('dissimilar summaries', (16, 18))]","[['our model', 'likely to generate', 'dissimilar summaries']]","[['different templates given', 'has', 'our model']]",[],[],[],"[['soft templates', 'with', 'different templates given']]",[],[],[],text_summarization,5,222
research-problem,Deep Recurrent Generative Decoder for Abstractive Text Summarization,[],"[('Abstractive Text Summarization', (5, 8))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Text Summarization']]",[],[],[],[],text_summarization,6,2
research-problem,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,[],"[('Automatic summarization', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Automatic summarization']]",[],[],[],[],text_summarization,6,11
model,"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .","[('design', (8, 9)), ('based on', (12, 14)), ('equipped with', (23, 25))]","[('new framework', (10, 12)), ('sequence to - sequence oriented encoder - decoder model', (14, 23)), ('latent structure modeling component', (26, 30))]","[['new framework', 'based on', 'sequence to - sequence oriented encoder - decoder model'], ['sequence to - sequence oriented encoder - decoder model', 'equipped with', 'latent structure modeling component']]",[],"[['Model', 'design', 'new framework']]",[],[],[],[],[],[],text_summarization,6,26
model,We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .,"[('employ', (1, 2)), ('as', (9, 10)), ('for', (13, 14)), ('can handle', (18, 20)), ('associated with', (23, 25))]","[('Variational Auto - Encoders ( VAEs )', (2, 9)), ('base model', (11, 13)), ('our generative framework', (14, 17)), ('inference problem', (21, 23)), ('complex generative modeling', (25, 28))]","[['Variational Auto - Encoders ( VAEs )', 'as', 'base model'], ['base model', 'for', 'our generative framework'], ['our generative framework', 'can handle', 'inference problem'], ['inference problem', 'associated with', 'complex generative modeling']]",[],"[['Model', 'employ', 'Variational Auto - Encoders ( VAEs )']]",[],[],[],[],[],[],text_summarization,6,27
model,"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .","[('add', (4, 5)), ('on', (7, 8)), ('of', (11, 12)), ('propose', (14, 15)), ('for', (23, 24))]","[('historical dependencies', (5, 7)), ('latent variables', (9, 11)), ('VAEs', (12, 13)), ('deep recurrent generative decoder ( DRGD )', (16, 23)), ('latent structure modeling', (24, 27))]","[['historical dependencies', 'on', 'latent variables'], ['latent variables', 'of', 'VAEs'], ['deep recurrent generative decoder ( DRGD )', 'for', 'latent structure modeling']]",[],"[['Model', 'add', 'historical dependencies'], ['Model', 'propose', 'deep recurrent generative decoder ( DRGD )']]",[],[],[],[],[],[],text_summarization,6,29
model,Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .,"[('integrated', (12, 13)), ('into', (13, 14))]","[('standard discriminative deterministic decoder and the recurrent generative decoder', (2, 11)), ('unified decoding framework', (15, 18))]","[['standard discriminative deterministic decoder and the recurrent generative decoder', 'into', 'unified decoding framework']]",[],"[['Model', 'integrated', 'standard discriminative deterministic decoder and the recurrent generative decoder']]",[],[],[],[],[],[],text_summarization,6,30
model,The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .,"[('decoded', (5, 6)), ('based on both', (6, 9))]","[('target summaries', (1, 3)), ('discriminative deterministic variables', (10, 13)), ('generative latent structural information', (15, 19))]","[['target summaries', 'based on both', 'discriminative deterministic variables'], ['target summaries', 'based on both', 'generative latent structural information']]",[],"[['Model', 'decoded', 'target summaries']]",[],[],[],[],[],[],text_summarization,6,31
research-problem,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,[],[],[],[],[],[],[],[],[],[],[],text_summarization,6,39
baselines,TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .,"[('for', (9, 10))]","[('TOPIARY', (0, 1)), ('compressive text summarization', (10, 13))]","[['TOPIARY', 'for', 'compressive text summarization']]",[],[],"[['Baselines', 'has', 'TOPIARY']]",[],[],[],[],[],text_summarization,6,192
baselines,It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .,"[('combines', (1, 2))]","[('system using linguistic based transformations', (3, 8)), ('an unsupervised topic detection algorithm', (9, 14))]",[],[],[],[],[],"[['compressive text summarization', 'combines', 'system using linguistic based transformations'], ['compressive text summarization', 'combines', 'an unsupervised topic detection algorithm']]",[],[],[],text_summarization,6,193
baselines,MOSES + uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries .,"[('uses', (2, 3)), ('trained on', (9, 11)), ('to produce', (12, 14))]","[('MOSES +', (0, 2)), ('phrasebased statistical machine translation system', (4, 9)), ('Gigaword', (11, 12)), ('summaries', (14, 15))]","[['MOSES +', 'uses', 'phrasebased statistical machine translation system'], ['phrasebased statistical machine translation system', 'trained on', 'Gigaword'], ['Gigaword', 'to produce', 'summaries']]",[],[],"[['Baselines', 'has', 'MOSES +']]",[],[],[],[],[],text_summarization,6,194
baselines,ABS and ABS + are both the neural network based models with local attention modeling for abstractive sentence summarization .,"[('with', (11, 12)), ('for', (15, 16))]","[('ABS and ABS +', (0, 4)), ('local attention modeling', (12, 15)), ('abstractive sentence summarization', (16, 19))]","[['ABS and ABS +', 'with', 'local attention modeling'], ['local attention modeling', 'for', 'abstractive sentence summarization']]",[],[],"[['Baselines', 'has', 'ABS and ABS +']]",[],[],[],[],[],text_summarization,6,196
baselines,"ABS + is trained on the Gigaword corpus , but combined with an additional log - linear extractive summarization model with handcrafted features .","[('trained on', (3, 5)), ('combined with', (10, 12)), ('with', (20, 21))]","[('ABS +', (0, 2)), ('Gigaword corpus', (6, 8)), ('additional log - linear extractive summarization model', (13, 20)), ('handcrafted features', (21, 23))]","[['ABS +', 'combined with', 'additional log - linear extractive summarization model'], ['additional log - linear extractive summarization model', 'with', 'handcrafted features'], ['ABS +', 'trained on', 'Gigaword corpus']]",[],[],"[['Baselines', 'has', 'ABS +']]",[],[],[],[],[],text_summarization,6,197
baselines,RNN and RNN - context are two seq2seq architectures .,"[('are', (5, 6))]","[('RNN and RNN - context', (0, 5)), ('two seq2seq architectures', (6, 9))]","[['RNN and RNN - context', 'are', 'two seq2seq architectures']]",[],[],"[['Baselines', 'has', 'RNN and RNN - context']]",[],[],[],[],[],text_summarization,6,198
baselines,Copy Net integrates a copying mechanism into the sequence - to sequence framework .,"[('integrates', (2, 3)), ('into', (6, 7))]","[('Copy Net', (0, 2)), ('copying mechanism', (4, 6)), ('sequence - to sequence framework', (8, 13))]","[['Copy Net', 'integrates', 'copying mechanism'], ['copying mechanism', 'into', 'sequence - to sequence framework']]",[],[],"[['Baselines', 'has', 'Copy Net']]",[],[],[],[],[],text_summarization,6,200
baselines,RNN - distract uses a new attention mechanism by distracting the historical attention in the decoding steps .,"[('uses', (3, 4)), ('by distracting', (8, 10)), ('in', (13, 14))]","[('RNN - distract', (0, 3)), ('new attention mechanism', (5, 8)), ('historical attention', (11, 13)), ('decoding steps', (15, 17))]","[['RNN - distract', 'uses', 'new attention mechanism'], ['new attention mechanism', 'by distracting', 'historical attention'], ['historical attention', 'in', 'decoding steps']]",[],[],"[['Baselines', 'has', 'RNN - distract']]",[],[],[],[],[],text_summarization,6,201
baselines,RAS - LSTM and RAS - Elman both consider words and word positions as input and use convolutional encoders to handle the source information .,"[('consider', (8, 9)), ('as', (13, 14)), ('use', (16, 17)), ('to handle', (19, 21))]","[('RAS - LSTM and RAS - Elman', (0, 7)), ('words and word positions', (9, 13)), ('input', (14, 15)), ('convolutional encoders', (17, 19)), ('source information', (22, 24))]","[['RAS - LSTM and RAS - Elman', 'use', 'convolutional encoders'], ['convolutional encoders', 'to handle', 'source information'], ['RAS - LSTM and RAS - Elman', 'consider', 'words and word positions'], ['words and word positions', 'as', 'input']]",[],[],"[['Baselines', 'has', 'RAS - LSTM and RAS - Elman']]",[],[],[],[],[],text_summarization,6,202
baselines,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .,"[('control', (5, 6)), ('by considering', (9, 11)), ('as', (15, 16))]","[('LenEmb', (0, 1)), ('summary length', (7, 9)), ('length embedding vector', (12, 15)), ('input', (17, 18))]","[['LenEmb', 'control', 'summary length'], ['summary length', 'by considering', 'length embedding vector'], ['length embedding vector', 'as', 'input']]",[],[],"[['Baselines', 'has', 'LenEmb']]",[],[],[],[],[],text_summarization,6,204
baselines,ASC+ FSC 1 ) uses a generative model with attention mechanism to conduct the sentence compression problem .,"[('uses', (4, 5)), ('with', (8, 9)), ('to conduct', (11, 13))]","[('ASC+ FSC', (0, 2)), ('generative model', (6, 8)), ('attention mechanism', (9, 11)), ('sentence compression problem', (14, 17))]","[['ASC+ FSC', 'uses', 'generative model'], ['generative model', 'with', 'attention mechanism'], ['attention mechanism', 'to conduct', 'sentence compression problem']]",[],[],"[['Baselines', 'has', 'ASC+ FSC']]",[],[],[],[],[],text_summarization,6,205
baselines,lvt2k - 1sent and lvt5k - 1sent utilize a trick to control the vocabulary size to improve the training efficiency .,"[('utilize', (7, 8)), ('to control', (10, 12)), ('to improve', (15, 17))]","[('lvt2k - 1sent and lvt5k - 1sent', (0, 7)), ('trick', (9, 10)), ('vocabulary size', (13, 15)), ('training efficiency', (18, 20))]","[['lvt2k - 1sent and lvt5k - 1sent', 'utilize', 'trick'], ['trick', 'to control', 'vocabulary size'], ['vocabulary size', 'to improve', 'training efficiency']]",[],[],"[['Baselines', 'has', 'lvt2k - 1sent and lvt5k - 1sent']]",[],[],[],[],[],text_summarization,6,207
experimental-setup,"For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .",[],[],"[['hidden states and latent variables', 'to', '500'], ['English dataset Gigawords', 'set', 'dimension'], ['dimension', 'of', 'hidden states and latent variables'], ['hidden states and latent variables', 'to', '500'], ['dimension', 'of', 'word embeddings'], ['word embeddings', 'to', '300']]",[],"[['Experimental setup', 'on', 'English dataset Gigawords']]",[],[],[],[],[],"[['English dataset Gigawords', 'has', 'batch size']]",text_summarization,6,209
experimental-setup,The maximum length of documents and summaries is 100 and 50 respectively .,"[('maximum length of', (1, 4)), ('is', (7, 8))]","[('documents and summaries', (4, 7)), ('100 and 50', (8, 11))]","[['documents and summaries', 'is', '100 and 50']]",[],[],[],[],"[['English dataset Gigawords', 'maximum length of', 'documents and summaries']]",[],[],[],text_summarization,6,210
experimental-setup,The batch size of mini-batch training is 256 .,"[('is', (6, 7))]","[('batch size', (1, 3)), ('mini-batch training', (4, 6)), ('256', (7, 8))]","[['mini-batch training', 'is', '256']]","[['batch size', 'of', 'mini-batch training']]",[],[],[],[],[],[],[],text_summarization,6,211
experimental-setup,"For DUC - 2004 , the maximum length of summaries is 75 bytes .","[('For', (0, 1)), ('maximum length of', (6, 9)), ('is', (10, 11))]","[('DUC - 2004', (1, 4)), ('summaries', (9, 10)), ('75 bytes', (11, 13))]","[['DUC - 2004', 'maximum length of', 'summaries'], ['summaries', 'is', '75 bytes']]",[],"[['Experimental setup', 'For', 'DUC - 2004']]",[],[],[],[],[],[],text_summarization,6,212
experimental-setup,"For the dataset of LCSTS , the dimension of word embeddings is 350 .","[('dimension of', (7, 9)), ('is', (11, 12))]","[('dataset of LCSTS', (2, 5)), ('word embeddings', (9, 11)), ('350', (12, 13))]","[['dataset of LCSTS', 'dimension of', 'word embeddings'], ['word embeddings', 'is', '350']]",[],[],"[['Experimental setup', 'For', 'dataset of LCSTS']]",[],[],[],[],[],text_summarization,6,213
experimental-setup,We also set the dimension of hidden states and latent variables to 500 .,[],[],[],[],[],[],[],[],[],"[['dataset of LCSTS', 'dimension of', 'hidden states and latent variables']]",[],text_summarization,6,214
experimental-setup,"The maximum length of documents and summaries is 120 and 25 respectively , and the batch size is also 256 .","[('maximum length of', (1, 4)), ('is', (7, 8))]","[('documents and summaries', (4, 7)), ('120 and 25', (8, 11))]","[['documents and summaries', 'is', '120 and 25']]",[],[],[],[],"[['dataset of LCSTS', 'maximum length of', 'documents and summaries']]",[],[],[],text_summarization,6,215
experimental-setup,The beam size of the decoder was set to be 10 .,"[('beam size of', (1, 4)), ('set to', (7, 9))]","[('decoder', (5, 6)), ('10', (10, 11))]","[['decoder', 'set to', '10']]",[],"[['Experimental setup', 'beam size of', 'decoder']]",[],[],[],[],[],[],text_summarization,6,216
experimental-setup,Adadelta with hyperparameter ? = 0.95 and = 1 e ? 6 is used for gradient based optimization .,"[('with', (1, 2)), ('used for', (13, 15))]","[('Adadelta', (0, 1)), ('hyperparameter ? = 0.95', (2, 6)), ('gradient based optimization', (15, 18))]","[['Adadelta', 'with', 'hyperparameter ? = 0.95'], ['hyperparameter ? = 0.95', 'used for', 'gradient based optimization']]",[],[],"[['Experimental setup', 'has', 'Adadelta']]",[],[],[],[],[],text_summarization,6,217
experimental-setup,"Our neural network based framework is implemented using Theano ( Theano Development Team , 2016 ) .","[('implemented using', (6, 8))]","[('neural network based framework', (1, 5)), ('Theano', (8, 9))]","[['neural network based framework', 'implemented using', 'Theano']]",[],[],"[['Experimental setup', 'has', 'neural network based framework']]",[],[],[],[],[],text_summarization,6,218
results,ROUGE Evaluation,[],"[('ROUGE Evaluation', (0, 2))]",[],[],[],"[['Results', 'has', 'ROUGE Evaluation']]",[],[],[],[],[],text_summarization,6,229
results,The results on the Chinese dataset LCSTS are shown in .,"[('on', (2, 3))]","[('Chinese dataset LCSTS', (4, 7))]",[],[],[],[],[],"[['ROUGE Evaluation', 'on', 'Chinese dataset LCSTS']]",[],[],"[['Chinese dataset LCSTS', 'has', 'Our model DRGD']]",text_summarization,6,230
results,Our model DRGD also achieves the best performance .,"[('achieves', (4, 5))]","[('Our model DRGD', (0, 3)), ('best performance', (6, 8))]","[['Our model DRGD', 'achieves', 'best performance']]",[],[],[],[],[],[],[],[],text_summarization,6,231
research-problem,Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization,[],"[('Neural Abstractive Summarization', (7, 10))]",[],[],[],[],"[['Contribution', 'has research problem', 'Neural Abstractive Summarization']]",[],[],[],[],text_summarization,7,2
research-problem,"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .",[],"[('abstractive summarization ( ABS )', (35, 40))]",[],[],[],[],"[['Contribution', 'has research problem', 'abstractive summarization ( ABS )']]",[],[],[],[],text_summarization,7,8
model,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,"[('of', (3, 4)), ('jointly estimate', (8, 10)), ('that can occur in', (17, 21)), ('during', (23, 24)), ('control', (32, 33)), ('in', (36, 37))]","[('upper-bound frequency', (11, 13)), ('each target vocabulary', (14, 17)), ('summary', (22, 23)), ('encoding process', (25, 27)), ('output words', (34, 36)), ('each decoding step', (37, 40))]","[['upper-bound frequency', 'of', 'each target vocabulary'], ['each target vocabulary', 'that can occur in', 'summary'], ['summary', 'during', 'encoding process'], ['upper-bound frequency', 'control', 'output words'], ['output words', 'in', 'each decoding step']]",[],"[['Model', 'jointly estimate', 'upper-bound frequency']]",[],[],[],[],[],[],text_summarization,7,17
model,We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,"[('additional component', (4, 6))]","[('wordfrequency estimation ( WFE ) sub-model', (8, 14))]",[],[],"[['Model', 'additional component', 'wordfrequency estimation ( WFE ) sub-model']]",[],[],[],[],[],[],text_summarization,7,18
model,The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .,"[('explicitly manages', (3, 5))]","[('WFE sub-model', (1, 3)), ('how many times each word has been generated so far', (5, 15)), ('might be generated in the future', (16, 22))]","[['WFE sub-model', 'explicitly manages', 'how many times each word has been generated so far'], ['WFE sub-model', 'explicitly manages', 'might be generated in the future']]",[],[],"[['Model', 'has', 'WFE sub-model']]",[],[],[],[],[],text_summarization,7,19
research-problem,Bottom - Up Abstractive Summarization,[],"[('Abstractive Summarization', (3, 5))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Summarization']]",[],[],[],[],text_summarization,8,2
research-problem,Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,[],"[('Text summarization', (0, 2))]",[],[],[],[],"[['Contribution', 'has research problem', 'Text summarization']]",[],[],[],[],text_summarization,8,11
research-problem,Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,[],"[('neural abstractive summarization', (8, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'neural abstractive summarization']]",[],[],[],[],text_summarization,8,13
approach,"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .","[('consider', (6, 7))]","[('bottom - up attention', (7, 11))]",[],[],"[['Approach', 'consider', 'bottom - up attention']]",[],[],[],[],[],[],text_summarization,8,29
approach,Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,"[('first selects', (2, 4)), ('for', (7, 8)), ('by this mask', (18, 21))]","[('selection mask', (5, 7)), ('source document', (9, 11)), ('constrains a standard neural model', (13, 18))]","[['selection mask', 'by this mask', 'constrains a standard neural model'], ['selection mask', 'for', 'source document']]",[],"[['Approach', 'first selects', 'selection mask']]",[],[],[],[],[],[],text_summarization,8,30
approach,Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,"[('incorporates', (3, 4)), ('to decide', (9, 11)), ('of', (14, 15))]","[('separate content selection system', (5, 9)), ('relevant aspects', (12, 14)), ('source document', (16, 18))]","[['separate content selection system', 'to decide', 'relevant aspects'], ['relevant aspects', 'of', 'source document']]",[],"[['Approach', 'incorporates', 'separate content selection system']]",[],[],[],[],[],[],text_summarization,8,33
approach,"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .","[('frame', (1, 2)), ('as a', (5, 7)), ('with the objective of', (12, 16)), ('from', (18, 19)), ('part of', (23, 25))]","[('selection task', (3, 5)), ('sequence - tagging problem', (7, 11)), ('identifying tokens', (16, 18)), ('document', (20, 21)), ('summary', (26, 27))]","[['selection task', 'as a', 'sequence - tagging problem'], ['sequence - tagging problem', 'with the objective of', 'identifying tokens'], ['identifying tokens', 'from', 'document'], ['identifying tokens', 'part of', 'summary']]",[],"[['Approach', 'frame', 'selection task']]",[],[],[],[],[],[],text_summarization,8,34
approach,"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .","[('employ', (12, 13)), ('to constrain', (14, 16)), ('to', (18, 19)), ('of', (22, 23)), ('produces', (27, 28))]","[('masking', (13, 14)), ('copying words', (16, 18)), ('selected parts', (20, 22)), ('text', (24, 25)), ('grammatical outputs', (28, 30))]","[['masking', 'to constrain', 'copying words'], ['copying words', 'to', 'selected parts'], ['selected parts', 'of', 'text'], ['masking', 'produces', 'grammatical outputs']]",[],"[['Approach', 'employ', 'masking']]",[],[],[],[],[],[],text_summarization,8,36
experimental-setup,All inference parameters are tuned on a 200 example subset of the validation set .,"[('tuned on', (4, 6))]","[('inference parameters', (1, 3)), ('200 example subset', (7, 10))]","[['inference parameters', 'tuned on', '200 example subset']]",[],[],"[['Experimental setup', 'has', 'inference parameters']]",[],[],[],[],[],text_summarization,8,186
experimental-setup,"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .",[],[],"[['Length penalty parameter', 'ranging from', '0.6 to 1.4'], ['copy mask', 'ranging from', '0.1 to 0.2']]",[],[],"[['Experimental setup', 'has', 'Length penalty parameter'], ['Experimental setup', 'has', 'copy mask']]",[],[],[],[],[],text_summarization,8,187
experimental-setup,The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,[],[],"[['generated summary', 'is set to', '35'], ['35', 'for', 'CNN - DM'], ['generated summary', 'is set to', '6'], ['6', 'for', 'NYT']]",[],"[['Experimental setup', 'minimum length of', 'generated summary']]",[],[],[],[],[],[],text_summarization,8,188
experimental-setup,"The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .","[('set to', (6, 8)), ('to', (17, 18))]","[('coverage penalty parameter', (1, 4)), ('10', (8, 9)), ('copy attention normalization parameter', (12, 16)), ('2', (18, 19))]","[['copy attention normalization parameter', 'to', '2'], ['coverage penalty parameter', 'set to', '10']]",[],[],"[['Experimental setup', 'has', 'copy attention normalization parameter'], ['Experimental setup', 'has', 'coverage penalty parameter']]",[],[],[],[],[],text_summarization,8,190
experimental-setup,"We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .",[],[],"[['AllenNLP', 'for', 'content selector'], ['Open NMT - py', 'for', 'abstractive models']]",[],"[['Experimental setup', 'use', 'AllenNLP'], ['Experimental setup', 'use', 'Open NMT - py']]",[],[],[],[],[],[],text_summarization,8,191
results,"3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .","[('on', (6, 7))]","[('CNN - DM corpus', (8, 12))]",[],[],"[['Results', 'on', 'CNN - DM corpus']]",[],[],[],[],[],[],text_summarization,8,192
results,"We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .","[('using', (4, 5)), ('scores', (9, 10)), ('as a', (12, 14)), ('without requiring', (18, 20))]","[('coverage inference penalty', (6, 9)), ('the same', (10, 12)), ('full coverage mechanism', (14, 17)), ('additional model parameters', (21, 24)), ('model fine - tuning', (25, 29))]","[['coverage inference penalty', 'scores', 'the same'], ['the same', 'as a', 'full coverage mechanism'], ['full coverage mechanism', 'without requiring', 'additional model parameters'], ['full coverage mechanism', 'without requiring', 'model fine - tuning']]",[],[],[],[],"[['CNN - DM corpus', 'using', 'coverage inference penalty']]",[],[],[],text_summarization,8,193
results,"The results with the CopyTransformer and coverage penalty indicate a slight improvement across all three scores , but we observe no significant difference between Pointer - Generator and CopyTransformer with bottom - up attention .","[('with', (2, 3)), ('indicate', (8, 9)), ('across', (12, 13)), ('observe', (19, 20)), ('between', (23, 24))]","[('CopyTransformer and coverage penalty', (4, 8)), ('slight improvement', (10, 12)), ('all three scores', (13, 16)), ('no significant difference', (20, 23)), ('Pointer - Generator and CopyTransformer with bottom - up attention', (24, 34))]","[['CopyTransformer and coverage penalty', 'indicate', 'slight improvement'], ['slight improvement', 'across', 'all three scores'], ['no significant difference', 'between', 'Pointer - Generator and CopyTransformer with bottom - up attention']]",[],"[['Results', 'with', 'CopyTransformer and coverage penalty'], ['Results', 'observe', 'no significant difference']]",[],[],[],[],[],[],text_summarization,8,194
research-problem,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,[],"[('Abstractive Sentence Summarization', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Abstractive Sentence Summarization']]",[],[],[],[],text_summarization,9,2
research-problem,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,[],"[('text summarization', (14, 16))]",[],[],[],[],"[['Contribution', 'has research problem', 'text summarization']]",[],[],[],[],text_summarization,9,10
model,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .","[('consists of', (12, 14)), ('acts as', (21, 23)), ('to generate', (25, 27)), ('of', (29, 30))]","[('conditional recurrent neural network', (15, 19)), ('decoder', (24, 25)), ('summary', (28, 29)), ('input sentence', (31, 33))]","[['conditional recurrent neural network', 'acts as', 'decoder'], ['decoder', 'to generate', 'summary'], ['summary', 'of', 'input sentence']]",[],"[['Model', 'consists of', 'conditional recurrent neural network']]",[],[],[],[],[],[],text_summarization,9,16
model,"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .","[('at', (3, 4)), ('takes', (10, 11)), ('output of', (17, 19))]","[('every time step', (4, 7)), ('decoder', (8, 9)), ('conditioning input', (12, 14)), ('encoder module', (20, 22))]","[['decoder', 'takes', 'conditioning input'], ['conditioning input', 'at', 'every time step'], ['conditioning input', 'output of', 'encoder module']]",[],[],"[['Model', 'has', 'decoder']]",[],[],[],[],[],text_summarization,9,17
model,"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .","[('computes scores over', (11, 14)), ('in', (16, 17))]","[('encoder', (10, 11)), ('words', (15, 16)), ('input sentence', (18, 20))]","[['encoder', 'computes scores over', 'words'], ['words', 'in', 'input sentence']]",[],[],"[['Model', 'has', 'encoder']]",[],[],[],[],[],text_summarization,9,18
model,Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,"[('jointly trained on', (6, 9)), ('consisting of', (12, 14))]","[('decoder and encoder', (2, 5)), ('data set', (10, 12)), ('sentence - summary pairs', (14, 18))]","[['decoder and encoder', 'jointly trained on', 'data set'], ['data set', 'consisting of', 'sentence - summary pairs']]",[],[],"[['Model', 'has', 'decoder and encoder']]",[],[],[],[],[],text_summarization,9,20
model,"Lastly , our encoder uses a convolutional network to encode input words .","[('uses', (4, 5)), ('to encode', (8, 10))]","[('convolutional network', (6, 8)), ('input words', (10, 12))]","[['convolutional network', 'to encode', 'input words']]",[],[],[],[],"[['encoder', 'uses', 'convolutional network']]",[],[],[],text_summarization,9,24
experimental-setup,We implemented our models in the Torch library ( http://torch.ch/),"[('implemented', (1, 2)), ('in', (4, 5))]","[('our models', (2, 4)), ('Torch library', (6, 8))]","[['our models', 'in', 'Torch library']]",[],"[['Experimental setup', 'implemented', 'our models']]",[],[],[],[],[],[],text_summarization,9,96
experimental-setup,2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,"[('optimize', (3, 4)), ('used', (11, 12)), ('with', (15, 16)), ('of size', (17, 19))]","[('our loss', (4, 6)), ('stochastic gradient descent', (12, 15)), ('mini-batches', (16, 17)), ('32', (19, 20))]","[['our loss', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'mini-batches'], ['mini-batches', 'of size', '32']]",[],"[['Experimental setup', 'optimize', 'our loss']]",[],[],[],[],[],[],text_summarization,9,97
experimental-setup,"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .","[('During', (0, 1)), ('measure', (3, 4)), ('of', (6, 7)), ('in', (9, 10)), ('adjust', (14, 15)), ('such as', (20, 22))]","[('training', (1, 2)), ('perplexity', (5, 6)), ('summaries', (8, 9)), ('validation set', (11, 13)), ('hyper - parameters', (16, 19)), ('learning rate', (23, 25))]","[['training', 'measure', 'perplexity'], ['perplexity', 'of', 'summaries'], ['summaries', 'in', 'validation set'], ['training', 'adjust', 'hyper - parameters'], ['hyper - parameters', 'such as', 'learning rate']]",[],"[['Experimental setup', 'During', 'training']]",[],[],[],[],[],[],text_summarization,9,98
experimental-setup,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,"[('For', (0, 1)), ('experimented with', (4, 6))]","[('decoder', (2, 3)), ('Elman RNN', (8, 10)), ('Long - Short Term Memory ( LSTM ) architecture', (12, 21))]","[['decoder', 'experimented with', 'Elman RNN'], ['decoder', 'experimented with', 'Long - Short Term Memory ( LSTM ) architecture']]",[],"[['Experimental setup', 'For', 'decoder']]",[],[],[],[],[],[],text_summarization,9,99
experimental-setup,We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,"[('chose', (1, 2)), ('based on', (5, 7)), ('picked the one which gave', (11, 16)), ('on', (19, 20))]","[('hyper - parameters', (2, 5)), ('grid search', (8, 10)), ('best perplexity', (17, 19)), ('validation set', (21, 23))]","[['hyper - parameters', 'based on', 'grid search'], ['hyper - parameters', 'picked the one which gave', 'best perplexity'], ['best perplexity', 'on', 'validation set']]",[],"[['Experimental setup', 'chose', 'hyper - parameters']]",[],[],[],[],[],[],text_summarization,9,100
,"Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 . ",[],[],[],[],[],[],[],[],[],[],[],text_summarization,9,104
experimental-setup,"The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .","[('with', (13, 14))]","[('LSTM model ( RAS - LSTM )', (1, 8)), ('single layer', (11, 13)), ('H = 512 , ? = 0.1 , ? = 2 , and ? = 10', (14, 30))]","[['single layer', 'with', 'H = 512 , ? = 0.1 , ? = 2 , and ? = 10']]","[['LSTM model ( RAS - LSTM )', 'has', 'single layer']]",[],"[['Experimental setup', 'has', 'LSTM model ( RAS - LSTM )']]",[],[],[],[],[],text_summarization,9,105
results,shows that both our RAS - Elman and RAS - LSTM models achieve lower perplexity than ABS as well as other models reported in .,"[('shows', (0, 1)), ('achieve', (12, 13)), ('than', (15, 16))]","[('RAS - Elman and RAS - LSTM models', (4, 12)), ('lower perplexity', (13, 15)), ('ABS', (16, 17))]","[['RAS - Elman and RAS - LSTM models', 'achieve', 'lower perplexity'], ['lower perplexity', 'than', 'ABS']]",[],"[['Results', 'shows', 'RAS - Elman and RAS - LSTM models']]",[],[],[],[],[],[],text_summarization,9,112
results,"The RAS - LSTM performs slightly worse than RAS - Elman , most likely due to over-fitting .","[('performs slightly worse than', (4, 8))]","[('RAS - LSTM', (1, 4)), ('RAS - Elman', (8, 11))]","[['RAS - LSTM', 'performs slightly worse than', 'RAS - Elman']]",[],[],"[['Results', 'has', 'RAS - LSTM']]",[],[],[],[],[],text_summarization,9,113
results,The ROUGE results show that our models comfortably outperform both ABS and ABS + by a wide margin on all metrics .,"[('show', (3, 4)), ('comfortably outperform', (7, 9)), ('by', (14, 15)), ('on', (18, 19))]","[('ROUGE results', (1, 3)), ('our models', (5, 7)), ('ABS and ABS +', (10, 14)), ('wide margin', (16, 18)), ('all metrics', (19, 21))]","[['ROUGE results', 'show', 'our models'], ['our models', 'comfortably outperform', 'ABS and ABS +'], ['ABS and ABS +', 'by', 'wide margin'], ['wide margin', 'on', 'all metrics']]",[],[],"[['Results', 'has', 'ROUGE results']]",[],[],[],[],[],text_summarization,9,115
results,On DUC - 2004 we report recall ROUGE as is customary on this dataset .,"[('On', (0, 1))]","[('DUC - 2004', (1, 4))]",[],[],"[['Results', 'On', 'DUC - 2004']]",[],[],[],[],[],[],text_summarization,9,118
results,The results ( Table 3 ) show that our models are better than ABS + .,"[('show', (6, 7)), ('better than', (11, 13))]","[('our models', (8, 10)), ('ABS +', (13, 15))]","[['our models', 'better than', 'ABS +']]",[],[],[],[],"[['DUC - 2004', 'show', 'our models']]",[],[],[],text_summarization,9,119
research-problem,Learning document embeddings along with their uncertainties,[],"[('Learning document embeddings', (0, 3))]",[],[],[],[],"[['Contribution', 'has research problem', 'Learning document embeddings']]",[],[],[],[],topic_models,0,2
research-problem,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,[],"[('topic identification', (9, 11))]",[],[],[],[],"[['Contribution', 'has research problem', 'topic identification']]",[],[],[],[],topic_models,0,8
research-problem,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,[],[],[],[],[],[],[],[],[],[],[],topic_models,0,18
research-problem,"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.",[],"[('L EARNING word and document embeddings', (0, 6))]",[],[],[],[],"[['Contribution', 'has research problem', 'L EARNING word and document embeddings']]",[],[],[],[],topic_models,0,24
model,"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .","[('present', (5, 6)), ('as a', (14, 16)), ('for', (18, 19))]","[('Bayesian subspace multinomial model ( Bayesian SMM )', (6, 14)), ('generative model', (16, 18)), ('bag - ofwords representation of documents', (19, 25))]","[['Bayesian subspace multinomial model ( Bayesian SMM )', 'as a', 'generative model'], ['generative model', 'for', 'bag - ofwords representation of documents']]",[],"[['Model', 'present', 'Bayesian subspace multinomial model ( Bayesian SMM )']]",[],[],[],[],[],[],topic_models,0,38
model,"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .","[('learn to represent', (6, 9)), ('in the form of', (11, 15)), ('encoding', (20, 21))]","[('each document', (9, 11)), ('Gaussian distribution', (16, 18)), ('uncertainty in its covariance', (22, 26))]","[['each document', 'in the form of', 'Gaussian distribution'], ['Gaussian distribution', 'encoding', 'uncertainty in its covariance']]",[],"[['Model', 'learn to represent', 'each document']]",[],[],[],[],[],[],topic_models,0,39
model,"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .","[('propose', (3, 4)), ('exploits', (9, 10)), ('for', (12, 13))]","[('generative Gaussian classifier', (5, 8)), ('uncertainty', (11, 12)), ('topic identification ( ID )', (13, 18))]","[['generative Gaussian classifier', 'exploits', 'uncertainty'], ['uncertainty', 'for', 'topic identification ( ID )']]",[],"[['Model', 'propose', 'generative Gaussian classifier']]",[],[],[],[],[],[],topic_models,0,40
model,"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .","[('extended', (6, 7)), ('for', (11, 12)), ('can model', (17, 19))]","[('proposed VB framework', (1, 4)), ('subspace n-gram model', (12, 15)), ('n-gram distribution of words in sentences', (19, 25))]","[['proposed VB framework', 'for', 'subspace n-gram model'], ['subspace n-gram model', 'can model', 'n-gram distribution of words in sentences']]",[],"[['Model', 'extended', 'proposed VB framework']]",[],[],[],[],[],[],topic_models,0,41
hyperparameters,"The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.","[('chosen from', (4, 6)), ('from', (21, 22))]","[('embedding dimension', (1, 3)), ('K = { 100 , . . . , 800 }', (6, 17)), ('regularization weight', (19, 21))]","[['embedding dimension', 'chosen from', 'K = { 100 , . . . , 800 }']]",[],[],"[['Hyperparameters', 'has', 'regularization weight'], ['Hyperparameters', 'has', 'embedding dimension']]",[],[],"[['regularization weight', 'from', '? = { 0.0001 , . . . , 10.0 }']]",[],[],topic_models,0,274
baselines,"1 ) NVDM : Since NVDM and our proposed Bayesian SMM share similarities , we chose to extract the embeddings from NVDM and use them for training linear classifiers .","[('extract', (17, 18)), ('use them for', (23, 26))]","[('NVDM', (2, 3)), ('embeddings from NVDM', (19, 22)), ('training linear classifiers', (26, 29))]","[['NVDM', 'extract', 'embeddings from NVDM'], ['embeddings from NVDM', 'use them for', 'training linear classifiers']]",[],[],"[['Baselines', 'has', 'NVDM']]",[],[],[],[],[],topic_models,0,285
baselines,"2 ) SMM : Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix , i.e. , 1 SMM .","[('is', (8, 9))]","[('SMM', (2, 3)), ('non-Bayesian SMM with 1 regularization over the rows in T matrix', (9, 20))]","[['SMM', 'is', 'non-Bayesian SMM with 1 regularization over the rows in T matrix']]",[],[],"[['Baselines', 'has', 'SMM']]",[],[],[],[],[],topic_models,0,293
baselines,3 ) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .,"[('is', (8, 9)), ('fine - tuned for', (13, 17))]","[('ULMFiT', (2, 3)), ('universal language model', (10, 13)), ('classification', (17, 18))]","[['ULMFiT', 'is', 'universal language model'], ['universal language model', 'fine - tuned for', 'classification']]",[],[],"[['Baselines', 'has', 'ULMFiT']]",[],[],[],[],[],topic_models,0,299
baselines,4 ) TF - IDF :,[],"[('TF - IDF', (2, 5))]",[],[],[],"[['Baselines', 'has', 'TF - IDF']]",[],[],[],[],[],topic_models,0,306
baselines,"The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .","[('is a', (4, 6)), ('followed by', (22, 24))]","[('standard term frequency - inverse document frequency ( TF - IDF ) based document representation', (6, 21)), ('multi-class logistic regression ( LR )', (24, 30))]","[['standard term frequency - inverse document frequency ( TF - IDF ) based document representation', 'followed by', 'multi-class logistic regression ( LR )']]",[],[],[],[],"[['TF - IDF', 'is a', 'standard term frequency - inverse document frequency ( TF - IDF ) based document representation']]",[],[],[],topic_models,0,307
results,"presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .","[('with', (8, 9))]","[('Fisher speech corpora', (5, 8)), ('manual and automatic transcriptions', (9, 13))]","[['Fisher speech corpora', 'with', 'manual and automatic transcriptions']]",[],[],"[['Results', 'has', 'Fisher speech corpora']]",[],[],[],[],[],topic_models,0,362
results,"We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .","[('see that', (2, 4)), ('achieve', (7, 8)), ('exploits', (16, 17)), ('than', (28, 29))]","[('our proposed systems', (4, 7)), ('consistently better accuracies', (8, 11)), ('GLCU', (14, 15)), ('uncertainty in document embeddings', (18, 22)), ('much lower cross - entropy', (23, 28)), ('GLC', (32, 33))]","[['much lower cross - entropy', 'than', 'GLC'], ['GLCU', 'exploits', 'uncertainty in document embeddings'], ['our proposed systems', 'achieve', 'consistently better accuracies']]","[['GLCU', 'has', 'much lower cross - entropy']]",[],[],[],"[['Fisher speech corpora', 'see that', 'GLCU'], ['Fisher speech corpora', 'see that', 'our proposed systems']]",[],[],[],topic_models,0,367
results,presents classification results on 20 Newsgroups dataset .,[],"[('20 Newsgroups dataset', (4, 7))]",[],[],[],"[['Results', 'has', '20 Newsgroups dataset']]",[],[],[],[],"[['20 Newsgroups dataset', 'see that', 'topic ID systems based on Bayesian SMM']]",topic_models,0,370
results,"We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .","[('see that', (1, 3)), ('better than', (15, 17)), ('except for', (22, 24))]","[('topic ID systems based on Bayesian SMM and logistic regression', (4, 14)), ('all the other models', (17, 21)), ('purely discriminative CNN model', (25, 29))]","[['topic ID systems based on Bayesian SMM and logistic regression', 'better than', 'all the other models'], ['all the other models', 'except for', 'purely discriminative CNN model']]",[],[],[],[],"[['20 Newsgroups dataset', 'see that', 'topic ID systems based on Bayesian SMM and logistic regression']]",[],[],[],topic_models,0,376
results,"We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .","[('consistently better than', (15, 18))]","[('topic ID systems based on Bayesian SMM', (7, 14)), ('variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM', (18, 29))]","[['topic ID systems based on Bayesian SMM', 'consistently better than', 'variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM']]",[],[],[],[],[],[],[],[],topic_models,0,377
